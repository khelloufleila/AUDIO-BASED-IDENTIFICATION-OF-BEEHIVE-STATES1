{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# python -m pip install SoundFile\n",
    "import glob\n",
    "import os\n",
    "from info import i, printb, printr, printp, print\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import pdb\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from info import i, printb, printr, printp, print\n",
    "import muda\n",
    "import jams\n",
    "from sklearn import svm\n",
    "import librosa\n",
    "import keras\n",
    "import scipy.io as sio\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------- parameters to change-----------------------------------#\n",
    "block_size=1 # blocks of 60 seconds\n",
    "thresholds=[0, 0.5]  # minimum length for nobee intervals: 0 or 5 seconds (creates one label file per threshold value)\n",
    "path_audioFiles=\"C:\\\\Users\\PC\\python\\Stage\\To Bee or not to Bee_the annotated dataset\"+os.sep  # path to audio files\n",
    "annotations_path=\"C:\\\\Users\\PC\\python\\Stage\\To Bee or not to Bee_the annotated dataset\"+os.sep # path to .lab files\n",
    "path_save_audio_labels= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+os.sep  # path where to save audio segments and labels files.\n",
    "#-------------------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if path doesn't exist then create one !!!\n",
    "if not os.path.exists(path_save_audio_labels):\n",
    "    os.makedirs(path_save_audio_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_beeNotBee_annotations_saves_labels(audiofilename, block_name,  blockStart, blockfinish, annotations_path, threshold=0):\n",
    "  \n",
    "    block_length=blockfinish-blockStart\n",
    "    #print(\"block_length: \", block_length)\n",
    "    if audiofilename.startswith('#'):\n",
    "        annotation_filename=audiofilename[1:-4]+'.lab'\n",
    "    else :\n",
    "        annotation_filename=audiofilename[0:-4]+'.lab'\n",
    "        \n",
    "        \n",
    "    try:    \n",
    "        with open(annotations_path + os.sep + annotation_filename,'r') as f:\n",
    "            # EXAMPLE FILE:\n",
    "            \n",
    "            # CF003 - Active - Day - (223)\n",
    "            # 0\t8.0\tbee\n",
    "            # 8.01\t15.05\tnobee\n",
    "            # 15.06\t300.0\tbee \n",
    "            # .\n",
    "            #\n",
    "            \n",
    "            # all files end with a dot followed by an empty line.\n",
    "\n",
    "            print(annotations_path + os.sep + annotation_filename)\n",
    "            lines = f.read().split('\\n')\n",
    "        \n",
    "            labels_th=['bee', 0.0]\n",
    "            label2assign='bee'\n",
    "            label_strength=0\n",
    "            intersected_s=0\n",
    "                            \n",
    "            for line in lines:\n",
    "                if (line == annotation_filename[0:-4]) or (line == '.') or (line ==''):\n",
    "                    #ignores title, '.', or empty line on the file.\n",
    "                    continue\n",
    "                \n",
    "               ## print(\"#############################  line \",line)\n",
    "                parsed_line= line.split('\\t')    \n",
    "                \n",
    "                assert (len(parsed_line)==3), ('expected 3 fields in each line, got: '+str(len(parsed_line))) \n",
    "                \n",
    "                \n",
    "                tp0=float(parsed_line[0])\n",
    "               ## print(\"tp0: \",tp0)\n",
    "                tp1=float(parsed_line[1])\n",
    "               ## print(\"tp1: \",tp1)\n",
    "                annotation_label=parsed_line[2]\n",
    "                if blockfinish < tp0: # no need to read further nobee intervals since annotation line is already after block finishes\n",
    "                    break\n",
    "                    \n",
    "                if annotation_label== 'nobee':\n",
    "               \n",
    "                   ## print(\"...............annotation_label=='nobee'..........................\")    \n",
    "                        \n",
    "                    if tp1-tp0 >= threshold:  # only progress if nobee interval is longer than defined threshold.\n",
    "                    \n",
    "                        if tp0 > blockStart and tp0 <= blockfinish and tp1 >= blockfinish:\n",
    "                            \n",
    "                            intersected_s=intersected_s + (blockfinish-tp0)  \n",
    "                           ## print(\" # bs          tp0 ######## bf########tp1\")\n",
    "                            # |____________########|########\n",
    "                            # bs          tp0      bf      tp1 \n",
    "                        \n",
    "                        elif tp1 >= blockStart and tp1 < blockfinish and tp0 <= blockStart:\n",
    "                            \n",
    "                            intersected_s=intersected_s+ (tp1-blockStart)\n",
    "                           ## print(\"# tp0 ######## bs  ########   tp1    bf\")\n",
    "                            # #####|########_____|\n",
    "                            # tp0  bs     tp1    bf\n",
    "                            \n",
    "                            \n",
    "                        elif tp1 >= blockStart and tp1 <= blockfinish and tp0 >= blockStart and tp0 <= blockfinish:\n",
    "                            \n",
    "                            intersected_s=intersected_s+ (tp1-tp0)\n",
    "                          ##  print(\"#bs   tp0  ########  tp1    bf\")\n",
    "                            # |_____########_____|\n",
    "                            # bs   tp0    tp1    bf\n",
    "                        \n",
    "                        elif tp0 <= blockStart and tp1 >= blockfinish:\n",
    "                            \n",
    "                            intersected_s=intersected_s + (blockfinish-blockStart)\n",
    "                          ##  print(\"tp0 #### bs############bf #### tp1\")\n",
    "                            #  ####|############|####\n",
    "                            # tp0  bs           bf  tp1\n",
    "                            \n",
    "                    if intersected_s > 0:\n",
    "                        label2assign='nobee'\n",
    "                    label_strength= intersected_s/block_length # proportion of nobee length in the block\n",
    "                    \n",
    "                    \n",
    "                    labels_th= [label2assign, round(label_strength,3)]  # if label_strehgth ==0 --> bee segment \n",
    "                    \n",
    "                    \n",
    "            assert (blockfinish <=tp1 ), ('the end of the request block falls outside the file: block ending: '+ str(blockfinish)+' end of file at: '+ str(tp1))\n",
    "            \n",
    "                \n",
    "    except FileNotFoundError as e:\n",
    "        print(e, '--Anotation file does not exist! label as unknown')\n",
    "        #print(annotation_filename=audiofilename[0:-4]+'.lab')\n",
    "            \n",
    "        label2assign = 'unknown'\n",
    "        label_strength=-1\n",
    "        \n",
    "        labels_th = [label2assign, label_strength]\n",
    "            \n",
    "    except Exception as e1:\n",
    "        print('unknown exception: '+str(e1))\n",
    "        #quit\n",
    "    \n",
    "    \n",
    "    return labels_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audioFiles_saves_segments( path_audioFiles,path_save_audio_labels, block_size , thresholds, annotations_path, read_beeNotBee_annotations ='yes', save_audioSegments='yes'):\n",
    "\n",
    "    \n",
    "    audiofilenames_list = [os.path.basename(x) for x in glob.glob(path_audioFiles+'*.mp3')]\n",
    "    audiofilenames_list.extend([os.path.basename(x) for x in glob.glob(path_audioFiles+'*.wav')])\n",
    "    \n",
    "    printb(\"Number of audiofiles in folder: \"+str(len(audiofilenames_list)))\n",
    "   # print(\"audiofilenames_list \",audiofilenames_list)\n",
    "    \n",
    "    fi=0\n",
    "    for file_name in audiofilenames_list:\n",
    "        fi=fi+1\n",
    "       # print('\\n')\n",
    "       # printb('Processing '+ file_name+'          :::file number:  '+str(fi)+' --------->of '+str(len(audiofilenames_list)))\n",
    "          \n",
    "\n",
    "        offset=0\n",
    "        block_id =0\n",
    "        \n",
    "        \n",
    "        while 1:\n",
    "                    \n",
    "            # READ ONE BLOCK OF THE AUDIO FILE\n",
    "            try:\n",
    "                ## Read one block of 60 seconds \n",
    "                block,sr = librosa.core.load(path_audioFiles+file_name, offset=offset, duration=block_size)\n",
    "               # print(block.shape , sr)\n",
    "               # print('-----------------Reading segment '+str(block_id))\n",
    "            except ValueError as e:\n",
    "                e\n",
    "                if 'Input signal length' in str(e):\n",
    "                    block=np.arange(0)\n",
    "            except FileNotFoundError as e1:\n",
    "                print(e1, ' but continuing anyway')\n",
    "                \n",
    "            ##print(\"test\")\n",
    "            if block.shape[0] > 0:    #when total length = multiple of blocksize, results that last block is 0-lenght, this if bypasses those cases.\n",
    "                \n",
    "                block_name=file_name[0:-4]+'__segment'+str(block_id)\n",
    "               ## print(block_name)\n",
    "                \n",
    "                # READ BEE NOT_BEE ANNOTATIONS:\n",
    "                if read_beeNotBee_annotations == 'yes':\n",
    "                   # print('---------------------Will read BeeNotbee anotations and create labels for segment'+str(block_id))\n",
    "                    blockStart=offset\n",
    "                    ##print(\"blockStart: \",blockStart)\n",
    "                    blockfinish=offset+block_size\n",
    "                    ##print(\"blockfinish: \",blockfinish)\n",
    "                    \n",
    "                    for th in thresholds:\n",
    "                        #print(\"th::::::::::\", th)\n",
    "                        label_file_exists = os.path.isfile(path_save_audio_labels+'labels_BeeNotBee_th'+str(th)+'.csv')\n",
    "                        with open(path_save_audio_labels+'labels_BeeNotBee_th'+str(th)+'.csv','a', newline='') as label_file:\n",
    "                            writer =csv.DictWriter(label_file, fieldnames=['sample_name', 'segment_start','segment_finish', 'label_strength', 'label'], delimiter=',')\n",
    "                            if not label_file_exists:\n",
    "                                writer.writeheader()\n",
    "                          ##  print(\"start read_beeNotBee_annotation_saves_labels\")\n",
    "                            label_block_th=read_beeNotBee_annotations_saves_labels(file_name, block_name,  blockStart, blockfinish, annotations_path, th)                            \n",
    "                           # print(\"label_block_th : \", label_block_th)                           \n",
    "                            writer.writerow({'sample_name': block_name, 'segment_start': blockStart, 'segment_finish': blockfinish , 'label_strength': label_block_th[1],  'label': label_block_th[0]} )\n",
    "                           # print('-----------------Wrote label for th '+ str(th)+' seconds of segment'+str(block_id)  ) \n",
    "                    \n",
    "               \n",
    "                # MAKE BLOCK OF THE SAME SIZE:\n",
    "                if block.shape[0] < block_size*sr:   \n",
    "                    block = uniform_block_size(block, block_size*sr, 'repeat')\n",
    "                   # print('-----------------Uniformizing block length of segment'+str(block_id)  ) \n",
    "\n",
    "                        \n",
    "            \n",
    "                # Save audio segment:\n",
    "                if save_audioSegments=='yes' and (not os.path.exists(path_save_audio_labels+block_name+'.wav')): #saves only if option is chosen and if block file doesn't already exist.\n",
    "                    librosa.output.write_wav(path_save_audio_labels+block_name+'.wav', block, sr)\n",
    "                    #print( '-----------------Saved wav file for segment '+str(block_id))\n",
    "                \n",
    "                    \n",
    "                    \n",
    "            else :\n",
    "                #print('----------------- no more segments for this file--------------------------------------')\n",
    "               # print('\\n')\n",
    "                break\n",
    "            offset += block_size\n",
    "            block_id += 1\n",
    "    printb('______________________________No more audioFiles___________________________________________________')\n",
    "       \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_block_size(undersized_block, block_size_samples, method='repeat' ):\n",
    "\n",
    "    lengthTofill=(block_size_samples)-(undersized_block.size)\n",
    "    if method == 'zero_padding':\n",
    "        new_block=np.pad(undersized_block, (0,lengthTofill), 'constant', constant_values=(0) )\n",
    "\n",
    "    elif method=='mean_padding':\n",
    "        new_block=np.pad(undersized_block, (0,lengthTofill), 'mean' )\n",
    "    \n",
    "    elif method=='repeat':        \n",
    "        new_block= np.pad(undersized_block, (0,lengthTofill), 'reflect')\n",
    "    else:\n",
    "       # print('methods to choose are: \\'zero_padding\\' ,\\'mean_padding\\' and \\'repeat\\' ' )\n",
    "        new_block=0\n",
    "              \n",
    "    return new_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_audioFiles_saves_segments( path_audioFiles, path_save_audio_labels, block_size , thresholds, annotations_path, read_beeNotBee_annotations='yes', save_audioSegments='yes')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 19:08:00 RAM65.7% 0.94GB] path_beeNotbee_labels:  C:\\Users\\PC\\python\\Stage\\dataset_BeeNoBee_2_second1sec\\labels_BeeNotBee_th0.5.csv\n"
     ]
    }
   ],
   "source": [
    "# read only labels_BeeNotBee_th0.5\n",
    "path_beeNotbee_labels = path_save_audio_labels + 'labels_BeeNotBee_th'+str(thresholds[1])+'.csv'\n",
    "print(\"path_beeNotbee_labels: \",path_beeNotbee_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_HiveState_fromSampleName( filename, states):   #states: state_labels=['active','missing queen','swarm' ]\n",
    "    label_state='other'\n",
    "    for state in states:\n",
    "        if state in filename.lower():\n",
    "           # print(\"1 \", filename)\n",
    "            label_state = state\n",
    "    #incorporate condition for Nu-hive recordings which do not follow the same annotation: 'QueenBee' or 'NO_QueenBee'\n",
    "    \n",
    "    if label_state=='other':\n",
    "        if 'NO_QueenBee' in filename:\n",
    "            ##print(\"NO_QueenBee\",label_state )\n",
    "            label_state = states[1]\n",
    "        else:\n",
    "            label_state=states[0]\n",
    "    return label_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_Statelabels_from_beeNotBeelabels(path_save, path_labels_BeeNotBee, states=['active','missing queen','swarm' ]):\n",
    "    \n",
    "    #label_file_exists = os.path.isfile(path_save+'state_labels.csv')\n",
    "    liste=[]\n",
    "    with open(path_labels_BeeNotBee, 'r' ) as rfile, \\\n",
    "    open(path_save+'state_labels.csv', 'w', newline='') as f_out:\n",
    "        csvreader = csv.reader(rfile, delimiter=',')\n",
    "        writer= csv.DictWriter(f_out, fieldnames=['sample_name', 'label'], delimiter=',') \n",
    "        #if not label_file_exists:\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in csvreader:\n",
    "            if not row[0]=='sample_name':\n",
    "                if row[4]=='bee':\n",
    "                    label_state=read_HiveState_fromSampleName(row[0], states)\n",
    "                    #print(row[0],\"label_state : \", label_state)\n",
    "                    writer.writerow({'sample_name':row[0], 'label':label_state})\n",
    "                else:   liste.append(row[0])  \n",
    "    return liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    " # reads labels beeNotBee files and creates corresponding states label file\n",
    "liste= write_Statelabels_from_beeNotBeelabels(path_save_audio_labels, path_beeNotbee_labels, states=['active','missing queen','swarm' ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#for i in range(len(liste)):\n",
    "#    os.remove(path_save_audio_labels+liste[i]+'.wav')\n",
    "#liste[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_samples_names(path_audioSegments_folder, extension='.wav'):\n",
    "    # Recupèrer tout les audios d'extention .wav\"\"\"\"\"\" glob.glob(path_audioSegments_folder+'*'+extension)\"\"\"\"\"\"\n",
    "    sample_ids=[os.path.basename(x) for x in glob.glob(path_audioSegments_folder+'*'+extension)]\n",
    "    return sample_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-27 13:18:49 RAM56.8% 0.29GB] 17295\n"
     ]
    }
   ],
   "source": [
    "sample_ids=get_list_samples_names(path_save_audio_labels) # get sample ids from audio segments folder.\n",
    "print(len(sample_ids) ) \n",
    "#sample_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------- parameters to change-----------------------------------#\n",
    "block_size=1 # blocks of 60 seconds\n",
    "path_workingFolder='C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+os.sep  # path where to save audio segments and labels files.\n",
    "labels2read= 'state_labels'\n",
    "feature = 'MFCCs20'\n",
    "path_working_MFCCs20= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\MFCCs20_matrix.mat'+os.sep\n",
    "path_working_stft= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\stft_matrix.mat'+os.sep\n",
    "path_working_TTBOX= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\TTBOX_matrix.mat'+os.sep\n",
    "path_working_cqt= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\cqt_matrix.mat'+os.sep\n",
    "\n",
    "\n",
    "#-----------------------------------parameters for TTBOX -----------------------------------#\n",
    "import timbre_descriptor as td\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import my_tools as mt\n",
    "from collections import namedtuple\n",
    "import scipy\n",
    "import scipy.signal\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "from scipy.sparse import csr_matrix\n",
    "from os.path import dirname, join as pjoin\n",
    "import matplotlib\n",
    "import swipep as swp             # used for sing le-F0 estimation\n",
    "import warnings                 # used for warning removal\n",
    "import time               # used performance benchmark\n",
    "import librosa\n",
    "nbits = 16;\n",
    "MAX_VAL = pow(2,(nbits-1)) * 1.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_sampless(path_audio_samples, sample_ids, raw_feature, normalization, high_level_features ): \n",
    "    #normalization = NO, z_norm, min_max\n",
    "    ## function to extract features \n",
    "    #high_level_features = 0 or 1 \n",
    "    #file_path = os.path.isfile(path_save_audio_labels+'matrix.mat'+'.csv') \n",
    "    n_samples_set = len(sample_ids) # 4\n",
    "    feature_Maps = []\n",
    "    if raw_feature== 'MFCCs20': \n",
    "         path_working= path_working_MFCCs20\n",
    "    else: \n",
    "        path_working= path_working_stft\n",
    "    \n",
    "    for sample in sample_ids:\n",
    "        # raw feature extraction:\n",
    "        print(\"sample: \",sample)\n",
    "        x = raw_feature_fromSample( path_audio_samples+sample, raw_feature ) # x.shape: (4, 20, 2584)\n",
    "       # print(\"x.shape: \",x.shape)\n",
    "       # Sauvgarder les x dans un fichier .mat \n",
    "     \n",
    "        #b = csr_matrix(x)\n",
    "        savemat(path_working+ sample[0:-4]+ '.mat', {'b': x})\n",
    "         \n",
    "                         \n",
    "                    \n",
    "        ##normalization here:si on veut les résultats pour Conv1D on utlise la normalisation \n",
    "        ##normalization here:\n",
    "        if not normalization == 'NO':\n",
    "             x_norm = featureMap_normalization_block_level(x, normalizationType = normalization) \n",
    "        else: x_norm = x\n",
    "        \n",
    "        if high_level_features:\n",
    "            # high level feature extraction:\n",
    "            if 'MFCCs' in raw_feature:\n",
    "                X = compute_statistics_overMFCCs(x_norm, 'yes') # X.shape: (4 , 120)\n",
    "            else: \n",
    "                X = compute_statistics_overSpectogram(x_norm)\n",
    "                \n",
    "            feature_map=X\n",
    "        else:\n",
    "            feature_map=x_norm\n",
    "        \n",
    "        \n",
    "        feature_Maps.append(feature_map)\n",
    "        \n",
    "    return feature_Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_feature_fromSample( path_audio_sample, feature2extract ):\n",
    "        \n",
    "    s, Fs = librosa.core.load(path_audio_sample) # sr= 22050\n",
    "    #m = re.match(r\"\\w+s(\\d+)\", feature2extract) #  m:  <re.Match object; span=(0, 7), match='MFCCs20'>\n",
    "    #n_freqs=int(m.groups()[0]) ## n_freqs= 20\n",
    "    #Melspec = librosa.feature.melspectrogram(audio_sample, n_mels = n_freqs) # computes mel spectrograms from audio sample, \n",
    "    \n",
    "    if 'MFCCs' in feature2extract:\n",
    "        n_freqs = int(feature2extract[5:len(feature2extract)]) ## n_freqs==20\n",
    "        Melspec = librosa.feature.melspectrogram(s, sr=Fs)\n",
    "        x = librosa.feature.mfcc(S=librosa.power_to_db(Melspec),sr=Fs, n_mfcc = n_freqs)\n",
    "        #x = librosa.feature.mfcc(s ,Fs=Fs, n_mfcc = n_freqs)\n",
    "        #print(x.shape)\n",
    "    elif 'TTBOX' in feature2extract:\n",
    "        r_Fs = librosa.resample(s, Fs, 44100)\n",
    "        r_Fs=44100\n",
    "        s= s/MAX_VAL;\n",
    "        N = len(s);\n",
    "       \n",
    "        desc_TEE, desc_AS, desc_STFTmag, desc_STFTpow, desc_Har, desc_ERB, desc_GAM =td.compute_all_descriptor(s, r_Fs);\n",
    "        desc  = [desc_TEE, desc_AS, desc_STFTmag, desc_STFTpow, desc_Har, desc_ERB, desc_GAM];\n",
    "\n",
    "        ### Time serie integration\n",
    "        param_val, field_name = td.temporalmodeling(desc);\n",
    "        x=param_val\n",
    "    else:\n",
    "        x = Melspec\n",
    "\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_feature_fromSample( path_audio_sample, feature2extract ):\n",
    "        \n",
    "    s, Fs = librosa.core.load(path_audio_sample) # sr= 22050\n",
    "    #m = re.match(r\"\\w+s(\\d+)\", feature2extract) #  m:  <re.Match object; span=(0, 7), match='MFCCs20'>\n",
    "    #n_freqs=int(m.groups()[0]) ## n_freqs= 20\n",
    "    #Melspec = librosa.feature.melspectrogram(audio_sample, n_mels = n_freqs) # computes mel spectrograms from audio sample, \n",
    "    \n",
    "    if 'MFCCs' in feature2extract:\n",
    "        n_freqs = int(feature2extract[5:len(feature2extract)]) ## n_freqs==20\n",
    "        Melspec = librosa.feature.melspectrogram(s, sr=Fs)\n",
    "        x = librosa.feature.mfcc(S=librosa.power_to_db(Melspec),sr=Fs, n_mfcc = n_freqs)\n",
    "        #x = librosa.feature.mfcc(s ,Fs=Fs, n_mfcc = n_freqs)\n",
    "        #print(x.shape)\n",
    "    # Short-time Fourier transform    \n",
    "    elif 'stft' in feature2extract:\n",
    "        x= librosa.stft(s , n_fft=2048)\n",
    "    # constante Q transform    \n",
    "    elif 'cqt' in  feature2extract:\n",
    "        x=  np.abs(librosa.cqt(s, Fs))\n",
    "    # timber tool box     \n",
    "    elif 'TTBOX' in feature2extract:\n",
    "        r_Fs = librosa.resample(s, Fs, 44100)\n",
    "        r_Fs=44100\n",
    "        s= s/MAX_VAL;\n",
    "        N = len(s);\n",
    "       \n",
    "        desc_TEE, desc_AS, desc_STFTmag, desc_STFTpow, desc_Har, desc_ERB, desc_GAM =td.compute_all_descriptor(s, r_Fs);\n",
    "        desc  = [desc_TEE, desc_AS, desc_STFTmag, desc_STFTpow, desc_Har, desc_ERB, desc_GAM];\n",
    "\n",
    "        ### Time serie integration\n",
    "        param_val, field_name = td.temporalmodeling(desc);\n",
    "        x=param_val\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        x = Melspec\n",
    "\n",
    "    return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_samples(path_audio_samples, sample_ids, raw_feature, normalization, high_level_features ): \n",
    "    #normalization = NO, z_norm, min_max\n",
    "    ## function to extract features \n",
    "    #high_level_features = 0 or 1 \n",
    "    #file_path = os.path.isfile(path_save_audio_labels+'matrix.mat'+'.csv') \n",
    "    n_samples_set = len(sample_ids) # 4\n",
    "    feature_Maps = []\n",
    "    if raw_feature== 'MFCCs20': \n",
    "         path_working= path_working_MFCC20\n",
    "    elif  raw_feature=='TTBOX': \n",
    "         path_working= path_working_TTBox\n",
    "    elif raw_feature=='stft':\n",
    "         path_working= path_working_stft\n",
    "    else: \n",
    "        path_working= path_working_cqt\n",
    "    \n",
    "    for sample in sample_ids:\n",
    "        # raw feature extraction:\n",
    "        print(\"sample: \",sample)\n",
    "        x = raw_feature_fromSample( path_audio_samples+sample, raw_feature ) # x.shape: (4, 20, 2584)\n",
    "       # print(\"x.shape: \",x.shape)\n",
    "       # Sauvgarder les x dans un fichier .mat \n",
    "     \n",
    "        b = csr_matrix(x)\n",
    "        savemat(path_working+ sample + '.mat', {'b': b})\n",
    "         \n",
    "                         \n",
    "                    \n",
    "        ##normalization here:si on veut les résultats pour Conv1D on utlise la normalisation \n",
    "        ##normalization here:\n",
    "        if not normalization == 'NO':\n",
    "             x_norm = featureMap_normalization_block_level(x, normalizationType = normalization) \n",
    "        else: x_norm = x\n",
    "        \n",
    "        if high_level_features:\n",
    "            # high level feature extraction:\n",
    "            if 'MFCCs' in raw_feature:\n",
    "                X = compute_statistics_overMFCCs(x_norm, 'yes') # X.shape: (4 , 120)\n",
    "            else: \n",
    "                X = compute_statistics_overSpectogram(x_norm)\n",
    "                \n",
    "            feature_map=X\n",
    "        else:\n",
    "            feature_map=x_norm\n",
    "        \n",
    "        \n",
    "        feature_Maps.append(feature_map)\n",
    "        \n",
    "    return feature_Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=get_features_from_samples(path_workingFolder, sample_ids, 'cqt', 'NO', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2binary(pos_label, list_labels):  # pos_label = missing queen / nobee\n",
    "    list_binary_labels=[]\n",
    "    for l in list_labels:\n",
    "        #print(\"l=\", l, \"pos_label= \", pos_label)\n",
    "        if l == pos_label:\n",
    "            list_binary_labels.append(1)\n",
    "        else:\n",
    "            list_binary_labels.append(0)\n",
    "    return list_binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GT_labels_fromFiles(path_labels, sample_ids, labels2read) : #labels2read =  name of the label file    \n",
    "    \n",
    "    ##reads labels files and retreives labels for a sample set given by sample_ids\n",
    "    # input:  path_labels: where label file is \n",
    "    #         sample_ids: list of sample names that we want the label\n",
    "    #         labels2read: name of the labels file: state_labels, labels_BeeNotBee_th0 ...\n",
    "    \n",
    "    # output: list of string labels, in same order as sample_ids list\n",
    "    \n",
    "    labels = []\n",
    "    fileAsdict={}\n",
    "    \n",
    "    with open(path_labels + labels2read+'.csv', 'r') as labfile:\n",
    "        csvreader = csv.reader(labfile, delimiter=',')    \n",
    "        for row in csvreader:\n",
    "            if not row[0] == 'sample_name':\n",
    "               # print(\"row[0]\",row[0]) # CF001 - Missing Queen - Day -__segment0\n",
    "               # print(\"row[-1]:\", row[-1])  # bee or nobee\n",
    "                fileAsdict[row[0]]=row[-1]   # row[-1] = '/missing queen/active' or 'bee/nobee'\n",
    "            \n",
    "    for sample in sample_ids:\n",
    "        #print(sample)\n",
    "        #print(fileAsdict[sample[0:-4]]) # bee nobee or unknown \n",
    "        labels.append(fileAsdict[sample[0:-4]])  #remove .wav extension\n",
    "    \n",
    "       \n",
    "    return labels  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_GT_labels_fromFiles(path_workingFolder, sample_idss, labels2read)\n",
    "Y= labels2binary('active', labels)\n",
    "#labels[0:10], Y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items2replicate(list_Binary_labels, list_sample_ids):\n",
    "    \n",
    "    # get the samples to be replicated.\n",
    "    # input: list of labels and sample_ids with same oreder!\n",
    "    # ouptut: dictionary keys:name of samples to be replicated,  value: Number of times to replicate that sample.\n",
    "    \n",
    "    #assert( len(list_Binary_labels) - len(list_sample_ids) == 0), ('arguments should have the same number of elements)\n",
    "    dict_items_replicate={}\n",
    "    \n",
    "    n_samples = len(list_Binary_labels)# 193\n",
    "    #print(\"n_samples: \", list_Binary_labels)\n",
    "    n_positive_labels = sum(list_Binary_labels)#158 = le nbr de 1\n",
    "    #print(\"n_positive_labels: \",n_positive_labels)\n",
    "    n_negative_labels = n_samples - n_positive_labels #35= le nbr de 0\n",
    "    #print(\"n_negative_labels: \",n_negative_labels)\n",
    "    \n",
    "    pos_samples=[]\n",
    "    neg_samples=[]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if list_Binary_labels[i] == 1 :\n",
    "            #print(\"list_sample_ids[i]= : \", list_sample_ids[i])\n",
    "            pos_samples.append(list_sample_ids[i])\n",
    "        else: \n",
    "            neg_samples.append(list_sample_ids[i])\n",
    "            \n",
    "    if n_positive_labels > n_negative_labels:\n",
    "        #print(n_positive_labels, n_negative_labels)\n",
    "        # Replicate negative samples as needed:\n",
    "        dif=n_positive_labels-n_negative_labels\n",
    "        items_replicate=random.choices(neg_samples, k=dif)\n",
    "       # print(\"neg_samples= \",neg_samples, \"items_replicate=\",items_replicate)\n",
    " \n",
    "    elif n_positive_labels < n_negative_labels:\n",
    "        dif=n_negative_labels-n_positive_labels\n",
    "        items_replicate=random.choices(pos_samples, k=dif)\n",
    "              \n",
    "    dict_items_replicate=Counter(items_replicate)\n",
    "    #print(dict_items_replicate)\n",
    "    return dict_items_replicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BalanceData_online(y_set, x_set, sample_ids_set):\n",
    "    \n",
    "    ## balances already processed data (X and Y, just before classifier) by replicating samples of the least represented class.\n",
    "    # input: y_set - binary labels of set, x_set - feature_maps of set, sample_ids_set - sample names in set, ( all have the same order!)\n",
    "    # output: X, Y and sample_ids with replicated samples concatenated \n",
    "    \n",
    " \n",
    "    printb( 'Balancing training data:' )\n",
    "    print('will randomly replicate samples from least represented class')\n",
    "    \n",
    "    x2concatenate = x_set\n",
    "    y2concatenate = y_set\n",
    "    sample_ids2concatenate = sample_ids_set\n",
    "    \n",
    "    dict_items_replicate = get_items2replicate(y_set,sample_ids_set )\n",
    "    #print(\"dict_items_replicate: \",dict_items_replicate)\n",
    "    \n",
    "    for i in range(len(sample_ids_set)):\n",
    "        if sample_ids_set[i] in dict_items_replicate.keys() :\n",
    "            \n",
    "            sample_ids2concatenate =np.concatenate([sample_ids2concatenate, [sample_ids_set[i]]*dict_items_replicate[sample_ids_set[i]]])\n",
    "            y2concatenate = np.concatenate([y2concatenate, [y_set[i]]*dict_items_replicate[sample_ids_set[i]]])\n",
    "            x2concatenate = np.concatenate([x2concatenate, [x_set[i]]*dict_items_replicate[sample_ids_set[i]]])\n",
    "            \n",
    "    return y2concatenate, x2concatenate, sample_ids2concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_audio_MFCCs= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\MFCCs20_matrix.mat'+os.sep  # path where \n",
    "\n",
    "def get_list_samples_name_MFCC(path_audioSegments, extension='.mat'):\n",
    "    states=['active','missing queen','swarm' ]\n",
    "    X_ttbox=[]\n",
    "    labels=[]\n",
    "    Y=[]\n",
    "    sample_ids=[]\n",
    "    # Recupèrer tout les audios d'extention .wav\"\"\"\"\"\" glob.glob(path_audioSegments_folder+'*'+extension)\"\"\"\"\"\"\n",
    "    #list_mfcc=[os.path.basename(x) for x in glob.glob(path_audioSegments+'*'+extension)]\n",
    "    \n",
    "    for x in glob.glob(path_audioSegments+'*'+extension): \n",
    "        sample=x[74:]\n",
    "        sample_ids.append(sample)\n",
    "        l= read_HiveState_fromSampleName( sample, states)\n",
    "        labels.append(l)\n",
    "        m=scipy.io.loadmat(x)\n",
    "        X_ttbox.append(m['b'])\n",
    "    \n",
    "    Y= labels2binary('active', labels)\n",
    "        \n",
    "    return X_ttbox,  labels , Y, sample_ids\n",
    "  \n",
    "X, labels , Y , sample_idss =get_list_samples_name_MFCC(path_save_audio_MFCCs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: MFCCs+SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 22:39:39 RAM61.9% 0.43GB] Balancing training data:\n",
      "[2020-05-18 22:39:39 RAM61.9% 0.43GB] will randomly replicate samples from least represented class\n"
     ]
    }
   ],
   "source": [
    "y_concat, x_concat, sample_ids_concat= BalanceData_online(Y, X, sample_idss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13183, 880), (4518, 880), (13183,), (4518,))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z= x_concat.shape\n",
    "X=x_concat.reshape(x, y*z)\n",
    "Y=y_concat\n",
    "Y.shape,X.shape\n",
    "# split the dataset \n",
    "# split the dataset \n",
    "x_train= X[4519:]\n",
    "x_test=X[: 4518]\n",
    "y_test=Y[:4518]\n",
    "y_train= Y[4519:]\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state = 2020, shuffle=False)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_Classification_BeehiveSTATE(X_flat_train, y_train, X_flat_test, y_test, kerneloption='rbf'):\n",
    "\n",
    "    print('\\n')\n",
    "    printb('Starting classification with SVM:')\n",
    "    Test_Preds=[]\n",
    "    Train_Preds=[]\n",
    "    Test_Preds_Proba=[]\n",
    "    Train_Preds_Proba=[]\n",
    "    Test_GroundT=[]\n",
    "    Train_GroundT=[]\n",
    "   \n",
    "    print('\\n')\n",
    "    printb('classification Beehive State into : Active or Missing Queen')\n",
    "        \n",
    "    #train :\n",
    "    CLF = svm.SVC(kernel=kerneloption, probability=True)\n",
    "    CLF.fit(X_flat_train, y_train)\n",
    "    y_pred_train = CLF.predict(X_flat_train)\n",
    "    y_pred_proba_train = CLF.predict_proba(X_flat_train)\n",
    "    \n",
    "    Train_GroundT = y_train\n",
    "    Train_Preds = y_pred_train\n",
    "    Train_Preds_Proba = y_pred_proba_train[:,1]\n",
    "    \n",
    "    # test:\n",
    "    y_pred_test = CLF.predict(X_flat_test)\n",
    "    y_pred_proba_test = CLF.predict_proba(X_flat_test)\n",
    "    Test_GroundT= y_test\n",
    "    Test_Preds = y_pred_test\n",
    "    Test_Preds_Proba = y_pred_proba_test[:,1]\n",
    "\n",
    "    return CLF, Test_GroundT, Train_GroundT, Test_Preds, Train_Preds, Test_Preds_Proba, Train_Preds_Proba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:52:35 RAM77.4% 1.67GB] \n",
      "\n",
      "[2020-05-18 16:52:35 RAM77.4% 1.67GB] Starting classification with SVM:\n",
      "[2020-05-18 16:52:35 RAM77.4% 1.67GB] \n",
      "\n",
      "[2020-05-18 16:52:35 RAM77.4% 1.67GB] classification Beehive State into : Active or Missing Queen\n"
     ]
    }
   ],
   "source": [
    "CLF, Test_GroundT, Train_GroundT, Test_Preds, Train_Preds, Test_Preds_Proba, Train_Preds_Proba = SVM_Classification_BeehiveSTATE(x_train, y_train , x_test, y_test, kerneloption='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4518,), (13183,), (4518,), (13183,), (4518,), (13183,))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_GroundT.shape, Train_GroundT.shape, Test_Preds.shape, Train_Preds.shape, Test_Preds_Proba.shape, Train_Preds_Proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 17:11:44 RAM65.2% 1.46GB] Accuracy:  0.7403718459495352\n",
      "[2020-05-18 17:11:44 RAM65.2% 1.46GB] Precision: 0.8037989901418611\n",
      "[2020-05-18 17:11:44 RAM65.2% 1.46GB] Recall: 0.9035135135135135\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score( Test_GroundT, Test_Preds))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(Test_GroundT, Test_Preds))\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(Test_GroundT, Test_Preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names= ['missing queen', 'active' ]\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=25)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 17:11:58 RAM65.1% 1.46GB] Confusion matrix, without normalization\n",
      "[2020-05-18 17:11:58 RAM65.1% 1.46GB] [[   2  816]\n",
      " [ 357 3343]]\n",
      "[2020-05-18 17:11:59 RAM65.1% 1.46GB] Normalized confusion matrix\n",
      "[2020-05-18 17:11:59 RAM65.1% 1.46GB] [[0.  1. ]\n",
      " [0.1 0.9]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 17:11:59 RAM65.1% 1.47GB] \n",
      "Clasification report MFCCs + SVM:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "missing_queen       0.01      0.00      0.00       818\n",
      "       active       0.80      0.90      0.85      3700\n",
      "\n",
      "     accuracy                           0.74      4518\n",
      "    macro avg       0.40      0.45      0.43      4518\n",
      " weighted avg       0.66      0.74      0.70      4518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Test_GroundT, Test_Preds )\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='MFCCs + SVM Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "target_names=['missing_queen', 'active']\n",
    "print ('\\nClasification report MFCCs + SVM:\\n', classification_report(Test_GroundT, Test_Preds , target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: MFCCs+ CNN Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential, Input, Model \n",
    "from keras.layers import Dense, Dropout, Flatten, Activation \n",
    "from keras.layers import Conv2D , MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "###################################################################\n",
    "# Set early stopping critiria\n",
    "pat=5 # this is the number of epocks with no improvment after which the training will stop \n",
    "early_stopping= EarlyStopping(monitor='val_loss', patience=pat, verbose=1 )\n",
    "n_folds=10\n",
    "epochs=50\n",
    "batch_size=145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 12:28:49 RAM78.3% 1.68GB] Balancing training data:\n",
      "[2020-05-18 12:28:49 RAM78.3% 1.68GB] will randomly replicate samples from least represented class\n"
     ]
    }
   ],
   "source": [
    "y_concat, x_concat, sample_ids_concat= BalanceData_online(Y, X, sample_idss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17702,), (17702, 20, 44))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z= x_concat.shape\n",
    "#X=x_concat.reshape(x, y*z)\n",
    "#Y=y_concat\n",
    "#Y.shape,X.shape\n",
    "y_concat.shape, x_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17702, 20, 44, 1), (17702, 1))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X= x_concat.reshape(-1, 20, 44, 1)\n",
    "Y=y_concat.reshape(-1, 1)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13183, 20, 44, 1), (4518, 20, 44, 1), (13183, 2), (4518, 2))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "\n",
    "X = np.array(X.tolist())\n",
    "y = np.array(Y.tolist())\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    " \n",
    "x_train= X[4519:]\n",
    "x_test=X[: 4518]\n",
    "y_test=yy[:4518]\n",
    "y_train= yy[4519:]\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.3, random_state = 2020, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12391, 20, 44, 1), (5311, 20, 44, 1), (12391, 2), (5311, 2))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(size):\n",
    "    # Neural Network Architecture \n",
    "    model=Sequential()\n",
    "    # size= ( 20,44, 1)\n",
    "   # for i in range(1, num_cnn_layers+1):\n",
    "         \n",
    "    model.add(Conv2D(16, kernel_size=(3,3), activation='relu', input_shape=size , padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    # we add the dropout to skip the overfitting \n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "    # we add the dropout to skip the overfitting \n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(3,1), activation='relu', padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(3,1), activation='relu', padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "    # we add the dropout to skip the overfitting \n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32 , activation='relu'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_49 (Conv2D)           (None, 20, 44, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 20, 44, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 10, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 10, 22, 16)        2320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 10, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 5, 11, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 5, 11, 16)         784       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 5, 11, 16)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 3, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 3, 6, 16)          784       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)   (None, 3, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 2, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 2, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               24832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 37,170\n",
      "Trainable params: 37,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=deep_model(( 20,44, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model check point callback -> this will keep saving the model as a physical file \n",
    "model_checkpoint= ModelCheckpoint('cnn_bee_1.h5', verbose=1, save_best_only=True )\n",
    "\n",
    "# define a function to fit the model\n",
    "\n",
    "def fit_and_evaluate(train_x, val_x, train_y, val_y, EPOCHS=50, BATCH_SIZE=145 ):\n",
    "    model=None\n",
    "    model=deep_model(( 20,44, 1))\n",
    "    results= model.fit(train_x, train_y, epochs=EPOCHS, batch_size= BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], verbose=1, validation_split=0.1)\n",
    "    print(\"Val Score :\", model.evaluate(val_x, val_y))\n",
    "    return results \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:16:03 RAM66.0% 0.94GB] Training on Fold : 1\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 23s - loss: 2.8729 - accuracy: 0.455 - ETA: 14s - loss: 3.0554 - accuracy: 0.544 - ETA: 11s - loss: 2.3802 - accuracy: 0.547 - ETA: 10s - loss: 2.0339 - accuracy: 0.529 - ETA: 9s - loss: 1.8274 - accuracy: 0.536 - ETA: 8s - loss: 1.6496 - accuracy: 0.53 - ETA: 7s - loss: 1.5263 - accuracy: 0.53 - ETA: 7s - loss: 1.4337 - accuracy: 0.53 - ETA: 7s - loss: 1.3556 - accuracy: 0.54 - ETA: 6s - loss: 1.2955 - accuracy: 0.54 - ETA: 6s - loss: 1.2435 - accuracy: 0.54 - ETA: 6s - loss: 1.1991 - accuracy: 0.54 - ETA: 6s - loss: 1.1623 - accuracy: 0.54 - ETA: 5s - loss: 1.1269 - accuracy: 0.55 - ETA: 5s - loss: 1.1019 - accuracy: 0.55 - ETA: 5s - loss: 1.0752 - accuracy: 0.55 - ETA: 5s - loss: 1.0502 - accuracy: 0.55 - ETA: 5s - loss: 1.0290 - accuracy: 0.56 - ETA: 5s - loss: 1.0071 - accuracy: 0.56 - ETA: 5s - loss: 0.9888 - accuracy: 0.57 - ETA: 4s - loss: 0.9733 - accuracy: 0.57 - ETA: 4s - loss: 0.9622 - accuracy: 0.57 - ETA: 4s - loss: 0.9479 - accuracy: 0.57 - ETA: 4s - loss: 0.9391 - accuracy: 0.57 - ETA: 4s - loss: 0.9280 - accuracy: 0.57 - ETA: 4s - loss: 0.9141 - accuracy: 0.58 - ETA: 4s - loss: 0.9022 - accuracy: 0.58 - ETA: 4s - loss: 0.8896 - accuracy: 0.59 - ETA: 4s - loss: 0.8808 - accuracy: 0.59 - ETA: 4s - loss: 0.8760 - accuracy: 0.59 - ETA: 3s - loss: 0.8694 - accuracy: 0.59 - ETA: 3s - loss: 0.8593 - accuracy: 0.59 - ETA: 3s - loss: 0.8494 - accuracy: 0.60 - ETA: 3s - loss: 0.8380 - accuracy: 0.60 - ETA: 3s - loss: 0.8276 - accuracy: 0.61 - ETA: 3s - loss: 0.8199 - accuracy: 0.61 - ETA: 3s - loss: 0.8152 - accuracy: 0.61 - ETA: 3s - loss: 0.8117 - accuracy: 0.61 - ETA: 3s - loss: 0.8022 - accuracy: 0.61 - ETA: 3s - loss: 0.7945 - accuracy: 0.62 - ETA: 3s - loss: 0.7861 - accuracy: 0.62 - ETA: 2s - loss: 0.7771 - accuracy: 0.63 - ETA: 2s - loss: 0.7679 - accuracy: 0.63 - ETA: 2s - loss: 0.7578 - accuracy: 0.64 - ETA: 2s - loss: 0.7500 - accuracy: 0.64 - ETA: 2s - loss: 0.7491 - accuracy: 0.64 - ETA: 2s - loss: 0.7446 - accuracy: 0.64 - ETA: 2s - loss: 0.7372 - accuracy: 0.64 - ETA: 2s - loss: 0.7294 - accuracy: 0.65 - ETA: 2s - loss: 0.7213 - accuracy: 0.65 - ETA: 2s - loss: 0.7134 - accuracy: 0.66 - ETA: 1s - loss: 0.7052 - accuracy: 0.66 - ETA: 1s - loss: 0.6991 - accuracy: 0.67 - ETA: 1s - loss: 0.6968 - accuracy: 0.67 - ETA: 1s - loss: 0.6920 - accuracy: 0.67 - ETA: 1s - loss: 0.6883 - accuracy: 0.67 - ETA: 1s - loss: 0.6817 - accuracy: 0.67 - ETA: 1s - loss: 0.6749 - accuracy: 0.68 - ETA: 1s - loss: 0.6677 - accuracy: 0.68 - ETA: 1s - loss: 0.6602 - accuracy: 0.68 - ETA: 1s - loss: 0.6537 - accuracy: 0.69 - ETA: 1s - loss: 0.6462 - accuracy: 0.69 - ETA: 0s - loss: 0.6393 - accuracy: 0.69 - ETA: 0s - loss: 0.6325 - accuracy: 0.70 - ETA: 0s - loss: 0.6257 - accuracy: 0.70 - ETA: 0s - loss: 0.6192 - accuracy: 0.71 - ETA: 0s - loss: 0.6123 - accuracy: 0.71 - ETA: 0s - loss: 0.6070 - accuracy: 0.71 - ETA: 0s - loss: 0.6039 - accuracy: 0.71 - ETA: 0s - loss: 0.6044 - accuracy: 0.71 - ETA: 0s - loss: 0.6049 - accuracy: 0.71 - ETA: 0s - loss: 0.6022 - accuracy: 0.71 - ETA: 0s - loss: 0.5975 - accuracy: 0.72 - 7s 648us/step - loss: 0.5944 - accuracy: 0.7222 - val_loss: 0.1398 - val_accuracy: 0.9671\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13981, saving model to cnn_bee_1.h5\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.2161 - accuracy: 0.92 - ETA: 6s - loss: 0.2159 - accuracy: 0.92 - ETA: 5s - loss: 0.1977 - accuracy: 0.93 - ETA: 5s - loss: 0.1806 - accuracy: 0.93 - ETA: 5s - loss: 0.1709 - accuracy: 0.94 - ETA: 5s - loss: 0.1684 - accuracy: 0.94 - ETA: 5s - loss: 0.1772 - accuracy: 0.94 - ETA: 5s - loss: 0.1716 - accuracy: 0.94 - ETA: 5s - loss: 0.1753 - accuracy: 0.94 - ETA: 5s - loss: 0.1732 - accuracy: 0.94 - ETA: 5s - loss: 0.1709 - accuracy: 0.94 - ETA: 5s - loss: 0.1690 - accuracy: 0.94 - ETA: 5s - loss: 0.1648 - accuracy: 0.94 - ETA: 5s - loss: 0.1645 - accuracy: 0.94 - ETA: 4s - loss: 0.1628 - accuracy: 0.94 - ETA: 4s - loss: 0.1628 - accuracy: 0.94 - ETA: 4s - loss: 0.1788 - accuracy: 0.93 - ETA: 4s - loss: 0.1966 - accuracy: 0.92 - ETA: 4s - loss: 0.2179 - accuracy: 0.91 - ETA: 4s - loss: 0.2251 - accuracy: 0.91 - ETA: 4s - loss: 0.2271 - accuracy: 0.91 - ETA: 4s - loss: 0.2240 - accuracy: 0.91 - ETA: 4s - loss: 0.2192 - accuracy: 0.91 - ETA: 4s - loss: 0.2166 - accuracy: 0.91 - ETA: 4s - loss: 0.2149 - accuracy: 0.91 - ETA: 4s - loss: 0.2114 - accuracy: 0.91 - ETA: 3s - loss: 0.2078 - accuracy: 0.92 - ETA: 3s - loss: 0.2036 - accuracy: 0.92 - ETA: 3s - loss: 0.2019 - accuracy: 0.92 - ETA: 3s - loss: 0.1992 - accuracy: 0.92 - ETA: 3s - loss: 0.1952 - accuracy: 0.92 - ETA: 3s - loss: 0.1926 - accuracy: 0.92 - ETA: 3s - loss: 0.1902 - accuracy: 0.92 - ETA: 3s - loss: 0.1884 - accuracy: 0.92 - ETA: 3s - loss: 0.1889 - accuracy: 0.92 - ETA: 3s - loss: 0.1870 - accuracy: 0.92 - ETA: 3s - loss: 0.1888 - accuracy: 0.92 - ETA: 3s - loss: 0.1882 - accuracy: 0.92 - ETA: 2s - loss: 0.1877 - accuracy: 0.92 - ETA: 2s - loss: 0.1867 - accuracy: 0.92 - ETA: 2s - loss: 0.1863 - accuracy: 0.92 - ETA: 2s - loss: 0.1845 - accuracy: 0.92 - ETA: 2s - loss: 0.1829 - accuracy: 0.92 - ETA: 2s - loss: 0.1820 - accuracy: 0.92 - ETA: 2s - loss: 0.1814 - accuracy: 0.92 - ETA: 2s - loss: 0.1803 - accuracy: 0.92 - ETA: 2s - loss: 0.1787 - accuracy: 0.92 - ETA: 2s - loss: 0.1762 - accuracy: 0.93 - ETA: 2s - loss: 0.1744 - accuracy: 0.93 - ETA: 2s - loss: 0.1727 - accuracy: 0.93 - ETA: 1s - loss: 0.1712 - accuracy: 0.93 - ETA: 1s - loss: 0.1712 - accuracy: 0.93 - ETA: 1s - loss: 0.1729 - accuracy: 0.93 - ETA: 1s - loss: 0.1756 - accuracy: 0.93 - ETA: 1s - loss: 0.1774 - accuracy: 0.93 - ETA: 1s - loss: 0.1809 - accuracy: 0.92 - ETA: 1s - loss: 0.1846 - accuracy: 0.92 - ETA: 1s - loss: 0.1840 - accuracy: 0.92 - ETA: 1s - loss: 0.1825 - accuracy: 0.92 - ETA: 1s - loss: 0.1820 - accuracy: 0.92 - ETA: 1s - loss: 0.1806 - accuracy: 0.92 - ETA: 0s - loss: 0.1792 - accuracy: 0.92 - ETA: 0s - loss: 0.1781 - accuracy: 0.92 - ETA: 0s - loss: 0.1764 - accuracy: 0.93 - ETA: 0s - loss: 0.1752 - accuracy: 0.93 - ETA: 0s - loss: 0.1742 - accuracy: 0.93 - ETA: 0s - loss: 0.1731 - accuracy: 0.93 - ETA: 0s - loss: 0.1730 - accuracy: 0.93 - ETA: 0s - loss: 0.1729 - accuracy: 0.93 - ETA: 0s - loss: 0.1743 - accuracy: 0.93 - ETA: 0s - loss: 0.1739 - accuracy: 0.93 - ETA: 0s - loss: 0.1738 - accuracy: 0.93 - ETA: 0s - loss: 0.1724 - accuracy: 0.93 - 6s 603us/step - loss: 0.1716 - accuracy: 0.9330 - val_loss: 0.0898 - val_accuracy: 0.9705\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13981 to 0.08980, saving model to cnn_bee_1.h5\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0727 - accuracy: 0.95 - ETA: 6s - loss: 0.0820 - accuracy: 0.96 - ETA: 6s - loss: 0.0850 - accuracy: 0.96 - ETA: 5s - loss: 0.0889 - accuracy: 0.96 - ETA: 5s - loss: 0.0859 - accuracy: 0.96 - ETA: 5s - loss: 0.0870 - accuracy: 0.96 - ETA: 5s - loss: 0.0902 - accuracy: 0.95 - ETA: 5s - loss: 0.0924 - accuracy: 0.95 - ETA: 5s - loss: 0.0908 - accuracy: 0.95 - ETA: 5s - loss: 0.0899 - accuracy: 0.96 - ETA: 5s - loss: 0.0913 - accuracy: 0.96 - ETA: 5s - loss: 0.0886 - accuracy: 0.96 - ETA: 4s - loss: 0.0870 - accuracy: 0.96 - ETA: 4s - loss: 0.0879 - accuracy: 0.96 - ETA: 4s - loss: 0.0899 - accuracy: 0.96 - ETA: 4s - loss: 0.0952 - accuracy: 0.95 - ETA: 4s - loss: 0.1042 - accuracy: 0.95 - ETA: 4s - loss: 0.1151 - accuracy: 0.95 - ETA: 4s - loss: 0.1210 - accuracy: 0.95 - ETA: 4s - loss: 0.1211 - accuracy: 0.95 - ETA: 4s - loss: 0.1200 - accuracy: 0.95 - ETA: 4s - loss: 0.1187 - accuracy: 0.95 - ETA: 4s - loss: 0.1168 - accuracy: 0.95 - ETA: 4s - loss: 0.1145 - accuracy: 0.95 - ETA: 4s - loss: 0.1133 - accuracy: 0.95 - ETA: 3s - loss: 0.1105 - accuracy: 0.95 - ETA: 3s - loss: 0.1082 - accuracy: 0.95 - ETA: 3s - loss: 0.1072 - accuracy: 0.95 - ETA: 3s - loss: 0.1061 - accuracy: 0.95 - ETA: 3s - loss: 0.1093 - accuracy: 0.95 - ETA: 3s - loss: 0.1090 - accuracy: 0.95 - ETA: 3s - loss: 0.1085 - accuracy: 0.95 - ETA: 3s - loss: 0.1068 - accuracy: 0.95 - ETA: 3s - loss: 0.1047 - accuracy: 0.95 - ETA: 3s - loss: 0.1029 - accuracy: 0.95 - ETA: 3s - loss: 0.1030 - accuracy: 0.96 - ETA: 3s - loss: 0.1024 - accuracy: 0.96 - ETA: 2s - loss: 0.1018 - accuracy: 0.96 - ETA: 2s - loss: 0.1011 - accuracy: 0.96 - ETA: 2s - loss: 0.1018 - accuracy: 0.96 - ETA: 2s - loss: 0.1061 - accuracy: 0.95 - ETA: 2s - loss: 0.1082 - accuracy: 0.95 - ETA: 2s - loss: 0.1080 - accuracy: 0.95 - ETA: 2s - loss: 0.1083 - accuracy: 0.95 - ETA: 2s - loss: 0.1082 - accuracy: 0.95 - ETA: 2s - loss: 0.1072 - accuracy: 0.95 - ETA: 2s - loss: 0.1066 - accuracy: 0.95 - ETA: 2s - loss: 0.1067 - accuracy: 0.95 - ETA: 2s - loss: 0.1075 - accuracy: 0.95 - ETA: 1s - loss: 0.1067 - accuracy: 0.95 - ETA: 1s - loss: 0.1061 - accuracy: 0.95 - ETA: 1s - loss: 0.1065 - accuracy: 0.95 - ETA: 1s - loss: 0.1084 - accuracy: 0.95 - ETA: 1s - loss: 0.1088 - accuracy: 0.95 - ETA: 1s - loss: 0.1084 - accuracy: 0.95 - ETA: 1s - loss: 0.1091 - accuracy: 0.95 - ETA: 1s - loss: 0.1085 - accuracy: 0.95 - ETA: 1s - loss: 0.1073 - accuracy: 0.95 - ETA: 1s - loss: 0.1071 - accuracy: 0.95 - ETA: 1s - loss: 0.1072 - accuracy: 0.95 - ETA: 1s - loss: 0.1067 - accuracy: 0.95 - ETA: 0s - loss: 0.1067 - accuracy: 0.95 - ETA: 0s - loss: 0.1057 - accuracy: 0.95 - ETA: 0s - loss: 0.1048 - accuracy: 0.95 - ETA: 0s - loss: 0.1038 - accuracy: 0.95 - ETA: 0s - loss: 0.1036 - accuracy: 0.95 - ETA: 0s - loss: 0.1032 - accuracy: 0.96 - ETA: 0s - loss: 0.1022 - accuracy: 0.96 - ETA: 0s - loss: 0.1020 - accuracy: 0.96 - ETA: 0s - loss: 0.1022 - accuracy: 0.96 - ETA: 0s - loss: 0.1022 - accuracy: 0.96 - ETA: 0s - loss: 0.1021 - accuracy: 0.96 - ETA: 0s - loss: 0.1025 - accuracy: 0.96 - 6s 592us/step - loss: 0.1028 - accuracy: 0.9602 - val_loss: 0.0918 - val_accuracy: 0.9671\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.08980\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0965 - accuracy: 0.96 - ETA: 5s - loss: 0.1621 - accuracy: 0.93 - ETA: 5s - loss: 0.1649 - accuracy: 0.92 - ETA: 5s - loss: 0.1614 - accuracy: 0.92 - ETA: 5s - loss: 0.1456 - accuracy: 0.93 - ETA: 5s - loss: 0.1375 - accuracy: 0.93 - ETA: 5s - loss: 0.1293 - accuracy: 0.94 - ETA: 5s - loss: 0.1178 - accuracy: 0.94 - ETA: 5s - loss: 0.1092 - accuracy: 0.95 - ETA: 5s - loss: 0.1034 - accuracy: 0.95 - ETA: 5s - loss: 0.0978 - accuracy: 0.95 - ETA: 4s - loss: 0.0944 - accuracy: 0.95 - ETA: 4s - loss: 0.1025 - accuracy: 0.95 - ETA: 4s - loss: 0.1076 - accuracy: 0.95 - ETA: 4s - loss: 0.1034 - accuracy: 0.95 - ETA: 4s - loss: 0.1003 - accuracy: 0.95 - ETA: 4s - loss: 0.0976 - accuracy: 0.95 - ETA: 4s - loss: 0.0957 - accuracy: 0.95 - ETA: 4s - loss: 0.0963 - accuracy: 0.96 - ETA: 4s - loss: 0.0941 - accuracy: 0.96 - ETA: 4s - loss: 0.0936 - accuracy: 0.96 - ETA: 4s - loss: 0.0934 - accuracy: 0.96 - ETA: 4s - loss: 0.0915 - accuracy: 0.96 - ETA: 4s - loss: 0.0920 - accuracy: 0.96 - ETA: 3s - loss: 0.0954 - accuracy: 0.96 - ETA: 3s - loss: 0.0962 - accuracy: 0.96 - ETA: 3s - loss: 0.0960 - accuracy: 0.96 - ETA: 3s - loss: 0.0957 - accuracy: 0.96 - ETA: 3s - loss: 0.0937 - accuracy: 0.96 - ETA: 3s - loss: 0.0919 - accuracy: 0.96 - ETA: 3s - loss: 0.0897 - accuracy: 0.96 - ETA: 3s - loss: 0.0888 - accuracy: 0.96 - ETA: 3s - loss: 0.0893 - accuracy: 0.96 - ETA: 3s - loss: 0.0884 - accuracy: 0.96 - ETA: 3s - loss: 0.0910 - accuracy: 0.96 - ETA: 3s - loss: 0.0912 - accuracy: 0.96 - ETA: 3s - loss: 0.0916 - accuracy: 0.96 - ETA: 2s - loss: 0.0922 - accuracy: 0.96 - ETA: 2s - loss: 0.0922 - accuracy: 0.96 - ETA: 2s - loss: 0.0917 - accuracy: 0.96 - ETA: 2s - loss: 0.0913 - accuracy: 0.96 - ETA: 2s - loss: 0.0905 - accuracy: 0.96 - ETA: 2s - loss: 0.0895 - accuracy: 0.96 - ETA: 2s - loss: 0.0888 - accuracy: 0.96 - ETA: 2s - loss: 0.0895 - accuracy: 0.96 - ETA: 2s - loss: 0.0883 - accuracy: 0.96 - ETA: 2s - loss: 0.0886 - accuracy: 0.96 - ETA: 2s - loss: 0.0889 - accuracy: 0.96 - ETA: 2s - loss: 0.0897 - accuracy: 0.96 - ETA: 1s - loss: 0.0890 - accuracy: 0.96 - ETA: 1s - loss: 0.0888 - accuracy: 0.96 - ETA: 1s - loss: 0.0879 - accuracy: 0.96 - ETA: 1s - loss: 0.0880 - accuracy: 0.96 - ETA: 1s - loss: 0.0872 - accuracy: 0.96 - ETA: 1s - loss: 0.0861 - accuracy: 0.96 - ETA: 1s - loss: 0.0864 - accuracy: 0.96 - ETA: 1s - loss: 0.0906 - accuracy: 0.96 - ETA: 1s - loss: 0.0996 - accuracy: 0.96 - ETA: 1s - loss: 0.0997 - accuracy: 0.96 - ETA: 1s - loss: 0.0984 - accuracy: 0.96 - ETA: 1s - loss: 0.0983 - accuracy: 0.96 - ETA: 0s - loss: 0.0971 - accuracy: 0.96 - ETA: 0s - loss: 0.0959 - accuracy: 0.96 - ETA: 0s - loss: 0.0957 - accuracy: 0.96 - ETA: 0s - loss: 0.0956 - accuracy: 0.96 - ETA: 0s - loss: 0.0955 - accuracy: 0.96 - ETA: 0s - loss: 0.0948 - accuracy: 0.96 - ETA: 0s - loss: 0.0945 - accuracy: 0.96 - ETA: 0s - loss: 0.0936 - accuracy: 0.96 - ETA: 0s - loss: 0.0930 - accuracy: 0.96 - ETA: 0s - loss: 0.0923 - accuracy: 0.96 - ETA: 0s - loss: 0.0917 - accuracy: 0.96 - ETA: 0s - loss: 0.0912 - accuracy: 0.96 - 6s 588us/step - loss: 0.0906 - accuracy: 0.9657 - val_loss: 0.0400 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08980 to 0.03999, saving model to cnn_bee_1.h5\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0195 - accuracy: 0.99 - ETA: 5s - loss: 0.0348 - accuracy: 0.98 - ETA: 5s - loss: 0.0308 - accuracy: 0.98 - ETA: 5s - loss: 0.0328 - accuracy: 0.98 - ETA: 5s - loss: 0.0329 - accuracy: 0.98 - ETA: 5s - loss: 0.0341 - accuracy: 0.98 - ETA: 5s - loss: 0.0308 - accuracy: 0.98 - ETA: 5s - loss: 0.0297 - accuracy: 0.98 - ETA: 5s - loss: 0.0335 - accuracy: 0.98 - ETA: 5s - loss: 0.0335 - accuracy: 0.98 - ETA: 5s - loss: 0.0396 - accuracy: 0.98 - ETA: 5s - loss: 0.0411 - accuracy: 0.98 - ETA: 5s - loss: 0.0426 - accuracy: 0.98 - ETA: 5s - loss: 0.0560 - accuracy: 0.97 - ETA: 4s - loss: 0.0644 - accuracy: 0.97 - ETA: 4s - loss: 0.0635 - accuracy: 0.97 - ETA: 4s - loss: 0.0658 - accuracy: 0.97 - ETA: 4s - loss: 0.0635 - accuracy: 0.97 - ETA: 4s - loss: 0.0615 - accuracy: 0.97 - ETA: 4s - loss: 0.0624 - accuracy: 0.97 - ETA: 4s - loss: 0.0655 - accuracy: 0.97 - ETA: 4s - loss: 0.0655 - accuracy: 0.97 - ETA: 4s - loss: 0.0695 - accuracy: 0.97 - ETA: 4s - loss: 0.0697 - accuracy: 0.97 - ETA: 4s - loss: 0.0707 - accuracy: 0.97 - ETA: 4s - loss: 0.0699 - accuracy: 0.97 - ETA: 3s - loss: 0.0697 - accuracy: 0.97 - ETA: 3s - loss: 0.0683 - accuracy: 0.97 - ETA: 3s - loss: 0.0678 - accuracy: 0.97 - ETA: 3s - loss: 0.0672 - accuracy: 0.97 - ETA: 3s - loss: 0.0660 - accuracy: 0.97 - ETA: 3s - loss: 0.0662 - accuracy: 0.97 - ETA: 3s - loss: 0.0664 - accuracy: 0.97 - ETA: 3s - loss: 0.0655 - accuracy: 0.97 - ETA: 3s - loss: 0.0658 - accuracy: 0.97 - ETA: 3s - loss: 0.0673 - accuracy: 0.97 - ETA: 3s - loss: 0.0674 - accuracy: 0.97 - ETA: 3s - loss: 0.0671 - accuracy: 0.97 - ETA: 2s - loss: 0.0666 - accuracy: 0.97 - ETA: 2s - loss: 0.0656 - accuracy: 0.97 - ETA: 2s - loss: 0.0662 - accuracy: 0.97 - ETA: 2s - loss: 0.0658 - accuracy: 0.97 - ETA: 2s - loss: 0.0651 - accuracy: 0.97 - ETA: 2s - loss: 0.0641 - accuracy: 0.97 - ETA: 2s - loss: 0.0641 - accuracy: 0.97 - ETA: 2s - loss: 0.0631 - accuracy: 0.97 - ETA: 2s - loss: 0.0624 - accuracy: 0.97 - ETA: 2s - loss: 0.0625 - accuracy: 0.97 - ETA: 2s - loss: 0.0664 - accuracy: 0.97 - ETA: 2s - loss: 0.0712 - accuracy: 0.97 - ETA: 1s - loss: 0.0753 - accuracy: 0.97 - ETA: 1s - loss: 0.0753 - accuracy: 0.97 - ETA: 1s - loss: 0.0746 - accuracy: 0.97 - ETA: 1s - loss: 0.0740 - accuracy: 0.97 - ETA: 1s - loss: 0.0748 - accuracy: 0.97 - ETA: 1s - loss: 0.0742 - accuracy: 0.97 - ETA: 1s - loss: 0.0737 - accuracy: 0.97 - ETA: 1s - loss: 0.0734 - accuracy: 0.97 - ETA: 1s - loss: 0.0742 - accuracy: 0.97 - ETA: 1s - loss: 0.0748 - accuracy: 0.97 - ETA: 1s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0743 - accuracy: 0.97 - ETA: 0s - loss: 0.0736 - accuracy: 0.97 - ETA: 0s - loss: 0.0733 - accuracy: 0.97 - ETA: 0s - loss: 0.0735 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0740 - accuracy: 0.97 - ETA: 0s - loss: 0.0743 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0734 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.97 - 7s 616us/step - loss: 0.0721 - accuracy: 0.9737 - val_loss: 0.0397 - val_accuracy: 0.9848\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03999 to 0.03968, saving model to cnn_bee_1.h5\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0119 - accuracy: 1.00 - ETA: 5s - loss: 0.0279 - accuracy: 0.98 - ETA: 5s - loss: 0.0580 - accuracy: 0.98 - ETA: 5s - loss: 0.0814 - accuracy: 0.97 - ETA: 5s - loss: 0.0757 - accuracy: 0.97 - ETA: 5s - loss: 0.0838 - accuracy: 0.97 - ETA: 5s - loss: 0.0970 - accuracy: 0.96 - ETA: 5s - loss: 0.0979 - accuracy: 0.96 - ETA: 5s - loss: 0.0890 - accuracy: 0.96 - ETA: 5s - loss: 0.0838 - accuracy: 0.97 - ETA: 5s - loss: 0.0827 - accuracy: 0.96 - ETA: 5s - loss: 0.0800 - accuracy: 0.97 - ETA: 5s - loss: 0.0784 - accuracy: 0.97 - ETA: 5s - loss: 0.0742 - accuracy: 0.97 - ETA: 4s - loss: 0.0717 - accuracy: 0.97 - ETA: 4s - loss: 0.0693 - accuracy: 0.97 - ETA: 4s - loss: 0.0669 - accuracy: 0.97 - ETA: 4s - loss: 0.0648 - accuracy: 0.97 - ETA: 4s - loss: 0.0651 - accuracy: 0.97 - ETA: 4s - loss: 0.0660 - accuracy: 0.97 - ETA: 4s - loss: 0.0643 - accuracy: 0.97 - ETA: 4s - loss: 0.0642 - accuracy: 0.97 - ETA: 4s - loss: 0.0636 - accuracy: 0.97 - ETA: 4s - loss: 0.0637 - accuracy: 0.97 - ETA: 4s - loss: 0.0621 - accuracy: 0.97 - ETA: 4s - loss: 0.0617 - accuracy: 0.97 - ETA: 3s - loss: 0.0638 - accuracy: 0.97 - ETA: 3s - loss: 0.0661 - accuracy: 0.97 - ETA: 3s - loss: 0.0687 - accuracy: 0.97 - ETA: 3s - loss: 0.0680 - accuracy: 0.97 - ETA: 3s - loss: 0.0664 - accuracy: 0.97 - ETA: 3s - loss: 0.0651 - accuracy: 0.97 - ETA: 3s - loss: 0.0637 - accuracy: 0.97 - ETA: 3s - loss: 0.0650 - accuracy: 0.97 - ETA: 3s - loss: 0.0682 - accuracy: 0.97 - ETA: 3s - loss: 0.0689 - accuracy: 0.97 - ETA: 3s - loss: 0.0688 - accuracy: 0.97 - ETA: 3s - loss: 0.0685 - accuracy: 0.97 - ETA: 2s - loss: 0.0680 - accuracy: 0.97 - ETA: 2s - loss: 0.0677 - accuracy: 0.97 - ETA: 2s - loss: 0.0675 - accuracy: 0.97 - ETA: 2s - loss: 0.0666 - accuracy: 0.97 - ETA: 2s - loss: 0.0660 - accuracy: 0.97 - ETA: 2s - loss: 0.0682 - accuracy: 0.97 - ETA: 2s - loss: 0.0688 - accuracy: 0.97 - ETA: 2s - loss: 0.0679 - accuracy: 0.97 - ETA: 2s - loss: 0.0677 - accuracy: 0.97 - ETA: 2s - loss: 0.0673 - accuracy: 0.97 - ETA: 2s - loss: 0.0663 - accuracy: 0.97 - ETA: 2s - loss: 0.0666 - accuracy: 0.97 - ETA: 1s - loss: 0.0673 - accuracy: 0.97 - ETA: 1s - loss: 0.0688 - accuracy: 0.97 - ETA: 1s - loss: 0.0682 - accuracy: 0.97 - ETA: 1s - loss: 0.0674 - accuracy: 0.97 - ETA: 1s - loss: 0.0668 - accuracy: 0.97 - ETA: 1s - loss: 0.0675 - accuracy: 0.97 - ETA: 1s - loss: 0.0682 - accuracy: 0.97 - ETA: 1s - loss: 0.0703 - accuracy: 0.97 - ETA: 1s - loss: 0.0695 - accuracy: 0.97 - ETA: 1s - loss: 0.0690 - accuracy: 0.97 - ETA: 1s - loss: 0.0688 - accuracy: 0.97 - ETA: 0s - loss: 0.0689 - accuracy: 0.97 - ETA: 0s - loss: 0.0686 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0677 - accuracy: 0.97 - ETA: 0s - loss: 0.0669 - accuracy: 0.97 - ETA: 0s - loss: 0.0669 - accuracy: 0.97 - ETA: 0s - loss: 0.0668 - accuracy: 0.97 - ETA: 0s - loss: 0.0667 - accuracy: 0.97 - ETA: 0s - loss: 0.0667 - accuracy: 0.97 - ETA: 0s - loss: 0.0662 - accuracy: 0.97 - ETA: 0s - loss: 0.0660 - accuracy: 0.97 - ETA: 0s - loss: 0.0652 - accuracy: 0.97 - 7s 610us/step - loss: 0.0647 - accuracy: 0.9763 - val_loss: 0.0301 - val_accuracy: 0.9907\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03968 to 0.03013, saving model to cnn_bee_1.h5\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0358 - accuracy: 0.98 - ETA: 5s - loss: 0.0320 - accuracy: 0.99 - ETA: 6s - loss: 0.0556 - accuracy: 0.97 - ETA: 6s - loss: 0.0775 - accuracy: 0.96 - ETA: 6s - loss: 0.0721 - accuracy: 0.96 - ETA: 6s - loss: 0.0661 - accuracy: 0.97 - ETA: 6s - loss: 0.0585 - accuracy: 0.97 - ETA: 6s - loss: 0.0554 - accuracy: 0.97 - ETA: 6s - loss: 0.0521 - accuracy: 0.97 - ETA: 6s - loss: 0.0546 - accuracy: 0.97 - ETA: 5s - loss: 0.0520 - accuracy: 0.97 - ETA: 5s - loss: 0.0512 - accuracy: 0.97 - ETA: 5s - loss: 0.0516 - accuracy: 0.97 - ETA: 5s - loss: 0.0570 - accuracy: 0.97 - ETA: 5s - loss: 0.0661 - accuracy: 0.97 - ETA: 5s - loss: 0.0670 - accuracy: 0.97 - ETA: 5s - loss: 0.0652 - accuracy: 0.97 - ETA: 5s - loss: 0.0652 - accuracy: 0.97 - ETA: 5s - loss: 0.0635 - accuracy: 0.97 - ETA: 4s - loss: 0.0615 - accuracy: 0.97 - ETA: 4s - loss: 0.0605 - accuracy: 0.97 - ETA: 4s - loss: 0.0596 - accuracy: 0.97 - ETA: 4s - loss: 0.0597 - accuracy: 0.97 - ETA: 4s - loss: 0.0596 - accuracy: 0.97 - ETA: 4s - loss: 0.0586 - accuracy: 0.97 - ETA: 4s - loss: 0.0568 - accuracy: 0.97 - ETA: 4s - loss: 0.0571 - accuracy: 0.97 - ETA: 4s - loss: 0.0575 - accuracy: 0.97 - ETA: 4s - loss: 0.0587 - accuracy: 0.97 - ETA: 3s - loss: 0.0609 - accuracy: 0.97 - ETA: 3s - loss: 0.0603 - accuracy: 0.97 - ETA: 3s - loss: 0.0597 - accuracy: 0.97 - ETA: 3s - loss: 0.0588 - accuracy: 0.97 - ETA: 3s - loss: 0.0582 - accuracy: 0.97 - ETA: 3s - loss: 0.0577 - accuracy: 0.97 - ETA: 3s - loss: 0.0570 - accuracy: 0.97 - ETA: 3s - loss: 0.0558 - accuracy: 0.97 - ETA: 3s - loss: 0.0546 - accuracy: 0.97 - ETA: 3s - loss: 0.0541 - accuracy: 0.97 - ETA: 3s - loss: 0.0549 - accuracy: 0.97 - ETA: 2s - loss: 0.0545 - accuracy: 0.97 - ETA: 2s - loss: 0.0542 - accuracy: 0.97 - ETA: 2s - loss: 0.0541 - accuracy: 0.97 - ETA: 2s - loss: 0.0547 - accuracy: 0.97 - ETA: 2s - loss: 0.0568 - accuracy: 0.97 - ETA: 2s - loss: 0.0572 - accuracy: 0.97 - ETA: 2s - loss: 0.0587 - accuracy: 0.97 - ETA: 2s - loss: 0.0583 - accuracy: 0.97 - ETA: 2s - loss: 0.0573 - accuracy: 0.97 - ETA: 2s - loss: 0.0563 - accuracy: 0.97 - ETA: 2s - loss: 0.0559 - accuracy: 0.97 - ETA: 1s - loss: 0.0561 - accuracy: 0.97 - ETA: 1s - loss: 0.0552 - accuracy: 0.97 - ETA: 1s - loss: 0.0546 - accuracy: 0.97 - ETA: 1s - loss: 0.0546 - accuracy: 0.97 - ETA: 1s - loss: 0.0544 - accuracy: 0.97 - ETA: 1s - loss: 0.0544 - accuracy: 0.97 - ETA: 1s - loss: 0.0545 - accuracy: 0.97 - ETA: 1s - loss: 0.0568 - accuracy: 0.97 - ETA: 1s - loss: 0.0607 - accuracy: 0.97 - ETA: 1s - loss: 0.0611 - accuracy: 0.97 - ETA: 1s - loss: 0.0611 - accuracy: 0.97 - ETA: 0s - loss: 0.0604 - accuracy: 0.97 - ETA: 0s - loss: 0.0602 - accuracy: 0.97 - ETA: 0s - loss: 0.0595 - accuracy: 0.97 - ETA: 0s - loss: 0.0590 - accuracy: 0.97 - ETA: 0s - loss: 0.0586 - accuracy: 0.97 - ETA: 0s - loss: 0.0582 - accuracy: 0.97 - ETA: 0s - loss: 0.0579 - accuracy: 0.97 - ETA: 0s - loss: 0.0578 - accuracy: 0.97 - ETA: 0s - loss: 0.0576 - accuracy: 0.97 - ETA: 0s - loss: 0.0576 - accuracy: 0.97 - ETA: 0s - loss: 0.0579 - accuracy: 0.97 - 7s 627us/step - loss: 0.0579 - accuracy: 0.9778 - val_loss: 0.0318 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03013\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0290 - accuracy: 0.98 - ETA: 6s - loss: 0.0445 - accuracy: 0.98 - ETA: 5s - loss: 0.0490 - accuracy: 0.98 - ETA: 5s - loss: 0.0475 - accuracy: 0.98 - ETA: 5s - loss: 0.0509 - accuracy: 0.98 - ETA: 5s - loss: 0.0483 - accuracy: 0.98 - ETA: 5s - loss: 0.0459 - accuracy: 0.98 - ETA: 5s - loss: 0.0412 - accuracy: 0.98 - ETA: 5s - loss: 0.0418 - accuracy: 0.98 - ETA: 5s - loss: 0.0390 - accuracy: 0.98 - ETA: 5s - loss: 0.0403 - accuracy: 0.98 - ETA: 5s - loss: 0.0406 - accuracy: 0.98 - ETA: 4s - loss: 0.0388 - accuracy: 0.98 - ETA: 4s - loss: 0.0389 - accuracy: 0.98 - ETA: 4s - loss: 0.0392 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0403 - accuracy: 0.98 - ETA: 4s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0422 - accuracy: 0.98 - ETA: 4s - loss: 0.0431 - accuracy: 0.98 - ETA: 4s - loss: 0.0440 - accuracy: 0.98 - ETA: 4s - loss: 0.0532 - accuracy: 0.97 - ETA: 4s - loss: 0.0598 - accuracy: 0.97 - ETA: 3s - loss: 0.0581 - accuracy: 0.97 - ETA: 3s - loss: 0.0579 - accuracy: 0.97 - ETA: 3s - loss: 0.0584 - accuracy: 0.97 - ETA: 3s - loss: 0.0581 - accuracy: 0.97 - ETA: 3s - loss: 0.0576 - accuracy: 0.97 - ETA: 3s - loss: 0.0572 - accuracy: 0.97 - ETA: 3s - loss: 0.0559 - accuracy: 0.97 - ETA: 3s - loss: 0.0546 - accuracy: 0.97 - ETA: 3s - loss: 0.0535 - accuracy: 0.97 - ETA: 3s - loss: 0.0527 - accuracy: 0.97 - ETA: 3s - loss: 0.0531 - accuracy: 0.97 - ETA: 3s - loss: 0.0532 - accuracy: 0.97 - ETA: 2s - loss: 0.0531 - accuracy: 0.97 - ETA: 2s - loss: 0.0522 - accuracy: 0.97 - ETA: 2s - loss: 0.0523 - accuracy: 0.97 - ETA: 2s - loss: 0.0517 - accuracy: 0.98 - ETA: 2s - loss: 0.0517 - accuracy: 0.98 - ETA: 2s - loss: 0.0511 - accuracy: 0.98 - ETA: 2s - loss: 0.0505 - accuracy: 0.98 - ETA: 2s - loss: 0.0500 - accuracy: 0.98 - ETA: 2s - loss: 0.0494 - accuracy: 0.98 - ETA: 2s - loss: 0.0505 - accuracy: 0.98 - ETA: 2s - loss: 0.0510 - accuracy: 0.98 - ETA: 2s - loss: 0.0522 - accuracy: 0.97 - ETA: 1s - loss: 0.0529 - accuracy: 0.97 - ETA: 1s - loss: 0.0531 - accuracy: 0.97 - ETA: 1s - loss: 0.0526 - accuracy: 0.97 - ETA: 1s - loss: 0.0521 - accuracy: 0.97 - ETA: 1s - loss: 0.0522 - accuracy: 0.97 - ETA: 1s - loss: 0.0521 - accuracy: 0.97 - ETA: 1s - loss: 0.0515 - accuracy: 0.98 - ETA: 1s - loss: 0.0516 - accuracy: 0.98 - ETA: 1s - loss: 0.0527 - accuracy: 0.98 - ETA: 1s - loss: 0.0527 - accuracy: 0.97 - ETA: 1s - loss: 0.0522 - accuracy: 0.98 - ETA: 1s - loss: 0.0520 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0517 - accuracy: 0.98 - ETA: 0s - loss: 0.0515 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0504 - accuracy: 0.98 - ETA: 0s - loss: 0.0502 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0496 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0497 - accuracy: 0.98 - 6s 588us/step - loss: 0.0499 - accuracy: 0.9808 - val_loss: 0.0338 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03013\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0494 - accuracy: 0.98 - ETA: 5s - loss: 0.0442 - accuracy: 0.98 - ETA: 5s - loss: 0.0382 - accuracy: 0.98 - ETA: 5s - loss: 0.0315 - accuracy: 0.99 - ETA: 5s - loss: 0.0308 - accuracy: 0.98 - ETA: 5s - loss: 0.0425 - accuracy: 0.98 - ETA: 5s - loss: 0.0398 - accuracy: 0.98 - ETA: 5s - loss: 0.0435 - accuracy: 0.98 - ETA: 5s - loss: 0.0436 - accuracy: 0.98 - ETA: 5s - loss: 0.0440 - accuracy: 0.98 - ETA: 5s - loss: 0.0443 - accuracy: 0.98 - ETA: 5s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0411 - accuracy: 0.98 - ETA: 4s - loss: 0.0404 - accuracy: 0.98 - ETA: 4s - loss: 0.0407 - accuracy: 0.98 - ETA: 4s - loss: 0.0391 - accuracy: 0.98 - ETA: 4s - loss: 0.0385 - accuracy: 0.98 - ETA: 4s - loss: 0.0368 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0353 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0401 - accuracy: 0.98 - ETA: 4s - loss: 0.0504 - accuracy: 0.98 - ETA: 4s - loss: 0.0507 - accuracy: 0.98 - ETA: 3s - loss: 0.0500 - accuracy: 0.98 - ETA: 3s - loss: 0.0491 - accuracy: 0.98 - ETA: 3s - loss: 0.0491 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 3s - loss: 0.0473 - accuracy: 0.98 - ETA: 3s - loss: 0.0472 - accuracy: 0.98 - ETA: 3s - loss: 0.0466 - accuracy: 0.98 - ETA: 3s - loss: 0.0455 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 3s - loss: 0.0439 - accuracy: 0.98 - ETA: 3s - loss: 0.0435 - accuracy: 0.98 - ETA: 3s - loss: 0.0430 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0421 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0445 - accuracy: 0.98 - ETA: 2s - loss: 0.0465 - accuracy: 0.98 - ETA: 2s - loss: 0.0459 - accuracy: 0.98 - ETA: 2s - loss: 0.0453 - accuracy: 0.98 - ETA: 2s - loss: 0.0450 - accuracy: 0.98 - ETA: 2s - loss: 0.0442 - accuracy: 0.98 - ETA: 2s - loss: 0.0440 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0435 - accuracy: 0.98 - ETA: 1s - loss: 0.0433 - accuracy: 0.98 - ETA: 1s - loss: 0.0425 - accuracy: 0.98 - ETA: 1s - loss: 0.0423 - accuracy: 0.98 - ETA: 1s - loss: 0.0423 - accuracy: 0.98 - ETA: 1s - loss: 0.0422 - accuracy: 0.98 - ETA: 1s - loss: 0.0419 - accuracy: 0.98 - ETA: 1s - loss: 0.0412 - accuracy: 0.98 - ETA: 1s - loss: 0.0431 - accuracy: 0.98 - ETA: 1s - loss: 0.0446 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0464 - accuracy: 0.98 - ETA: 0s - loss: 0.0466 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0471 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0464 - accuracy: 0.98 - ETA: 0s - loss: 0.0465 - accuracy: 0.98 - ETA: 0s - loss: 0.0471 - accuracy: 0.98 - ETA: 0s - loss: 0.0467 - accuracy: 0.98 - ETA: 0s - loss: 0.0462 - accuracy: 0.98 - ETA: 0s - loss: 0.0457 - accuracy: 0.98 - ETA: 0s - loss: 0.0454 - accuracy: 0.98 - 6s 594us/step - loss: 0.0451 - accuracy: 0.9855 - val_loss: 0.0247 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03013 to 0.02475, saving model to cnn_bee_1.h5\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0213 - accuracy: 0.99 - ETA: 5s - loss: 0.0188 - accuracy: 0.99 - ETA: 5s - loss: 0.0376 - accuracy: 0.98 - ETA: 5s - loss: 0.0470 - accuracy: 0.98 - ETA: 5s - loss: 0.0566 - accuracy: 0.97 - ETA: 5s - loss: 0.0499 - accuracy: 0.98 - ETA: 5s - loss: 0.0467 - accuracy: 0.98 - ETA: 5s - loss: 0.0460 - accuracy: 0.98 - ETA: 5s - loss: 0.0461 - accuracy: 0.98 - ETA: 5s - loss: 0.0433 - accuracy: 0.98 - ETA: 5s - loss: 0.0403 - accuracy: 0.98 - ETA: 5s - loss: 0.0390 - accuracy: 0.98 - ETA: 5s - loss: 0.0365 - accuracy: 0.98 - ETA: 5s - loss: 0.0373 - accuracy: 0.98 - ETA: 5s - loss: 0.0361 - accuracy: 0.98 - ETA: 4s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0334 - accuracy: 0.98 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0316 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0301 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.98 - ETA: 4s - loss: 0.0353 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0389 - accuracy: 0.98 - ETA: 3s - loss: 0.0705 - accuracy: 0.97 - ETA: 3s - loss: 0.0792 - accuracy: 0.97 - ETA: 3s - loss: 0.0775 - accuracy: 0.97 - ETA: 3s - loss: 0.0752 - accuracy: 0.97 - ETA: 3s - loss: 0.0768 - accuracy: 0.97 - ETA: 3s - loss: 0.0758 - accuracy: 0.97 - ETA: 3s - loss: 0.0743 - accuracy: 0.97 - ETA: 3s - loss: 0.0739 - accuracy: 0.97 - ETA: 3s - loss: 0.0728 - accuracy: 0.97 - ETA: 3s - loss: 0.0718 - accuracy: 0.97 - ETA: 2s - loss: 0.0711 - accuracy: 0.97 - ETA: 2s - loss: 0.0700 - accuracy: 0.97 - ETA: 2s - loss: 0.0694 - accuracy: 0.97 - ETA: 2s - loss: 0.0679 - accuracy: 0.97 - ETA: 2s - loss: 0.0667 - accuracy: 0.97 - ETA: 2s - loss: 0.0658 - accuracy: 0.97 - ETA: 2s - loss: 0.0644 - accuracy: 0.97 - ETA: 2s - loss: 0.0633 - accuracy: 0.97 - ETA: 2s - loss: 0.0625 - accuracy: 0.97 - ETA: 2s - loss: 0.0613 - accuracy: 0.97 - ETA: 2s - loss: 0.0604 - accuracy: 0.98 - ETA: 2s - loss: 0.0601 - accuracy: 0.97 - ETA: 1s - loss: 0.0592 - accuracy: 0.98 - ETA: 1s - loss: 0.0582 - accuracy: 0.98 - ETA: 1s - loss: 0.0575 - accuracy: 0.98 - ETA: 1s - loss: 0.0570 - accuracy: 0.98 - ETA: 1s - loss: 0.0567 - accuracy: 0.98 - ETA: 1s - loss: 0.0562 - accuracy: 0.98 - ETA: 1s - loss: 0.0557 - accuracy: 0.98 - ETA: 1s - loss: 0.0556 - accuracy: 0.98 - ETA: 1s - loss: 0.0548 - accuracy: 0.98 - ETA: 1s - loss: 0.0547 - accuracy: 0.98 - ETA: 1s - loss: 0.0540 - accuracy: 0.98 - ETA: 1s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0533 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0517 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0507 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - ETA: 0s - loss: 0.0503 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0494 - accuracy: 0.98 - ETA: 0s - loss: 0.0492 - accuracy: 0.98 - 6s 594us/step - loss: 0.0495 - accuracy: 0.9836 - val_loss: 0.1060 - val_accuracy: 0.9612\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02475\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0882 - accuracy: 0.96 - ETA: 5s - loss: 0.0814 - accuracy: 0.97 - ETA: 5s - loss: 0.0761 - accuracy: 0.97 - ETA: 5s - loss: 0.0625 - accuracy: 0.97 - ETA: 5s - loss: 0.0546 - accuracy: 0.97 - ETA: 5s - loss: 0.0527 - accuracy: 0.97 - ETA: 5s - loss: 0.0533 - accuracy: 0.98 - ETA: 5s - loss: 0.0480 - accuracy: 0.98 - ETA: 5s - loss: 0.0489 - accuracy: 0.98 - ETA: 5s - loss: 0.0481 - accuracy: 0.98 - ETA: 5s - loss: 0.0468 - accuracy: 0.98 - ETA: 5s - loss: 0.0450 - accuracy: 0.98 - ETA: 5s - loss: 0.0497 - accuracy: 0.98 - ETA: 4s - loss: 0.0479 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 4s - loss: 0.0457 - accuracy: 0.98 - ETA: 4s - loss: 0.0463 - accuracy: 0.98 - ETA: 4s - loss: 0.0447 - accuracy: 0.98 - ETA: 4s - loss: 0.0440 - accuracy: 0.98 - ETA: 4s - loss: 0.0427 - accuracy: 0.98 - ETA: 4s - loss: 0.0411 - accuracy: 0.98 - ETA: 4s - loss: 0.0424 - accuracy: 0.98 - ETA: 4s - loss: 0.0423 - accuracy: 0.98 - ETA: 4s - loss: 0.0413 - accuracy: 0.98 - ETA: 4s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0380 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0364 - accuracy: 0.98 - ETA: 3s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0386 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 2s - loss: 0.0385 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 2s - loss: 0.0400 - accuracy: 0.98 - ETA: 2s - loss: 0.0417 - accuracy: 0.98 - ETA: 1s - loss: 0.0436 - accuracy: 0.98 - ETA: 1s - loss: 0.0440 - accuracy: 0.98 - ETA: 1s - loss: 0.0432 - accuracy: 0.98 - ETA: 1s - loss: 0.0429 - accuracy: 0.98 - ETA: 1s - loss: 0.0423 - accuracy: 0.98 - ETA: 1s - loss: 0.0422 - accuracy: 0.98 - ETA: 1s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0417 - accuracy: 0.98 - ETA: 1s - loss: 0.0412 - accuracy: 0.98 - ETA: 1s - loss: 0.0407 - accuracy: 0.98 - ETA: 1s - loss: 0.0429 - accuracy: 0.98 - ETA: 1s - loss: 0.0426 - accuracy: 0.98 - ETA: 0s - loss: 0.0426 - accuracy: 0.98 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0421 - accuracy: 0.98 - ETA: 0s - loss: 0.0427 - accuracy: 0.98 - ETA: 0s - loss: 0.0423 - accuracy: 0.98 - ETA: 0s - loss: 0.0420 - accuracy: 0.98 - ETA: 0s - loss: 0.0418 - accuracy: 0.98 - ETA: 0s - loss: 0.0417 - accuracy: 0.98 - ETA: 0s - loss: 0.0413 - accuracy: 0.98 - ETA: 0s - loss: 0.0414 - accuracy: 0.98 - ETA: 0s - loss: 0.0412 - accuracy: 0.98 - ETA: 0s - loss: 0.0408 - accuracy: 0.98 - 6s 585us/step - loss: 0.0405 - accuracy: 0.9853 - val_loss: 0.0280 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02475\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0218 - accuracy: 0.98 - ETA: 5s - loss: 0.0346 - accuracy: 0.97 - ETA: 6s - loss: 0.0280 - accuracy: 0.97 - ETA: 5s - loss: 0.0244 - accuracy: 0.98 - ETA: 5s - loss: 0.0235 - accuracy: 0.98 - ETA: 5s - loss: 0.0199 - accuracy: 0.98 - ETA: 5s - loss: 0.0216 - accuracy: 0.98 - ETA: 5s - loss: 0.0199 - accuracy: 0.98 - ETA: 5s - loss: 0.0220 - accuracy: 0.99 - ETA: 5s - loss: 0.0252 - accuracy: 0.98 - ETA: 5s - loss: 0.0283 - accuracy: 0.98 - ETA: 5s - loss: 0.0302 - accuracy: 0.98 - ETA: 5s - loss: 0.0303 - accuracy: 0.98 - ETA: 5s - loss: 0.0289 - accuracy: 0.98 - ETA: 4s - loss: 0.0294 - accuracy: 0.98 - ETA: 4s - loss: 0.0298 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0323 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 4s - loss: 0.0358 - accuracy: 0.98 - ETA: 4s - loss: 0.0348 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 3s - loss: 0.0330 - accuracy: 0.98 - ETA: 3s - loss: 0.0329 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0319 - accuracy: 0.98 - ETA: 3s - loss: 0.0310 - accuracy: 0.98 - ETA: 3s - loss: 0.0301 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0331 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0364 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0364 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 2s - loss: 0.0369 - accuracy: 0.98 - ETA: 2s - loss: 0.0365 - accuracy: 0.98 - ETA: 2s - loss: 0.0360 - accuracy: 0.98 - ETA: 2s - loss: 0.0363 - accuracy: 0.98 - ETA: 2s - loss: 0.0361 - accuracy: 0.98 - ETA: 2s - loss: 0.0360 - accuracy: 0.98 - ETA: 2s - loss: 0.0359 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 1s - loss: 0.0350 - accuracy: 0.98 - ETA: 1s - loss: 0.0352 - accuracy: 0.98 - ETA: 1s - loss: 0.0352 - accuracy: 0.98 - ETA: 1s - loss: 0.0350 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 1s - loss: 0.0355 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 1s - loss: 0.0358 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0384 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0375 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0377 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - 6s 593us/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 0.0217 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02475 to 0.02169, saving model to cnn_bee_1.h5\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0136 - accuracy: 0.99 - ETA: 5s - loss: 0.0118 - accuracy: 0.99 - ETA: 5s - loss: 0.0083 - accuracy: 0.99 - ETA: 5s - loss: 0.0089 - accuracy: 0.99 - ETA: 5s - loss: 0.0103 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0333 - accuracy: 0.98 - ETA: 5s - loss: 0.0475 - accuracy: 0.98 - ETA: 5s - loss: 0.0609 - accuracy: 0.98 - ETA: 5s - loss: 0.0572 - accuracy: 0.98 - ETA: 5s - loss: 0.0575 - accuracy: 0.98 - ETA: 5s - loss: 0.0555 - accuracy: 0.98 - ETA: 4s - loss: 0.0517 - accuracy: 0.98 - ETA: 4s - loss: 0.0482 - accuracy: 0.98 - ETA: 4s - loss: 0.0462 - accuracy: 0.98 - ETA: 4s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0430 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 4s - loss: 0.0395 - accuracy: 0.98 - ETA: 4s - loss: 0.0400 - accuracy: 0.98 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 4s - loss: 0.0399 - accuracy: 0.98 - ETA: 4s - loss: 0.0430 - accuracy: 0.98 - ETA: 4s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0423 - accuracy: 0.98 - ETA: 3s - loss: 0.0417 - accuracy: 0.98 - ETA: 3s - loss: 0.0412 - accuracy: 0.98 - ETA: 3s - loss: 0.0423 - accuracy: 0.98 - ETA: 3s - loss: 0.0414 - accuracy: 0.98 - ETA: 3s - loss: 0.0407 - accuracy: 0.98 - ETA: 3s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0402 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0386 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0404 - accuracy: 0.98 - ETA: 3s - loss: 0.0404 - accuracy: 0.98 - ETA: 2s - loss: 0.0400 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 2s - loss: 0.0393 - accuracy: 0.98 - ETA: 2s - loss: 0.0385 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0368 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0363 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0355 - accuracy: 0.98 - ETA: 1s - loss: 0.0353 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0359 - accuracy: 0.98 - ETA: 1s - loss: 0.0379 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0386 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0382 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0388 - accuracy: 0.98 - ETA: 0s - loss: 0.0386 - accuracy: 0.98 - 6s 590us/step - loss: 0.0384 - accuracy: 0.9860 - val_loss: 0.0378 - val_accuracy: 0.9857\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02169\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0343 - accuracy: 0.97 - ETA: 6s - loss: 0.0675 - accuracy: 0.96 - ETA: 5s - loss: 0.0514 - accuracy: 0.97 - ETA: 5s - loss: 0.0477 - accuracy: 0.98 - ETA: 5s - loss: 0.0421 - accuracy: 0.98 - ETA: 5s - loss: 0.0406 - accuracy: 0.98 - ETA: 5s - loss: 0.0410 - accuracy: 0.98 - ETA: 5s - loss: 0.0420 - accuracy: 0.98 - ETA: 5s - loss: 0.0392 - accuracy: 0.98 - ETA: 5s - loss: 0.0387 - accuracy: 0.98 - ETA: 5s - loss: 0.0364 - accuracy: 0.98 - ETA: 5s - loss: 0.0397 - accuracy: 0.98 - ETA: 5s - loss: 0.0378 - accuracy: 0.98 - ETA: 5s - loss: 0.0363 - accuracy: 0.98 - ETA: 5s - loss: 0.0348 - accuracy: 0.98 - ETA: 4s - loss: 0.0345 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0414 - accuracy: 0.98 - ETA: 4s - loss: 0.0397 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 4s - loss: 0.0357 - accuracy: 0.98 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 3s - loss: 0.0333 - accuracy: 0.98 - ETA: 3s - loss: 0.0335 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0327 - accuracy: 0.98 - ETA: 3s - loss: 0.0321 - accuracy: 0.98 - ETA: 3s - loss: 0.0314 - accuracy: 0.98 - ETA: 3s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0298 - accuracy: 0.99 - ETA: 3s - loss: 0.0291 - accuracy: 0.99 - ETA: 3s - loss: 0.0303 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0354 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 2s - loss: 0.0336 - accuracy: 0.98 - ETA: 2s - loss: 0.0337 - accuracy: 0.98 - ETA: 2s - loss: 0.0330 - accuracy: 0.98 - ETA: 2s - loss: 0.0325 - accuracy: 0.98 - ETA: 2s - loss: 0.0323 - accuracy: 0.98 - ETA: 2s - loss: 0.0332 - accuracy: 0.98 - ETA: 2s - loss: 0.0332 - accuracy: 0.98 - ETA: 2s - loss: 0.0325 - accuracy: 0.98 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 1s - loss: 0.0336 - accuracy: 0.98 - ETA: 1s - loss: 0.0342 - accuracy: 0.98 - ETA: 1s - loss: 0.0346 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 1s - loss: 0.0340 - accuracy: 0.98 - ETA: 1s - loss: 0.0339 - accuracy: 0.98 - ETA: 1s - loss: 0.0335 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0324 - accuracy: 0.98 - ETA: 0s - loss: 0.0322 - accuracy: 0.98 - ETA: 0s - loss: 0.0319 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0324 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0317 - accuracy: 0.98 - ETA: 0s - loss: 0.0325 - accuracy: 0.98 - 6s 590us/step - loss: 0.0332 - accuracy: 0.9889 - val_loss: 0.0317 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02169\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0380 - accuracy: 0.99 - ETA: 5s - loss: 0.0400 - accuracy: 0.98 - ETA: 5s - loss: 0.0316 - accuracy: 0.99 - ETA: 6s - loss: 0.0286 - accuracy: 0.99 - ETA: 6s - loss: 0.0299 - accuracy: 0.99 - ETA: 6s - loss: 0.0299 - accuracy: 0.99 - ETA: 5s - loss: 0.0276 - accuracy: 0.99 - ETA: 5s - loss: 0.0260 - accuracy: 0.99 - ETA: 5s - loss: 0.0238 - accuracy: 0.99 - ETA: 5s - loss: 0.0220 - accuracy: 0.99 - ETA: 5s - loss: 0.0203 - accuracy: 0.99 - ETA: 5s - loss: 0.0189 - accuracy: 0.99 - ETA: 5s - loss: 0.0182 - accuracy: 0.99 - ETA: 5s - loss: 0.0176 - accuracy: 0.99 - ETA: 5s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0175 - accuracy: 0.99 - ETA: 4s - loss: 0.0175 - accuracy: 0.99 - ETA: 4s - loss: 0.0181 - accuracy: 0.99 - ETA: 4s - loss: 0.0178 - accuracy: 0.99 - ETA: 4s - loss: 0.0208 - accuracy: 0.99 - ETA: 4s - loss: 0.0295 - accuracy: 0.99 - ETA: 4s - loss: 0.0404 - accuracy: 0.98 - ETA: 4s - loss: 0.0431 - accuracy: 0.98 - ETA: 4s - loss: 0.0425 - accuracy: 0.98 - ETA: 4s - loss: 0.0414 - accuracy: 0.98 - ETA: 4s - loss: 0.0402 - accuracy: 0.98 - ETA: 3s - loss: 0.0390 - accuracy: 0.98 - ETA: 3s - loss: 0.0381 - accuracy: 0.98 - ETA: 3s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0364 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0350 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0330 - accuracy: 0.98 - ETA: 3s - loss: 0.0333 - accuracy: 0.98 - ETA: 3s - loss: 0.0331 - accuracy: 0.98 - ETA: 3s - loss: 0.0331 - accuracy: 0.98 - ETA: 3s - loss: 0.0333 - accuracy: 0.98 - ETA: 2s - loss: 0.0326 - accuracy: 0.98 - ETA: 2s - loss: 0.0329 - accuracy: 0.98 - ETA: 2s - loss: 0.0330 - accuracy: 0.98 - ETA: 2s - loss: 0.0323 - accuracy: 0.99 - ETA: 2s - loss: 0.0322 - accuracy: 0.98 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0319 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0315 - accuracy: 0.99 - ETA: 2s - loss: 0.0313 - accuracy: 0.99 - ETA: 2s - loss: 0.0312 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.99 - ETA: 1s - loss: 0.0301 - accuracy: 0.99 - ETA: 1s - loss: 0.0299 - accuracy: 0.99 - ETA: 1s - loss: 0.0295 - accuracy: 0.99 - ETA: 1s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0302 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0315 - accuracy: 0.98 - ETA: 1s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0311 - accuracy: 0.98 - ETA: 0s - loss: 0.0308 - accuracy: 0.98 - ETA: 0s - loss: 0.0308 - accuracy: 0.98 - ETA: 0s - loss: 0.0303 - accuracy: 0.98 - ETA: 0s - loss: 0.0299 - accuracy: 0.99 - ETA: 0s - loss: 0.0296 - accuracy: 0.99 - ETA: 0s - loss: 0.0294 - accuracy: 0.99 - ETA: 0s - loss: 0.0294 - accuracy: 0.99 - ETA: 0s - loss: 0.0306 - accuracy: 0.98 - ETA: 0s - loss: 0.0310 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - 6s 596us/step - loss: 0.0372 - accuracy: 0.9878 - val_loss: 0.0327 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02169\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0435 - accuracy: 0.97 - ETA: 5s - loss: 0.0286 - accuracy: 0.98 - ETA: 5s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0159 - accuracy: 0.99 - ETA: 5s - loss: 0.0139 - accuracy: 0.99 - ETA: 5s - loss: 0.0143 - accuracy: 0.99 - ETA: 5s - loss: 0.0147 - accuracy: 0.99 - ETA: 5s - loss: 0.0150 - accuracy: 0.99 - ETA: 5s - loss: 0.0137 - accuracy: 0.99 - ETA: 5s - loss: 0.0130 - accuracy: 0.99 - ETA: 5s - loss: 0.0130 - accuracy: 0.99 - ETA: 5s - loss: 0.0121 - accuracy: 0.99 - ETA: 5s - loss: 0.0118 - accuracy: 0.99 - ETA: 4s - loss: 0.0111 - accuracy: 0.99 - ETA: 4s - loss: 0.0127 - accuracy: 0.99 - ETA: 4s - loss: 0.0146 - accuracy: 0.99 - ETA: 4s - loss: 0.0152 - accuracy: 0.99 - ETA: 4s - loss: 0.0160 - accuracy: 0.99 - ETA: 4s - loss: 0.0158 - accuracy: 0.99 - ETA: 4s - loss: 0.0157 - accuracy: 0.99 - ETA: 4s - loss: 0.0153 - accuracy: 0.99 - ETA: 4s - loss: 0.0167 - accuracy: 0.99 - ETA: 4s - loss: 0.0199 - accuracy: 0.99 - ETA: 4s - loss: 0.0199 - accuracy: 0.99 - ETA: 4s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0217 - accuracy: 0.99 - ETA: 3s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0208 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0208 - accuracy: 0.99 - ETA: 3s - loss: 0.0289 - accuracy: 0.99 - ETA: 3s - loss: 0.0286 - accuracy: 0.99 - ETA: 3s - loss: 0.0280 - accuracy: 0.99 - ETA: 3s - loss: 0.0292 - accuracy: 0.99 - ETA: 3s - loss: 0.0292 - accuracy: 0.99 - ETA: 3s - loss: 0.0288 - accuracy: 0.99 - ETA: 3s - loss: 0.0286 - accuracy: 0.99 - ETA: 2s - loss: 0.0289 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0281 - accuracy: 0.99 - ETA: 2s - loss: 0.0279 - accuracy: 0.99 - ETA: 2s - loss: 0.0294 - accuracy: 0.99 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.99 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 2s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 2s - loss: 0.0302 - accuracy: 0.99 - ETA: 1s - loss: 0.0302 - accuracy: 0.98 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0294 - accuracy: 0.99 - ETA: 1s - loss: 0.0291 - accuracy: 0.99 - ETA: 1s - loss: 0.0289 - accuracy: 0.99 - ETA: 1s - loss: 0.0291 - accuracy: 0.99 - ETA: 1s - loss: 0.0289 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 1s - loss: 0.0277 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0270 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - 6s 595us/step - loss: 0.0270 - accuracy: 0.9915 - val_loss: 0.0449 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02169\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0148 - accuracy: 0.99 - ETA: 6s - loss: 0.0098 - accuracy: 0.99 - ETA: 6s - loss: 0.0077 - accuracy: 0.99 - ETA: 5s - loss: 0.0139 - accuracy: 0.99 - ETA: 5s - loss: 0.0189 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0302 - accuracy: 0.98 - ETA: 5s - loss: 0.0327 - accuracy: 0.98 - ETA: 5s - loss: 0.0359 - accuracy: 0.98 - ETA: 5s - loss: 0.0381 - accuracy: 0.98 - ETA: 5s - loss: 0.0396 - accuracy: 0.98 - ETA: 5s - loss: 0.0391 - accuracy: 0.98 - ETA: 5s - loss: 0.0370 - accuracy: 0.98 - ETA: 5s - loss: 0.0393 - accuracy: 0.98 - ETA: 5s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0323 - accuracy: 0.98 - ETA: 4s - loss: 0.0324 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.98 - ETA: 4s - loss: 0.0301 - accuracy: 0.98 - ETA: 4s - loss: 0.0353 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 3s - loss: 0.0353 - accuracy: 0.98 - ETA: 3s - loss: 0.0345 - accuracy: 0.98 - ETA: 3s - loss: 0.0347 - accuracy: 0.98 - ETA: 3s - loss: 0.0338 - accuracy: 0.98 - ETA: 3s - loss: 0.0336 - accuracy: 0.98 - ETA: 3s - loss: 0.0330 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0340 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 2s - loss: 0.0325 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0302 - accuracy: 0.98 - ETA: 2s - loss: 0.0299 - accuracy: 0.98 - ETA: 2s - loss: 0.0295 - accuracy: 0.98 - ETA: 2s - loss: 0.0290 - accuracy: 0.98 - ETA: 1s - loss: 0.0285 - accuracy: 0.98 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0304 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 1s - loss: 0.0325 - accuracy: 0.98 - ETA: 1s - loss: 0.0325 - accuracy: 0.98 - ETA: 1s - loss: 0.0321 - accuracy: 0.98 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0317 - accuracy: 0.98 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0311 - accuracy: 0.98 - ETA: 0s - loss: 0.0307 - accuracy: 0.98 - ETA: 0s - loss: 0.0303 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.98 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0291 - accuracy: 0.98 - ETA: 0s - loss: 0.0295 - accuracy: 0.98 - ETA: 0s - loss: 0.0291 - accuracy: 0.98 - ETA: 0s - loss: 0.0289 - accuracy: 0.98 - ETA: 0s - loss: 0.0297 - accuracy: 0.98 - ETA: 0s - loss: 0.0343 - accuracy: 0.98 - ETA: 0s - loss: 0.0353 - accuracy: 0.98 - 6s 592us/step - loss: 0.0351 - accuracy: 0.9883 - val_loss: 0.0180 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02169 to 0.01796, saving model to cnn_bee_1.h5\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0184 - accuracy: 0.99 - ETA: 5s - loss: 0.0131 - accuracy: 0.99 - ETA: 5s - loss: 0.0133 - accuracy: 0.99 - ETA: 5s - loss: 0.0119 - accuracy: 0.99 - ETA: 5s - loss: 0.0098 - accuracy: 0.99 - ETA: 5s - loss: 0.0087 - accuracy: 0.99 - ETA: 5s - loss: 0.0077 - accuracy: 0.99 - ETA: 5s - loss: 0.0119 - accuracy: 0.99 - ETA: 5s - loss: 0.0112 - accuracy: 0.99 - ETA: 5s - loss: 0.0142 - accuracy: 0.99 - ETA: 5s - loss: 0.0132 - accuracy: 0.99 - ETA: 5s - loss: 0.0122 - accuracy: 0.99 - ETA: 4s - loss: 0.0116 - accuracy: 0.99 - ETA: 4s - loss: 0.0114 - accuracy: 0.99 - ETA: 4s - loss: 0.0128 - accuracy: 0.99 - ETA: 4s - loss: 0.0137 - accuracy: 0.99 - ETA: 4s - loss: 0.0132 - accuracy: 0.99 - ETA: 4s - loss: 0.0130 - accuracy: 0.99 - ETA: 4s - loss: 0.0130 - accuracy: 0.99 - ETA: 4s - loss: 0.0137 - accuracy: 0.99 - ETA: 4s - loss: 0.0171 - accuracy: 0.99 - ETA: 4s - loss: 0.0169 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0160 - accuracy: 0.99 - ETA: 3s - loss: 0.0163 - accuracy: 0.99 - ETA: 3s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0188 - accuracy: 0.99 - ETA: 3s - loss: 0.0186 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0190 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0213 - accuracy: 0.99 - ETA: 2s - loss: 0.0218 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0232 - accuracy: 0.99 - ETA: 1s - loss: 0.0241 - accuracy: 0.99 - ETA: 1s - loss: 0.0237 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0228 - accuracy: 0.99 - ETA: 1s - loss: 0.0229 - accuracy: 0.99 - ETA: 1s - loss: 0.0239 - accuracy: 0.99 - ETA: 1s - loss: 0.0236 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0237 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0231 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0241 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0237 - accuracy: 0.99 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - 6s 587us/step - loss: 0.0238 - accuracy: 0.9919 - val_loss: 0.0161 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01796 to 0.01607, saving model to cnn_bee_1.h5\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0177 - accuracy: 0.99 - ETA: 5s - loss: 0.0228 - accuracy: 0.98 - ETA: 5s - loss: 0.0210 - accuracy: 0.98 - ETA: 5s - loss: 0.0165 - accuracy: 0.99 - ETA: 5s - loss: 0.0136 - accuracy: 0.99 - ETA: 5s - loss: 0.0125 - accuracy: 0.99 - ETA: 5s - loss: 0.0123 - accuracy: 0.99 - ETA: 5s - loss: 0.0116 - accuracy: 0.99 - ETA: 5s - loss: 0.0111 - accuracy: 0.99 - ETA: 5s - loss: 0.0103 - accuracy: 0.99 - ETA: 5s - loss: 0.0133 - accuracy: 0.99 - ETA: 5s - loss: 0.0151 - accuracy: 0.99 - ETA: 4s - loss: 0.0175 - accuracy: 0.99 - ETA: 4s - loss: 0.0176 - accuracy: 0.99 - ETA: 4s - loss: 0.0171 - accuracy: 0.99 - ETA: 4s - loss: 0.0165 - accuracy: 0.99 - ETA: 4s - loss: 0.0171 - accuracy: 0.99 - ETA: 4s - loss: 0.0168 - accuracy: 0.99 - ETA: 4s - loss: 0.0172 - accuracy: 0.99 - ETA: 4s - loss: 0.0165 - accuracy: 0.99 - ETA: 4s - loss: 0.0177 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0190 - accuracy: 0.99 - ETA: 4s - loss: 0.0184 - accuracy: 0.99 - ETA: 3s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0238 - accuracy: 0.99 - ETA: 3s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0230 - accuracy: 0.99 - ETA: 3s - loss: 0.0256 - accuracy: 0.99 - ETA: 3s - loss: 0.0263 - accuracy: 0.99 - ETA: 3s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0264 - accuracy: 0.99 - ETA: 2s - loss: 0.0258 - accuracy: 0.99 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 2s - loss: 0.0250 - accuracy: 0.99 - ETA: 2s - loss: 0.0251 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 1s - loss: 0.0243 - accuracy: 0.99 - ETA: 1s - loss: 0.0249 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0317 - accuracy: 0.98 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0311 - accuracy: 0.98 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0304 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0302 - accuracy: 0.98 - ETA: 1s - loss: 0.0299 - accuracy: 0.98 - ETA: 0s - loss: 0.0297 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0291 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0289 - accuracy: 0.98 - ETA: 0s - loss: 0.0288 - accuracy: 0.98 - ETA: 0s - loss: 0.0284 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.98 - ETA: 0s - loss: 0.0280 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - 6s 591us/step - loss: 0.0275 - accuracy: 0.9903 - val_loss: 0.0155 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01607 to 0.01554, saving model to cnn_bee_1.h5\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0065 - accuracy: 1.00 - ETA: 5s - loss: 0.0119 - accuracy: 0.99 - ETA: 5s - loss: 0.0094 - accuracy: 0.99 - ETA: 5s - loss: 0.0078 - accuracy: 0.99 - ETA: 5s - loss: 0.0065 - accuracy: 0.99 - ETA: 5s - loss: 0.0062 - accuracy: 0.99 - ETA: 5s - loss: 0.0058 - accuracy: 0.99 - ETA: 5s - loss: 0.0065 - accuracy: 0.99 - ETA: 5s - loss: 0.0064 - accuracy: 0.99 - ETA: 5s - loss: 0.0076 - accuracy: 0.99 - ETA: 5s - loss: 0.0104 - accuracy: 0.99 - ETA: 5s - loss: 0.0118 - accuracy: 0.99 - ETA: 5s - loss: 0.0148 - accuracy: 0.99 - ETA: 5s - loss: 0.0165 - accuracy: 0.99 - ETA: 4s - loss: 0.0175 - accuracy: 0.99 - ETA: 4s - loss: 0.0165 - accuracy: 0.99 - ETA: 4s - loss: 0.0160 - accuracy: 0.99 - ETA: 4s - loss: 0.0179 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0225 - accuracy: 0.99 - ETA: 4s - loss: 0.0219 - accuracy: 0.99 - ETA: 4s - loss: 0.0211 - accuracy: 0.99 - ETA: 4s - loss: 0.0209 - accuracy: 0.99 - ETA: 4s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0191 - accuracy: 0.99 - ETA: 3s - loss: 0.0190 - accuracy: 0.99 - ETA: 3s - loss: 0.0184 - accuracy: 0.99 - ETA: 3s - loss: 0.0184 - accuracy: 0.99 - ETA: 3s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0205 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0222 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0230 - accuracy: 0.99 - ETA: 2s - loss: 0.0230 - accuracy: 0.99 - ETA: 2s - loss: 0.0250 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 1s - loss: 0.0239 - accuracy: 0.99 - ETA: 1s - loss: 0.0236 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0231 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0237 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0231 - accuracy: 0.99 - ETA: 0s - loss: 0.0229 - accuracy: 0.99 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0231 - accuracy: 0.99 - ETA: 0s - loss: 0.0228 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0230 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - 6s 603us/step - loss: 0.0233 - accuracy: 0.9920 - val_loss: 0.0145 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01554 to 0.01451, saving model to cnn_bee_1.h5\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0135 - accuracy: 0.99 - ETA: 5s - loss: 0.0085 - accuracy: 0.99 - ETA: 5s - loss: 0.0081 - accuracy: 0.99 - ETA: 5s - loss: 0.0063 - accuracy: 0.99 - ETA: 5s - loss: 0.0130 - accuracy: 0.99 - ETA: 5s - loss: 0.0902 - accuracy: 0.97 - ETA: 5s - loss: 0.0811 - accuracy: 0.97 - ETA: 5s - loss: 0.0717 - accuracy: 0.98 - ETA: 5s - loss: 0.0639 - accuracy: 0.98 - ETA: 5s - loss: 0.0579 - accuracy: 0.98 - ETA: 5s - loss: 0.0560 - accuracy: 0.98 - ETA: 4s - loss: 0.0515 - accuracy: 0.98 - ETA: 4s - loss: 0.0485 - accuracy: 0.98 - ETA: 4s - loss: 0.0454 - accuracy: 0.98 - ETA: 4s - loss: 0.0450 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0419 - accuracy: 0.98 - ETA: 4s - loss: 0.0399 - accuracy: 0.98 - ETA: 4s - loss: 0.0383 - accuracy: 0.98 - ETA: 4s - loss: 0.0370 - accuracy: 0.98 - ETA: 4s - loss: 0.0369 - accuracy: 0.99 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 4s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 3s - loss: 0.0373 - accuracy: 0.98 - ETA: 3s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.99 - ETA: 3s - loss: 0.0346 - accuracy: 0.99 - ETA: 3s - loss: 0.0336 - accuracy: 0.99 - ETA: 3s - loss: 0.0327 - accuracy: 0.99 - ETA: 3s - loss: 0.0319 - accuracy: 0.99 - ETA: 3s - loss: 0.0315 - accuracy: 0.99 - ETA: 2s - loss: 0.0307 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 2s - loss: 0.0295 - accuracy: 0.99 - ETA: 2s - loss: 0.0290 - accuracy: 0.99 - ETA: 2s - loss: 0.0285 - accuracy: 0.99 - ETA: 2s - loss: 0.0280 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0265 - accuracy: 0.99 - ETA: 2s - loss: 0.0265 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0255 - accuracy: 0.99 - ETA: 1s - loss: 0.0250 - accuracy: 0.99 - ETA: 1s - loss: 0.0250 - accuracy: 0.99 - ETA: 1s - loss: 0.0254 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - 6s 584us/step - loss: 0.0249 - accuracy: 0.9927 - val_loss: 0.1169 - val_accuracy: 0.9655\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01451\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1236 - accuracy: 0.95 - ETA: 5s - loss: 0.0791 - accuracy: 0.96 - ETA: 5s - loss: 0.0817 - accuracy: 0.96 - ETA: 5s - loss: 0.0649 - accuracy: 0.97 - ETA: 5s - loss: 0.0660 - accuracy: 0.97 - ETA: 5s - loss: 0.0662 - accuracy: 0.97 - ETA: 5s - loss: 0.0571 - accuracy: 0.97 - ETA: 5s - loss: 0.0540 - accuracy: 0.97 - ETA: 5s - loss: 0.0494 - accuracy: 0.98 - ETA: 5s - loss: 0.0455 - accuracy: 0.98 - ETA: 5s - loss: 0.0439 - accuracy: 0.98 - ETA: 5s - loss: 0.0404 - accuracy: 0.98 - ETA: 4s - loss: 0.0377 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0295 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.98 - ETA: 4s - loss: 0.0297 - accuracy: 0.98 - ETA: 4s - loss: 0.0289 - accuracy: 0.98 - ETA: 4s - loss: 0.0280 - accuracy: 0.98 - ETA: 4s - loss: 0.0273 - accuracy: 0.99 - ETA: 4s - loss: 0.0267 - accuracy: 0.99 - ETA: 4s - loss: 0.0263 - accuracy: 0.99 - ETA: 3s - loss: 0.0265 - accuracy: 0.99 - ETA: 3s - loss: 0.0265 - accuracy: 0.98 - ETA: 3s - loss: 0.0260 - accuracy: 0.98 - ETA: 3s - loss: 0.0252 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.98 - ETA: 3s - loss: 0.0332 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0352 - accuracy: 0.98 - ETA: 3s - loss: 0.0344 - accuracy: 0.98 - ETA: 3s - loss: 0.0336 - accuracy: 0.98 - ETA: 3s - loss: 0.0329 - accuracy: 0.98 - ETA: 3s - loss: 0.0331 - accuracy: 0.98 - ETA: 3s - loss: 0.0327 - accuracy: 0.98 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 2s - loss: 0.0317 - accuracy: 0.98 - ETA: 2s - loss: 0.0314 - accuracy: 0.98 - ETA: 2s - loss: 0.0308 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.98 - ETA: 2s - loss: 0.0296 - accuracy: 0.98 - ETA: 2s - loss: 0.0291 - accuracy: 0.98 - ETA: 2s - loss: 0.0286 - accuracy: 0.98 - ETA: 2s - loss: 0.0282 - accuracy: 0.98 - ETA: 2s - loss: 0.0278 - accuracy: 0.98 - ETA: 2s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0258 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0263 - accuracy: 0.99 - ETA: 0s - loss: 0.0260 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - 6s 587us/step - loss: 0.0262 - accuracy: 0.9904 - val_loss: 0.0165 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01451\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0033 - accuracy: 1.00 - ETA: 5s - loss: 0.0310 - accuracy: 0.98 - ETA: 5s - loss: 0.0488 - accuracy: 0.98 - ETA: 5s - loss: 0.0401 - accuracy: 0.98 - ETA: 5s - loss: 0.0344 - accuracy: 0.98 - ETA: 5s - loss: 0.0346 - accuracy: 0.98 - ETA: 5s - loss: 0.0304 - accuracy: 0.98 - ETA: 5s - loss: 0.0270 - accuracy: 0.98 - ETA: 5s - loss: 0.0241 - accuracy: 0.99 - ETA: 5s - loss: 0.0218 - accuracy: 0.99 - ETA: 5s - loss: 0.0201 - accuracy: 0.99 - ETA: 5s - loss: 0.0185 - accuracy: 0.99 - ETA: 5s - loss: 0.0176 - accuracy: 0.99 - ETA: 5s - loss: 0.0168 - accuracy: 0.99 - ETA: 4s - loss: 0.0157 - accuracy: 0.99 - ETA: 4s - loss: 0.0155 - accuracy: 0.99 - ETA: 4s - loss: 0.0149 - accuracy: 0.99 - ETA: 4s - loss: 0.0146 - accuracy: 0.99 - ETA: 4s - loss: 0.0143 - accuracy: 0.99 - ETA: 4s - loss: 0.0136 - accuracy: 0.99 - ETA: 4s - loss: 0.0141 - accuracy: 0.99 - ETA: 4s - loss: 0.0136 - accuracy: 0.99 - ETA: 4s - loss: 0.0132 - accuracy: 0.99 - ETA: 4s - loss: 0.0130 - accuracy: 0.99 - ETA: 4s - loss: 0.0149 - accuracy: 0.99 - ETA: 4s - loss: 0.0290 - accuracy: 0.99 - ETA: 3s - loss: 0.0296 - accuracy: 0.99 - ETA: 3s - loss: 0.0291 - accuracy: 0.99 - ETA: 3s - loss: 0.0300 - accuracy: 0.99 - ETA: 3s - loss: 0.0307 - accuracy: 0.99 - ETA: 3s - loss: 0.0302 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0286 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0267 - accuracy: 0.99 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0250 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0249 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 1s - loss: 0.0241 - accuracy: 0.99 - ETA: 1s - loss: 0.0245 - accuracy: 0.99 - ETA: 1s - loss: 0.0249 - accuracy: 0.99 - ETA: 1s - loss: 0.0245 - accuracy: 0.99 - ETA: 1s - loss: 0.0242 - accuracy: 0.99 - ETA: 1s - loss: 0.0241 - accuracy: 0.99 - ETA: 1s - loss: 0.0237 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0229 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0221 - accuracy: 0.99 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0263 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0260 - accuracy: 0.99 - 6s 588us/step - loss: 0.0258 - accuracy: 0.9928 - val_loss: 0.0154 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01451\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0031 - accuracy: 1.00 - ETA: 5s - loss: 0.0056 - accuracy: 1.00 - ETA: 5s - loss: 0.0117 - accuracy: 0.99 - ETA: 5s - loss: 0.0114 - accuracy: 0.99 - ETA: 5s - loss: 0.0109 - accuracy: 0.99 - ETA: 5s - loss: 0.0094 - accuracy: 0.99 - ETA: 5s - loss: 0.0083 - accuracy: 0.99 - ETA: 5s - loss: 0.0075 - accuracy: 0.99 - ETA: 5s - loss: 0.0067 - accuracy: 0.99 - ETA: 5s - loss: 0.0082 - accuracy: 0.99 - ETA: 5s - loss: 0.0091 - accuracy: 0.99 - ETA: 5s - loss: 0.0095 - accuracy: 0.99 - ETA: 4s - loss: 0.0100 - accuracy: 0.99 - ETA: 4s - loss: 0.0124 - accuracy: 0.99 - ETA: 4s - loss: 0.0141 - accuracy: 0.99 - ETA: 4s - loss: 0.0158 - accuracy: 0.99 - ETA: 4s - loss: 0.0176 - accuracy: 0.99 - ETA: 4s - loss: 0.0227 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0293 - accuracy: 0.98 - ETA: 4s - loss: 0.0284 - accuracy: 0.98 - ETA: 4s - loss: 0.0275 - accuracy: 0.98 - ETA: 4s - loss: 0.0271 - accuracy: 0.98 - ETA: 3s - loss: 0.0273 - accuracy: 0.98 - ETA: 3s - loss: 0.0264 - accuracy: 0.98 - ETA: 3s - loss: 0.0264 - accuracy: 0.98 - ETA: 3s - loss: 0.0262 - accuracy: 0.98 - ETA: 3s - loss: 0.0254 - accuracy: 0.98 - ETA: 3s - loss: 0.0256 - accuracy: 0.98 - ETA: 3s - loss: 0.0252 - accuracy: 0.98 - ETA: 3s - loss: 0.0247 - accuracy: 0.98 - ETA: 3s - loss: 0.0245 - accuracy: 0.98 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0244 - accuracy: 0.99 - ETA: 3s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0238 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0232 - accuracy: 0.99 - ETA: 2s - loss: 0.0228 - accuracy: 0.99 - ETA: 2s - loss: 0.0223 - accuracy: 0.99 - ETA: 2s - loss: 0.0218 - accuracy: 0.99 - ETA: 2s - loss: 0.0216 - accuracy: 0.99 - ETA: 2s - loss: 0.0223 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0217 - accuracy: 0.99 - ETA: 1s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0236 - accuracy: 0.99 - ETA: 1s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0241 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0243 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - 6s 593us/step - loss: 0.0241 - accuracy: 0.9909 - val_loss: 0.0215 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01451\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0128 - accuracy: 1.00 - ETA: 5s - loss: 0.0090 - accuracy: 1.00 - ETA: 5s - loss: 0.0077 - accuracy: 1.00 - ETA: 5s - loss: 0.0066 - accuracy: 1.00 - ETA: 5s - loss: 0.0067 - accuracy: 1.00 - ETA: 5s - loss: 0.0079 - accuracy: 0.99 - ETA: 5s - loss: 0.0083 - accuracy: 0.99 - ETA: 5s - loss: 0.0085 - accuracy: 0.99 - ETA: 5s - loss: 0.0083 - accuracy: 0.99 - ETA: 5s - loss: 0.0101 - accuracy: 0.99 - ETA: 5s - loss: 0.0097 - accuracy: 0.99 - ETA: 5s - loss: 0.0118 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 4s - loss: 0.0186 - accuracy: 0.99 - ETA: 4s - loss: 0.0179 - accuracy: 0.99 - ETA: 4s - loss: 0.0171 - accuracy: 0.99 - ETA: 4s - loss: 0.0171 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0178 - accuracy: 0.99 - ETA: 4s - loss: 0.0173 - accuracy: 0.99 - ETA: 4s - loss: 0.0172 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0164 - accuracy: 0.99 - ETA: 4s - loss: 0.0163 - accuracy: 0.99 - ETA: 3s - loss: 0.0165 - accuracy: 0.99 - ETA: 3s - loss: 0.0182 - accuracy: 0.99 - ETA: 3s - loss: 0.0186 - accuracy: 0.99 - ETA: 3s - loss: 0.0190 - accuracy: 0.99 - ETA: 3s - loss: 0.0185 - accuracy: 0.99 - ETA: 3s - loss: 0.0179 - accuracy: 0.99 - ETA: 3s - loss: 0.0180 - accuracy: 0.99 - ETA: 3s - loss: 0.0187 - accuracy: 0.99 - ETA: 3s - loss: 0.0182 - accuracy: 0.99 - ETA: 3s - loss: 0.0181 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0207 - accuracy: 0.99 - ETA: 2s - loss: 0.0215 - accuracy: 0.99 - ETA: 2s - loss: 0.0217 - accuracy: 0.99 - ETA: 2s - loss: 0.0217 - accuracy: 0.99 - ETA: 2s - loss: 0.0218 - accuracy: 0.99 - ETA: 2s - loss: 0.0216 - accuracy: 0.99 - ETA: 2s - loss: 0.0214 - accuracy: 0.99 - ETA: 2s - loss: 0.0210 - accuracy: 0.99 - ETA: 2s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 1s - loss: 0.0201 - accuracy: 0.99 - ETA: 1s - loss: 0.0210 - accuracy: 0.99 - ETA: 1s - loss: 0.0210 - accuracy: 0.99 - ETA: 1s - loss: 0.0207 - accuracy: 0.99 - ETA: 1s - loss: 0.0204 - accuracy: 0.99 - ETA: 1s - loss: 0.0201 - accuracy: 0.99 - ETA: 1s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0202 - accuracy: 0.99 - ETA: 1s - loss: 0.0201 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0200 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0201 - accuracy: 0.99 - ETA: 0s - loss: 0.0199 - accuracy: 0.99 - ETA: 0s - loss: 0.0200 - accuracy: 0.99 - ETA: 0s - loss: 0.0198 - accuracy: 0.99 - ETA: 0s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0194 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0207 - accuracy: 0.99 - 6s 591us/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.0276 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01451\n",
      "Epoch 00025: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 229us/step\n",
      "[2020-05-18 16:18:44 RAM66.2% 1.05GB] Val Score : [0.03474609951106043, 0.9893859028816223]\n",
      "[2020-05-18 16:18:44 RAM66.2% 1.05GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:18:44 RAM66.2% 1.05GB] Training on Fold : 2\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 20s - loss: 3.6025 - accuracy: 0.510 - ETA: 13s - loss: 5.3786 - accuracy: 0.541 - ETA: 10s - loss: 3.9670 - accuracy: 0.547 - ETA: 9s - loss: 3.2061 - accuracy: 0.551 - ETA: 8s - loss: 2.8058 - accuracy: 0.54 - ETA: 7s - loss: 2.4882 - accuracy: 0.54 - ETA: 7s - loss: 2.2419 - accuracy: 0.54 - ETA: 7s - loss: 2.0667 - accuracy: 0.54 - ETA: 6s - loss: 1.9224 - accuracy: 0.54 - ETA: 6s - loss: 1.8134 - accuracy: 0.53 - ETA: 6s - loss: 1.7146 - accuracy: 0.53 - ETA: 6s - loss: 1.6337 - accuracy: 0.54 - ETA: 5s - loss: 1.5600 - accuracy: 0.54 - ETA: 5s - loss: 1.4970 - accuracy: 0.55 - ETA: 5s - loss: 1.4421 - accuracy: 0.55 - ETA: 5s - loss: 1.3964 - accuracy: 0.56 - ETA: 5s - loss: 1.3520 - accuracy: 0.56 - ETA: 5s - loss: 1.3121 - accuracy: 0.57 - ETA: 5s - loss: 1.2762 - accuracy: 0.57 - ETA: 4s - loss: 1.2480 - accuracy: 0.57 - ETA: 4s - loss: 1.2181 - accuracy: 0.58 - ETA: 4s - loss: 1.1928 - accuracy: 0.58 - ETA: 4s - loss: 1.1693 - accuracy: 0.58 - ETA: 4s - loss: 1.1469 - accuracy: 0.59 - ETA: 4s - loss: 1.1297 - accuracy: 0.59 - ETA: 4s - loss: 1.1123 - accuracy: 0.59 - ETA: 4s - loss: 1.0941 - accuracy: 0.59 - ETA: 4s - loss: 1.0775 - accuracy: 0.59 - ETA: 3s - loss: 1.0633 - accuracy: 0.59 - ETA: 3s - loss: 1.0491 - accuracy: 0.59 - ETA: 3s - loss: 1.0362 - accuracy: 0.60 - ETA: 3s - loss: 1.0241 - accuracy: 0.60 - ETA: 3s - loss: 1.0122 - accuracy: 0.60 - ETA: 3s - loss: 1.0012 - accuracy: 0.60 - ETA: 3s - loss: 0.9900 - accuracy: 0.60 - ETA: 3s - loss: 0.9776 - accuracy: 0.61 - ETA: 3s - loss: 0.9670 - accuracy: 0.61 - ETA: 3s - loss: 0.9578 - accuracy: 0.61 - ETA: 3s - loss: 0.9492 - accuracy: 0.61 - ETA: 2s - loss: 0.9395 - accuracy: 0.61 - ETA: 2s - loss: 0.9315 - accuracy: 0.61 - ETA: 2s - loss: 0.9216 - accuracy: 0.62 - ETA: 2s - loss: 0.9142 - accuracy: 0.62 - ETA: 2s - loss: 0.9087 - accuracy: 0.62 - ETA: 2s - loss: 0.9023 - accuracy: 0.62 - ETA: 2s - loss: 0.8946 - accuracy: 0.62 - ETA: 2s - loss: 0.8857 - accuracy: 0.62 - ETA: 2s - loss: 0.8803 - accuracy: 0.62 - ETA: 2s - loss: 0.8740 - accuracy: 0.63 - ETA: 2s - loss: 0.8665 - accuracy: 0.63 - ETA: 1s - loss: 0.8578 - accuracy: 0.63 - ETA: 1s - loss: 0.8512 - accuracy: 0.63 - ETA: 1s - loss: 0.8448 - accuracy: 0.63 - ETA: 1s - loss: 0.8408 - accuracy: 0.63 - ETA: 1s - loss: 0.8347 - accuracy: 0.64 - ETA: 1s - loss: 0.8283 - accuracy: 0.64 - ETA: 1s - loss: 0.8216 - accuracy: 0.64 - ETA: 1s - loss: 0.8167 - accuracy: 0.64 - ETA: 1s - loss: 0.8124 - accuracy: 0.64 - ETA: 1s - loss: 0.8071 - accuracy: 0.64 - ETA: 1s - loss: 0.7996 - accuracy: 0.65 - ETA: 0s - loss: 0.7963 - accuracy: 0.65 - ETA: 0s - loss: 0.7947 - accuracy: 0.65 - ETA: 0s - loss: 0.7907 - accuracy: 0.65 - ETA: 0s - loss: 0.7864 - accuracy: 0.65 - ETA: 0s - loss: 0.7804 - accuracy: 0.65 - ETA: 0s - loss: 0.7744 - accuracy: 0.66 - ETA: 0s - loss: 0.7686 - accuracy: 0.66 - ETA: 0s - loss: 0.7644 - accuracy: 0.66 - ETA: 0s - loss: 0.7626 - accuracy: 0.66 - ETA: 0s - loss: 0.7583 - accuracy: 0.66 - ETA: 0s - loss: 0.7540 - accuracy: 0.66 - ETA: 0s - loss: 0.7483 - accuracy: 0.66 - 6s 605us/step - loss: 0.7451 - accuracy: 0.6713 - val_loss: 0.3319 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.01451\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.3528 - accuracy: 0.86 - ETA: 5s - loss: 0.3677 - accuracy: 0.84 - ETA: 5s - loss: 0.3634 - accuracy: 0.84 - ETA: 5s - loss: 0.3709 - accuracy: 0.83 - ETA: 5s - loss: 0.3713 - accuracy: 0.83 - ETA: 5s - loss: 0.4170 - accuracy: 0.80 - ETA: 5s - loss: 0.4533 - accuracy: 0.78 - ETA: 5s - loss: 0.4530 - accuracy: 0.78 - ETA: 5s - loss: 0.4417 - accuracy: 0.79 - ETA: 5s - loss: 0.4395 - accuracy: 0.79 - ETA: 5s - loss: 0.4377 - accuracy: 0.78 - ETA: 5s - loss: 0.4429 - accuracy: 0.78 - ETA: 4s - loss: 0.4362 - accuracy: 0.78 - ETA: 4s - loss: 0.4254 - accuracy: 0.79 - ETA: 4s - loss: 0.4216 - accuracy: 0.79 - ETA: 4s - loss: 0.4134 - accuracy: 0.80 - ETA: 4s - loss: 0.4064 - accuracy: 0.80 - ETA: 4s - loss: 0.4015 - accuracy: 0.81 - ETA: 4s - loss: 0.4013 - accuracy: 0.80 - ETA: 4s - loss: 0.3955 - accuracy: 0.81 - ETA: 4s - loss: 0.3950 - accuracy: 0.81 - ETA: 4s - loss: 0.3963 - accuracy: 0.80 - ETA: 4s - loss: 0.3939 - accuracy: 0.81 - ETA: 4s - loss: 0.3907 - accuracy: 0.81 - ETA: 3s - loss: 0.3870 - accuracy: 0.81 - ETA: 3s - loss: 0.3823 - accuracy: 0.81 - ETA: 3s - loss: 0.3790 - accuracy: 0.82 - ETA: 3s - loss: 0.3785 - accuracy: 0.82 - ETA: 3s - loss: 0.3832 - accuracy: 0.81 - ETA: 3s - loss: 0.3799 - accuracy: 0.82 - ETA: 3s - loss: 0.3785 - accuracy: 0.82 - ETA: 3s - loss: 0.3759 - accuracy: 0.82 - ETA: 3s - loss: 0.3707 - accuracy: 0.82 - ETA: 3s - loss: 0.3654 - accuracy: 0.82 - ETA: 3s - loss: 0.3626 - accuracy: 0.83 - ETA: 3s - loss: 0.3632 - accuracy: 0.83 - ETA: 2s - loss: 0.3727 - accuracy: 0.82 - ETA: 2s - loss: 0.3720 - accuracy: 0.82 - ETA: 2s - loss: 0.3697 - accuracy: 0.82 - ETA: 2s - loss: 0.3697 - accuracy: 0.82 - ETA: 2s - loss: 0.3691 - accuracy: 0.82 - ETA: 2s - loss: 0.3660 - accuracy: 0.83 - ETA: 2s - loss: 0.3636 - accuracy: 0.83 - ETA: 2s - loss: 0.3604 - accuracy: 0.83 - ETA: 2s - loss: 0.3595 - accuracy: 0.83 - ETA: 2s - loss: 0.3570 - accuracy: 0.83 - ETA: 2s - loss: 0.3567 - accuracy: 0.83 - ETA: 2s - loss: 0.3597 - accuracy: 0.83 - ETA: 2s - loss: 0.3580 - accuracy: 0.83 - ETA: 1s - loss: 0.3561 - accuracy: 0.83 - ETA: 1s - loss: 0.3531 - accuracy: 0.83 - ETA: 1s - loss: 0.3500 - accuracy: 0.83 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3469 - accuracy: 0.84 - ETA: 1s - loss: 0.3493 - accuracy: 0.83 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3445 - accuracy: 0.84 - ETA: 1s - loss: 0.3413 - accuracy: 0.84 - ETA: 1s - loss: 0.3392 - accuracy: 0.84 - ETA: 1s - loss: 0.3356 - accuracy: 0.84 - ETA: 0s - loss: 0.3331 - accuracy: 0.84 - ETA: 0s - loss: 0.3320 - accuracy: 0.84 - ETA: 0s - loss: 0.3345 - accuracy: 0.84 - ETA: 0s - loss: 0.3377 - accuracy: 0.84 - ETA: 0s - loss: 0.3366 - accuracy: 0.84 - ETA: 0s - loss: 0.3346 - accuracy: 0.84 - ETA: 0s - loss: 0.3333 - accuracy: 0.84 - ETA: 0s - loss: 0.3320 - accuracy: 0.85 - ETA: 0s - loss: 0.3306 - accuracy: 0.85 - ETA: 0s - loss: 0.3293 - accuracy: 0.85 - ETA: 0s - loss: 0.3293 - accuracy: 0.85 - ETA: 0s - loss: 0.3270 - accuracy: 0.85 - 6s 585us/step - loss: 0.3256 - accuracy: 0.8534 - val_loss: 0.2578 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.01451\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.4199 - accuracy: 0.80 - ETA: 5s - loss: 0.4310 - accuracy: 0.78 - ETA: 5s - loss: 0.3491 - accuracy: 0.84 - ETA: 5s - loss: 0.3081 - accuracy: 0.86 - ETA: 5s - loss: 0.2860 - accuracy: 0.87 - ETA: 5s - loss: 0.2709 - accuracy: 0.88 - ETA: 5s - loss: 0.2593 - accuracy: 0.88 - ETA: 5s - loss: 0.2538 - accuracy: 0.89 - ETA: 5s - loss: 0.2446 - accuracy: 0.89 - ETA: 5s - loss: 0.2458 - accuracy: 0.89 - ETA: 5s - loss: 0.2486 - accuracy: 0.89 - ETA: 4s - loss: 0.2490 - accuracy: 0.89 - ETA: 4s - loss: 0.2442 - accuracy: 0.89 - ETA: 4s - loss: 0.2410 - accuracy: 0.89 - ETA: 4s - loss: 0.2391 - accuracy: 0.89 - ETA: 4s - loss: 0.2394 - accuracy: 0.89 - ETA: 4s - loss: 0.2399 - accuracy: 0.89 - ETA: 4s - loss: 0.2387 - accuracy: 0.89 - ETA: 4s - loss: 0.2327 - accuracy: 0.90 - ETA: 4s - loss: 0.2332 - accuracy: 0.90 - ETA: 4s - loss: 0.2478 - accuracy: 0.89 - ETA: 4s - loss: 0.2550 - accuracy: 0.88 - ETA: 4s - loss: 0.2586 - accuracy: 0.88 - ETA: 4s - loss: 0.2562 - accuracy: 0.88 - ETA: 3s - loss: 0.2555 - accuracy: 0.88 - ETA: 3s - loss: 0.2526 - accuracy: 0.88 - ETA: 3s - loss: 0.2488 - accuracy: 0.89 - ETA: 3s - loss: 0.2455 - accuracy: 0.89 - ETA: 3s - loss: 0.2427 - accuracy: 0.89 - ETA: 3s - loss: 0.2408 - accuracy: 0.89 - ETA: 3s - loss: 0.2375 - accuracy: 0.89 - ETA: 3s - loss: 0.2349 - accuracy: 0.89 - ETA: 3s - loss: 0.2331 - accuracy: 0.89 - ETA: 3s - loss: 0.2319 - accuracy: 0.90 - ETA: 3s - loss: 0.2317 - accuracy: 0.90 - ETA: 3s - loss: 0.2307 - accuracy: 0.90 - ETA: 3s - loss: 0.2308 - accuracy: 0.90 - ETA: 2s - loss: 0.2319 - accuracy: 0.90 - ETA: 2s - loss: 0.2380 - accuracy: 0.89 - ETA: 2s - loss: 0.2436 - accuracy: 0.89 - ETA: 2s - loss: 0.2484 - accuracy: 0.89 - ETA: 2s - loss: 0.2492 - accuracy: 0.89 - ETA: 2s - loss: 0.2478 - accuracy: 0.89 - ETA: 2s - loss: 0.2466 - accuracy: 0.89 - ETA: 2s - loss: 0.2443 - accuracy: 0.89 - ETA: 2s - loss: 0.2439 - accuracy: 0.89 - ETA: 2s - loss: 0.2428 - accuracy: 0.89 - ETA: 2s - loss: 0.2416 - accuracy: 0.89 - ETA: 2s - loss: 0.2398 - accuracy: 0.89 - ETA: 1s - loss: 0.2379 - accuracy: 0.89 - ETA: 1s - loss: 0.2357 - accuracy: 0.89 - ETA: 1s - loss: 0.2369 - accuracy: 0.89 - ETA: 1s - loss: 0.2355 - accuracy: 0.89 - ETA: 1s - loss: 0.2347 - accuracy: 0.90 - ETA: 1s - loss: 0.2322 - accuracy: 0.90 - ETA: 1s - loss: 0.2316 - accuracy: 0.90 - ETA: 1s - loss: 0.2301 - accuracy: 0.90 - ETA: 1s - loss: 0.2304 - accuracy: 0.90 - ETA: 1s - loss: 0.2290 - accuracy: 0.90 - ETA: 1s - loss: 0.2286 - accuracy: 0.90 - ETA: 1s - loss: 0.2263 - accuracy: 0.90 - ETA: 0s - loss: 0.2269 - accuracy: 0.90 - ETA: 0s - loss: 0.2278 - accuracy: 0.90 - ETA: 0s - loss: 0.2290 - accuracy: 0.90 - ETA: 0s - loss: 0.2279 - accuracy: 0.90 - ETA: 0s - loss: 0.2276 - accuracy: 0.90 - ETA: 0s - loss: 0.2268 - accuracy: 0.90 - ETA: 0s - loss: 0.2252 - accuracy: 0.90 - ETA: 0s - loss: 0.2242 - accuracy: 0.90 - ETA: 0s - loss: 0.2232 - accuracy: 0.90 - ETA: 0s - loss: 0.2219 - accuracy: 0.90 - ETA: 0s - loss: 0.2207 - accuracy: 0.90 - ETA: 0s - loss: 0.2196 - accuracy: 0.90 - 6s 590us/step - loss: 0.2191 - accuracy: 0.9071 - val_loss: 0.1187 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01451\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0900 - accuracy: 0.97 - ETA: 5s - loss: 0.1255 - accuracy: 0.95 - ETA: 5s - loss: 0.1200 - accuracy: 0.95 - ETA: 5s - loss: 0.1235 - accuracy: 0.95 - ETA: 5s - loss: 0.1224 - accuracy: 0.95 - ETA: 5s - loss: 0.1298 - accuracy: 0.94 - ETA: 5s - loss: 0.1320 - accuracy: 0.94 - ETA: 5s - loss: 0.1474 - accuracy: 0.93 - ETA: 5s - loss: 0.1854 - accuracy: 0.92 - ETA: 5s - loss: 0.1920 - accuracy: 0.91 - ETA: 5s - loss: 0.1986 - accuracy: 0.91 - ETA: 5s - loss: 0.2035 - accuracy: 0.91 - ETA: 4s - loss: 0.2014 - accuracy: 0.91 - ETA: 4s - loss: 0.2000 - accuracy: 0.91 - ETA: 4s - loss: 0.1972 - accuracy: 0.91 - ETA: 4s - loss: 0.1943 - accuracy: 0.91 - ETA: 4s - loss: 0.1922 - accuracy: 0.91 - ETA: 4s - loss: 0.1900 - accuracy: 0.91 - ETA: 4s - loss: 0.1881 - accuracy: 0.92 - ETA: 4s - loss: 0.1845 - accuracy: 0.92 - ETA: 4s - loss: 0.1827 - accuracy: 0.92 - ETA: 4s - loss: 0.1845 - accuracy: 0.92 - ETA: 4s - loss: 0.1907 - accuracy: 0.92 - ETA: 4s - loss: 0.1980 - accuracy: 0.91 - ETA: 4s - loss: 0.2008 - accuracy: 0.91 - ETA: 3s - loss: 0.2009 - accuracy: 0.91 - ETA: 3s - loss: 0.2028 - accuracy: 0.91 - ETA: 3s - loss: 0.2018 - accuracy: 0.91 - ETA: 3s - loss: 0.1987 - accuracy: 0.91 - ETA: 3s - loss: 0.1963 - accuracy: 0.91 - ETA: 3s - loss: 0.1947 - accuracy: 0.91 - ETA: 3s - loss: 0.1917 - accuracy: 0.91 - ETA: 3s - loss: 0.1889 - accuracy: 0.92 - ETA: 3s - loss: 0.1885 - accuracy: 0.92 - ETA: 3s - loss: 0.1885 - accuracy: 0.92 - ETA: 3s - loss: 0.1888 - accuracy: 0.91 - ETA: 3s - loss: 0.1882 - accuracy: 0.92 - ETA: 2s - loss: 0.1871 - accuracy: 0.92 - ETA: 2s - loss: 0.1851 - accuracy: 0.92 - ETA: 2s - loss: 0.1827 - accuracy: 0.92 - ETA: 2s - loss: 0.1804 - accuracy: 0.92 - ETA: 2s - loss: 0.1782 - accuracy: 0.92 - ETA: 2s - loss: 0.1763 - accuracy: 0.92 - ETA: 2s - loss: 0.1778 - accuracy: 0.92 - ETA: 2s - loss: 0.1791 - accuracy: 0.92 - ETA: 2s - loss: 0.1781 - accuracy: 0.92 - ETA: 2s - loss: 0.1781 - accuracy: 0.92 - ETA: 2s - loss: 0.1801 - accuracy: 0.92 - ETA: 2s - loss: 0.1822 - accuracy: 0.92 - ETA: 1s - loss: 0.1834 - accuracy: 0.92 - ETA: 1s - loss: 0.1829 - accuracy: 0.92 - ETA: 1s - loss: 0.1824 - accuracy: 0.92 - ETA: 1s - loss: 0.1811 - accuracy: 0.92 - ETA: 1s - loss: 0.1810 - accuracy: 0.92 - ETA: 1s - loss: 0.1803 - accuracy: 0.92 - ETA: 1s - loss: 0.1791 - accuracy: 0.92 - ETA: 1s - loss: 0.1786 - accuracy: 0.92 - ETA: 1s - loss: 0.1778 - accuracy: 0.92 - ETA: 1s - loss: 0.1774 - accuracy: 0.92 - ETA: 1s - loss: 0.1772 - accuracy: 0.92 - ETA: 1s - loss: 0.1776 - accuracy: 0.92 - ETA: 0s - loss: 0.1776 - accuracy: 0.92 - ETA: 0s - loss: 0.1762 - accuracy: 0.92 - ETA: 0s - loss: 0.1751 - accuracy: 0.92 - ETA: 0s - loss: 0.1739 - accuracy: 0.92 - ETA: 0s - loss: 0.1728 - accuracy: 0.92 - ETA: 0s - loss: 0.1720 - accuracy: 0.92 - ETA: 0s - loss: 0.1715 - accuracy: 0.92 - ETA: 0s - loss: 0.1707 - accuracy: 0.92 - ETA: 0s - loss: 0.1703 - accuracy: 0.92 - ETA: 0s - loss: 0.1751 - accuracy: 0.92 - ETA: 0s - loss: 0.1803 - accuracy: 0.92 - ETA: 0s - loss: 0.1823 - accuracy: 0.92 - 6s 588us/step - loss: 0.1815 - accuracy: 0.9234 - val_loss: 0.1011 - val_accuracy: 0.9621\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01451\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1069 - accuracy: 0.95 - ETA: 5s - loss: 0.1266 - accuracy: 0.94 - ETA: 5s - loss: 0.1180 - accuracy: 0.95 - ETA: 5s - loss: 0.1228 - accuracy: 0.94 - ETA: 5s - loss: 0.1295 - accuracy: 0.94 - ETA: 5s - loss: 0.1267 - accuracy: 0.94 - ETA: 5s - loss: 0.1247 - accuracy: 0.94 - ETA: 5s - loss: 0.1204 - accuracy: 0.94 - ETA: 5s - loss: 0.1160 - accuracy: 0.94 - ETA: 5s - loss: 0.1104 - accuracy: 0.95 - ETA: 5s - loss: 0.1100 - accuracy: 0.95 - ETA: 5s - loss: 0.1059 - accuracy: 0.95 - ETA: 4s - loss: 0.1066 - accuracy: 0.95 - ETA: 4s - loss: 0.1052 - accuracy: 0.95 - ETA: 4s - loss: 0.1045 - accuracy: 0.95 - ETA: 4s - loss: 0.1067 - accuracy: 0.95 - ETA: 4s - loss: 0.1150 - accuracy: 0.95 - ETA: 4s - loss: 0.1286 - accuracy: 0.94 - ETA: 4s - loss: 0.1494 - accuracy: 0.93 - ETA: 4s - loss: 0.1481 - accuracy: 0.93 - ETA: 4s - loss: 0.1485 - accuracy: 0.93 - ETA: 4s - loss: 0.1458 - accuracy: 0.94 - ETA: 4s - loss: 0.1455 - accuracy: 0.94 - ETA: 4s - loss: 0.1428 - accuracy: 0.94 - ETA: 4s - loss: 0.1433 - accuracy: 0.94 - ETA: 3s - loss: 0.1406 - accuracy: 0.94 - ETA: 3s - loss: 0.1436 - accuracy: 0.94 - ETA: 3s - loss: 0.1473 - accuracy: 0.94 - ETA: 3s - loss: 0.1454 - accuracy: 0.94 - ETA: 3s - loss: 0.1451 - accuracy: 0.94 - ETA: 3s - loss: 0.1420 - accuracy: 0.94 - ETA: 3s - loss: 0.1392 - accuracy: 0.94 - ETA: 3s - loss: 0.1367 - accuracy: 0.94 - ETA: 3s - loss: 0.1344 - accuracy: 0.94 - ETA: 3s - loss: 0.1328 - accuracy: 0.94 - ETA: 3s - loss: 0.1314 - accuracy: 0.94 - ETA: 3s - loss: 0.1296 - accuracy: 0.94 - ETA: 2s - loss: 0.1286 - accuracy: 0.94 - ETA: 2s - loss: 0.1277 - accuracy: 0.94 - ETA: 2s - loss: 0.1266 - accuracy: 0.94 - ETA: 2s - loss: 0.1269 - accuracy: 0.94 - ETA: 2s - loss: 0.1295 - accuracy: 0.94 - ETA: 2s - loss: 0.1305 - accuracy: 0.94 - ETA: 2s - loss: 0.1301 - accuracy: 0.94 - ETA: 2s - loss: 0.1297 - accuracy: 0.94 - ETA: 2s - loss: 0.1284 - accuracy: 0.94 - ETA: 2s - loss: 0.1283 - accuracy: 0.94 - ETA: 2s - loss: 0.1271 - accuracy: 0.94 - ETA: 2s - loss: 0.1259 - accuracy: 0.94 - ETA: 1s - loss: 0.1252 - accuracy: 0.94 - ETA: 1s - loss: 0.1266 - accuracy: 0.94 - ETA: 1s - loss: 0.1295 - accuracy: 0.94 - ETA: 1s - loss: 0.1298 - accuracy: 0.94 - ETA: 1s - loss: 0.1291 - accuracy: 0.94 - ETA: 1s - loss: 0.1286 - accuracy: 0.94 - ETA: 1s - loss: 0.1275 - accuracy: 0.94 - ETA: 1s - loss: 0.1260 - accuracy: 0.94 - ETA: 1s - loss: 0.1254 - accuracy: 0.94 - ETA: 1s - loss: 0.1253 - accuracy: 0.94 - ETA: 1s - loss: 0.1252 - accuracy: 0.94 - ETA: 1s - loss: 0.1257 - accuracy: 0.94 - ETA: 0s - loss: 0.1263 - accuracy: 0.94 - ETA: 0s - loss: 0.1274 - accuracy: 0.94 - ETA: 0s - loss: 0.1277 - accuracy: 0.94 - ETA: 0s - loss: 0.1272 - accuracy: 0.94 - ETA: 0s - loss: 0.1259 - accuracy: 0.94 - ETA: 0s - loss: 0.1263 - accuracy: 0.94 - ETA: 0s - loss: 0.1263 - accuracy: 0.94 - ETA: 0s - loss: 0.1267 - accuracy: 0.94 - ETA: 0s - loss: 0.1264 - accuracy: 0.94 - ETA: 0s - loss: 0.1256 - accuracy: 0.94 - ETA: 0s - loss: 0.1255 - accuracy: 0.94 - ETA: 0s - loss: 0.1254 - accuracy: 0.94 - 6s 595us/step - loss: 0.1257 - accuracy: 0.9494 - val_loss: 0.0689 - val_accuracy: 0.9789\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01451\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1129 - accuracy: 0.95 - ETA: 5s - loss: 0.1092 - accuracy: 0.95 - ETA: 5s - loss: 0.0934 - accuracy: 0.96 - ETA: 5s - loss: 0.0926 - accuracy: 0.96 - ETA: 5s - loss: 0.0795 - accuracy: 0.96 - ETA: 5s - loss: 0.0762 - accuracy: 0.97 - ETA: 5s - loss: 0.0772 - accuracy: 0.97 - ETA: 5s - loss: 0.0738 - accuracy: 0.97 - ETA: 5s - loss: 0.0778 - accuracy: 0.97 - ETA: 5s - loss: 0.0810 - accuracy: 0.96 - ETA: 5s - loss: 0.0899 - accuracy: 0.96 - ETA: 5s - loss: 0.0962 - accuracy: 0.96 - ETA: 4s - loss: 0.1073 - accuracy: 0.95 - ETA: 4s - loss: 0.1113 - accuracy: 0.95 - ETA: 4s - loss: 0.1095 - accuracy: 0.95 - ETA: 4s - loss: 0.1064 - accuracy: 0.95 - ETA: 4s - loss: 0.1091 - accuracy: 0.95 - ETA: 4s - loss: 0.1074 - accuracy: 0.95 - ETA: 4s - loss: 0.1074 - accuracy: 0.95 - ETA: 4s - loss: 0.1055 - accuracy: 0.95 - ETA: 4s - loss: 0.1027 - accuracy: 0.95 - ETA: 4s - loss: 0.0999 - accuracy: 0.96 - ETA: 4s - loss: 0.0993 - accuracy: 0.96 - ETA: 4s - loss: 0.0999 - accuracy: 0.96 - ETA: 3s - loss: 0.1003 - accuracy: 0.96 - ETA: 3s - loss: 0.0995 - accuracy: 0.96 - ETA: 3s - loss: 0.0965 - accuracy: 0.96 - ETA: 3s - loss: 0.0958 - accuracy: 0.96 - ETA: 3s - loss: 0.0942 - accuracy: 0.96 - ETA: 3s - loss: 0.0931 - accuracy: 0.96 - ETA: 3s - loss: 0.0956 - accuracy: 0.96 - ETA: 3s - loss: 0.1007 - accuracy: 0.96 - ETA: 3s - loss: 0.1101 - accuracy: 0.95 - ETA: 3s - loss: 0.1122 - accuracy: 0.95 - ETA: 3s - loss: 0.1114 - accuracy: 0.95 - ETA: 3s - loss: 0.1108 - accuracy: 0.95 - ETA: 3s - loss: 0.1105 - accuracy: 0.95 - ETA: 2s - loss: 0.1099 - accuracy: 0.95 - ETA: 2s - loss: 0.1092 - accuracy: 0.95 - ETA: 2s - loss: 0.1086 - accuracy: 0.95 - ETA: 2s - loss: 0.1079 - accuracy: 0.95 - ETA: 2s - loss: 0.1062 - accuracy: 0.95 - ETA: 2s - loss: 0.1047 - accuracy: 0.96 - ETA: 2s - loss: 0.1040 - accuracy: 0.96 - ETA: 2s - loss: 0.1035 - accuracy: 0.96 - ETA: 2s - loss: 0.1023 - accuracy: 0.96 - ETA: 2s - loss: 0.1018 - accuracy: 0.96 - ETA: 2s - loss: 0.1018 - accuracy: 0.96 - ETA: 2s - loss: 0.1026 - accuracy: 0.96 - ETA: 1s - loss: 0.1032 - accuracy: 0.96 - ETA: 1s - loss: 0.1030 - accuracy: 0.96 - ETA: 1s - loss: 0.1027 - accuracy: 0.96 - ETA: 1s - loss: 0.1018 - accuracy: 0.96 - ETA: 1s - loss: 0.1017 - accuracy: 0.96 - ETA: 1s - loss: 0.1020 - accuracy: 0.96 - ETA: 1s - loss: 0.1012 - accuracy: 0.96 - ETA: 1s - loss: 0.1009 - accuracy: 0.96 - ETA: 1s - loss: 0.1007 - accuracy: 0.96 - ETA: 1s - loss: 0.0998 - accuracy: 0.96 - ETA: 1s - loss: 0.0987 - accuracy: 0.96 - ETA: 1s - loss: 0.0990 - accuracy: 0.96 - ETA: 0s - loss: 0.1014 - accuracy: 0.96 - ETA: 0s - loss: 0.1065 - accuracy: 0.95 - ETA: 0s - loss: 0.1167 - accuracy: 0.95 - ETA: 0s - loss: 0.1196 - accuracy: 0.95 - ETA: 0s - loss: 0.1183 - accuracy: 0.95 - ETA: 0s - loss: 0.1176 - accuracy: 0.95 - ETA: 0s - loss: 0.1169 - accuracy: 0.95 - ETA: 0s - loss: 0.1161 - accuracy: 0.95 - ETA: 0s - loss: 0.1152 - accuracy: 0.95 - ETA: 0s - loss: 0.1146 - accuracy: 0.95 - ETA: 0s - loss: 0.1140 - accuracy: 0.95 - ETA: 0s - loss: 0.1133 - accuracy: 0.95 - 6s 585us/step - loss: 0.1127 - accuracy: 0.9578 - val_loss: 0.0535 - val_accuracy: 0.9773\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01451\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0580 - accuracy: 0.97 - ETA: 5s - loss: 0.0584 - accuracy: 0.98 - ETA: 5s - loss: 0.0530 - accuracy: 0.98 - ETA: 5s - loss: 0.0595 - accuracy: 0.97 - ETA: 5s - loss: 0.0573 - accuracy: 0.97 - ETA: 5s - loss: 0.0602 - accuracy: 0.97 - ETA: 5s - loss: 0.0594 - accuracy: 0.97 - ETA: 5s - loss: 0.0574 - accuracy: 0.97 - ETA: 5s - loss: 0.0578 - accuracy: 0.97 - ETA: 5s - loss: 0.0631 - accuracy: 0.97 - ETA: 5s - loss: 0.0674 - accuracy: 0.97 - ETA: 5s - loss: 0.0658 - accuracy: 0.97 - ETA: 5s - loss: 0.0669 - accuracy: 0.97 - ETA: 4s - loss: 0.0656 - accuracy: 0.97 - ETA: 4s - loss: 0.0658 - accuracy: 0.97 - ETA: 4s - loss: 0.0657 - accuracy: 0.97 - ETA: 4s - loss: 0.0651 - accuracy: 0.97 - ETA: 4s - loss: 0.0655 - accuracy: 0.97 - ETA: 4s - loss: 0.0672 - accuracy: 0.97 - ETA: 4s - loss: 0.0691 - accuracy: 0.97 - ETA: 4s - loss: 0.0682 - accuracy: 0.97 - ETA: 4s - loss: 0.0686 - accuracy: 0.97 - ETA: 4s - loss: 0.0677 - accuracy: 0.97 - ETA: 4s - loss: 0.0662 - accuracy: 0.97 - ETA: 4s - loss: 0.0664 - accuracy: 0.97 - ETA: 3s - loss: 0.0716 - accuracy: 0.97 - ETA: 3s - loss: 0.0798 - accuracy: 0.97 - ETA: 3s - loss: 0.0796 - accuracy: 0.97 - ETA: 3s - loss: 0.0797 - accuracy: 0.97 - ETA: 3s - loss: 0.0793 - accuracy: 0.97 - ETA: 3s - loss: 0.0812 - accuracy: 0.97 - ETA: 3s - loss: 0.0798 - accuracy: 0.97 - ETA: 3s - loss: 0.0794 - accuracy: 0.97 - ETA: 3s - loss: 0.0817 - accuracy: 0.97 - ETA: 3s - loss: 0.0820 - accuracy: 0.97 - ETA: 3s - loss: 0.0814 - accuracy: 0.97 - ETA: 3s - loss: 0.0812 - accuracy: 0.97 - ETA: 2s - loss: 0.0825 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0804 - accuracy: 0.97 - ETA: 2s - loss: 0.0800 - accuracy: 0.97 - ETA: 2s - loss: 0.0794 - accuracy: 0.97 - ETA: 2s - loss: 0.0792 - accuracy: 0.97 - ETA: 2s - loss: 0.0792 - accuracy: 0.97 - ETA: 2s - loss: 0.0798 - accuracy: 0.97 - ETA: 2s - loss: 0.0824 - accuracy: 0.97 - ETA: 2s - loss: 0.0857 - accuracy: 0.96 - ETA: 2s - loss: 0.0854 - accuracy: 0.97 - ETA: 2s - loss: 0.0849 - accuracy: 0.97 - ETA: 1s - loss: 0.0851 - accuracy: 0.97 - ETA: 1s - loss: 0.0846 - accuracy: 0.97 - ETA: 1s - loss: 0.0841 - accuracy: 0.97 - ETA: 1s - loss: 0.0833 - accuracy: 0.97 - ETA: 1s - loss: 0.0826 - accuracy: 0.97 - ETA: 1s - loss: 0.0821 - accuracy: 0.97 - ETA: 1s - loss: 0.0812 - accuracy: 0.97 - ETA: 1s - loss: 0.0804 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 1s - loss: 0.0794 - accuracy: 0.97 - ETA: 1s - loss: 0.0783 - accuracy: 0.97 - ETA: 1s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0803 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0797 - accuracy: 0.97 - ETA: 0s - loss: 0.0793 - accuracy: 0.97 - ETA: 0s - loss: 0.0788 - accuracy: 0.97 - ETA: 0s - loss: 0.0783 - accuracy: 0.97 - ETA: 0s - loss: 0.0794 - accuracy: 0.97 - ETA: 0s - loss: 0.0805 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0800 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - 6s 596us/step - loss: 0.0808 - accuracy: 0.9715 - val_loss: 0.1495 - val_accuracy: 0.9469\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01451\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1362 - accuracy: 0.95 - ETA: 5s - loss: 0.1292 - accuracy: 0.96 - ETA: 5s - loss: 0.1085 - accuracy: 0.96 - ETA: 5s - loss: 0.0892 - accuracy: 0.97 - ETA: 5s - loss: 0.0794 - accuracy: 0.97 - ETA: 5s - loss: 0.0774 - accuracy: 0.97 - ETA: 5s - loss: 0.0705 - accuracy: 0.97 - ETA: 5s - loss: 0.0689 - accuracy: 0.97 - ETA: 5s - loss: 0.0813 - accuracy: 0.97 - ETA: 5s - loss: 0.0792 - accuracy: 0.97 - ETA: 5s - loss: 0.0757 - accuracy: 0.97 - ETA: 5s - loss: 0.0758 - accuracy: 0.97 - ETA: 4s - loss: 0.0730 - accuracy: 0.97 - ETA: 4s - loss: 0.0734 - accuracy: 0.97 - ETA: 4s - loss: 0.0703 - accuracy: 0.97 - ETA: 4s - loss: 0.0749 - accuracy: 0.97 - ETA: 4s - loss: 0.1016 - accuracy: 0.96 - ETA: 4s - loss: 0.1103 - accuracy: 0.96 - ETA: 4s - loss: 0.1128 - accuracy: 0.96 - ETA: 4s - loss: 0.1125 - accuracy: 0.96 - ETA: 4s - loss: 0.1084 - accuracy: 0.96 - ETA: 4s - loss: 0.1061 - accuracy: 0.96 - ETA: 4s - loss: 0.1048 - accuracy: 0.96 - ETA: 4s - loss: 0.1027 - accuracy: 0.96 - ETA: 3s - loss: 0.1008 - accuracy: 0.96 - ETA: 3s - loss: 0.0987 - accuracy: 0.96 - ETA: 3s - loss: 0.0959 - accuracy: 0.96 - ETA: 3s - loss: 0.0935 - accuracy: 0.97 - ETA: 3s - loss: 0.0913 - accuracy: 0.97 - ETA: 3s - loss: 0.0896 - accuracy: 0.97 - ETA: 3s - loss: 0.0885 - accuracy: 0.97 - ETA: 3s - loss: 0.0884 - accuracy: 0.97 - ETA: 3s - loss: 0.0878 - accuracy: 0.97 - ETA: 3s - loss: 0.0882 - accuracy: 0.97 - ETA: 3s - loss: 0.0872 - accuracy: 0.97 - ETA: 3s - loss: 0.0860 - accuracy: 0.97 - ETA: 2s - loss: 0.0845 - accuracy: 0.97 - ETA: 2s - loss: 0.0843 - accuracy: 0.97 - ETA: 2s - loss: 0.0832 - accuracy: 0.97 - ETA: 2s - loss: 0.0819 - accuracy: 0.97 - ETA: 2s - loss: 0.0810 - accuracy: 0.97 - ETA: 2s - loss: 0.0802 - accuracy: 0.97 - ETA: 2s - loss: 0.0796 - accuracy: 0.97 - ETA: 2s - loss: 0.0784 - accuracy: 0.97 - ETA: 2s - loss: 0.0778 - accuracy: 0.97 - ETA: 2s - loss: 0.0771 - accuracy: 0.97 - ETA: 2s - loss: 0.0763 - accuracy: 0.97 - ETA: 2s - loss: 0.0772 - accuracy: 0.97 - ETA: 2s - loss: 0.0783 - accuracy: 0.97 - ETA: 1s - loss: 0.0777 - accuracy: 0.97 - ETA: 1s - loss: 0.0771 - accuracy: 0.97 - ETA: 1s - loss: 0.0766 - accuracy: 0.97 - ETA: 1s - loss: 0.0765 - accuracy: 0.97 - ETA: 1s - loss: 0.0758 - accuracy: 0.97 - ETA: 1s - loss: 0.0752 - accuracy: 0.97 - ETA: 1s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0740 - accuracy: 0.97 - ETA: 1s - loss: 0.0759 - accuracy: 0.97 - ETA: 1s - loss: 0.0836 - accuracy: 0.97 - ETA: 1s - loss: 0.0846 - accuracy: 0.97 - ETA: 1s - loss: 0.0838 - accuracy: 0.97 - ETA: 0s - loss: 0.0831 - accuracy: 0.97 - ETA: 0s - loss: 0.0824 - accuracy: 0.97 - ETA: 0s - loss: 0.0824 - accuracy: 0.97 - ETA: 0s - loss: 0.0829 - accuracy: 0.97 - ETA: 0s - loss: 0.0826 - accuracy: 0.97 - ETA: 0s - loss: 0.0819 - accuracy: 0.97 - ETA: 0s - loss: 0.0810 - accuracy: 0.97 - ETA: 0s - loss: 0.0808 - accuracy: 0.97 - ETA: 0s - loss: 0.0800 - accuracy: 0.97 - ETA: 0s - loss: 0.0797 - accuracy: 0.97 - ETA: 0s - loss: 0.0790 - accuracy: 0.97 - ETA: 0s - loss: 0.0786 - accuracy: 0.97 - 6s 582us/step - loss: 0.0782 - accuracy: 0.9735 - val_loss: 0.0530 - val_accuracy: 0.9756\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01451\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0810 - accuracy: 0.96 - ETA: 5s - loss: 0.0882 - accuracy: 0.96 - ETA: 5s - loss: 0.0718 - accuracy: 0.97 - ETA: 5s - loss: 0.0647 - accuracy: 0.97 - ETA: 5s - loss: 0.0607 - accuracy: 0.97 - ETA: 5s - loss: 0.0543 - accuracy: 0.97 - ETA: 5s - loss: 0.0488 - accuracy: 0.98 - ETA: 5s - loss: 0.0460 - accuracy: 0.98 - ETA: 5s - loss: 0.0427 - accuracy: 0.98 - ETA: 5s - loss: 0.0478 - accuracy: 0.98 - ETA: 5s - loss: 0.0550 - accuracy: 0.97 - ETA: 5s - loss: 0.0625 - accuracy: 0.97 - ETA: 5s - loss: 0.0661 - accuracy: 0.97 - ETA: 4s - loss: 0.0663 - accuracy: 0.97 - ETA: 4s - loss: 0.0722 - accuracy: 0.97 - ETA: 4s - loss: 0.0771 - accuracy: 0.96 - ETA: 4s - loss: 0.0778 - accuracy: 0.97 - ETA: 4s - loss: 0.0754 - accuracy: 0.97 - ETA: 4s - loss: 0.0732 - accuracy: 0.97 - ETA: 4s - loss: 0.0715 - accuracy: 0.97 - ETA: 4s - loss: 0.0695 - accuracy: 0.97 - ETA: 4s - loss: 0.0687 - accuracy: 0.97 - ETA: 4s - loss: 0.0676 - accuracy: 0.97 - ETA: 4s - loss: 0.0663 - accuracy: 0.97 - ETA: 4s - loss: 0.0666 - accuracy: 0.97 - ETA: 3s - loss: 0.0686 - accuracy: 0.97 - ETA: 3s - loss: 0.0687 - accuracy: 0.97 - ETA: 3s - loss: 0.0682 - accuracy: 0.97 - ETA: 3s - loss: 0.0683 - accuracy: 0.97 - ETA: 3s - loss: 0.0678 - accuracy: 0.97 - ETA: 3s - loss: 0.0675 - accuracy: 0.97 - ETA: 3s - loss: 0.0701 - accuracy: 0.97 - ETA: 3s - loss: 0.0764 - accuracy: 0.97 - ETA: 3s - loss: 0.0783 - accuracy: 0.97 - ETA: 3s - loss: 0.0778 - accuracy: 0.97 - ETA: 3s - loss: 0.0760 - accuracy: 0.97 - ETA: 3s - loss: 0.0746 - accuracy: 0.97 - ETA: 2s - loss: 0.0735 - accuracy: 0.97 - ETA: 2s - loss: 0.0729 - accuracy: 0.97 - ETA: 2s - loss: 0.0716 - accuracy: 0.97 - ETA: 2s - loss: 0.0708 - accuracy: 0.97 - ETA: 2s - loss: 0.0698 - accuracy: 0.97 - ETA: 2s - loss: 0.0692 - accuracy: 0.97 - ETA: 2s - loss: 0.0692 - accuracy: 0.97 - ETA: 2s - loss: 0.0685 - accuracy: 0.97 - ETA: 2s - loss: 0.0673 - accuracy: 0.97 - ETA: 2s - loss: 0.0668 - accuracy: 0.97 - ETA: 2s - loss: 0.0663 - accuracy: 0.97 - ETA: 2s - loss: 0.0654 - accuracy: 0.97 - ETA: 1s - loss: 0.0644 - accuracy: 0.97 - ETA: 1s - loss: 0.0638 - accuracy: 0.97 - ETA: 1s - loss: 0.0637 - accuracy: 0.97 - ETA: 1s - loss: 0.0628 - accuracy: 0.97 - ETA: 1s - loss: 0.0621 - accuracy: 0.97 - ETA: 1s - loss: 0.0615 - accuracy: 0.97 - ETA: 1s - loss: 0.0615 - accuracy: 0.97 - ETA: 1s - loss: 0.0615 - accuracy: 0.97 - ETA: 1s - loss: 0.0666 - accuracy: 0.97 - ETA: 1s - loss: 0.0688 - accuracy: 0.97 - ETA: 1s - loss: 0.0692 - accuracy: 0.97 - ETA: 1s - loss: 0.0723 - accuracy: 0.97 - ETA: 0s - loss: 0.0745 - accuracy: 0.97 - ETA: 0s - loss: 0.0752 - accuracy: 0.97 - ETA: 0s - loss: 0.0744 - accuracy: 0.97 - ETA: 0s - loss: 0.0743 - accuracy: 0.97 - ETA: 0s - loss: 0.0744 - accuracy: 0.97 - ETA: 0s - loss: 0.0736 - accuracy: 0.97 - ETA: 0s - loss: 0.0731 - accuracy: 0.97 - ETA: 0s - loss: 0.0731 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - 6s 587us/step - loss: 0.0725 - accuracy: 0.9733 - val_loss: 0.0361 - val_accuracy: 0.9857\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01451\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0358 - accuracy: 0.98 - ETA: 6s - loss: 0.0356 - accuracy: 0.98 - ETA: 5s - loss: 0.0356 - accuracy: 0.98 - ETA: 5s - loss: 0.0410 - accuracy: 0.98 - ETA: 5s - loss: 0.0487 - accuracy: 0.98 - ETA: 5s - loss: 0.0499 - accuracy: 0.98 - ETA: 5s - loss: 0.0598 - accuracy: 0.97 - ETA: 5s - loss: 0.0615 - accuracy: 0.97 - ETA: 5s - loss: 0.0578 - accuracy: 0.97 - ETA: 5s - loss: 0.0591 - accuracy: 0.97 - ETA: 5s - loss: 0.0590 - accuracy: 0.97 - ETA: 5s - loss: 0.0668 - accuracy: 0.97 - ETA: 4s - loss: 0.0761 - accuracy: 0.97 - ETA: 4s - loss: 0.0718 - accuracy: 0.97 - ETA: 4s - loss: 0.0708 - accuracy: 0.97 - ETA: 4s - loss: 0.0682 - accuracy: 0.97 - ETA: 4s - loss: 0.0659 - accuracy: 0.97 - ETA: 4s - loss: 0.0631 - accuracy: 0.97 - ETA: 4s - loss: 0.0620 - accuracy: 0.97 - ETA: 4s - loss: 0.0636 - accuracy: 0.97 - ETA: 4s - loss: 0.0629 - accuracy: 0.97 - ETA: 4s - loss: 0.0631 - accuracy: 0.97 - ETA: 4s - loss: 0.0689 - accuracy: 0.97 - ETA: 4s - loss: 0.0726 - accuracy: 0.97 - ETA: 3s - loss: 0.0743 - accuracy: 0.97 - ETA: 3s - loss: 0.0764 - accuracy: 0.97 - ETA: 3s - loss: 0.0769 - accuracy: 0.97 - ETA: 3s - loss: 0.0764 - accuracy: 0.97 - ETA: 3s - loss: 0.0759 - accuracy: 0.97 - ETA: 3s - loss: 0.0754 - accuracy: 0.97 - ETA: 3s - loss: 0.0743 - accuracy: 0.97 - ETA: 3s - loss: 0.0744 - accuracy: 0.97 - ETA: 3s - loss: 0.0727 - accuracy: 0.97 - ETA: 3s - loss: 0.0717 - accuracy: 0.97 - ETA: 3s - loss: 0.0715 - accuracy: 0.97 - ETA: 3s - loss: 0.0700 - accuracy: 0.97 - ETA: 2s - loss: 0.0688 - accuracy: 0.97 - ETA: 2s - loss: 0.0677 - accuracy: 0.97 - ETA: 2s - loss: 0.0669 - accuracy: 0.97 - ETA: 2s - loss: 0.0673 - accuracy: 0.97 - ETA: 2s - loss: 0.0672 - accuracy: 0.97 - ETA: 2s - loss: 0.0665 - accuracy: 0.97 - ETA: 2s - loss: 0.0655 - accuracy: 0.97 - ETA: 2s - loss: 0.0648 - accuracy: 0.97 - ETA: 2s - loss: 0.0645 - accuracy: 0.97 - ETA: 2s - loss: 0.0645 - accuracy: 0.97 - ETA: 2s - loss: 0.0661 - accuracy: 0.97 - ETA: 2s - loss: 0.0663 - accuracy: 0.97 - ETA: 2s - loss: 0.0669 - accuracy: 0.97 - ETA: 1s - loss: 0.0673 - accuracy: 0.97 - ETA: 1s - loss: 0.0685 - accuracy: 0.97 - ETA: 1s - loss: 0.0697 - accuracy: 0.97 - ETA: 1s - loss: 0.0696 - accuracy: 0.97 - ETA: 1s - loss: 0.0689 - accuracy: 0.97 - ETA: 1s - loss: 0.0679 - accuracy: 0.97 - ETA: 1s - loss: 0.0671 - accuracy: 0.97 - ETA: 1s - loss: 0.0671 - accuracy: 0.97 - ETA: 1s - loss: 0.0673 - accuracy: 0.97 - ETA: 1s - loss: 0.0669 - accuracy: 0.97 - ETA: 1s - loss: 0.0660 - accuracy: 0.97 - ETA: 1s - loss: 0.0657 - accuracy: 0.97 - ETA: 0s - loss: 0.0650 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0640 - accuracy: 0.97 - ETA: 0s - loss: 0.0642 - accuracy: 0.97 - ETA: 0s - loss: 0.0638 - accuracy: 0.97 - ETA: 0s - loss: 0.0635 - accuracy: 0.97 - ETA: 0s - loss: 0.0636 - accuracy: 0.97 - ETA: 0s - loss: 0.0644 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0629 - accuracy: 0.97 - ETA: 0s - loss: 0.0625 - accuracy: 0.97 - 6s 586us/step - loss: 0.0630 - accuracy: 0.9768 - val_loss: 0.2289 - val_accuracy: 0.9309\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01451\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.3486 - accuracy: 0.89 - ETA: 5s - loss: 0.2864 - accuracy: 0.88 - ETA: 5s - loss: 0.1977 - accuracy: 0.92 - ETA: 5s - loss: 0.1525 - accuracy: 0.94 - ETA: 5s - loss: 0.1314 - accuracy: 0.94 - ETA: 5s - loss: 0.1132 - accuracy: 0.95 - ETA: 5s - loss: 0.0997 - accuracy: 0.96 - ETA: 5s - loss: 0.0939 - accuracy: 0.96 - ETA: 5s - loss: 0.0844 - accuracy: 0.96 - ETA: 5s - loss: 0.0785 - accuracy: 0.97 - ETA: 5s - loss: 0.0758 - accuracy: 0.97 - ETA: 5s - loss: 0.0789 - accuracy: 0.96 - ETA: 4s - loss: 0.0763 - accuracy: 0.96 - ETA: 4s - loss: 0.0745 - accuracy: 0.97 - ETA: 4s - loss: 0.0715 - accuracy: 0.97 - ETA: 4s - loss: 0.0732 - accuracy: 0.97 - ETA: 4s - loss: 0.0727 - accuracy: 0.97 - ETA: 4s - loss: 0.0710 - accuracy: 0.97 - ETA: 4s - loss: 0.0705 - accuracy: 0.97 - ETA: 4s - loss: 0.0700 - accuracy: 0.97 - ETA: 4s - loss: 0.0687 - accuracy: 0.97 - ETA: 4s - loss: 0.0667 - accuracy: 0.97 - ETA: 4s - loss: 0.0651 - accuracy: 0.97 - ETA: 4s - loss: 0.0639 - accuracy: 0.97 - ETA: 4s - loss: 0.0625 - accuracy: 0.97 - ETA: 3s - loss: 0.0612 - accuracy: 0.97 - ETA: 3s - loss: 0.0598 - accuracy: 0.97 - ETA: 3s - loss: 0.0602 - accuracy: 0.97 - ETA: 3s - loss: 0.0625 - accuracy: 0.97 - ETA: 3s - loss: 0.0618 - accuracy: 0.97 - ETA: 3s - loss: 0.0603 - accuracy: 0.97 - ETA: 3s - loss: 0.0595 - accuracy: 0.97 - ETA: 3s - loss: 0.0589 - accuracy: 0.97 - ETA: 3s - loss: 0.0591 - accuracy: 0.97 - ETA: 3s - loss: 0.0587 - accuracy: 0.97 - ETA: 3s - loss: 0.0594 - accuracy: 0.97 - ETA: 3s - loss: 0.0602 - accuracy: 0.97 - ETA: 2s - loss: 0.0622 - accuracy: 0.97 - ETA: 2s - loss: 0.0664 - accuracy: 0.97 - ETA: 2s - loss: 0.0682 - accuracy: 0.97 - ETA: 2s - loss: 0.0673 - accuracy: 0.97 - ETA: 2s - loss: 0.0668 - accuracy: 0.97 - ETA: 2s - loss: 0.0662 - accuracy: 0.97 - ETA: 2s - loss: 0.0648 - accuracy: 0.97 - ETA: 2s - loss: 0.0641 - accuracy: 0.97 - ETA: 2s - loss: 0.0630 - accuracy: 0.97 - ETA: 2s - loss: 0.0618 - accuracy: 0.97 - ETA: 2s - loss: 0.0616 - accuracy: 0.97 - ETA: 2s - loss: 0.0616 - accuracy: 0.97 - ETA: 1s - loss: 0.0609 - accuracy: 0.97 - ETA: 1s - loss: 0.0607 - accuracy: 0.97 - ETA: 1s - loss: 0.0607 - accuracy: 0.97 - ETA: 1s - loss: 0.0599 - accuracy: 0.97 - ETA: 1s - loss: 0.0606 - accuracy: 0.97 - ETA: 1s - loss: 0.0605 - accuracy: 0.97 - ETA: 1s - loss: 0.0606 - accuracy: 0.97 - ETA: 1s - loss: 0.0615 - accuracy: 0.97 - ETA: 1s - loss: 0.0608 - accuracy: 0.97 - ETA: 1s - loss: 0.0602 - accuracy: 0.97 - ETA: 1s - loss: 0.0599 - accuracy: 0.97 - ETA: 1s - loss: 0.0593 - accuracy: 0.97 - ETA: 0s - loss: 0.0590 - accuracy: 0.98 - ETA: 0s - loss: 0.0584 - accuracy: 0.98 - ETA: 0s - loss: 0.0583 - accuracy: 0.98 - ETA: 0s - loss: 0.0584 - accuracy: 0.98 - ETA: 0s - loss: 0.0577 - accuracy: 0.98 - ETA: 0s - loss: 0.0571 - accuracy: 0.98 - ETA: 0s - loss: 0.0570 - accuracy: 0.98 - ETA: 0s - loss: 0.0574 - accuracy: 0.98 - ETA: 0s - loss: 0.0605 - accuracy: 0.97 - ETA: 0s - loss: 0.0665 - accuracy: 0.97 - ETA: 0s - loss: 0.0699 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - 6s 581us/step - loss: 0.0729 - accuracy: 0.9751 - val_loss: 0.0353 - val_accuracy: 0.9874\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01451\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0271 - accuracy: 0.99 - ETA: 6s - loss: 0.0515 - accuracy: 0.98 - ETA: 6s - loss: 0.0451 - accuracy: 0.98 - ETA: 6s - loss: 0.0376 - accuracy: 0.98 - ETA: 5s - loss: 0.0419 - accuracy: 0.98 - ETA: 5s - loss: 0.0409 - accuracy: 0.98 - ETA: 5s - loss: 0.0411 - accuracy: 0.98 - ETA: 5s - loss: 0.0397 - accuracy: 0.98 - ETA: 5s - loss: 0.0380 - accuracy: 0.98 - ETA: 5s - loss: 0.0356 - accuracy: 0.98 - ETA: 5s - loss: 0.0345 - accuracy: 0.98 - ETA: 5s - loss: 0.0362 - accuracy: 0.98 - ETA: 5s - loss: 0.0408 - accuracy: 0.98 - ETA: 5s - loss: 0.0413 - accuracy: 0.98 - ETA: 4s - loss: 0.0428 - accuracy: 0.98 - ETA: 4s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0455 - accuracy: 0.98 - ETA: 4s - loss: 0.0447 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0430 - accuracy: 0.98 - ETA: 4s - loss: 0.0425 - accuracy: 0.98 - ETA: 4s - loss: 0.0414 - accuracy: 0.98 - ETA: 4s - loss: 0.0418 - accuracy: 0.98 - ETA: 4s - loss: 0.0413 - accuracy: 0.98 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0402 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0400 - accuracy: 0.98 - ETA: 3s - loss: 0.0389 - accuracy: 0.98 - ETA: 3s - loss: 0.0385 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0379 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0358 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0373 - accuracy: 0.98 - ETA: 2s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0366 - accuracy: 0.98 - ETA: 2s - loss: 0.0363 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0404 - accuracy: 0.98 - ETA: 2s - loss: 0.0412 - accuracy: 0.98 - ETA: 2s - loss: 0.0428 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0451 - accuracy: 0.98 - ETA: 2s - loss: 0.0449 - accuracy: 0.98 - ETA: 1s - loss: 0.0459 - accuracy: 0.98 - ETA: 1s - loss: 0.0455 - accuracy: 0.98 - ETA: 1s - loss: 0.0451 - accuracy: 0.98 - ETA: 1s - loss: 0.0455 - accuracy: 0.98 - ETA: 1s - loss: 0.0454 - accuracy: 0.98 - ETA: 1s - loss: 0.0451 - accuracy: 0.98 - ETA: 1s - loss: 0.0451 - accuracy: 0.98 - ETA: 1s - loss: 0.0457 - accuracy: 0.98 - ETA: 1s - loss: 0.0456 - accuracy: 0.98 - ETA: 1s - loss: 0.0450 - accuracy: 0.98 - ETA: 1s - loss: 0.0443 - accuracy: 0.98 - ETA: 1s - loss: 0.0443 - accuracy: 0.98 - ETA: 0s - loss: 0.0446 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0519 - accuracy: 0.98 - ETA: 0s - loss: 0.0515 - accuracy: 0.98 - ETA: 0s - loss: 0.0519 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0507 - accuracy: 0.98 - ETA: 0s - loss: 0.0503 - accuracy: 0.98 - ETA: 0s - loss: 0.0501 - accuracy: 0.98 - 7s 611us/step - loss: 0.0499 - accuracy: 0.9826 - val_loss: 0.0263 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01451\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 8s - loss: 0.0284 - accuracy: 0.99 - ETA: 7s - loss: 0.0287 - accuracy: 0.98 - ETA: 6s - loss: 0.0359 - accuracy: 0.98 - ETA: 6s - loss: 0.0389 - accuracy: 0.98 - ETA: 6s - loss: 0.0363 - accuracy: 0.98 - ETA: 6s - loss: 0.0411 - accuracy: 0.98 - ETA: 6s - loss: 0.0484 - accuracy: 0.98 - ETA: 6s - loss: 0.0494 - accuracy: 0.98 - ETA: 6s - loss: 0.0487 - accuracy: 0.98 - ETA: 5s - loss: 0.0454 - accuracy: 0.98 - ETA: 5s - loss: 0.0435 - accuracy: 0.98 - ETA: 5s - loss: 0.0447 - accuracy: 0.98 - ETA: 5s - loss: 0.0425 - accuracy: 0.98 - ETA: 5s - loss: 0.0402 - accuracy: 0.98 - ETA: 5s - loss: 0.0395 - accuracy: 0.98 - ETA: 5s - loss: 0.0395 - accuracy: 0.98 - ETA: 5s - loss: 0.0391 - accuracy: 0.98 - ETA: 5s - loss: 0.0395 - accuracy: 0.98 - ETA: 5s - loss: 0.0387 - accuracy: 0.98 - ETA: 4s - loss: 0.0525 - accuracy: 0.98 - ETA: 4s - loss: 0.0689 - accuracy: 0.97 - ETA: 4s - loss: 0.0689 - accuracy: 0.97 - ETA: 4s - loss: 0.0683 - accuracy: 0.97 - ETA: 4s - loss: 0.0669 - accuracy: 0.97 - ETA: 4s - loss: 0.0662 - accuracy: 0.97 - ETA: 4s - loss: 0.0653 - accuracy: 0.97 - ETA: 4s - loss: 0.0637 - accuracy: 0.98 - ETA: 4s - loss: 0.0628 - accuracy: 0.98 - ETA: 3s - loss: 0.0618 - accuracy: 0.98 - ETA: 3s - loss: 0.0621 - accuracy: 0.98 - ETA: 3s - loss: 0.0619 - accuracy: 0.97 - ETA: 3s - loss: 0.0621 - accuracy: 0.97 - ETA: 3s - loss: 0.0612 - accuracy: 0.97 - ETA: 3s - loss: 0.0605 - accuracy: 0.97 - ETA: 3s - loss: 0.0599 - accuracy: 0.97 - ETA: 3s - loss: 0.0590 - accuracy: 0.98 - ETA: 3s - loss: 0.0576 - accuracy: 0.98 - ETA: 3s - loss: 0.0587 - accuracy: 0.98 - ETA: 3s - loss: 0.0577 - accuracy: 0.98 - ETA: 2s - loss: 0.0565 - accuracy: 0.98 - ETA: 2s - loss: 0.0561 - accuracy: 0.98 - ETA: 2s - loss: 0.0554 - accuracy: 0.98 - ETA: 2s - loss: 0.0546 - accuracy: 0.98 - ETA: 2s - loss: 0.0543 - accuracy: 0.98 - ETA: 2s - loss: 0.0539 - accuracy: 0.98 - ETA: 2s - loss: 0.0539 - accuracy: 0.98 - ETA: 2s - loss: 0.0532 - accuracy: 0.98 - ETA: 2s - loss: 0.0523 - accuracy: 0.98 - ETA: 2s - loss: 0.0514 - accuracy: 0.98 - ETA: 2s - loss: 0.0509 - accuracy: 0.98 - ETA: 1s - loss: 0.0503 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 1s - loss: 0.0508 - accuracy: 0.98 - ETA: 1s - loss: 0.0514 - accuracy: 0.98 - ETA: 1s - loss: 0.0513 - accuracy: 0.98 - ETA: 1s - loss: 0.0507 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0501 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0504 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0501 - accuracy: 0.98 - ETA: 0s - loss: 0.0497 - accuracy: 0.98 - ETA: 0s - loss: 0.0496 - accuracy: 0.98 - ETA: 0s - loss: 0.0493 - accuracy: 0.98 - ETA: 0s - loss: 0.0487 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - 6s 596us/step - loss: 0.0497 - accuracy: 0.9824 - val_loss: 0.0447 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01451\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0376 - accuracy: 0.98 - ETA: 5s - loss: 0.0207 - accuracy: 0.99 - ETA: 6s - loss: 0.0180 - accuracy: 0.99 - ETA: 5s - loss: 0.0164 - accuracy: 0.99 - ETA: 5s - loss: 0.0147 - accuracy: 0.99 - ETA: 5s - loss: 0.0135 - accuracy: 0.99 - ETA: 5s - loss: 0.0161 - accuracy: 0.99 - ETA: 5s - loss: 0.0305 - accuracy: 0.98 - ETA: 5s - loss: 0.0481 - accuracy: 0.98 - ETA: 5s - loss: 0.0579 - accuracy: 0.97 - ETA: 5s - loss: 0.0650 - accuracy: 0.97 - ETA: 5s - loss: 0.0635 - accuracy: 0.97 - ETA: 5s - loss: 0.0719 - accuracy: 0.97 - ETA: 4s - loss: 0.0689 - accuracy: 0.97 - ETA: 4s - loss: 0.0647 - accuracy: 0.97 - ETA: 4s - loss: 0.0613 - accuracy: 0.97 - ETA: 4s - loss: 0.0601 - accuracy: 0.97 - ETA: 4s - loss: 0.0571 - accuracy: 0.97 - ETA: 4s - loss: 0.0551 - accuracy: 0.97 - ETA: 4s - loss: 0.0533 - accuracy: 0.97 - ETA: 4s - loss: 0.0521 - accuracy: 0.98 - ETA: 4s - loss: 0.0536 - accuracy: 0.97 - ETA: 4s - loss: 0.0520 - accuracy: 0.97 - ETA: 4s - loss: 0.0519 - accuracy: 0.98 - ETA: 4s - loss: 0.0513 - accuracy: 0.98 - ETA: 4s - loss: 0.0497 - accuracy: 0.98 - ETA: 3s - loss: 0.0486 - accuracy: 0.98 - ETA: 3s - loss: 0.0482 - accuracy: 0.98 - ETA: 3s - loss: 0.0489 - accuracy: 0.98 - ETA: 3s - loss: 0.0485 - accuracy: 0.98 - ETA: 3s - loss: 0.0477 - accuracy: 0.98 - ETA: 3s - loss: 0.0464 - accuracy: 0.98 - ETA: 3s - loss: 0.0454 - accuracy: 0.98 - ETA: 3s - loss: 0.0453 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 3s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0478 - accuracy: 0.98 - ETA: 2s - loss: 0.0474 - accuracy: 0.98 - ETA: 2s - loss: 0.0480 - accuracy: 0.98 - ETA: 2s - loss: 0.0484 - accuracy: 0.98 - ETA: 2s - loss: 0.0483 - accuracy: 0.98 - ETA: 2s - loss: 0.0488 - accuracy: 0.98 - ETA: 2s - loss: 0.0506 - accuracy: 0.98 - ETA: 2s - loss: 0.0511 - accuracy: 0.98 - ETA: 2s - loss: 0.0506 - accuracy: 0.98 - ETA: 2s - loss: 0.0502 - accuracy: 0.98 - ETA: 2s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0489 - accuracy: 0.98 - ETA: 1s - loss: 0.0491 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0488 - accuracy: 0.98 - ETA: 1s - loss: 0.0492 - accuracy: 0.98 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 1s - loss: 0.0482 - accuracy: 0.98 - ETA: 1s - loss: 0.0483 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0466 - accuracy: 0.98 - ETA: 0s - loss: 0.0463 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0493 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0488 - accuracy: 0.98 - ETA: 0s - loss: 0.0483 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0476 - accuracy: 0.98 - 6s 582us/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.0420 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01451\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0341 - accuracy: 0.97 - ETA: 6s - loss: 0.0220 - accuracy: 0.98 - ETA: 6s - loss: 0.0256 - accuracy: 0.98 - ETA: 6s - loss: 0.0471 - accuracy: 0.98 - ETA: 6s - loss: 0.0665 - accuracy: 0.97 - ETA: 6s - loss: 0.0682 - accuracy: 0.97 - ETA: 6s - loss: 0.0630 - accuracy: 0.97 - ETA: 5s - loss: 0.0582 - accuracy: 0.97 - ETA: 5s - loss: 0.0524 - accuracy: 0.97 - ETA: 5s - loss: 0.0481 - accuracy: 0.97 - ETA: 5s - loss: 0.0457 - accuracy: 0.98 - ETA: 5s - loss: 0.0449 - accuracy: 0.98 - ETA: 5s - loss: 0.0465 - accuracy: 0.97 - ETA: 5s - loss: 0.0527 - accuracy: 0.97 - ETA: 5s - loss: 0.0503 - accuracy: 0.98 - ETA: 5s - loss: 0.0496 - accuracy: 0.97 - ETA: 4s - loss: 0.0475 - accuracy: 0.98 - ETA: 4s - loss: 0.0474 - accuracy: 0.98 - ETA: 4s - loss: 0.0477 - accuracy: 0.98 - ETA: 4s - loss: 0.0498 - accuracy: 0.97 - ETA: 4s - loss: 0.0483 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0499 - accuracy: 0.98 - ETA: 4s - loss: 0.0497 - accuracy: 0.98 - ETA: 4s - loss: 0.0485 - accuracy: 0.98 - ETA: 4s - loss: 0.0478 - accuracy: 0.98 - ETA: 3s - loss: 0.0466 - accuracy: 0.98 - ETA: 3s - loss: 0.0456 - accuracy: 0.98 - ETA: 3s - loss: 0.0444 - accuracy: 0.98 - ETA: 3s - loss: 0.0432 - accuracy: 0.98 - ETA: 3s - loss: 0.0434 - accuracy: 0.98 - ETA: 3s - loss: 0.0428 - accuracy: 0.98 - ETA: 3s - loss: 0.0422 - accuracy: 0.98 - ETA: 3s - loss: 0.0429 - accuracy: 0.98 - ETA: 3s - loss: 0.0440 - accuracy: 0.98 - ETA: 3s - loss: 0.0445 - accuracy: 0.98 - ETA: 3s - loss: 0.0442 - accuracy: 0.98 - ETA: 2s - loss: 0.0443 - accuracy: 0.98 - ETA: 2s - loss: 0.0448 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0442 - accuracy: 0.98 - ETA: 2s - loss: 0.0438 - accuracy: 0.98 - ETA: 2s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0430 - accuracy: 0.98 - ETA: 2s - loss: 0.0423 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0422 - accuracy: 0.98 - ETA: 2s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0409 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0398 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0411 - accuracy: 0.98 - ETA: 1s - loss: 0.0411 - accuracy: 0.98 - ETA: 1s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0418 - accuracy: 0.98 - ETA: 1s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0418 - accuracy: 0.98 - ETA: 1s - loss: 0.0413 - accuracy: 0.98 - ETA: 0s - loss: 0.0411 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0429 - accuracy: 0.98 - ETA: 0s - loss: 0.0440 - accuracy: 0.98 - ETA: 0s - loss: 0.0443 - accuracy: 0.98 - ETA: 0s - loss: 0.0459 - accuracy: 0.98 - ETA: 0s - loss: 0.0454 - accuracy: 0.98 - ETA: 0s - loss: 0.0460 - accuracy: 0.98 - ETA: 0s - loss: 0.0459 - accuracy: 0.98 - ETA: 0s - loss: 0.0454 - accuracy: 0.98 - ETA: 0s - loss: 0.0450 - accuracy: 0.98 - ETA: 0s - loss: 0.0447 - accuracy: 0.98 - 6s 574us/step - loss: 0.0455 - accuracy: 0.9829 - val_loss: 0.0571 - val_accuracy: 0.9756\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01451\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0732 - accuracy: 0.96 - ETA: 5s - loss: 0.0875 - accuracy: 0.96 - ETA: 5s - loss: 0.0776 - accuracy: 0.96 - ETA: 5s - loss: 0.0747 - accuracy: 0.97 - ETA: 5s - loss: 0.0646 - accuracy: 0.97 - ETA: 5s - loss: 0.0556 - accuracy: 0.98 - ETA: 5s - loss: 0.0487 - accuracy: 0.98 - ETA: 5s - loss: 0.0449 - accuracy: 0.98 - ETA: 5s - loss: 0.0424 - accuracy: 0.98 - ETA: 4s - loss: 0.0400 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0426 - accuracy: 0.98 - ETA: 4s - loss: 0.0440 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0428 - accuracy: 0.98 - ETA: 4s - loss: 0.0416 - accuracy: 0.98 - ETA: 4s - loss: 0.0414 - accuracy: 0.98 - ETA: 4s - loss: 0.0402 - accuracy: 0.98 - ETA: 4s - loss: 0.0395 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 3s - loss: 0.0439 - accuracy: 0.98 - ETA: 3s - loss: 0.0460 - accuracy: 0.98 - ETA: 3s - loss: 0.0452 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0446 - accuracy: 0.98 - ETA: 3s - loss: 0.0439 - accuracy: 0.98 - ETA: 3s - loss: 0.0426 - accuracy: 0.98 - ETA: 3s - loss: 0.0413 - accuracy: 0.98 - ETA: 3s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0481 - accuracy: 0.98 - ETA: 3s - loss: 0.0500 - accuracy: 0.98 - ETA: 3s - loss: 0.0503 - accuracy: 0.98 - ETA: 3s - loss: 0.0493 - accuracy: 0.98 - ETA: 2s - loss: 0.0481 - accuracy: 0.98 - ETA: 2s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0464 - accuracy: 0.98 - ETA: 2s - loss: 0.0457 - accuracy: 0.98 - ETA: 2s - loss: 0.0451 - accuracy: 0.98 - ETA: 2s - loss: 0.0443 - accuracy: 0.98 - ETA: 2s - loss: 0.0443 - accuracy: 0.98 - ETA: 2s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0431 - accuracy: 0.98 - ETA: 2s - loss: 0.0432 - accuracy: 0.98 - ETA: 2s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0423 - accuracy: 0.98 - ETA: 2s - loss: 0.0419 - accuracy: 0.98 - ETA: 1s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0408 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0415 - accuracy: 0.98 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 1s - loss: 0.0407 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 1s - loss: 0.0428 - accuracy: 0.98 - ETA: 0s - loss: 0.0435 - accuracy: 0.98 - ETA: 0s - loss: 0.0434 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0434 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0453 - accuracy: 0.98 - ETA: 0s - loss: 0.0452 - accuracy: 0.98 - ETA: 0s - loss: 0.0451 - accuracy: 0.98 - ETA: 0s - loss: 0.0448 - accuracy: 0.98 - ETA: 0s - loss: 0.0444 - accuracy: 0.98 - ETA: 0s - loss: 0.0440 - accuracy: 0.98 - 6s 555us/step - loss: 0.0438 - accuracy: 0.9853 - val_loss: 0.0239 - val_accuracy: 0.9907\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01451\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0332 - accuracy: 0.98 - ETA: 5s - loss: 0.0299 - accuracy: 0.98 - ETA: 5s - loss: 0.0340 - accuracy: 0.98 - ETA: 5s - loss: 0.0347 - accuracy: 0.98 - ETA: 5s - loss: 0.0387 - accuracy: 0.98 - ETA: 5s - loss: 0.0383 - accuracy: 0.98 - ETA: 5s - loss: 0.0349 - accuracy: 0.98 - ETA: 5s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0314 - accuracy: 0.98 - ETA: 4s - loss: 0.0297 - accuracy: 0.98 - ETA: 4s - loss: 0.0294 - accuracy: 0.98 - ETA: 4s - loss: 0.0284 - accuracy: 0.98 - ETA: 4s - loss: 0.0278 - accuracy: 0.98 - ETA: 4s - loss: 0.0266 - accuracy: 0.98 - ETA: 4s - loss: 0.0285 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0549 - accuracy: 0.98 - ETA: 4s - loss: 0.0569 - accuracy: 0.97 - ETA: 4s - loss: 0.0578 - accuracy: 0.97 - ETA: 4s - loss: 0.0560 - accuracy: 0.97 - ETA: 4s - loss: 0.0555 - accuracy: 0.97 - ETA: 4s - loss: 0.0559 - accuracy: 0.97 - ETA: 3s - loss: 0.0538 - accuracy: 0.97 - ETA: 3s - loss: 0.0527 - accuracy: 0.98 - ETA: 3s - loss: 0.0519 - accuracy: 0.98 - ETA: 3s - loss: 0.0506 - accuracy: 0.98 - ETA: 3s - loss: 0.0496 - accuracy: 0.98 - ETA: 3s - loss: 0.0486 - accuracy: 0.98 - ETA: 3s - loss: 0.0492 - accuracy: 0.98 - ETA: 3s - loss: 0.0477 - accuracy: 0.98 - ETA: 3s - loss: 0.0463 - accuracy: 0.98 - ETA: 3s - loss: 0.0458 - accuracy: 0.98 - ETA: 3s - loss: 0.0455 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 2s - loss: 0.0442 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0435 - accuracy: 0.98 - ETA: 2s - loss: 0.0434 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0419 - accuracy: 0.98 - ETA: 2s - loss: 0.0416 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0407 - accuracy: 0.98 - ETA: 2s - loss: 0.0412 - accuracy: 0.98 - ETA: 2s - loss: 0.0408 - accuracy: 0.98 - ETA: 2s - loss: 0.0402 - accuracy: 0.98 - ETA: 2s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0387 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0373 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0364 - accuracy: 0.98 - ETA: 0s - loss: 0.0360 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0371 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - 6s 556us/step - loss: 0.0374 - accuracy: 0.9859 - val_loss: 0.0399 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01451\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0236 - accuracy: 0.99 - ETA: 4s - loss: 0.0236 - accuracy: 0.99 - ETA: 5s - loss: 0.0260 - accuracy: 0.99 - ETA: 5s - loss: 0.0370 - accuracy: 0.98 - ETA: 5s - loss: 0.0578 - accuracy: 0.97 - ETA: 5s - loss: 0.0562 - accuracy: 0.98 - ETA: 5s - loss: 0.0491 - accuracy: 0.98 - ETA: 5s - loss: 0.0619 - accuracy: 0.98 - ETA: 4s - loss: 0.0617 - accuracy: 0.98 - ETA: 4s - loss: 0.0576 - accuracy: 0.98 - ETA: 4s - loss: 0.0562 - accuracy: 0.98 - ETA: 4s - loss: 0.0534 - accuracy: 0.98 - ETA: 4s - loss: 0.0504 - accuracy: 0.98 - ETA: 4s - loss: 0.0507 - accuracy: 0.98 - ETA: 4s - loss: 0.0480 - accuracy: 0.98 - ETA: 4s - loss: 0.0457 - accuracy: 0.98 - ETA: 4s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0434 - accuracy: 0.98 - ETA: 4s - loss: 0.0418 - accuracy: 0.98 - ETA: 4s - loss: 0.0399 - accuracy: 0.98 - ETA: 4s - loss: 0.0402 - accuracy: 0.98 - ETA: 4s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0407 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 3s - loss: 0.0385 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0402 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0388 - accuracy: 0.98 - ETA: 3s - loss: 0.0380 - accuracy: 0.98 - ETA: 3s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0390 - accuracy: 0.98 - ETA: 2s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0394 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0385 - accuracy: 0.98 - ETA: 2s - loss: 0.0382 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0382 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0389 - accuracy: 0.98 - ETA: 1s - loss: 0.0391 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0409 - accuracy: 0.98 - ETA: 1s - loss: 0.0409 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 0s - loss: 0.0400 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0388 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0364 - accuracy: 0.98 - 6s 557us/step - loss: 0.0369 - accuracy: 0.9884 - val_loss: 0.0467 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01451\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0620 - accuracy: 0.97 - ETA: 5s - loss: 0.0334 - accuracy: 0.98 - ETA: 5s - loss: 0.0312 - accuracy: 0.99 - ETA: 5s - loss: 0.0269 - accuracy: 0.99 - ETA: 5s - loss: 0.0273 - accuracy: 0.99 - ETA: 5s - loss: 0.0244 - accuracy: 0.99 - ETA: 5s - loss: 0.0243 - accuracy: 0.99 - ETA: 5s - loss: 0.0256 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 4s - loss: 0.0247 - accuracy: 0.99 - ETA: 4s - loss: 0.0293 - accuracy: 0.99 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0326 - accuracy: 0.98 - ETA: 4s - loss: 0.0306 - accuracy: 0.99 - ETA: 4s - loss: 0.0297 - accuracy: 0.99 - ETA: 4s - loss: 0.0298 - accuracy: 0.99 - ETA: 4s - loss: 0.0282 - accuracy: 0.99 - ETA: 4s - loss: 0.0271 - accuracy: 0.99 - ETA: 4s - loss: 0.0265 - accuracy: 0.99 - ETA: 4s - loss: 0.0253 - accuracy: 0.99 - ETA: 4s - loss: 0.0257 - accuracy: 0.99 - ETA: 4s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0371 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0381 - accuracy: 0.98 - ETA: 3s - loss: 0.0376 - accuracy: 0.98 - ETA: 3s - loss: 0.0370 - accuracy: 0.98 - ETA: 3s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0362 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0346 - accuracy: 0.98 - ETA: 3s - loss: 0.0338 - accuracy: 0.98 - ETA: 2s - loss: 0.0331 - accuracy: 0.98 - ETA: 2s - loss: 0.0324 - accuracy: 0.98 - ETA: 2s - loss: 0.0322 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0316 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0306 - accuracy: 0.99 - ETA: 2s - loss: 0.0309 - accuracy: 0.99 - ETA: 2s - loss: 0.0318 - accuracy: 0.98 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 2s - loss: 0.0332 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0320 - accuracy: 0.98 - ETA: 1s - loss: 0.0321 - accuracy: 0.98 - ETA: 1s - loss: 0.0317 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.99 - ETA: 1s - loss: 0.0313 - accuracy: 0.99 - ETA: 1s - loss: 0.0310 - accuracy: 0.99 - ETA: 1s - loss: 0.0308 - accuracy: 0.99 - ETA: 0s - loss: 0.0311 - accuracy: 0.99 - ETA: 0s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0324 - accuracy: 0.98 - ETA: 0s - loss: 0.0324 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0319 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0336 - accuracy: 0.98 - ETA: 0s - loss: 0.0335 - accuracy: 0.98 - 6s 556us/step - loss: 0.0337 - accuracy: 0.9889 - val_loss: 0.0303 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01451\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0609 - accuracy: 0.97 - ETA: 5s - loss: 0.0575 - accuracy: 0.98 - ETA: 5s - loss: 0.0442 - accuracy: 0.98 - ETA: 5s - loss: 0.0369 - accuracy: 0.98 - ETA: 5s - loss: 0.0304 - accuracy: 0.98 - ETA: 5s - loss: 0.0288 - accuracy: 0.98 - ETA: 5s - loss: 0.0277 - accuracy: 0.98 - ETA: 5s - loss: 0.0258 - accuracy: 0.98 - ETA: 5s - loss: 0.0260 - accuracy: 0.98 - ETA: 5s - loss: 0.0285 - accuracy: 0.98 - ETA: 4s - loss: 0.0440 - accuracy: 0.98 - ETA: 4s - loss: 0.0512 - accuracy: 0.98 - ETA: 4s - loss: 0.0513 - accuracy: 0.98 - ETA: 4s - loss: 0.0494 - accuracy: 0.98 - ETA: 4s - loss: 0.0491 - accuracy: 0.98 - ETA: 4s - loss: 0.0495 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0463 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0462 - accuracy: 0.98 - ETA: 4s - loss: 0.0449 - accuracy: 0.98 - ETA: 4s - loss: 0.0442 - accuracy: 0.98 - ETA: 3s - loss: 0.0426 - accuracy: 0.98 - ETA: 3s - loss: 0.0416 - accuracy: 0.98 - ETA: 3s - loss: 0.0410 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0373 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 3s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 2s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0364 - accuracy: 0.98 - ETA: 2s - loss: 0.0357 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0346 - accuracy: 0.98 - ETA: 2s - loss: 0.0341 - accuracy: 0.98 - ETA: 2s - loss: 0.0336 - accuracy: 0.98 - ETA: 1s - loss: 0.0330 - accuracy: 0.98 - ETA: 1s - loss: 0.0336 - accuracy: 0.98 - ETA: 1s - loss: 0.0336 - accuracy: 0.98 - ETA: 1s - loss: 0.0334 - accuracy: 0.98 - ETA: 1s - loss: 0.0333 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 1s - loss: 0.0355 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 1s - loss: 0.0349 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 1s - loss: 0.0339 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0337 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0344 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0344 - accuracy: 0.98 - ETA: 0s - loss: 0.0339 - accuracy: 0.98 - ETA: 0s - loss: 0.0335 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0344 - accuracy: 0.98 - 6s 563us/step - loss: 0.0343 - accuracy: 0.9879 - val_loss: 0.0192 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01451\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0069 - accuracy: 1.00 - ETA: 5s - loss: 0.0077 - accuracy: 0.99 - ETA: 5s - loss: 0.0099 - accuracy: 0.99 - ETA: 5s - loss: 0.0106 - accuracy: 0.99 - ETA: 5s - loss: 0.0196 - accuracy: 0.99 - ETA: 5s - loss: 0.0189 - accuracy: 0.99 - ETA: 5s - loss: 0.0172 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 5s - loss: 0.0188 - accuracy: 0.99 - ETA: 5s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0191 - accuracy: 0.99 - ETA: 4s - loss: 0.0185 - accuracy: 0.99 - ETA: 4s - loss: 0.0173 - accuracy: 0.99 - ETA: 4s - loss: 0.0165 - accuracy: 0.99 - ETA: 4s - loss: 0.0158 - accuracy: 0.99 - ETA: 4s - loss: 0.0150 - accuracy: 0.99 - ETA: 4s - loss: 0.0152 - accuracy: 0.99 - ETA: 4s - loss: 0.0165 - accuracy: 0.99 - ETA: 4s - loss: 0.0265 - accuracy: 0.99 - ETA: 4s - loss: 0.0305 - accuracy: 0.98 - ETA: 4s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0342 - accuracy: 0.98 - ETA: 4s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0353 - accuracy: 0.98 - ETA: 3s - loss: 0.0347 - accuracy: 0.98 - ETA: 3s - loss: 0.0371 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0390 - accuracy: 0.98 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0380 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0369 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.99 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 2s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0360 - accuracy: 0.98 - ETA: 2s - loss: 0.0355 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0345 - accuracy: 0.98 - ETA: 2s - loss: 0.0366 - accuracy: 0.98 - ETA: 2s - loss: 0.0383 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0373 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0362 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0364 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0362 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0349 - accuracy: 0.98 - ETA: 0s - loss: 0.0348 - accuracy: 0.98 - ETA: 0s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0339 - accuracy: 0.98 - ETA: 0s - loss: 0.0337 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0325 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - 6s 570us/step - loss: 0.0457 - accuracy: 0.9877 - val_loss: 0.0403 - val_accuracy: 0.9823\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01451\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0264 - accuracy: 0.98 - ETA: 5s - loss: 0.0368 - accuracy: 0.98 - ETA: 5s - loss: 0.0359 - accuracy: 0.98 - ETA: 5s - loss: 0.0298 - accuracy: 0.98 - ETA: 5s - loss: 0.0266 - accuracy: 0.99 - ETA: 5s - loss: 0.0238 - accuracy: 0.99 - ETA: 5s - loss: 0.0215 - accuracy: 0.99 - ETA: 5s - loss: 0.0224 - accuracy: 0.99 - ETA: 5s - loss: 0.0211 - accuracy: 0.99 - ETA: 4s - loss: 0.0200 - accuracy: 0.99 - ETA: 4s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0176 - accuracy: 0.99 - ETA: 4s - loss: 0.0167 - accuracy: 0.99 - ETA: 4s - loss: 0.0159 - accuracy: 0.99 - ETA: 4s - loss: 0.0151 - accuracy: 0.99 - ETA: 4s - loss: 0.0144 - accuracy: 0.99 - ETA: 4s - loss: 0.0143 - accuracy: 0.99 - ETA: 4s - loss: 0.0138 - accuracy: 0.99 - ETA: 4s - loss: 0.0139 - accuracy: 0.99 - ETA: 4s - loss: 0.0135 - accuracy: 0.99 - ETA: 4s - loss: 0.0132 - accuracy: 0.99 - ETA: 4s - loss: 0.0127 - accuracy: 0.99 - ETA: 3s - loss: 0.0125 - accuracy: 0.99 - ETA: 3s - loss: 0.0122 - accuracy: 0.99 - ETA: 3s - loss: 0.0119 - accuracy: 0.99 - ETA: 3s - loss: 0.0118 - accuracy: 0.99 - ETA: 3s - loss: 0.0114 - accuracy: 0.99 - ETA: 3s - loss: 0.0110 - accuracy: 0.99 - ETA: 3s - loss: 0.0122 - accuracy: 0.99 - ETA: 3s - loss: 0.0143 - accuracy: 0.99 - ETA: 3s - loss: 0.0139 - accuracy: 0.99 - ETA: 3s - loss: 0.0138 - accuracy: 0.99 - ETA: 3s - loss: 0.0137 - accuracy: 0.99 - ETA: 3s - loss: 0.0135 - accuracy: 0.99 - ETA: 3s - loss: 0.0146 - accuracy: 0.99 - ETA: 2s - loss: 0.0164 - accuracy: 0.99 - ETA: 2s - loss: 0.0166 - accuracy: 0.99 - ETA: 2s - loss: 0.0166 - accuracy: 0.99 - ETA: 2s - loss: 0.0168 - accuracy: 0.99 - ETA: 2s - loss: 0.0175 - accuracy: 0.99 - ETA: 2s - loss: 0.0175 - accuracy: 0.99 - ETA: 2s - loss: 0.0172 - accuracy: 0.99 - ETA: 2s - loss: 0.0179 - accuracy: 0.99 - ETA: 2s - loss: 0.0178 - accuracy: 0.99 - ETA: 2s - loss: 0.0185 - accuracy: 0.99 - ETA: 2s - loss: 0.0184 - accuracy: 0.99 - ETA: 2s - loss: 0.0184 - accuracy: 0.99 - ETA: 1s - loss: 0.0187 - accuracy: 0.99 - ETA: 1s - loss: 0.0205 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 1s - loss: 0.0210 - accuracy: 0.99 - ETA: 1s - loss: 0.0232 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0255 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0255 - accuracy: 0.99 - ETA: 1s - loss: 0.0252 - accuracy: 0.99 - ETA: 1s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0245 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0248 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - ETA: 0s - loss: 0.0245 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - 6s 556us/step - loss: 0.0261 - accuracy: 0.9912 - val_loss: 0.0241 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01451\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0395 - accuracy: 0.97 - ETA: 5s - loss: 0.0349 - accuracy: 0.98 - ETA: 5s - loss: 0.0257 - accuracy: 0.98 - ETA: 5s - loss: 0.0252 - accuracy: 0.98 - ETA: 5s - loss: 0.0237 - accuracy: 0.98 - ETA: 5s - loss: 0.0285 - accuracy: 0.98 - ETA: 5s - loss: 0.0317 - accuracy: 0.98 - ETA: 5s - loss: 0.0323 - accuracy: 0.98 - ETA: 5s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0408 - accuracy: 0.98 - ETA: 4s - loss: 0.0416 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0427 - accuracy: 0.98 - ETA: 4s - loss: 0.0406 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0371 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0338 - accuracy: 0.98 - ETA: 3s - loss: 0.0330 - accuracy: 0.98 - ETA: 3s - loss: 0.0318 - accuracy: 0.98 - ETA: 3s - loss: 0.0310 - accuracy: 0.98 - ETA: 3s - loss: 0.0301 - accuracy: 0.98 - ETA: 3s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0335 - accuracy: 0.98 - ETA: 3s - loss: 0.0329 - accuracy: 0.98 - ETA: 3s - loss: 0.0320 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0347 - accuracy: 0.98 - ETA: 3s - loss: 0.0346 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0334 - accuracy: 0.98 - ETA: 2s - loss: 0.0328 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0312 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.98 - ETA: 2s - loss: 0.0298 - accuracy: 0.98 - ETA: 2s - loss: 0.0311 - accuracy: 0.98 - ETA: 2s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0346 - accuracy: 0.98 - ETA: 2s - loss: 0.0344 - accuracy: 0.98 - ETA: 2s - loss: 0.0340 - accuracy: 0.98 - ETA: 1s - loss: 0.0340 - accuracy: 0.98 - ETA: 1s - loss: 0.0334 - accuracy: 0.98 - ETA: 1s - loss: 0.0329 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0337 - accuracy: 0.98 - ETA: 1s - loss: 0.0339 - accuracy: 0.98 - ETA: 1s - loss: 0.0339 - accuracy: 0.98 - ETA: 1s - loss: 0.0337 - accuracy: 0.98 - ETA: 1s - loss: 0.0335 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0313 - accuracy: 0.98 - ETA: 0s - loss: 0.0313 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0310 - accuracy: 0.98 - ETA: 0s - loss: 0.0311 - accuracy: 0.98 - ETA: 0s - loss: 0.0308 - accuracy: 0.98 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.99 - 6s 567us/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.0179 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01451\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0022 - accuracy: 1.00 - ETA: 5s - loss: 0.0098 - accuracy: 0.99 - ETA: 5s - loss: 0.0189 - accuracy: 0.99 - ETA: 5s - loss: 0.0473 - accuracy: 0.98 - ETA: 5s - loss: 0.0411 - accuracy: 0.98 - ETA: 5s - loss: 0.0364 - accuracy: 0.98 - ETA: 5s - loss: 0.0434 - accuracy: 0.98 - ETA: 5s - loss: 0.0458 - accuracy: 0.98 - ETA: 4s - loss: 0.0437 - accuracy: 0.98 - ETA: 4s - loss: 0.0408 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0353 - accuracy: 0.98 - ETA: 4s - loss: 0.0345 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0292 - accuracy: 0.98 - ETA: 4s - loss: 0.0283 - accuracy: 0.98 - ETA: 4s - loss: 0.0285 - accuracy: 0.98 - ETA: 4s - loss: 0.0291 - accuracy: 0.98 - ETA: 4s - loss: 0.0291 - accuracy: 0.98 - ETA: 3s - loss: 0.0286 - accuracy: 0.98 - ETA: 3s - loss: 0.0283 - accuracy: 0.98 - ETA: 3s - loss: 0.0295 - accuracy: 0.98 - ETA: 3s - loss: 0.0287 - accuracy: 0.98 - ETA: 3s - loss: 0.0283 - accuracy: 0.98 - ETA: 3s - loss: 0.0284 - accuracy: 0.98 - ETA: 3s - loss: 0.0283 - accuracy: 0.98 - ETA: 3s - loss: 0.0282 - accuracy: 0.98 - ETA: 3s - loss: 0.0297 - accuracy: 0.98 - ETA: 3s - loss: 0.0301 - accuracy: 0.98 - ETA: 3s - loss: 0.0307 - accuracy: 0.98 - ETA: 3s - loss: 0.0312 - accuracy: 0.98 - ETA: 3s - loss: 0.0311 - accuracy: 0.98 - ETA: 2s - loss: 0.0304 - accuracy: 0.98 - ETA: 2s - loss: 0.0298 - accuracy: 0.98 - ETA: 2s - loss: 0.0291 - accuracy: 0.98 - ETA: 2s - loss: 0.0293 - accuracy: 0.98 - ETA: 2s - loss: 0.0290 - accuracy: 0.98 - ETA: 2s - loss: 0.0285 - accuracy: 0.98 - ETA: 2s - loss: 0.0282 - accuracy: 0.98 - ETA: 2s - loss: 0.0276 - accuracy: 0.98 - ETA: 2s - loss: 0.0283 - accuracy: 0.98 - ETA: 2s - loss: 0.0279 - accuracy: 0.98 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 1s - loss: 0.0258 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0255 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0250 - accuracy: 0.99 - ETA: 1s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0417 - accuracy: 0.98 - ETA: 1s - loss: 0.0420 - accuracy: 0.98 - ETA: 1s - loss: 0.0415 - accuracy: 0.98 - ETA: 1s - loss: 0.0419 - accuracy: 0.98 - ETA: 1s - loss: 0.0413 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0400 - accuracy: 0.98 - ETA: 0s - loss: 0.0396 - accuracy: 0.98 - ETA: 0s - loss: 0.0391 - accuracy: 0.98 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0375 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - ETA: 0s - loss: 0.0371 - accuracy: 0.98 - ETA: 0s - loss: 0.0371 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0360 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - 6s 550us/step - loss: 0.0359 - accuracy: 0.9897 - val_loss: 0.0278 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01451\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0029 - accuracy: 1.00 - ETA: 5s - loss: 0.0023 - accuracy: 1.00 - ETA: 5s - loss: 0.0030 - accuracy: 1.00 - ETA: 5s - loss: 0.0054 - accuracy: 0.99 - ETA: 5s - loss: 0.0054 - accuracy: 0.99 - ETA: 5s - loss: 0.0075 - accuracy: 0.99 - ETA: 5s - loss: 0.0074 - accuracy: 0.99 - ETA: 5s - loss: 0.0072 - accuracy: 0.99 - ETA: 4s - loss: 0.0072 - accuracy: 0.99 - ETA: 4s - loss: 0.0076 - accuracy: 0.99 - ETA: 4s - loss: 0.0072 - accuracy: 0.99 - ETA: 4s - loss: 0.0067 - accuracy: 0.99 - ETA: 4s - loss: 0.0084 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0206 - accuracy: 0.99 - ETA: 4s - loss: 0.0213 - accuracy: 0.99 - ETA: 4s - loss: 0.0213 - accuracy: 0.99 - ETA: 4s - loss: 0.0204 - accuracy: 0.99 - ETA: 4s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0250 - accuracy: 0.99 - ETA: 4s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0239 - accuracy: 0.99 - ETA: 3s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0221 - accuracy: 0.99 - ETA: 3s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0230 - accuracy: 0.99 - ETA: 3s - loss: 0.0225 - accuracy: 0.99 - ETA: 3s - loss: 0.0222 - accuracy: 0.99 - ETA: 3s - loss: 0.0219 - accuracy: 0.99 - ETA: 3s - loss: 0.0218 - accuracy: 0.99 - ETA: 3s - loss: 0.0218 - accuracy: 0.99 - ETA: 3s - loss: 0.0212 - accuracy: 0.99 - ETA: 2s - loss: 0.0208 - accuracy: 0.99 - ETA: 2s - loss: 0.0209 - accuracy: 0.99 - ETA: 2s - loss: 0.0205 - accuracy: 0.99 - ETA: 2s - loss: 0.0204 - accuracy: 0.99 - ETA: 2s - loss: 0.0201 - accuracy: 0.99 - ETA: 2s - loss: 0.0202 - accuracy: 0.99 - ETA: 2s - loss: 0.0207 - accuracy: 0.99 - ETA: 2s - loss: 0.0206 - accuracy: 0.99 - ETA: 2s - loss: 0.0208 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0264 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0257 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0255 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0254 - accuracy: 0.99 - ETA: 1s - loss: 0.0253 - accuracy: 0.99 - ETA: 1s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0248 - accuracy: 0.99 - ETA: 0s - loss: 0.0245 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0245 - accuracy: 0.99 - ETA: 0s - loss: 0.0245 - accuracy: 0.99 - ETA: 0s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - 6s 553us/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 0.0238 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01451\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0107 - accuracy: 0.99 - ETA: 5s - loss: 0.0097 - accuracy: 0.99 - ETA: 5s - loss: 0.0228 - accuracy: 0.99 - ETA: 5s - loss: 0.0295 - accuracy: 0.98 - ETA: 5s - loss: 0.0463 - accuracy: 0.98 - ETA: 5s - loss: 0.0467 - accuracy: 0.98 - ETA: 4s - loss: 0.0426 - accuracy: 0.98 - ETA: 4s - loss: 0.0384 - accuracy: 0.98 - ETA: 4s - loss: 0.0387 - accuracy: 0.98 - ETA: 4s - loss: 0.0397 - accuracy: 0.98 - ETA: 4s - loss: 0.0388 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0342 - accuracy: 0.98 - ETA: 4s - loss: 0.0322 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0295 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.99 - ETA: 4s - loss: 0.0267 - accuracy: 0.99 - ETA: 4s - loss: 0.0258 - accuracy: 0.99 - ETA: 4s - loss: 0.0259 - accuracy: 0.99 - ETA: 3s - loss: 0.0258 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0244 - accuracy: 0.99 - ETA: 3s - loss: 0.0259 - accuracy: 0.99 - ETA: 3s - loss: 0.0342 - accuracy: 0.98 - ETA: 3s - loss: 0.0407 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0386 - accuracy: 0.98 - ETA: 3s - loss: 0.0377 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0338 - accuracy: 0.98 - ETA: 2s - loss: 0.0337 - accuracy: 0.98 - ETA: 2s - loss: 0.0331 - accuracy: 0.98 - ETA: 2s - loss: 0.0324 - accuracy: 0.98 - ETA: 2s - loss: 0.0316 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.98 - ETA: 2s - loss: 0.0302 - accuracy: 0.98 - ETA: 1s - loss: 0.0296 - accuracy: 0.98 - ETA: 1s - loss: 0.0294 - accuracy: 0.98 - ETA: 1s - loss: 0.0296 - accuracy: 0.98 - ETA: 1s - loss: 0.0301 - accuracy: 0.98 - ETA: 1s - loss: 0.0298 - accuracy: 0.98 - ETA: 1s - loss: 0.0293 - accuracy: 0.98 - ETA: 1s - loss: 0.0291 - accuracy: 0.98 - ETA: 1s - loss: 0.0286 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0277 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0296 - accuracy: 0.99 - ETA: 0s - loss: 0.0336 - accuracy: 0.98 - ETA: 0s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0312 - accuracy: 0.98 - ETA: 0s - loss: 0.0310 - accuracy: 0.98 - 6s 553us/step - loss: 0.0308 - accuracy: 0.9897 - val_loss: 0.0147 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01451\n",
      "Epoch 27/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0223 - accuracy: 0.98 - ETA: 6s - loss: 0.0192 - accuracy: 0.98 - ETA: 5s - loss: 0.0136 - accuracy: 0.99 - ETA: 5s - loss: 0.0148 - accuracy: 0.99 - ETA: 5s - loss: 0.0149 - accuracy: 0.99 - ETA: 5s - loss: 0.0167 - accuracy: 0.99 - ETA: 5s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0200 - accuracy: 0.99 - ETA: 5s - loss: 0.0182 - accuracy: 0.99 - ETA: 4s - loss: 0.0229 - accuracy: 0.99 - ETA: 4s - loss: 0.0225 - accuracy: 0.99 - ETA: 4s - loss: 0.0207 - accuracy: 0.99 - ETA: 4s - loss: 0.0216 - accuracy: 0.99 - ETA: 4s - loss: 0.0220 - accuracy: 0.99 - ETA: 4s - loss: 0.0211 - accuracy: 0.99 - ETA: 4s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0186 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0243 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0240 - accuracy: 0.99 - ETA: 3s - loss: 0.0243 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 3s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0228 - accuracy: 0.99 - ETA: 3s - loss: 0.0220 - accuracy: 0.99 - ETA: 3s - loss: 0.0214 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0223 - accuracy: 0.99 - ETA: 3s - loss: 0.0222 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0221 - accuracy: 0.99 - ETA: 3s - loss: 0.0216 - accuracy: 0.99 - ETA: 2s - loss: 0.0221 - accuracy: 0.99 - ETA: 2s - loss: 0.0216 - accuracy: 0.99 - ETA: 2s - loss: 0.0218 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0281 - accuracy: 0.99 - ETA: 2s - loss: 0.0279 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0270 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0261 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0258 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0252 - accuracy: 0.99 - ETA: 1s - loss: 0.0249 - accuracy: 0.99 - ETA: 1s - loss: 0.0245 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0254 - accuracy: 0.99 - ETA: 1s - loss: 0.0253 - accuracy: 0.99 - ETA: 1s - loss: 0.0250 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0249 - accuracy: 0.99 - ETA: 1s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0237 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0243 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0268 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0263 - accuracy: 0.99 - 6s 556us/step - loss: 0.0261 - accuracy: 0.9916 - val_loss: 0.0304 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01451\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0952 - accuracy: 0.97 - ETA: 5s - loss: 0.0612 - accuracy: 0.98 - ETA: 5s - loss: 0.0431 - accuracy: 0.99 - ETA: 5s - loss: 0.0340 - accuracy: 0.99 - ETA: 5s - loss: 0.0287 - accuracy: 0.99 - ETA: 5s - loss: 0.0241 - accuracy: 0.99 - ETA: 4s - loss: 0.0237 - accuracy: 0.99 - ETA: 4s - loss: 0.0218 - accuracy: 0.99 - ETA: 4s - loss: 0.0228 - accuracy: 0.99 - ETA: 4s - loss: 0.0259 - accuracy: 0.99 - ETA: 4s - loss: 0.0266 - accuracy: 0.99 - ETA: 4s - loss: 0.0291 - accuracy: 0.99 - ETA: 4s - loss: 0.0292 - accuracy: 0.99 - ETA: 4s - loss: 0.0293 - accuracy: 0.99 - ETA: 4s - loss: 0.0282 - accuracy: 0.99 - ETA: 4s - loss: 0.0276 - accuracy: 0.99 - ETA: 4s - loss: 0.0267 - accuracy: 0.99 - ETA: 4s - loss: 0.0259 - accuracy: 0.99 - ETA: 4s - loss: 0.0257 - accuracy: 0.99 - ETA: 4s - loss: 0.0246 - accuracy: 0.99 - ETA: 4s - loss: 0.0237 - accuracy: 0.99 - ETA: 3s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0237 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0267 - accuracy: 0.99 - ETA: 3s - loss: 0.0285 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0269 - accuracy: 0.99 - ETA: 3s - loss: 0.0281 - accuracy: 0.99 - ETA: 3s - loss: 0.0277 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0286 - accuracy: 0.99 - ETA: 2s - loss: 0.0294 - accuracy: 0.99 - ETA: 2s - loss: 0.0305 - accuracy: 0.99 - ETA: 2s - loss: 0.0307 - accuracy: 0.99 - ETA: 2s - loss: 0.0308 - accuracy: 0.99 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0304 - accuracy: 0.99 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0297 - accuracy: 0.99 - ETA: 2s - loss: 0.0291 - accuracy: 0.99 - ETA: 2s - loss: 0.0288 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0288 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0274 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0263 - accuracy: 0.99 - ETA: 0s - loss: 0.0260 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0241 - accuracy: 0.99 - 6s 556us/step - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.0312 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01451\n",
      "Epoch 29/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0188 - accuracy: 0.98 - ETA: 6s - loss: 0.0195 - accuracy: 0.98 - ETA: 5s - loss: 0.0235 - accuracy: 0.98 - ETA: 5s - loss: 0.0554 - accuracy: 0.97 - ETA: 5s - loss: 0.0920 - accuracy: 0.96 - ETA: 5s - loss: 0.0800 - accuracy: 0.97 - ETA: 5s - loss: 0.0724 - accuracy: 0.97 - ETA: 5s - loss: 0.0640 - accuracy: 0.97 - ETA: 5s - loss: 0.0574 - accuracy: 0.98 - ETA: 5s - loss: 0.0526 - accuracy: 0.98 - ETA: 4s - loss: 0.0484 - accuracy: 0.98 - ETA: 4s - loss: 0.0452 - accuracy: 0.98 - ETA: 4s - loss: 0.0442 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 4s - loss: 0.0408 - accuracy: 0.98 - ETA: 4s - loss: 0.0407 - accuracy: 0.98 - ETA: 4s - loss: 0.0389 - accuracy: 0.98 - ETA: 4s - loss: 0.0368 - accuracy: 0.98 - ETA: 4s - loss: 0.0354 - accuracy: 0.98 - ETA: 4s - loss: 0.0340 - accuracy: 0.98 - ETA: 4s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0326 - accuracy: 0.98 - ETA: 3s - loss: 0.0319 - accuracy: 0.98 - ETA: 3s - loss: 0.0308 - accuracy: 0.98 - ETA: 3s - loss: 0.0298 - accuracy: 0.99 - ETA: 3s - loss: 0.0286 - accuracy: 0.99 - ETA: 3s - loss: 0.0290 - accuracy: 0.99 - ETA: 3s - loss: 0.0289 - accuracy: 0.99 - ETA: 3s - loss: 0.0280 - accuracy: 0.99 - ETA: 3s - loss: 0.0272 - accuracy: 0.99 - ETA: 3s - loss: 0.0265 - accuracy: 0.99 - ETA: 3s - loss: 0.0258 - accuracy: 0.99 - ETA: 3s - loss: 0.0253 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 3s - loss: 0.0243 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0258 - accuracy: 0.99 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0237 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0256 - accuracy: 0.99 - ETA: 2s - loss: 0.0252 - accuracy: 0.99 - ETA: 1s - loss: 0.0248 - accuracy: 0.99 - ETA: 1s - loss: 0.0257 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0286 - accuracy: 0.99 - ETA: 1s - loss: 0.0284 - accuracy: 0.99 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.99 - ETA: 0s - loss: 0.0280 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0263 - accuracy: 0.99 - ETA: 0s - loss: 0.0260 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0260 - accuracy: 0.99 - 6s 557us/step - loss: 0.0258 - accuracy: 0.9914 - val_loss: 0.0165 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01451\n",
      "Epoch 30/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0228 - accuracy: 0.99 - ETA: 6s - loss: 0.0124 - accuracy: 0.99 - ETA: 5s - loss: 0.0135 - accuracy: 0.99 - ETA: 6s - loss: 0.0147 - accuracy: 0.99 - ETA: 5s - loss: 0.0161 - accuracy: 0.99 - ETA: 5s - loss: 0.0166 - accuracy: 0.99 - ETA: 5s - loss: 0.0216 - accuracy: 0.99 - ETA: 5s - loss: 0.0194 - accuracy: 0.99 - ETA: 5s - loss: 0.0174 - accuracy: 0.99 - ETA: 5s - loss: 0.0184 - accuracy: 0.99 - ETA: 5s - loss: 0.0177 - accuracy: 0.99 - ETA: 5s - loss: 0.0189 - accuracy: 0.99 - ETA: 5s - loss: 0.0200 - accuracy: 0.99 - ETA: 5s - loss: 0.0212 - accuracy: 0.99 - ETA: 5s - loss: 0.0200 - accuracy: 0.99 - ETA: 4s - loss: 0.0204 - accuracy: 0.99 - ETA: 4s - loss: 0.0203 - accuracy: 0.99 - ETA: 4s - loss: 0.0206 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0189 - accuracy: 0.99 - ETA: 4s - loss: 0.0194 - accuracy: 0.99 - ETA: 4s - loss: 0.0211 - accuracy: 0.99 - ETA: 4s - loss: 0.0216 - accuracy: 0.99 - ETA: 4s - loss: 0.0215 - accuracy: 0.99 - ETA: 4s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0221 - accuracy: 0.99 - ETA: 3s - loss: 0.0242 - accuracy: 0.99 - ETA: 3s - loss: 0.0255 - accuracy: 0.99 - ETA: 3s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0242 - accuracy: 0.99 - ETA: 3s - loss: 0.0235 - accuracy: 0.99 - ETA: 3s - loss: 0.0240 - accuracy: 0.99 - ETA: 3s - loss: 0.0240 - accuracy: 0.99 - ETA: 3s - loss: 0.0233 - accuracy: 0.99 - ETA: 3s - loss: 0.0228 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0325 - accuracy: 0.99 - ETA: 2s - loss: 0.0321 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0308 - accuracy: 0.99 - ETA: 2s - loss: 0.0305 - accuracy: 0.99 - ETA: 2s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0298 - accuracy: 0.99 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 2s - loss: 0.0296 - accuracy: 0.99 - ETA: 2s - loss: 0.0290 - accuracy: 0.99 - ETA: 2s - loss: 0.0285 - accuracy: 0.99 - ETA: 1s - loss: 0.0286 - accuracy: 0.99 - ETA: 1s - loss: 0.0285 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0284 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0268 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - 6s 584us/step - loss: 0.0269 - accuracy: 0.9913 - val_loss: 0.0144 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01451 to 0.01438, saving model to cnn_bee_1.h5\n",
      "Epoch 31/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0158 - accuracy: 0.98 - ETA: 5s - loss: 0.0139 - accuracy: 0.98 - ETA: 5s - loss: 0.0115 - accuracy: 0.99 - ETA: 5s - loss: 0.0096 - accuracy: 0.99 - ETA: 5s - loss: 0.0089 - accuracy: 0.99 - ETA: 5s - loss: 0.0133 - accuracy: 0.99 - ETA: 5s - loss: 0.0119 - accuracy: 0.99 - ETA: 5s - loss: 0.0144 - accuracy: 0.99 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0357 - accuracy: 0.98 - ETA: 4s - loss: 0.0334 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0297 - accuracy: 0.98 - ETA: 4s - loss: 0.0283 - accuracy: 0.98 - ETA: 4s - loss: 0.0279 - accuracy: 0.98 - ETA: 4s - loss: 0.0265 - accuracy: 0.99 - ETA: 4s - loss: 0.0254 - accuracy: 0.99 - ETA: 4s - loss: 0.0246 - accuracy: 0.99 - ETA: 4s - loss: 0.0238 - accuracy: 0.99 - ETA: 4s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0240 - accuracy: 0.99 - ETA: 3s - loss: 0.0237 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0227 - accuracy: 0.99 - ETA: 3s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0272 - accuracy: 0.99 - ETA: 3s - loss: 0.0271 - accuracy: 0.99 - ETA: 3s - loss: 0.0267 - accuracy: 0.99 - ETA: 3s - loss: 0.0261 - accuracy: 0.99 - ETA: 3s - loss: 0.0254 - accuracy: 0.99 - ETA: 3s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0252 - accuracy: 0.99 - ETA: 2s - loss: 0.0257 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0250 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0227 - accuracy: 0.99 - ETA: 2s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0223 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0228 - accuracy: 0.99 - ETA: 1s - loss: 0.0252 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0258 - accuracy: 0.99 - ETA: 1s - loss: 0.0254 - accuracy: 0.99 - ETA: 1s - loss: 0.0258 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0248 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0241 - accuracy: 0.99 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0237 - accuracy: 0.99 - 6s 583us/step - loss: 0.0238 - accuracy: 0.9920 - val_loss: 0.0561 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01438\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0021 - accuracy: 1.00 - ETA: 5s - loss: 0.0174 - accuracy: 0.98 - ETA: 5s - loss: 0.0503 - accuracy: 0.98 - ETA: 5s - loss: 0.0541 - accuracy: 0.97 - ETA: 5s - loss: 0.0465 - accuracy: 0.98 - ETA: 5s - loss: 0.0409 - accuracy: 0.98 - ETA: 5s - loss: 0.0365 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0316 - accuracy: 0.98 - ETA: 4s - loss: 0.0286 - accuracy: 0.98 - ETA: 4s - loss: 0.0270 - accuracy: 0.99 - ETA: 4s - loss: 0.0266 - accuracy: 0.99 - ETA: 4s - loss: 0.0247 - accuracy: 0.99 - ETA: 4s - loss: 0.0233 - accuracy: 0.99 - ETA: 4s - loss: 0.0218 - accuracy: 0.99 - ETA: 4s - loss: 0.0213 - accuracy: 0.99 - ETA: 4s - loss: 0.0203 - accuracy: 0.99 - ETA: 4s - loss: 0.0192 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0180 - accuracy: 0.99 - ETA: 4s - loss: 0.0174 - accuracy: 0.99 - ETA: 4s - loss: 0.0168 - accuracy: 0.99 - ETA: 4s - loss: 0.0162 - accuracy: 0.99 - ETA: 3s - loss: 0.0170 - accuracy: 0.99 - ETA: 3s - loss: 0.0166 - accuracy: 0.99 - ETA: 3s - loss: 0.0171 - accuracy: 0.99 - ETA: 3s - loss: 0.0167 - accuracy: 0.99 - ETA: 3s - loss: 0.0162 - accuracy: 0.99 - ETA: 3s - loss: 0.0157 - accuracy: 0.99 - ETA: 3s - loss: 0.0152 - accuracy: 0.99 - ETA: 3s - loss: 0.0148 - accuracy: 0.99 - ETA: 3s - loss: 0.0145 - accuracy: 0.99 - ETA: 3s - loss: 0.0142 - accuracy: 0.99 - ETA: 3s - loss: 0.0138 - accuracy: 0.99 - ETA: 3s - loss: 0.0135 - accuracy: 0.99 - ETA: 2s - loss: 0.0132 - accuracy: 0.99 - ETA: 2s - loss: 0.0129 - accuracy: 0.99 - ETA: 2s - loss: 0.0127 - accuracy: 0.99 - ETA: 2s - loss: 0.0137 - accuracy: 0.99 - ETA: 2s - loss: 0.0202 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0216 - accuracy: 0.99 - ETA: 2s - loss: 0.0224 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0227 - accuracy: 0.99 - ETA: 2s - loss: 0.0224 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0219 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0223 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0217 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0215 - accuracy: 0.99 - ETA: 1s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0212 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0206 - accuracy: 0.99 - ETA: 0s - loss: 0.0204 - accuracy: 0.99 - ETA: 0s - loss: 0.0215 - accuracy: 0.99 - ETA: 0s - loss: 0.0212 - accuracy: 0.99 - ETA: 0s - loss: 0.0211 - accuracy: 0.99 - ETA: 0s - loss: 0.0211 - accuracy: 0.99 - ETA: 0s - loss: 0.0208 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0207 - accuracy: 0.99 - ETA: 0s - loss: 0.0205 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - 6s 560us/step - loss: 0.0202 - accuracy: 0.9942 - val_loss: 0.0237 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01438\n",
      "Epoch 33/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0037 - accuracy: 1.00 - ETA: 4s - loss: 0.0021 - accuracy: 1.00 - ETA: 5s - loss: 0.0023 - accuracy: 1.00 - ETA: 5s - loss: 0.0029 - accuracy: 1.00 - ETA: 5s - loss: 0.0026 - accuracy: 1.00 - ETA: 5s - loss: 0.0041 - accuracy: 1.00 - ETA: 4s - loss: 0.0039 - accuracy: 1.00 - ETA: 4s - loss: 0.0043 - accuracy: 1.00 - ETA: 4s - loss: 0.0041 - accuracy: 1.00 - ETA: 4s - loss: 0.0058 - accuracy: 0.99 - ETA: 4s - loss: 0.0099 - accuracy: 0.99 - ETA: 4s - loss: 0.0228 - accuracy: 0.99 - ETA: 4s - loss: 0.0309 - accuracy: 0.99 - ETA: 4s - loss: 0.0304 - accuracy: 0.99 - ETA: 4s - loss: 0.0296 - accuracy: 0.99 - ETA: 4s - loss: 0.0280 - accuracy: 0.99 - ETA: 4s - loss: 0.0270 - accuracy: 0.99 - ETA: 4s - loss: 0.0290 - accuracy: 0.99 - ETA: 4s - loss: 0.0278 - accuracy: 0.99 - ETA: 4s - loss: 0.0265 - accuracy: 0.99 - ETA: 4s - loss: 0.0286 - accuracy: 0.99 - ETA: 3s - loss: 0.0323 - accuracy: 0.99 - ETA: 3s - loss: 0.0328 - accuracy: 0.99 - ETA: 3s - loss: 0.0318 - accuracy: 0.99 - ETA: 3s - loss: 0.0315 - accuracy: 0.99 - ETA: 3s - loss: 0.0307 - accuracy: 0.99 - ETA: 3s - loss: 0.0302 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0297 - accuracy: 0.99 - ETA: 3s - loss: 0.0300 - accuracy: 0.99 - ETA: 3s - loss: 0.0291 - accuracy: 0.99 - ETA: 3s - loss: 0.0284 - accuracy: 0.99 - ETA: 3s - loss: 0.0288 - accuracy: 0.99 - ETA: 3s - loss: 0.0298 - accuracy: 0.99 - ETA: 3s - loss: 0.0290 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0279 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0265 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0264 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - 6s 561us/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.0156 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01438\n",
      "Epoch 34/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0028 - accuracy: 1.00 - ETA: 6s - loss: 0.0033 - accuracy: 1.00 - ETA: 5s - loss: 0.0036 - accuracy: 1.00 - ETA: 5s - loss: 0.0076 - accuracy: 0.99 - ETA: 5s - loss: 0.0166 - accuracy: 0.99 - ETA: 5s - loss: 0.0160 - accuracy: 0.99 - ETA: 5s - loss: 0.0226 - accuracy: 0.99 - ETA: 5s - loss: 0.0456 - accuracy: 0.98 - ETA: 5s - loss: 0.0425 - accuracy: 0.98 - ETA: 5s - loss: 0.0386 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.99 - ETA: 4s - loss: 0.0344 - accuracy: 0.99 - ETA: 4s - loss: 0.0346 - accuracy: 0.99 - ETA: 4s - loss: 0.0337 - accuracy: 0.99 - ETA: 4s - loss: 0.0315 - accuracy: 0.99 - ETA: 4s - loss: 0.0296 - accuracy: 0.99 - ETA: 4s - loss: 0.0283 - accuracy: 0.99 - ETA: 4s - loss: 0.0269 - accuracy: 0.99 - ETA: 4s - loss: 0.0257 - accuracy: 0.99 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - ETA: 4s - loss: 0.0244 - accuracy: 0.99 - ETA: 4s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0237 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 3s - loss: 0.0239 - accuracy: 0.99 - ETA: 3s - loss: 0.0233 - accuracy: 0.99 - ETA: 3s - loss: 0.0227 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0229 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0219 - accuracy: 0.99 - ETA: 3s - loss: 0.0214 - accuracy: 0.99 - ETA: 3s - loss: 0.0213 - accuracy: 0.99 - ETA: 3s - loss: 0.0219 - accuracy: 0.99 - ETA: 3s - loss: 0.0214 - accuracy: 0.99 - ETA: 2s - loss: 0.0228 - accuracy: 0.99 - ETA: 2s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0281 - accuracy: 0.99 - ETA: 2s - loss: 0.0276 - accuracy: 0.99 - ETA: 2s - loss: 0.0273 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0265 - accuracy: 0.99 - ETA: 2s - loss: 0.0261 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0265 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0257 - accuracy: 0.99 - ETA: 1s - loss: 0.0253 - accuracy: 0.99 - ETA: 1s - loss: 0.0248 - accuracy: 0.99 - ETA: 1s - loss: 0.0245 - accuracy: 0.99 - ETA: 1s - loss: 0.0246 - accuracy: 0.99 - ETA: 1s - loss: 0.0243 - accuracy: 0.99 - ETA: 1s - loss: 0.0246 - accuracy: 0.99 - ETA: 1s - loss: 0.0248 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0243 - accuracy: 0.99 - ETA: 0s - loss: 0.0241 - accuracy: 0.99 - 6s 559us/step - loss: 0.0239 - accuracy: 0.9921 - val_loss: 0.0142 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01438 to 0.01416, saving model to cnn_bee_1.h5\n",
      "Epoch 35/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0034 - accuracy: 1.00 - ETA: 4s - loss: 0.0030 - accuracy: 1.00 - ETA: 5s - loss: 0.0031 - accuracy: 1.00 - ETA: 5s - loss: 0.0049 - accuracy: 0.99 - ETA: 5s - loss: 0.0066 - accuracy: 0.99 - ETA: 4s - loss: 0.0197 - accuracy: 0.99 - ETA: 4s - loss: 0.0371 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0332 - accuracy: 0.98 - ETA: 4s - loss: 0.0333 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0332 - accuracy: 0.98 - ETA: 4s - loss: 0.0333 - accuracy: 0.98 - ETA: 4s - loss: 0.0358 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0333 - accuracy: 0.98 - ETA: 4s - loss: 0.0332 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0320 - accuracy: 0.98 - ETA: 4s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0292 - accuracy: 0.98 - ETA: 3s - loss: 0.0281 - accuracy: 0.98 - ETA: 3s - loss: 0.0274 - accuracy: 0.98 - ETA: 3s - loss: 0.0288 - accuracy: 0.98 - ETA: 3s - loss: 0.0300 - accuracy: 0.98 - ETA: 3s - loss: 0.0310 - accuracy: 0.98 - ETA: 3s - loss: 0.0304 - accuracy: 0.98 - ETA: 3s - loss: 0.0295 - accuracy: 0.98 - ETA: 3s - loss: 0.0291 - accuracy: 0.98 - ETA: 3s - loss: 0.0283 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0264 - accuracy: 0.99 - ETA: 2s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0261 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0261 - accuracy: 0.99 - ETA: 2s - loss: 0.0270 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.98 - ETA: 2s - loss: 0.0294 - accuracy: 0.99 - ETA: 2s - loss: 0.0309 - accuracy: 0.99 - ETA: 2s - loss: 0.0311 - accuracy: 0.98 - ETA: 2s - loss: 0.0312 - accuracy: 0.98 - ETA: 1s - loss: 0.0311 - accuracy: 0.98 - ETA: 1s - loss: 0.0317 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0312 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 1s - loss: 0.0302 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0285 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 0s - loss: 0.0288 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0285 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.99 - ETA: 0s - loss: 0.0280 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0274 - accuracy: 0.99 - ETA: 0s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0270 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - 6s 556us/step - loss: 0.0265 - accuracy: 0.9910 - val_loss: 0.0343 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01416\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0241 - accuracy: 0.99 - ETA: 5s - loss: 0.0141 - accuracy: 0.99 - ETA: 5s - loss: 0.0102 - accuracy: 0.99 - ETA: 5s - loss: 0.0230 - accuracy: 0.99 - ETA: 5s - loss: 0.0195 - accuracy: 0.99 - ETA: 5s - loss: 0.0181 - accuracy: 0.99 - ETA: 5s - loss: 0.0218 - accuracy: 0.99 - ETA: 5s - loss: 0.0233 - accuracy: 0.99 - ETA: 5s - loss: 0.0214 - accuracy: 0.99 - ETA: 5s - loss: 0.0204 - accuracy: 0.99 - ETA: 4s - loss: 0.0188 - accuracy: 0.99 - ETA: 4s - loss: 0.0185 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0170 - accuracy: 0.99 - ETA: 4s - loss: 0.0167 - accuracy: 0.99 - ETA: 4s - loss: 0.0162 - accuracy: 0.99 - ETA: 4s - loss: 0.0154 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0210 - accuracy: 0.99 - ETA: 4s - loss: 0.0206 - accuracy: 0.99 - ETA: 4s - loss: 0.0215 - accuracy: 0.99 - ETA: 4s - loss: 0.0213 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 3s - loss: 0.0288 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0277 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0261 - accuracy: 0.99 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 3s - loss: 0.0270 - accuracy: 0.99 - ETA: 3s - loss: 0.0265 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0232 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0224 - accuracy: 0.99 - ETA: 2s - loss: 0.0222 - accuracy: 0.99 - ETA: 2s - loss: 0.0222 - accuracy: 0.99 - ETA: 2s - loss: 0.0220 - accuracy: 0.99 - ETA: 2s - loss: 0.0216 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0228 - accuracy: 0.99 - ETA: 1s - loss: 0.0224 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0216 - accuracy: 0.99 - ETA: 1s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0216 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0219 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0258 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0245 - accuracy: 0.99 - ETA: 0s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0245 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - 6s 553us/step - loss: 0.0248 - accuracy: 0.9933 - val_loss: 0.0315 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01416\n",
      "Epoch 37/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0550 - accuracy: 0.98 - ETA: 5s - loss: 0.0297 - accuracy: 0.99 - ETA: 5s - loss: 0.0260 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 5s - loss: 0.0216 - accuracy: 0.99 - ETA: 5s - loss: 0.0183 - accuracy: 0.99 - ETA: 5s - loss: 0.0164 - accuracy: 0.99 - ETA: 5s - loss: 0.0145 - accuracy: 0.99 - ETA: 5s - loss: 0.0135 - accuracy: 0.99 - ETA: 4s - loss: 0.0154 - accuracy: 0.99 - ETA: 4s - loss: 0.0143 - accuracy: 0.99 - ETA: 4s - loss: 0.0135 - accuracy: 0.99 - ETA: 4s - loss: 0.0130 - accuracy: 0.99 - ETA: 4s - loss: 0.0127 - accuracy: 0.99 - ETA: 4s - loss: 0.0139 - accuracy: 0.99 - ETA: 4s - loss: 0.0131 - accuracy: 0.99 - ETA: 4s - loss: 0.0125 - accuracy: 0.99 - ETA: 4s - loss: 0.0125 - accuracy: 0.99 - ETA: 4s - loss: 0.0120 - accuracy: 0.99 - ETA: 4s - loss: 0.0118 - accuracy: 0.99 - ETA: 4s - loss: 0.0113 - accuracy: 0.99 - ETA: 4s - loss: 0.0116 - accuracy: 0.99 - ETA: 3s - loss: 0.0113 - accuracy: 0.99 - ETA: 3s - loss: 0.0124 - accuracy: 0.99 - ETA: 3s - loss: 0.0164 - accuracy: 0.99 - ETA: 3s - loss: 0.0168 - accuracy: 0.99 - ETA: 3s - loss: 0.0165 - accuracy: 0.99 - ETA: 3s - loss: 0.0163 - accuracy: 0.99 - ETA: 3s - loss: 0.0158 - accuracy: 0.99 - ETA: 3s - loss: 0.0160 - accuracy: 0.99 - ETA: 3s - loss: 0.0166 - accuracy: 0.99 - ETA: 3s - loss: 0.0163 - accuracy: 0.99 - ETA: 3s - loss: 0.0160 - accuracy: 0.99 - ETA: 3s - loss: 0.0156 - accuracy: 0.99 - ETA: 3s - loss: 0.0153 - accuracy: 0.99 - ETA: 2s - loss: 0.0150 - accuracy: 0.99 - ETA: 2s - loss: 0.0153 - accuracy: 0.99 - ETA: 2s - loss: 0.0164 - accuracy: 0.99 - ETA: 2s - loss: 0.0161 - accuracy: 0.99 - ETA: 2s - loss: 0.0160 - accuracy: 0.99 - ETA: 2s - loss: 0.0158 - accuracy: 0.99 - ETA: 2s - loss: 0.0162 - accuracy: 0.99 - ETA: 2s - loss: 0.0168 - accuracy: 0.99 - ETA: 2s - loss: 0.0178 - accuracy: 0.99 - ETA: 2s - loss: 0.0175 - accuracy: 0.99 - ETA: 2s - loss: 0.0171 - accuracy: 0.99 - ETA: 2s - loss: 0.0189 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0201 - accuracy: 0.99 - ETA: 1s - loss: 0.0208 - accuracy: 0.99 - ETA: 1s - loss: 0.0208 - accuracy: 0.99 - ETA: 1s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0212 - accuracy: 0.99 - ETA: 1s - loss: 0.0210 - accuracy: 0.99 - ETA: 1s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0213 - accuracy: 0.99 - ETA: 0s - loss: 0.0213 - accuracy: 0.99 - ETA: 0s - loss: 0.0211 - accuracy: 0.99 - ETA: 0s - loss: 0.0214 - accuracy: 0.99 - ETA: 0s - loss: 0.0211 - accuracy: 0.99 - ETA: 0s - loss: 0.0211 - accuracy: 0.99 - ETA: 0s - loss: 0.0212 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0225 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0225 - accuracy: 0.99 - 6s 564us/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 0.0332 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01416\n",
      "Epoch 38/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0189 - accuracy: 0.99 - ETA: 5s - loss: 0.0097 - accuracy: 0.99 - ETA: 5s - loss: 0.0096 - accuracy: 0.99 - ETA: 5s - loss: 0.0075 - accuracy: 0.99 - ETA: 5s - loss: 0.0084 - accuracy: 0.99 - ETA: 5s - loss: 0.0081 - accuracy: 0.99 - ETA: 5s - loss: 0.0085 - accuracy: 0.99 - ETA: 5s - loss: 0.0107 - accuracy: 0.99 - ETA: 5s - loss: 0.0110 - accuracy: 0.99 - ETA: 4s - loss: 0.0102 - accuracy: 0.99 - ETA: 4s - loss: 0.0104 - accuracy: 0.99 - ETA: 4s - loss: 0.0097 - accuracy: 0.99 - ETA: 4s - loss: 0.0093 - accuracy: 0.99 - ETA: 4s - loss: 0.0093 - accuracy: 0.99 - ETA: 4s - loss: 0.0102 - accuracy: 0.99 - ETA: 4s - loss: 0.0096 - accuracy: 0.99 - ETA: 4s - loss: 0.0091 - accuracy: 0.99 - ETA: 4s - loss: 0.0093 - accuracy: 0.99 - ETA: 4s - loss: 0.0100 - accuracy: 0.99 - ETA: 4s - loss: 0.0112 - accuracy: 0.99 - ETA: 4s - loss: 0.0126 - accuracy: 0.99 - ETA: 4s - loss: 0.0128 - accuracy: 0.99 - ETA: 3s - loss: 0.0127 - accuracy: 0.99 - ETA: 3s - loss: 0.0124 - accuracy: 0.99 - ETA: 3s - loss: 0.0124 - accuracy: 0.99 - ETA: 3s - loss: 0.0128 - accuracy: 0.99 - ETA: 3s - loss: 0.0123 - accuracy: 0.99 - ETA: 3s - loss: 0.0144 - accuracy: 0.99 - ETA: 3s - loss: 0.0140 - accuracy: 0.99 - ETA: 3s - loss: 0.0144 - accuracy: 0.99 - ETA: 3s - loss: 0.0149 - accuracy: 0.99 - ETA: 3s - loss: 0.0160 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0191 - accuracy: 0.99 - ETA: 3s - loss: 0.0190 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 2s - loss: 0.0190 - accuracy: 0.99 - ETA: 2s - loss: 0.0186 - accuracy: 0.99 - ETA: 2s - loss: 0.0186 - accuracy: 0.99 - ETA: 2s - loss: 0.0183 - accuracy: 0.99 - ETA: 2s - loss: 0.0179 - accuracy: 0.99 - ETA: 2s - loss: 0.0176 - accuracy: 0.99 - ETA: 2s - loss: 0.0173 - accuracy: 0.99 - ETA: 2s - loss: 0.0170 - accuracy: 0.99 - ETA: 2s - loss: 0.0167 - accuracy: 0.99 - ETA: 2s - loss: 0.0164 - accuracy: 0.99 - ETA: 2s - loss: 0.0161 - accuracy: 0.99 - ETA: 2s - loss: 0.0158 - accuracy: 0.99 - ETA: 1s - loss: 0.0162 - accuracy: 0.99 - ETA: 1s - loss: 0.0159 - accuracy: 0.99 - ETA: 1s - loss: 0.0162 - accuracy: 0.99 - ETA: 1s - loss: 0.0161 - accuracy: 0.99 - ETA: 1s - loss: 0.0158 - accuracy: 0.99 - ETA: 1s - loss: 0.0156 - accuracy: 0.99 - ETA: 1s - loss: 0.0159 - accuracy: 0.99 - ETA: 1s - loss: 0.0172 - accuracy: 0.99 - ETA: 1s - loss: 0.0173 - accuracy: 0.99 - ETA: 1s - loss: 0.0170 - accuracy: 0.99 - ETA: 1s - loss: 0.0168 - accuracy: 0.99 - ETA: 1s - loss: 0.0168 - accuracy: 0.99 - ETA: 0s - loss: 0.0166 - accuracy: 0.99 - ETA: 0s - loss: 0.0163 - accuracy: 0.99 - ETA: 0s - loss: 0.0164 - accuracy: 0.99 - ETA: 0s - loss: 0.0171 - accuracy: 0.99 - ETA: 0s - loss: 0.0180 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0190 - accuracy: 0.99 - ETA: 0s - loss: 0.0189 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0180 - accuracy: 0.99 - 6s 607us/step - loss: 0.0179 - accuracy: 0.9940 - val_loss: 0.0169 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01416\n",
      "Epoch 39/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 6.2207e-04 - accuracy: 1.00 - ETA: 5s - loss: 0.0038 - accuracy: 1.0000   - ETA: 5s - loss: 0.0044 - accuracy: 1.00 - ETA: 5s - loss: 0.0037 - accuracy: 1.00 - ETA: 5s - loss: 0.0046 - accuracy: 0.99 - ETA: 5s - loss: 0.0156 - accuracy: 0.99 - ETA: 5s - loss: 0.0138 - accuracy: 0.99 - ETA: 5s - loss: 0.0131 - accuracy: 0.99 - ETA: 5s - loss: 0.0121 - accuracy: 0.99 - ETA: 5s - loss: 0.0129 - accuracy: 0.99 - ETA: 4s - loss: 0.0120 - accuracy: 0.99 - ETA: 4s - loss: 0.0144 - accuracy: 0.99 - ETA: 4s - loss: 0.0219 - accuracy: 0.99 - ETA: 4s - loss: 0.0222 - accuracy: 0.99 - ETA: 4s - loss: 0.0218 - accuracy: 0.99 - ETA: 4s - loss: 0.0206 - accuracy: 0.99 - ETA: 4s - loss: 0.0203 - accuracy: 0.99 - ETA: 4s - loss: 0.0203 - accuracy: 0.99 - ETA: 4s - loss: 0.0194 - accuracy: 0.99 - ETA: 4s - loss: 0.0186 - accuracy: 0.99 - ETA: 4s - loss: 0.0184 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 3s - loss: 0.0177 - accuracy: 0.99 - ETA: 3s - loss: 0.0170 - accuracy: 0.99 - ETA: 3s - loss: 0.0165 - accuracy: 0.99 - ETA: 3s - loss: 0.0164 - accuracy: 0.99 - ETA: 3s - loss: 0.0185 - accuracy: 0.99 - ETA: 3s - loss: 0.0190 - accuracy: 0.99 - ETA: 3s - loss: 0.0190 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0201 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0200 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0206 - accuracy: 0.99 - ETA: 2s - loss: 0.0210 - accuracy: 0.99 - ETA: 2s - loss: 0.0212 - accuracy: 0.99 - ETA: 2s - loss: 0.0214 - accuracy: 0.99 - ETA: 2s - loss: 0.0210 - accuracy: 0.99 - ETA: 2s - loss: 0.0208 - accuracy: 0.99 - ETA: 2s - loss: 0.0206 - accuracy: 0.99 - ETA: 2s - loss: 0.0204 - accuracy: 0.99 - ETA: 2s - loss: 0.0206 - accuracy: 0.99 - ETA: 2s - loss: 0.0210 - accuracy: 0.99 - ETA: 2s - loss: 0.0208 - accuracy: 0.99 - ETA: 1s - loss: 0.0204 - accuracy: 0.99 - ETA: 1s - loss: 0.0205 - accuracy: 0.99 - ETA: 1s - loss: 0.0205 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 1s - loss: 0.0203 - accuracy: 0.99 - ETA: 1s - loss: 0.0201 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0210 - accuracy: 0.99 - ETA: 1s - loss: 0.0217 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0217 - accuracy: 0.99 - ETA: 1s - loss: 0.0217 - accuracy: 0.99 - ETA: 0s - loss: 0.0214 - accuracy: 0.99 - ETA: 0s - loss: 0.0214 - accuracy: 0.99 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0229 - accuracy: 0.99 - ETA: 0s - loss: 0.0227 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0216 - accuracy: 0.99 - ETA: 0s - loss: 0.0214 - accuracy: 0.99 - ETA: 0s - loss: 0.0212 - accuracy: 0.99 - 6s 584us/step - loss: 0.0211 - accuracy: 0.9941 - val_loss: 0.0202 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01416\n",
      "Epoch 00039: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 220us/step\n",
      "[2020-05-18 16:22:44 RAM67.4% 1.13GB] Val Score : [0.00858533431334037, 0.9954510927200317]\n",
      "[2020-05-18 16:22:44 RAM67.4% 1.13GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:22:44 RAM67.4% 1.13GB] Training on Fold : 3\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 25s - loss: 1.8623 - accuracy: 0.441 - ETA: 15s - loss: 1.8800 - accuracy: 0.513 - ETA: 12s - loss: 1.4933 - accuracy: 0.542 - ETA: 10s - loss: 1.2951 - accuracy: 0.562 - ETA: 9s - loss: 1.1890 - accuracy: 0.554 - ETA: 8s - loss: 1.1086 - accuracy: 0.55 - ETA: 7s - loss: 1.0463 - accuracy: 0.56 - ETA: 7s - loss: 1.0018 - accuracy: 0.56 - ETA: 7s - loss: 0.9655 - accuracy: 0.57 - ETA: 6s - loss: 0.9414 - accuracy: 0.57 - ETA: 6s - loss: 0.9192 - accuracy: 0.57 - ETA: 6s - loss: 0.8977 - accuracy: 0.58 - ETA: 6s - loss: 0.8846 - accuracy: 0.58 - ETA: 6s - loss: 0.8732 - accuracy: 0.57 - ETA: 5s - loss: 0.8618 - accuracy: 0.57 - ETA: 5s - loss: 0.8503 - accuracy: 0.57 - ETA: 5s - loss: 0.8382 - accuracy: 0.58 - ETA: 5s - loss: 0.8307 - accuracy: 0.58 - ETA: 5s - loss: 0.8206 - accuracy: 0.58 - ETA: 5s - loss: 0.8103 - accuracy: 0.58 - ETA: 5s - loss: 0.8032 - accuracy: 0.58 - ETA: 4s - loss: 0.7977 - accuracy: 0.58 - ETA: 4s - loss: 0.7927 - accuracy: 0.58 - ETA: 4s - loss: 0.7867 - accuracy: 0.59 - ETA: 4s - loss: 0.7811 - accuracy: 0.59 - ETA: 4s - loss: 0.7753 - accuracy: 0.59 - ETA: 4s - loss: 0.7707 - accuracy: 0.59 - ETA: 4s - loss: 0.7658 - accuracy: 0.59 - ETA: 4s - loss: 0.7618 - accuracy: 0.59 - ETA: 4s - loss: 0.7571 - accuracy: 0.60 - ETA: 3s - loss: 0.7534 - accuracy: 0.60 - ETA: 3s - loss: 0.7495 - accuracy: 0.60 - ETA: 3s - loss: 0.7448 - accuracy: 0.60 - ETA: 3s - loss: 0.7403 - accuracy: 0.61 - ETA: 3s - loss: 0.7448 - accuracy: 0.61 - ETA: 3s - loss: 0.7422 - accuracy: 0.60 - ETA: 3s - loss: 0.7386 - accuracy: 0.61 - ETA: 3s - loss: 0.7342 - accuracy: 0.61 - ETA: 3s - loss: 0.7286 - accuracy: 0.61 - ETA: 3s - loss: 0.7237 - accuracy: 0.62 - ETA: 2s - loss: 0.7190 - accuracy: 0.62 - ETA: 2s - loss: 0.7200 - accuracy: 0.62 - ETA: 2s - loss: 0.7196 - accuracy: 0.62 - ETA: 2s - loss: 0.7154 - accuracy: 0.62 - ETA: 2s - loss: 0.7108 - accuracy: 0.62 - ETA: 2s - loss: 0.7060 - accuracy: 0.62 - ETA: 2s - loss: 0.7011 - accuracy: 0.63 - ETA: 2s - loss: 0.6948 - accuracy: 0.63 - ETA: 2s - loss: 0.6913 - accuracy: 0.63 - ETA: 2s - loss: 0.6875 - accuracy: 0.64 - ETA: 2s - loss: 0.6867 - accuracy: 0.64 - ETA: 1s - loss: 0.6814 - accuracy: 0.64 - ETA: 1s - loss: 0.6759 - accuracy: 0.64 - ETA: 1s - loss: 0.6703 - accuracy: 0.65 - ETA: 1s - loss: 0.6687 - accuracy: 0.65 - ETA: 1s - loss: 0.6638 - accuracy: 0.65 - ETA: 1s - loss: 0.6596 - accuracy: 0.65 - ETA: 1s - loss: 0.6549 - accuracy: 0.66 - ETA: 1s - loss: 0.6496 - accuracy: 0.66 - ETA: 1s - loss: 0.6443 - accuracy: 0.66 - ETA: 1s - loss: 0.6407 - accuracy: 0.67 - ETA: 1s - loss: 0.6360 - accuracy: 0.67 - ETA: 0s - loss: 0.6306 - accuracy: 0.67 - ETA: 0s - loss: 0.6247 - accuracy: 0.68 - ETA: 0s - loss: 0.6233 - accuracy: 0.68 - ETA: 0s - loss: 0.6269 - accuracy: 0.68 - ETA: 0s - loss: 0.6232 - accuracy: 0.68 - ETA: 0s - loss: 0.6189 - accuracy: 0.68 - ETA: 0s - loss: 0.6143 - accuracy: 0.68 - ETA: 0s - loss: 0.6097 - accuracy: 0.69 - ETA: 0s - loss: 0.6052 - accuracy: 0.69 - ETA: 0s - loss: 0.6011 - accuracy: 0.69 - ETA: 0s - loss: 0.5969 - accuracy: 0.69 - 7s 626us/step - loss: 0.5946 - accuracy: 0.7010 - val_loss: 0.3130 - val_accuracy: 0.8475\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.01416\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.3468 - accuracy: 0.84 - ETA: 6s - loss: 0.3154 - accuracy: 0.86 - ETA: 6s - loss: 0.2826 - accuracy: 0.87 - ETA: 6s - loss: 0.2786 - accuracy: 0.88 - ETA: 5s - loss: 0.3025 - accuracy: 0.86 - ETA: 5s - loss: 0.2951 - accuracy: 0.87 - ETA: 5s - loss: 0.2907 - accuracy: 0.87 - ETA: 5s - loss: 0.2950 - accuracy: 0.88 - ETA: 5s - loss: 0.2863 - accuracy: 0.88 - ETA: 5s - loss: 0.2763 - accuracy: 0.88 - ETA: 5s - loss: 0.2755 - accuracy: 0.88 - ETA: 5s - loss: 0.2740 - accuracy: 0.88 - ETA: 5s - loss: 0.2764 - accuracy: 0.88 - ETA: 5s - loss: 0.2763 - accuracy: 0.87 - ETA: 4s - loss: 0.2822 - accuracy: 0.87 - ETA: 4s - loss: 0.2866 - accuracy: 0.87 - ETA: 4s - loss: 0.2879 - accuracy: 0.87 - ETA: 4s - loss: 0.2833 - accuracy: 0.87 - ETA: 4s - loss: 0.2829 - accuracy: 0.87 - ETA: 4s - loss: 0.2812 - accuracy: 0.87 - ETA: 4s - loss: 0.2776 - accuracy: 0.87 - ETA: 4s - loss: 0.2745 - accuracy: 0.88 - ETA: 4s - loss: 0.2738 - accuracy: 0.88 - ETA: 4s - loss: 0.2752 - accuracy: 0.88 - ETA: 4s - loss: 0.2713 - accuracy: 0.88 - ETA: 3s - loss: 0.2672 - accuracy: 0.88 - ETA: 3s - loss: 0.2646 - accuracy: 0.88 - ETA: 3s - loss: 0.2603 - accuracy: 0.88 - ETA: 3s - loss: 0.2575 - accuracy: 0.89 - ETA: 3s - loss: 0.2574 - accuracy: 0.89 - ETA: 3s - loss: 0.2683 - accuracy: 0.88 - ETA: 3s - loss: 0.2795 - accuracy: 0.88 - ETA: 3s - loss: 0.2773 - accuracy: 0.88 - ETA: 3s - loss: 0.2760 - accuracy: 0.88 - ETA: 3s - loss: 0.2744 - accuracy: 0.88 - ETA: 3s - loss: 0.2715 - accuracy: 0.88 - ETA: 3s - loss: 0.2678 - accuracy: 0.88 - ETA: 3s - loss: 0.2664 - accuracy: 0.88 - ETA: 2s - loss: 0.2641 - accuracy: 0.88 - ETA: 2s - loss: 0.2627 - accuracy: 0.88 - ETA: 2s - loss: 0.2619 - accuracy: 0.89 - ETA: 2s - loss: 0.2604 - accuracy: 0.89 - ETA: 2s - loss: 0.2581 - accuracy: 0.89 - ETA: 2s - loss: 0.2572 - accuracy: 0.89 - ETA: 2s - loss: 0.2566 - accuracy: 0.89 - ETA: 2s - loss: 0.2577 - accuracy: 0.89 - ETA: 2s - loss: 0.2565 - accuracy: 0.89 - ETA: 2s - loss: 0.2551 - accuracy: 0.89 - ETA: 2s - loss: 0.2546 - accuracy: 0.89 - ETA: 1s - loss: 0.2525 - accuracy: 0.89 - ETA: 1s - loss: 0.2511 - accuracy: 0.89 - ETA: 1s - loss: 0.2494 - accuracy: 0.89 - ETA: 1s - loss: 0.2477 - accuracy: 0.89 - ETA: 1s - loss: 0.2461 - accuracy: 0.89 - ETA: 1s - loss: 0.2445 - accuracy: 0.89 - ETA: 1s - loss: 0.2431 - accuracy: 0.89 - ETA: 1s - loss: 0.2407 - accuracy: 0.89 - ETA: 1s - loss: 0.2389 - accuracy: 0.90 - ETA: 1s - loss: 0.2391 - accuracy: 0.90 - ETA: 1s - loss: 0.2373 - accuracy: 0.90 - ETA: 1s - loss: 0.2355 - accuracy: 0.90 - ETA: 0s - loss: 0.2341 - accuracy: 0.90 - ETA: 0s - loss: 0.2334 - accuracy: 0.90 - ETA: 0s - loss: 0.2323 - accuracy: 0.90 - ETA: 0s - loss: 0.2335 - accuracy: 0.90 - ETA: 0s - loss: 0.2360 - accuracy: 0.90 - ETA: 0s - loss: 0.2373 - accuracy: 0.90 - ETA: 0s - loss: 0.2378 - accuracy: 0.90 - ETA: 0s - loss: 0.2358 - accuracy: 0.90 - ETA: 0s - loss: 0.2343 - accuracy: 0.90 - ETA: 0s - loss: 0.2323 - accuracy: 0.90 - ETA: 0s - loss: 0.2312 - accuracy: 0.90 - ETA: 0s - loss: 0.2301 - accuracy: 0.90 - 6s 582us/step - loss: 0.2303 - accuracy: 0.9065 - val_loss: 0.1686 - val_accuracy: 0.9393\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.01416\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1810 - accuracy: 0.93 - ETA: 5s - loss: 0.1958 - accuracy: 0.92 - ETA: 5s - loss: 0.1763 - accuracy: 0.93 - ETA: 5s - loss: 0.1692 - accuracy: 0.93 - ETA: 5s - loss: 0.1618 - accuracy: 0.93 - ETA: 5s - loss: 0.1727 - accuracy: 0.92 - ETA: 5s - loss: 0.1689 - accuracy: 0.93 - ETA: 5s - loss: 0.1704 - accuracy: 0.93 - ETA: 5s - loss: 0.1711 - accuracy: 0.93 - ETA: 4s - loss: 0.1656 - accuracy: 0.93 - ETA: 4s - loss: 0.1673 - accuracy: 0.93 - ETA: 4s - loss: 0.1757 - accuracy: 0.93 - ETA: 4s - loss: 0.1868 - accuracy: 0.92 - ETA: 4s - loss: 0.1915 - accuracy: 0.92 - ETA: 4s - loss: 0.1908 - accuracy: 0.92 - ETA: 4s - loss: 0.1847 - accuracy: 0.92 - ETA: 4s - loss: 0.1854 - accuracy: 0.92 - ETA: 4s - loss: 0.1844 - accuracy: 0.92 - ETA: 4s - loss: 0.1820 - accuracy: 0.92 - ETA: 4s - loss: 0.1803 - accuracy: 0.92 - ETA: 4s - loss: 0.1802 - accuracy: 0.92 - ETA: 4s - loss: 0.1773 - accuracy: 0.93 - ETA: 3s - loss: 0.1773 - accuracy: 0.93 - ETA: 3s - loss: 0.1746 - accuracy: 0.93 - ETA: 3s - loss: 0.1725 - accuracy: 0.93 - ETA: 3s - loss: 0.1715 - accuracy: 0.93 - ETA: 3s - loss: 0.1720 - accuracy: 0.93 - ETA: 3s - loss: 0.1728 - accuracy: 0.93 - ETA: 3s - loss: 0.1715 - accuracy: 0.93 - ETA: 3s - loss: 0.1726 - accuracy: 0.93 - ETA: 3s - loss: 0.1742 - accuracy: 0.92 - ETA: 3s - loss: 0.1751 - accuracy: 0.93 - ETA: 3s - loss: 0.1736 - accuracy: 0.93 - ETA: 3s - loss: 0.1710 - accuracy: 0.93 - ETA: 3s - loss: 0.1698 - accuracy: 0.93 - ETA: 2s - loss: 0.1730 - accuracy: 0.93 - ETA: 2s - loss: 0.1719 - accuracy: 0.93 - ETA: 2s - loss: 0.1713 - accuracy: 0.93 - ETA: 2s - loss: 0.1692 - accuracy: 0.93 - ETA: 2s - loss: 0.1677 - accuracy: 0.93 - ETA: 2s - loss: 0.1661 - accuracy: 0.93 - ETA: 2s - loss: 0.1652 - accuracy: 0.93 - ETA: 2s - loss: 0.1636 - accuracy: 0.93 - ETA: 2s - loss: 0.1622 - accuracy: 0.93 - ETA: 2s - loss: 0.1633 - accuracy: 0.93 - ETA: 2s - loss: 0.1662 - accuracy: 0.93 - ETA: 2s - loss: 0.1711 - accuracy: 0.93 - ETA: 2s - loss: 0.1699 - accuracy: 0.93 - ETA: 1s - loss: 0.1691 - accuracy: 0.93 - ETA: 1s - loss: 0.1680 - accuracy: 0.93 - ETA: 1s - loss: 0.1667 - accuracy: 0.93 - ETA: 1s - loss: 0.1658 - accuracy: 0.93 - ETA: 1s - loss: 0.1655 - accuracy: 0.93 - ETA: 1s - loss: 0.1641 - accuracy: 0.93 - ETA: 1s - loss: 0.1634 - accuracy: 0.93 - ETA: 1s - loss: 0.1625 - accuracy: 0.93 - ETA: 1s - loss: 0.1611 - accuracy: 0.93 - ETA: 1s - loss: 0.1609 - accuracy: 0.93 - ETA: 1s - loss: 0.1608 - accuracy: 0.93 - ETA: 1s - loss: 0.1620 - accuracy: 0.93 - ETA: 1s - loss: 0.1658 - accuracy: 0.93 - ETA: 0s - loss: 0.1657 - accuracy: 0.93 - ETA: 0s - loss: 0.1649 - accuracy: 0.93 - ETA: 0s - loss: 0.1633 - accuracy: 0.93 - ETA: 0s - loss: 0.1619 - accuracy: 0.93 - ETA: 0s - loss: 0.1608 - accuracy: 0.93 - ETA: 0s - loss: 0.1595 - accuracy: 0.93 - ETA: 0s - loss: 0.1585 - accuracy: 0.93 - ETA: 0s - loss: 0.1591 - accuracy: 0.93 - ETA: 0s - loss: 0.1595 - accuracy: 0.93 - ETA: 0s - loss: 0.1607 - accuracy: 0.93 - ETA: 0s - loss: 0.1601 - accuracy: 0.93 - ETA: 0s - loss: 0.1592 - accuracy: 0.93 - 6s 571us/step - loss: 0.1585 - accuracy: 0.9363 - val_loss: 0.0783 - val_accuracy: 0.9789\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01416\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0793 - accuracy: 0.97 - ETA: 5s - loss: 0.1150 - accuracy: 0.95 - ETA: 5s - loss: 0.1136 - accuracy: 0.95 - ETA: 5s - loss: 0.1042 - accuracy: 0.95 - ETA: 5s - loss: 0.0991 - accuracy: 0.96 - ETA: 5s - loss: 0.1007 - accuracy: 0.96 - ETA: 5s - loss: 0.0970 - accuracy: 0.96 - ETA: 5s - loss: 0.1031 - accuracy: 0.96 - ETA: 5s - loss: 0.1073 - accuracy: 0.95 - ETA: 5s - loss: 0.1182 - accuracy: 0.95 - ETA: 5s - loss: 0.1200 - accuracy: 0.95 - ETA: 4s - loss: 0.1233 - accuracy: 0.95 - ETA: 4s - loss: 0.1208 - accuracy: 0.95 - ETA: 4s - loss: 0.1181 - accuracy: 0.95 - ETA: 4s - loss: 0.1199 - accuracy: 0.95 - ETA: 4s - loss: 0.1187 - accuracy: 0.95 - ETA: 4s - loss: 0.1186 - accuracy: 0.95 - ETA: 4s - loss: 0.1159 - accuracy: 0.95 - ETA: 4s - loss: 0.1184 - accuracy: 0.95 - ETA: 4s - loss: 0.1169 - accuracy: 0.95 - ETA: 4s - loss: 0.1158 - accuracy: 0.95 - ETA: 4s - loss: 0.1138 - accuracy: 0.95 - ETA: 4s - loss: 0.1114 - accuracy: 0.95 - ETA: 3s - loss: 0.1090 - accuracy: 0.95 - ETA: 3s - loss: 0.1093 - accuracy: 0.95 - ETA: 3s - loss: 0.1088 - accuracy: 0.95 - ETA: 3s - loss: 0.1084 - accuracy: 0.95 - ETA: 3s - loss: 0.1084 - accuracy: 0.95 - ETA: 3s - loss: 0.1097 - accuracy: 0.95 - ETA: 3s - loss: 0.1131 - accuracy: 0.95 - ETA: 3s - loss: 0.1172 - accuracy: 0.95 - ETA: 3s - loss: 0.1162 - accuracy: 0.95 - ETA: 3s - loss: 0.1155 - accuracy: 0.95 - ETA: 3s - loss: 0.1155 - accuracy: 0.95 - ETA: 3s - loss: 0.1142 - accuracy: 0.95 - ETA: 2s - loss: 0.1127 - accuracy: 0.95 - ETA: 2s - loss: 0.1131 - accuracy: 0.95 - ETA: 2s - loss: 0.1136 - accuracy: 0.95 - ETA: 2s - loss: 0.1139 - accuracy: 0.95 - ETA: 2s - loss: 0.1139 - accuracy: 0.95 - ETA: 2s - loss: 0.1132 - accuracy: 0.95 - ETA: 2s - loss: 0.1119 - accuracy: 0.95 - ETA: 2s - loss: 0.1116 - accuracy: 0.95 - ETA: 2s - loss: 0.1113 - accuracy: 0.95 - ETA: 2s - loss: 0.1127 - accuracy: 0.95 - ETA: 2s - loss: 0.1133 - accuracy: 0.95 - ETA: 2s - loss: 0.1140 - accuracy: 0.95 - ETA: 2s - loss: 0.1131 - accuracy: 0.95 - ETA: 1s - loss: 0.1125 - accuracy: 0.95 - ETA: 1s - loss: 0.1129 - accuracy: 0.95 - ETA: 1s - loss: 0.1130 - accuracy: 0.95 - ETA: 1s - loss: 0.1126 - accuracy: 0.95 - ETA: 1s - loss: 0.1112 - accuracy: 0.95 - ETA: 1s - loss: 0.1104 - accuracy: 0.95 - ETA: 1s - loss: 0.1154 - accuracy: 0.95 - ETA: 1s - loss: 0.1162 - accuracy: 0.95 - ETA: 1s - loss: 0.1178 - accuracy: 0.95 - ETA: 1s - loss: 0.1170 - accuracy: 0.95 - ETA: 1s - loss: 0.1166 - accuracy: 0.95 - ETA: 1s - loss: 0.1157 - accuracy: 0.95 - ETA: 1s - loss: 0.1148 - accuracy: 0.95 - ETA: 0s - loss: 0.1140 - accuracy: 0.95 - ETA: 0s - loss: 0.1133 - accuracy: 0.95 - ETA: 0s - loss: 0.1124 - accuracy: 0.95 - ETA: 0s - loss: 0.1116 - accuracy: 0.95 - ETA: 0s - loss: 0.1114 - accuracy: 0.95 - ETA: 0s - loss: 0.1105 - accuracy: 0.95 - ETA: 0s - loss: 0.1104 - accuracy: 0.95 - ETA: 0s - loss: 0.1120 - accuracy: 0.95 - ETA: 0s - loss: 0.1129 - accuracy: 0.95 - ETA: 0s - loss: 0.1174 - accuracy: 0.95 - ETA: 0s - loss: 0.1181 - accuracy: 0.95 - ETA: 0s - loss: 0.1183 - accuracy: 0.95 - 6s 562us/step - loss: 0.1177 - accuracy: 0.9563 - val_loss: 0.0646 - val_accuracy: 0.9806\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01416\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1183 - accuracy: 0.94 - ETA: 5s - loss: 0.1150 - accuracy: 0.94 - ETA: 5s - loss: 0.0988 - accuracy: 0.95 - ETA: 5s - loss: 0.0922 - accuracy: 0.95 - ETA: 5s - loss: 0.0835 - accuracy: 0.96 - ETA: 5s - loss: 0.0784 - accuracy: 0.96 - ETA: 5s - loss: 0.0734 - accuracy: 0.96 - ETA: 5s - loss: 0.0748 - accuracy: 0.96 - ETA: 5s - loss: 0.0724 - accuracy: 0.96 - ETA: 5s - loss: 0.0697 - accuracy: 0.96 - ETA: 4s - loss: 0.0717 - accuracy: 0.96 - ETA: 4s - loss: 0.0800 - accuracy: 0.96 - ETA: 4s - loss: 0.0770 - accuracy: 0.96 - ETA: 4s - loss: 0.0804 - accuracy: 0.96 - ETA: 4s - loss: 0.0841 - accuracy: 0.96 - ETA: 4s - loss: 0.0861 - accuracy: 0.96 - ETA: 4s - loss: 0.0839 - accuracy: 0.96 - ETA: 4s - loss: 0.0815 - accuracy: 0.96 - ETA: 4s - loss: 0.0815 - accuracy: 0.96 - ETA: 4s - loss: 0.0803 - accuracy: 0.96 - ETA: 4s - loss: 0.0787 - accuracy: 0.96 - ETA: 4s - loss: 0.0776 - accuracy: 0.96 - ETA: 3s - loss: 0.0755 - accuracy: 0.96 - ETA: 3s - loss: 0.0748 - accuracy: 0.96 - ETA: 3s - loss: 0.0744 - accuracy: 0.96 - ETA: 3s - loss: 0.0742 - accuracy: 0.96 - ETA: 3s - loss: 0.0739 - accuracy: 0.96 - ETA: 3s - loss: 0.0738 - accuracy: 0.96 - ETA: 3s - loss: 0.0743 - accuracy: 0.96 - ETA: 3s - loss: 0.0759 - accuracy: 0.96 - ETA: 3s - loss: 0.0810 - accuracy: 0.96 - ETA: 3s - loss: 0.0937 - accuracy: 0.96 - ETA: 3s - loss: 0.1103 - accuracy: 0.95 - ETA: 3s - loss: 0.1116 - accuracy: 0.95 - ETA: 3s - loss: 0.1101 - accuracy: 0.95 - ETA: 2s - loss: 0.1086 - accuracy: 0.95 - ETA: 2s - loss: 0.1083 - accuracy: 0.95 - ETA: 2s - loss: 0.1074 - accuracy: 0.95 - ETA: 2s - loss: 0.1070 - accuracy: 0.95 - ETA: 2s - loss: 0.1059 - accuracy: 0.95 - ETA: 2s - loss: 0.1043 - accuracy: 0.95 - ETA: 2s - loss: 0.1027 - accuracy: 0.96 - ETA: 2s - loss: 0.1024 - accuracy: 0.96 - ETA: 2s - loss: 0.1016 - accuracy: 0.96 - ETA: 2s - loss: 0.1011 - accuracy: 0.96 - ETA: 2s - loss: 0.0993 - accuracy: 0.96 - ETA: 2s - loss: 0.0982 - accuracy: 0.96 - ETA: 2s - loss: 0.0981 - accuracy: 0.96 - ETA: 1s - loss: 0.0975 - accuracy: 0.96 - ETA: 1s - loss: 0.0973 - accuracy: 0.96 - ETA: 1s - loss: 0.0961 - accuracy: 0.96 - ETA: 1s - loss: 0.0953 - accuracy: 0.96 - ETA: 1s - loss: 0.0959 - accuracy: 0.96 - ETA: 1s - loss: 0.0952 - accuracy: 0.96 - ETA: 1s - loss: 0.0944 - accuracy: 0.96 - ETA: 1s - loss: 0.0934 - accuracy: 0.96 - ETA: 1s - loss: 0.0921 - accuracy: 0.96 - ETA: 1s - loss: 0.0913 - accuracy: 0.96 - ETA: 1s - loss: 0.0907 - accuracy: 0.96 - ETA: 1s - loss: 0.0902 - accuracy: 0.96 - ETA: 0s - loss: 0.0899 - accuracy: 0.96 - ETA: 0s - loss: 0.0907 - accuracy: 0.96 - ETA: 0s - loss: 0.0901 - accuracy: 0.96 - ETA: 0s - loss: 0.0895 - accuracy: 0.96 - ETA: 0s - loss: 0.0887 - accuracy: 0.96 - ETA: 0s - loss: 0.0885 - accuracy: 0.96 - ETA: 0s - loss: 0.0889 - accuracy: 0.96 - ETA: 0s - loss: 0.0895 - accuracy: 0.96 - ETA: 0s - loss: 0.0904 - accuracy: 0.96 - ETA: 0s - loss: 0.0909 - accuracy: 0.96 - ETA: 0s - loss: 0.0914 - accuracy: 0.96 - ETA: 0s - loss: 0.0911 - accuracy: 0.96 - ETA: 0s - loss: 0.0904 - accuracy: 0.96 - 6s 557us/step - loss: 0.0902 - accuracy: 0.9645 - val_loss: 0.0687 - val_accuracy: 0.9739\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01416\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.1142 - accuracy: 0.96 - ETA: 5s - loss: 0.1535 - accuracy: 0.94 - ETA: 5s - loss: 0.1899 - accuracy: 0.92 - ETA: 5s - loss: 0.1718 - accuracy: 0.93 - ETA: 5s - loss: 0.1466 - accuracy: 0.94 - ETA: 5s - loss: 0.1310 - accuracy: 0.94 - ETA: 5s - loss: 0.1221 - accuracy: 0.95 - ETA: 5s - loss: 0.1150 - accuracy: 0.95 - ETA: 4s - loss: 0.1113 - accuracy: 0.95 - ETA: 4s - loss: 0.1069 - accuracy: 0.95 - ETA: 4s - loss: 0.1090 - accuracy: 0.95 - ETA: 4s - loss: 0.1083 - accuracy: 0.95 - ETA: 4s - loss: 0.1079 - accuracy: 0.95 - ETA: 4s - loss: 0.1061 - accuracy: 0.95 - ETA: 4s - loss: 0.1047 - accuracy: 0.95 - ETA: 4s - loss: 0.1017 - accuracy: 0.95 - ETA: 4s - loss: 0.1020 - accuracy: 0.95 - ETA: 4s - loss: 0.1056 - accuracy: 0.95 - ETA: 4s - loss: 0.1021 - accuracy: 0.95 - ETA: 4s - loss: 0.1021 - accuracy: 0.95 - ETA: 4s - loss: 0.0998 - accuracy: 0.95 - ETA: 3s - loss: 0.0974 - accuracy: 0.95 - ETA: 3s - loss: 0.0958 - accuracy: 0.95 - ETA: 3s - loss: 0.0944 - accuracy: 0.96 - ETA: 3s - loss: 0.0936 - accuracy: 0.96 - ETA: 3s - loss: 0.0974 - accuracy: 0.95 - ETA: 3s - loss: 0.1063 - accuracy: 0.95 - ETA: 3s - loss: 0.1045 - accuracy: 0.95 - ETA: 3s - loss: 0.1032 - accuracy: 0.95 - ETA: 3s - loss: 0.1017 - accuracy: 0.95 - ETA: 3s - loss: 0.0997 - accuracy: 0.95 - ETA: 3s - loss: 0.0988 - accuracy: 0.95 - ETA: 3s - loss: 0.0971 - accuracy: 0.95 - ETA: 3s - loss: 0.0954 - accuracy: 0.96 - ETA: 3s - loss: 0.0938 - accuracy: 0.96 - ETA: 2s - loss: 0.0936 - accuracy: 0.96 - ETA: 2s - loss: 0.0927 - accuracy: 0.96 - ETA: 2s - loss: 0.0931 - accuracy: 0.96 - ETA: 2s - loss: 0.0930 - accuracy: 0.96 - ETA: 2s - loss: 0.0924 - accuracy: 0.96 - ETA: 2s - loss: 0.0911 - accuracy: 0.96 - ETA: 2s - loss: 0.0915 - accuracy: 0.96 - ETA: 2s - loss: 0.0915 - accuracy: 0.96 - ETA: 2s - loss: 0.0904 - accuracy: 0.96 - ETA: 2s - loss: 0.0897 - accuracy: 0.96 - ETA: 2s - loss: 0.0892 - accuracy: 0.96 - ETA: 2s - loss: 0.0895 - accuracy: 0.96 - ETA: 1s - loss: 0.0891 - accuracy: 0.96 - ETA: 1s - loss: 0.0888 - accuracy: 0.96 - ETA: 1s - loss: 0.0885 - accuracy: 0.96 - ETA: 1s - loss: 0.0919 - accuracy: 0.96 - ETA: 1s - loss: 0.0921 - accuracy: 0.96 - ETA: 1s - loss: 0.0924 - accuracy: 0.96 - ETA: 1s - loss: 0.0915 - accuracy: 0.96 - ETA: 1s - loss: 0.0917 - accuracy: 0.96 - ETA: 1s - loss: 0.0909 - accuracy: 0.96 - ETA: 1s - loss: 0.0898 - accuracy: 0.96 - ETA: 1s - loss: 0.0884 - accuracy: 0.96 - ETA: 1s - loss: 0.0879 - accuracy: 0.96 - ETA: 1s - loss: 0.0877 - accuracy: 0.96 - ETA: 0s - loss: 0.0877 - accuracy: 0.96 - ETA: 0s - loss: 0.0878 - accuracy: 0.96 - ETA: 0s - loss: 0.0890 - accuracy: 0.96 - ETA: 0s - loss: 0.0894 - accuracy: 0.96 - ETA: 0s - loss: 0.0889 - accuracy: 0.96 - ETA: 0s - loss: 0.0889 - accuracy: 0.96 - ETA: 0s - loss: 0.0884 - accuracy: 0.96 - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - ETA: 0s - loss: 0.0871 - accuracy: 0.96 - ETA: 0s - loss: 0.0874 - accuracy: 0.96 - ETA: 0s - loss: 0.0878 - accuracy: 0.96 - ETA: 0s - loss: 0.0882 - accuracy: 0.96 - ETA: 0s - loss: 0.0889 - accuracy: 0.96 - 6s 555us/step - loss: 0.0889 - accuracy: 0.9640 - val_loss: 0.0734 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01416\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0665 - accuracy: 0.97 - ETA: 5s - loss: 0.0427 - accuracy: 0.98 - ETA: 5s - loss: 0.0420 - accuracy: 0.98 - ETA: 5s - loss: 0.0483 - accuracy: 0.98 - ETA: 5s - loss: 0.0482 - accuracy: 0.98 - ETA: 5s - loss: 0.0596 - accuracy: 0.98 - ETA: 5s - loss: 0.0621 - accuracy: 0.97 - ETA: 5s - loss: 0.0673 - accuracy: 0.97 - ETA: 5s - loss: 0.0684 - accuracy: 0.97 - ETA: 5s - loss: 0.0667 - accuracy: 0.97 - ETA: 5s - loss: 0.0653 - accuracy: 0.97 - ETA: 4s - loss: 0.0654 - accuracy: 0.97 - ETA: 4s - loss: 0.0640 - accuracy: 0.97 - ETA: 4s - loss: 0.0646 - accuracy: 0.97 - ETA: 4s - loss: 0.0653 - accuracy: 0.97 - ETA: 4s - loss: 0.0656 - accuracy: 0.97 - ETA: 4s - loss: 0.0651 - accuracy: 0.97 - ETA: 4s - loss: 0.0633 - accuracy: 0.97 - ETA: 4s - loss: 0.0630 - accuracy: 0.97 - ETA: 4s - loss: 0.0634 - accuracy: 0.97 - ETA: 4s - loss: 0.0664 - accuracy: 0.97 - ETA: 4s - loss: 0.0647 - accuracy: 0.97 - ETA: 4s - loss: 0.0629 - accuracy: 0.97 - ETA: 3s - loss: 0.0610 - accuracy: 0.97 - ETA: 3s - loss: 0.0589 - accuracy: 0.97 - ETA: 3s - loss: 0.0578 - accuracy: 0.97 - ETA: 3s - loss: 0.0568 - accuracy: 0.97 - ETA: 3s - loss: 0.0559 - accuracy: 0.97 - ETA: 3s - loss: 0.0659 - accuracy: 0.97 - ETA: 3s - loss: 0.0693 - accuracy: 0.97 - ETA: 3s - loss: 0.0712 - accuracy: 0.97 - ETA: 3s - loss: 0.0702 - accuracy: 0.97 - ETA: 3s - loss: 0.0695 - accuracy: 0.97 - ETA: 3s - loss: 0.0696 - accuracy: 0.97 - ETA: 3s - loss: 0.0695 - accuracy: 0.97 - ETA: 2s - loss: 0.0705 - accuracy: 0.97 - ETA: 2s - loss: 0.0719 - accuracy: 0.97 - ETA: 2s - loss: 0.0714 - accuracy: 0.97 - ETA: 2s - loss: 0.0711 - accuracy: 0.97 - ETA: 2s - loss: 0.0706 - accuracy: 0.97 - ETA: 2s - loss: 0.0699 - accuracy: 0.97 - ETA: 2s - loss: 0.0695 - accuracy: 0.97 - ETA: 2s - loss: 0.0692 - accuracy: 0.97 - ETA: 2s - loss: 0.0689 - accuracy: 0.97 - ETA: 2s - loss: 0.0697 - accuracy: 0.97 - ETA: 2s - loss: 0.0695 - accuracy: 0.97 - ETA: 2s - loss: 0.0689 - accuracy: 0.97 - ETA: 2s - loss: 0.0681 - accuracy: 0.97 - ETA: 1s - loss: 0.0683 - accuracy: 0.97 - ETA: 1s - loss: 0.0680 - accuracy: 0.97 - ETA: 1s - loss: 0.0684 - accuracy: 0.97 - ETA: 1s - loss: 0.0687 - accuracy: 0.97 - ETA: 1s - loss: 0.0682 - accuracy: 0.97 - ETA: 1s - loss: 0.0673 - accuracy: 0.97 - ETA: 1s - loss: 0.0671 - accuracy: 0.97 - ETA: 1s - loss: 0.0672 - accuracy: 0.97 - ETA: 1s - loss: 0.0669 - accuracy: 0.97 - ETA: 1s - loss: 0.0670 - accuracy: 0.97 - ETA: 1s - loss: 0.0663 - accuracy: 0.97 - ETA: 1s - loss: 0.0659 - accuracy: 0.97 - ETA: 1s - loss: 0.0649 - accuracy: 0.97 - ETA: 0s - loss: 0.0648 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0646 - accuracy: 0.97 - ETA: 0s - loss: 0.0641 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - ETA: 0s - loss: 0.0633 - accuracy: 0.97 - ETA: 0s - loss: 0.0631 - accuracy: 0.97 - ETA: 0s - loss: 0.0633 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - ETA: 0s - loss: 0.0648 - accuracy: 0.97 - 6s 576us/step - loss: 0.0644 - accuracy: 0.9743 - val_loss: 0.0294 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01416\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0322 - accuracy: 0.98 - ETA: 5s - loss: 0.0466 - accuracy: 0.97 - ETA: 5s - loss: 0.0776 - accuracy: 0.96 - ETA: 5s - loss: 0.0881 - accuracy: 0.96 - ETA: 5s - loss: 0.0816 - accuracy: 0.96 - ETA: 5s - loss: 0.0739 - accuracy: 0.96 - ETA: 5s - loss: 0.0723 - accuracy: 0.96 - ETA: 5s - loss: 0.0696 - accuracy: 0.97 - ETA: 5s - loss: 0.0706 - accuracy: 0.97 - ETA: 5s - loss: 0.0694 - accuracy: 0.97 - ETA: 5s - loss: 0.0670 - accuracy: 0.97 - ETA: 4s - loss: 0.0667 - accuracy: 0.97 - ETA: 4s - loss: 0.0675 - accuracy: 0.97 - ETA: 4s - loss: 0.0762 - accuracy: 0.97 - ETA: 4s - loss: 0.0807 - accuracy: 0.96 - ETA: 4s - loss: 0.0811 - accuracy: 0.96 - ETA: 4s - loss: 0.0809 - accuracy: 0.96 - ETA: 4s - loss: 0.0809 - accuracy: 0.96 - ETA: 4s - loss: 0.0796 - accuracy: 0.96 - ETA: 4s - loss: 0.0779 - accuracy: 0.96 - ETA: 4s - loss: 0.0766 - accuracy: 0.96 - ETA: 4s - loss: 0.0744 - accuracy: 0.96 - ETA: 4s - loss: 0.0723 - accuracy: 0.97 - ETA: 4s - loss: 0.0703 - accuracy: 0.97 - ETA: 4s - loss: 0.0693 - accuracy: 0.97 - ETA: 3s - loss: 0.0685 - accuracy: 0.97 - ETA: 3s - loss: 0.0678 - accuracy: 0.97 - ETA: 3s - loss: 0.0674 - accuracy: 0.97 - ETA: 3s - loss: 0.0683 - accuracy: 0.97 - ETA: 3s - loss: 0.0669 - accuracy: 0.97 - ETA: 3s - loss: 0.0659 - accuracy: 0.97 - ETA: 3s - loss: 0.0653 - accuracy: 0.97 - ETA: 3s - loss: 0.0650 - accuracy: 0.97 - ETA: 3s - loss: 0.0641 - accuracy: 0.97 - ETA: 3s - loss: 0.0636 - accuracy: 0.97 - ETA: 3s - loss: 0.0651 - accuracy: 0.97 - ETA: 3s - loss: 0.0683 - accuracy: 0.97 - ETA: 2s - loss: 0.0705 - accuracy: 0.97 - ETA: 2s - loss: 0.0703 - accuracy: 0.97 - ETA: 2s - loss: 0.0709 - accuracy: 0.97 - ETA: 2s - loss: 0.0702 - accuracy: 0.97 - ETA: 2s - loss: 0.0699 - accuracy: 0.97 - ETA: 2s - loss: 0.0694 - accuracy: 0.97 - ETA: 2s - loss: 0.0685 - accuracy: 0.97 - ETA: 2s - loss: 0.0677 - accuracy: 0.97 - ETA: 2s - loss: 0.0669 - accuracy: 0.97 - ETA: 2s - loss: 0.0663 - accuracy: 0.97 - ETA: 2s - loss: 0.0653 - accuracy: 0.97 - ETA: 2s - loss: 0.0643 - accuracy: 0.97 - ETA: 2s - loss: 0.0648 - accuracy: 0.97 - ETA: 1s - loss: 0.0650 - accuracy: 0.97 - ETA: 1s - loss: 0.0676 - accuracy: 0.97 - ETA: 1s - loss: 0.0716 - accuracy: 0.97 - ETA: 1s - loss: 0.0715 - accuracy: 0.97 - ETA: 1s - loss: 0.0709 - accuracy: 0.97 - ETA: 1s - loss: 0.0702 - accuracy: 0.97 - ETA: 1s - loss: 0.0701 - accuracy: 0.97 - ETA: 1s - loss: 0.0697 - accuracy: 0.97 - ETA: 1s - loss: 0.0691 - accuracy: 0.97 - ETA: 1s - loss: 0.0687 - accuracy: 0.97 - ETA: 1s - loss: 0.0691 - accuracy: 0.97 - ETA: 0s - loss: 0.0731 - accuracy: 0.97 - ETA: 0s - loss: 0.0729 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - ETA: 0s - loss: 0.0724 - accuracy: 0.97 - ETA: 0s - loss: 0.0720 - accuracy: 0.97 - ETA: 0s - loss: 0.0716 - accuracy: 0.97 - ETA: 0s - loss: 0.0707 - accuracy: 0.97 - ETA: 0s - loss: 0.0701 - accuracy: 0.97 - ETA: 0s - loss: 0.0697 - accuracy: 0.97 - ETA: 0s - loss: 0.0694 - accuracy: 0.97 - ETA: 0s - loss: 0.0687 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - 6s 599us/step - loss: 0.0681 - accuracy: 0.9737 - val_loss: 0.0580 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01416\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0301 - accuracy: 0.98 - ETA: 6s - loss: 0.0403 - accuracy: 0.97 - ETA: 6s - loss: 0.0459 - accuracy: 0.97 - ETA: 6s - loss: 0.0391 - accuracy: 0.98 - ETA: 6s - loss: 0.0346 - accuracy: 0.98 - ETA: 6s - loss: 0.0356 - accuracy: 0.98 - ETA: 5s - loss: 0.0447 - accuracy: 0.98 - ETA: 5s - loss: 0.0486 - accuracy: 0.97 - ETA: 5s - loss: 0.0567 - accuracy: 0.97 - ETA: 5s - loss: 0.0537 - accuracy: 0.97 - ETA: 5s - loss: 0.0564 - accuracy: 0.97 - ETA: 5s - loss: 0.0533 - accuracy: 0.97 - ETA: 5s - loss: 0.0529 - accuracy: 0.97 - ETA: 5s - loss: 0.0549 - accuracy: 0.97 - ETA: 4s - loss: 0.0581 - accuracy: 0.97 - ETA: 4s - loss: 0.0576 - accuracy: 0.97 - ETA: 4s - loss: 0.0556 - accuracy: 0.97 - ETA: 4s - loss: 0.0545 - accuracy: 0.97 - ETA: 4s - loss: 0.0519 - accuracy: 0.97 - ETA: 4s - loss: 0.0510 - accuracy: 0.97 - ETA: 4s - loss: 0.0503 - accuracy: 0.98 - ETA: 4s - loss: 0.0502 - accuracy: 0.98 - ETA: 4s - loss: 0.0504 - accuracy: 0.97 - ETA: 4s - loss: 0.0487 - accuracy: 0.98 - ETA: 4s - loss: 0.0475 - accuracy: 0.98 - ETA: 3s - loss: 0.0476 - accuracy: 0.98 - ETA: 3s - loss: 0.0461 - accuracy: 0.98 - ETA: 3s - loss: 0.0447 - accuracy: 0.98 - ETA: 3s - loss: 0.0442 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0467 - accuracy: 0.98 - ETA: 3s - loss: 0.0604 - accuracy: 0.97 - ETA: 3s - loss: 0.0598 - accuracy: 0.97 - ETA: 3s - loss: 0.0590 - accuracy: 0.97 - ETA: 3s - loss: 0.0585 - accuracy: 0.97 - ETA: 3s - loss: 0.0587 - accuracy: 0.97 - ETA: 2s - loss: 0.0580 - accuracy: 0.97 - ETA: 2s - loss: 0.0571 - accuracy: 0.97 - ETA: 2s - loss: 0.0565 - accuracy: 0.97 - ETA: 2s - loss: 0.0571 - accuracy: 0.97 - ETA: 2s - loss: 0.0571 - accuracy: 0.97 - ETA: 2s - loss: 0.0566 - accuracy: 0.97 - ETA: 2s - loss: 0.0558 - accuracy: 0.97 - ETA: 2s - loss: 0.0551 - accuracy: 0.97 - ETA: 2s - loss: 0.0546 - accuracy: 0.97 - ETA: 2s - loss: 0.0542 - accuracy: 0.97 - ETA: 2s - loss: 0.0538 - accuracy: 0.97 - ETA: 2s - loss: 0.0536 - accuracy: 0.97 - ETA: 2s - loss: 0.0528 - accuracy: 0.97 - ETA: 1s - loss: 0.0525 - accuracy: 0.97 - ETA: 1s - loss: 0.0536 - accuracy: 0.97 - ETA: 1s - loss: 0.0548 - accuracy: 0.97 - ETA: 1s - loss: 0.0564 - accuracy: 0.97 - ETA: 1s - loss: 0.0566 - accuracy: 0.97 - ETA: 1s - loss: 0.0561 - accuracy: 0.97 - ETA: 1s - loss: 0.0563 - accuracy: 0.97 - ETA: 1s - loss: 0.0561 - accuracy: 0.97 - ETA: 1s - loss: 0.0559 - accuracy: 0.97 - ETA: 1s - loss: 0.0564 - accuracy: 0.97 - ETA: 1s - loss: 0.0562 - accuracy: 0.97 - ETA: 1s - loss: 0.0562 - accuracy: 0.97 - ETA: 0s - loss: 0.0572 - accuracy: 0.97 - ETA: 0s - loss: 0.0567 - accuracy: 0.97 - ETA: 0s - loss: 0.0562 - accuracy: 0.97 - ETA: 0s - loss: 0.0558 - accuracy: 0.97 - ETA: 0s - loss: 0.0552 - accuracy: 0.97 - ETA: 0s - loss: 0.0550 - accuracy: 0.97 - ETA: 0s - loss: 0.0545 - accuracy: 0.97 - ETA: 0s - loss: 0.0554 - accuracy: 0.97 - ETA: 0s - loss: 0.0552 - accuracy: 0.97 - ETA: 0s - loss: 0.0556 - accuracy: 0.97 - ETA: 0s - loss: 0.0556 - accuracy: 0.97 - ETA: 0s - loss: 0.0551 - accuracy: 0.97 - 6s 594us/step - loss: 0.0548 - accuracy: 0.9791 - val_loss: 0.0257 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01416\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0389 - accuracy: 0.97 - ETA: 5s - loss: 0.0371 - accuracy: 0.97 - ETA: 5s - loss: 0.0410 - accuracy: 0.97 - ETA: 5s - loss: 0.0567 - accuracy: 0.97 - ETA: 5s - loss: 0.0722 - accuracy: 0.96 - ETA: 5s - loss: 0.0692 - accuracy: 0.97 - ETA: 5s - loss: 0.0625 - accuracy: 0.97 - ETA: 5s - loss: 0.0577 - accuracy: 0.97 - ETA: 5s - loss: 0.0593 - accuracy: 0.97 - ETA: 5s - loss: 0.0572 - accuracy: 0.97 - ETA: 5s - loss: 0.0562 - accuracy: 0.97 - ETA: 5s - loss: 0.0564 - accuracy: 0.97 - ETA: 5s - loss: 0.0589 - accuracy: 0.97 - ETA: 5s - loss: 0.0558 - accuracy: 0.97 - ETA: 5s - loss: 0.0558 - accuracy: 0.97 - ETA: 4s - loss: 0.0530 - accuracy: 0.97 - ETA: 4s - loss: 0.0532 - accuracy: 0.97 - ETA: 4s - loss: 0.0534 - accuracy: 0.97 - ETA: 4s - loss: 0.0517 - accuracy: 0.98 - ETA: 4s - loss: 0.0518 - accuracy: 0.98 - ETA: 4s - loss: 0.0505 - accuracy: 0.98 - ETA: 4s - loss: 0.0487 - accuracy: 0.98 - ETA: 4s - loss: 0.0480 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0466 - accuracy: 0.98 - ETA: 4s - loss: 0.0462 - accuracy: 0.98 - ETA: 4s - loss: 0.0463 - accuracy: 0.98 - ETA: 3s - loss: 0.0471 - accuracy: 0.98 - ETA: 3s - loss: 0.0495 - accuracy: 0.98 - ETA: 3s - loss: 0.0509 - accuracy: 0.98 - ETA: 3s - loss: 0.0519 - accuracy: 0.98 - ETA: 3s - loss: 0.0538 - accuracy: 0.97 - ETA: 3s - loss: 0.0544 - accuracy: 0.97 - ETA: 3s - loss: 0.0566 - accuracy: 0.97 - ETA: 3s - loss: 0.0554 - accuracy: 0.97 - ETA: 3s - loss: 0.0541 - accuracy: 0.97 - ETA: 3s - loss: 0.0530 - accuracy: 0.98 - ETA: 2s - loss: 0.0531 - accuracy: 0.97 - ETA: 2s - loss: 0.0524 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 2s - loss: 0.0518 - accuracy: 0.98 - ETA: 2s - loss: 0.0512 - accuracy: 0.98 - ETA: 2s - loss: 0.0507 - accuracy: 0.98 - ETA: 2s - loss: 0.0514 - accuracy: 0.98 - ETA: 2s - loss: 0.0527 - accuracy: 0.98 - ETA: 2s - loss: 0.0527 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 1s - loss: 0.0527 - accuracy: 0.97 - ETA: 1s - loss: 0.0522 - accuracy: 0.98 - ETA: 1s - loss: 0.0521 - accuracy: 0.97 - ETA: 1s - loss: 0.0530 - accuracy: 0.97 - ETA: 1s - loss: 0.0523 - accuracy: 0.97 - ETA: 1s - loss: 0.0520 - accuracy: 0.97 - ETA: 1s - loss: 0.0516 - accuracy: 0.98 - ETA: 1s - loss: 0.0510 - accuracy: 0.98 - ETA: 1s - loss: 0.0514 - accuracy: 0.98 - ETA: 1s - loss: 0.0515 - accuracy: 0.98 - ETA: 1s - loss: 0.0512 - accuracy: 0.98 - ETA: 1s - loss: 0.0506 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0506 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0503 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0507 - accuracy: 0.98 - ETA: 0s - loss: 0.0503 - accuracy: 0.98 - ETA: 0s - loss: 0.0497 - accuracy: 0.98 - ETA: 0s - loss: 0.0494 - accuracy: 0.98 - ETA: 0s - loss: 0.0492 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - 6s 590us/step - loss: 0.0487 - accuracy: 0.9817 - val_loss: 0.0213 - val_accuracy: 0.9907\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01416\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0162 - accuracy: 0.99 - ETA: 6s - loss: 0.0210 - accuracy: 0.99 - ETA: 6s - loss: 0.0201 - accuracy: 0.99 - ETA: 6s - loss: 0.0212 - accuracy: 0.99 - ETA: 6s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0178 - accuracy: 0.99 - ETA: 5s - loss: 0.0177 - accuracy: 0.99 - ETA: 5s - loss: 0.0194 - accuracy: 0.99 - ETA: 5s - loss: 0.0219 - accuracy: 0.99 - ETA: 5s - loss: 0.0233 - accuracy: 0.99 - ETA: 5s - loss: 0.0504 - accuracy: 0.98 - ETA: 4s - loss: 0.0649 - accuracy: 0.97 - ETA: 4s - loss: 0.0618 - accuracy: 0.97 - ETA: 4s - loss: 0.0611 - accuracy: 0.97 - ETA: 4s - loss: 0.0585 - accuracy: 0.97 - ETA: 4s - loss: 0.0589 - accuracy: 0.97 - ETA: 4s - loss: 0.0558 - accuracy: 0.98 - ETA: 4s - loss: 0.0540 - accuracy: 0.98 - ETA: 4s - loss: 0.0528 - accuracy: 0.98 - ETA: 4s - loss: 0.0510 - accuracy: 0.98 - ETA: 4s - loss: 0.0492 - accuracy: 0.98 - ETA: 4s - loss: 0.0505 - accuracy: 0.98 - ETA: 4s - loss: 0.0490 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 3s - loss: 0.0475 - accuracy: 0.98 - ETA: 3s - loss: 0.0467 - accuracy: 0.98 - ETA: 3s - loss: 0.0470 - accuracy: 0.98 - ETA: 3s - loss: 0.0489 - accuracy: 0.98 - ETA: 3s - loss: 0.0486 - accuracy: 0.98 - ETA: 3s - loss: 0.0487 - accuracy: 0.98 - ETA: 3s - loss: 0.0485 - accuracy: 0.98 - ETA: 3s - loss: 0.0492 - accuracy: 0.98 - ETA: 3s - loss: 0.0488 - accuracy: 0.98 - ETA: 3s - loss: 0.0492 - accuracy: 0.98 - ETA: 3s - loss: 0.0490 - accuracy: 0.98 - ETA: 2s - loss: 0.0484 - accuracy: 0.98 - ETA: 2s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0467 - accuracy: 0.98 - ETA: 2s - loss: 0.0458 - accuracy: 0.98 - ETA: 2s - loss: 0.0448 - accuracy: 0.98 - ETA: 2s - loss: 0.0441 - accuracy: 0.98 - ETA: 2s - loss: 0.0432 - accuracy: 0.98 - ETA: 2s - loss: 0.0438 - accuracy: 0.98 - ETA: 2s - loss: 0.0443 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0438 - accuracy: 0.98 - ETA: 2s - loss: 0.0453 - accuracy: 0.98 - ETA: 2s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0478 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0465 - accuracy: 0.98 - ETA: 1s - loss: 0.0457 - accuracy: 0.98 - ETA: 1s - loss: 0.0452 - accuracy: 0.98 - ETA: 1s - loss: 0.0451 - accuracy: 0.98 - ETA: 1s - loss: 0.0451 - accuracy: 0.98 - ETA: 1s - loss: 0.0463 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0474 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0467 - accuracy: 0.98 - ETA: 0s - loss: 0.0462 - accuracy: 0.98 - ETA: 0s - loss: 0.0459 - accuracy: 0.98 - ETA: 0s - loss: 0.0453 - accuracy: 0.98 - ETA: 0s - loss: 0.0455 - accuracy: 0.98 - ETA: 0s - loss: 0.0452 - accuracy: 0.98 - ETA: 0s - loss: 0.0454 - accuracy: 0.98 - ETA: 0s - loss: 0.0455 - accuracy: 0.98 - ETA: 0s - loss: 0.0459 - accuracy: 0.98 - ETA: 0s - loss: 0.0455 - accuracy: 0.98 - ETA: 0s - loss: 0.0451 - accuracy: 0.98 - 6s 562us/step - loss: 0.0451 - accuracy: 0.9834 - val_loss: 0.0330 - val_accuracy: 0.9848\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01416\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0102 - accuracy: 1.00 - ETA: 5s - loss: 0.0146 - accuracy: 1.00 - ETA: 5s - loss: 0.0128 - accuracy: 1.00 - ETA: 5s - loss: 0.0201 - accuracy: 0.99 - ETA: 5s - loss: 0.0261 - accuracy: 0.99 - ETA: 5s - loss: 0.0383 - accuracy: 0.99 - ETA: 5s - loss: 0.0392 - accuracy: 0.99 - ETA: 5s - loss: 0.0376 - accuracy: 0.99 - ETA: 4s - loss: 0.0400 - accuracy: 0.99 - ETA: 4s - loss: 0.0442 - accuracy: 0.98 - ETA: 4s - loss: 0.0421 - accuracy: 0.98 - ETA: 4s - loss: 0.0424 - accuracy: 0.98 - ETA: 4s - loss: 0.0413 - accuracy: 0.98 - ETA: 4s - loss: 0.0410 - accuracy: 0.98 - ETA: 4s - loss: 0.0395 - accuracy: 0.98 - ETA: 4s - loss: 0.0398 - accuracy: 0.98 - ETA: 4s - loss: 0.0396 - accuracy: 0.98 - ETA: 4s - loss: 0.0389 - accuracy: 0.98 - ETA: 4s - loss: 0.0386 - accuracy: 0.98 - ETA: 4s - loss: 0.0374 - accuracy: 0.98 - ETA: 4s - loss: 0.0359 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0385 - accuracy: 0.98 - ETA: 3s - loss: 0.0376 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0350 - accuracy: 0.98 - ETA: 3s - loss: 0.0380 - accuracy: 0.98 - ETA: 3s - loss: 0.0413 - accuracy: 0.98 - ETA: 3s - loss: 0.0420 - accuracy: 0.98 - ETA: 2s - loss: 0.0438 - accuracy: 0.98 - ETA: 2s - loss: 0.0434 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0428 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0417 - accuracy: 0.98 - ETA: 2s - loss: 0.0420 - accuracy: 0.98 - ETA: 2s - loss: 0.0415 - accuracy: 0.98 - ETA: 2s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0399 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0388 - accuracy: 0.98 - ETA: 1s - loss: 0.0399 - accuracy: 0.98 - ETA: 1s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0394 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0389 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 0s - loss: 0.0395 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0391 - accuracy: 0.98 - ETA: 0s - loss: 0.0386 - accuracy: 0.98 - ETA: 0s - loss: 0.0382 - accuracy: 0.98 - ETA: 0s - loss: 0.0377 - accuracy: 0.98 - ETA: 0s - loss: 0.0379 - accuracy: 0.98 - ETA: 0s - loss: 0.0376 - accuracy: 0.98 - ETA: 0s - loss: 0.0377 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0395 - accuracy: 0.98 - ETA: 0s - loss: 0.0434 - accuracy: 0.98 - 6s 560us/step - loss: 0.0442 - accuracy: 0.9846 - val_loss: 0.0455 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01416\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0638 - accuracy: 0.97 - ETA: 5s - loss: 0.0587 - accuracy: 0.97 - ETA: 5s - loss: 0.0503 - accuracy: 0.98 - ETA: 5s - loss: 0.0478 - accuracy: 0.98 - ETA: 5s - loss: 0.0433 - accuracy: 0.98 - ETA: 5s - loss: 0.0379 - accuracy: 0.98 - ETA: 5s - loss: 0.0359 - accuracy: 0.98 - ETA: 5s - loss: 0.0347 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0315 - accuracy: 0.98 - ETA: 4s - loss: 0.0293 - accuracy: 0.98 - ETA: 4s - loss: 0.0289 - accuracy: 0.98 - ETA: 4s - loss: 0.0273 - accuracy: 0.98 - ETA: 4s - loss: 0.0276 - accuracy: 0.98 - ETA: 4s - loss: 0.0309 - accuracy: 0.98 - ETA: 4s - loss: 0.0307 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0338 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0335 - accuracy: 0.98 - ETA: 4s - loss: 0.0342 - accuracy: 0.98 - ETA: 3s - loss: 0.0426 - accuracy: 0.98 - ETA: 3s - loss: 0.0441 - accuracy: 0.98 - ETA: 3s - loss: 0.0463 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 3s - loss: 0.0438 - accuracy: 0.98 - ETA: 3s - loss: 0.0424 - accuracy: 0.98 - ETA: 3s - loss: 0.0415 - accuracy: 0.98 - ETA: 3s - loss: 0.0425 - accuracy: 0.98 - ETA: 3s - loss: 0.0433 - accuracy: 0.98 - ETA: 3s - loss: 0.0424 - accuracy: 0.98 - ETA: 3s - loss: 0.0433 - accuracy: 0.98 - ETA: 3s - loss: 0.0425 - accuracy: 0.98 - ETA: 3s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0417 - accuracy: 0.98 - ETA: 2s - loss: 0.0422 - accuracy: 0.98 - ETA: 2s - loss: 0.0434 - accuracy: 0.98 - ETA: 2s - loss: 0.0434 - accuracy: 0.98 - ETA: 2s - loss: 0.0430 - accuracy: 0.98 - ETA: 2s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0420 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0422 - accuracy: 0.98 - ETA: 2s - loss: 0.0417 - accuracy: 0.98 - ETA: 2s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 1s - loss: 0.0413 - accuracy: 0.98 - ETA: 1s - loss: 0.0417 - accuracy: 0.98 - ETA: 1s - loss: 0.0420 - accuracy: 0.98 - ETA: 1s - loss: 0.0418 - accuracy: 0.98 - ETA: 1s - loss: 0.0414 - accuracy: 0.98 - ETA: 1s - loss: 0.0409 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0409 - accuracy: 0.98 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 1s - loss: 0.0417 - accuracy: 0.98 - ETA: 1s - loss: 0.0412 - accuracy: 0.98 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - ETA: 0s - loss: 0.0415 - accuracy: 0.98 - ETA: 0s - loss: 0.0417 - accuracy: 0.98 - ETA: 0s - loss: 0.0418 - accuracy: 0.98 - ETA: 0s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0426 - accuracy: 0.98 - ETA: 0s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0433 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0428 - accuracy: 0.98 - ETA: 0s - loss: 0.0425 - accuracy: 0.98 - 6s 564us/step - loss: 0.0422 - accuracy: 0.9852 - val_loss: 0.0205 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01416\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0098 - accuracy: 1.00 - ETA: 5s - loss: 0.0198 - accuracy: 0.99 - ETA: 5s - loss: 0.0339 - accuracy: 0.98 - ETA: 5s - loss: 0.0619 - accuracy: 0.97 - ETA: 5s - loss: 0.0549 - accuracy: 0.97 - ETA: 5s - loss: 0.0536 - accuracy: 0.97 - ETA: 5s - loss: 0.0478 - accuracy: 0.98 - ETA: 4s - loss: 0.0430 - accuracy: 0.98 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 4s - loss: 0.0377 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0338 - accuracy: 0.98 - ETA: 4s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0365 - accuracy: 0.98 - ETA: 4s - loss: 0.0564 - accuracy: 0.98 - ETA: 4s - loss: 0.0597 - accuracy: 0.98 - ETA: 4s - loss: 0.0582 - accuracy: 0.98 - ETA: 3s - loss: 0.0576 - accuracy: 0.98 - ETA: 3s - loss: 0.0563 - accuracy: 0.98 - ETA: 3s - loss: 0.0546 - accuracy: 0.98 - ETA: 3s - loss: 0.0532 - accuracy: 0.98 - ETA: 3s - loss: 0.0529 - accuracy: 0.98 - ETA: 3s - loss: 0.0512 - accuracy: 0.98 - ETA: 3s - loss: 0.0500 - accuracy: 0.98 - ETA: 3s - loss: 0.0486 - accuracy: 0.98 - ETA: 3s - loss: 0.0488 - accuracy: 0.98 - ETA: 3s - loss: 0.0487 - accuracy: 0.98 - ETA: 3s - loss: 0.0488 - accuracy: 0.98 - ETA: 3s - loss: 0.0498 - accuracy: 0.98 - ETA: 3s - loss: 0.0493 - accuracy: 0.98 - ETA: 2s - loss: 0.0489 - accuracy: 0.98 - ETA: 2s - loss: 0.0479 - accuracy: 0.98 - ETA: 2s - loss: 0.0475 - accuracy: 0.98 - ETA: 2s - loss: 0.0468 - accuracy: 0.98 - ETA: 2s - loss: 0.0461 - accuracy: 0.98 - ETA: 2s - loss: 0.0455 - accuracy: 0.98 - ETA: 2s - loss: 0.0445 - accuracy: 0.98 - ETA: 2s - loss: 0.0439 - accuracy: 0.98 - ETA: 2s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0450 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0445 - accuracy: 0.98 - ETA: 1s - loss: 0.0446 - accuracy: 0.98 - ETA: 1s - loss: 0.0437 - accuracy: 0.98 - ETA: 1s - loss: 0.0432 - accuracy: 0.98 - ETA: 1s - loss: 0.0431 - accuracy: 0.98 - ETA: 1s - loss: 0.0440 - accuracy: 0.98 - ETA: 1s - loss: 0.0437 - accuracy: 0.98 - ETA: 1s - loss: 0.0437 - accuracy: 0.98 - ETA: 1s - loss: 0.0441 - accuracy: 0.98 - ETA: 1s - loss: 0.0436 - accuracy: 0.98 - ETA: 1s - loss: 0.0434 - accuracy: 0.98 - ETA: 1s - loss: 0.0436 - accuracy: 0.98 - ETA: 1s - loss: 0.0435 - accuracy: 0.98 - ETA: 1s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0429 - accuracy: 0.98 - ETA: 0s - loss: 0.0429 - accuracy: 0.98 - ETA: 0s - loss: 0.0425 - accuracy: 0.98 - ETA: 0s - loss: 0.0423 - accuracy: 0.98 - ETA: 0s - loss: 0.0451 - accuracy: 0.98 - ETA: 0s - loss: 0.0450 - accuracy: 0.98 - ETA: 0s - loss: 0.0445 - accuracy: 0.98 - ETA: 0s - loss: 0.0443 - accuracy: 0.98 - ETA: 0s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0433 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.98 - ETA: 0s - loss: 0.0428 - accuracy: 0.98 - 6s 556us/step - loss: 0.0431 - accuracy: 0.9855 - val_loss: 0.0235 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01416\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0235 - accuracy: 0.99 - ETA: 5s - loss: 0.0160 - accuracy: 0.99 - ETA: 5s - loss: 0.0140 - accuracy: 0.99 - ETA: 5s - loss: 0.0186 - accuracy: 0.99 - ETA: 5s - loss: 0.0301 - accuracy: 0.99 - ETA: 5s - loss: 0.0306 - accuracy: 0.98 - ETA: 5s - loss: 0.0324 - accuracy: 0.98 - ETA: 5s - loss: 0.0310 - accuracy: 0.98 - ETA: 5s - loss: 0.0295 - accuracy: 0.98 - ETA: 5s - loss: 0.0267 - accuracy: 0.99 - ETA: 5s - loss: 0.0285 - accuracy: 0.98 - ETA: 4s - loss: 0.0281 - accuracy: 0.98 - ETA: 4s - loss: 0.0269 - accuracy: 0.98 - ETA: 4s - loss: 0.0300 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0324 - accuracy: 0.98 - ETA: 4s - loss: 0.0313 - accuracy: 0.98 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0324 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0386 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0373 - accuracy: 0.98 - ETA: 3s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0353 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 3s - loss: 0.0347 - accuracy: 0.98 - ETA: 3s - loss: 0.0347 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0335 - accuracy: 0.98 - ETA: 2s - loss: 0.0334 - accuracy: 0.98 - ETA: 2s - loss: 0.0329 - accuracy: 0.98 - ETA: 2s - loss: 0.0322 - accuracy: 0.98 - ETA: 2s - loss: 0.0317 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0321 - accuracy: 0.98 - ETA: 2s - loss: 0.0317 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0318 - accuracy: 0.98 - ETA: 2s - loss: 0.0326 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 2s - loss: 0.0334 - accuracy: 0.98 - ETA: 1s - loss: 0.0332 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 1s - loss: 0.0330 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 1s - loss: 0.0327 - accuracy: 0.98 - ETA: 1s - loss: 0.0321 - accuracy: 0.98 - ETA: 1s - loss: 0.0324 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0321 - accuracy: 0.98 - ETA: 1s - loss: 0.0317 - accuracy: 0.98 - ETA: 1s - loss: 0.0320 - accuracy: 0.98 - ETA: 1s - loss: 0.0325 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0317 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0325 - accuracy: 0.98 - ETA: 0s - loss: 0.0325 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - 6s 571us/step - loss: 0.0332 - accuracy: 0.9877 - val_loss: 0.0143 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01416\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0391 - accuracy: 0.98 - ETA: 5s - loss: 0.0429 - accuracy: 0.97 - ETA: 5s - loss: 0.0333 - accuracy: 0.98 - ETA: 5s - loss: 0.0373 - accuracy: 0.98 - ETA: 5s - loss: 0.0339 - accuracy: 0.98 - ETA: 5s - loss: 0.0294 - accuracy: 0.98 - ETA: 4s - loss: 0.0268 - accuracy: 0.98 - ETA: 4s - loss: 0.0243 - accuracy: 0.98 - ETA: 4s - loss: 0.0219 - accuracy: 0.99 - ETA: 4s - loss: 0.0204 - accuracy: 0.99 - ETA: 4s - loss: 0.0190 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0259 - accuracy: 0.98 - ETA: 4s - loss: 0.0268 - accuracy: 0.98 - ETA: 4s - loss: 0.0273 - accuracy: 0.98 - ETA: 4s - loss: 0.0269 - accuracy: 0.98 - ETA: 4s - loss: 0.0265 - accuracy: 0.98 - ETA: 4s - loss: 0.0284 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0319 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0331 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0323 - accuracy: 0.98 - ETA: 3s - loss: 0.0324 - accuracy: 0.98 - ETA: 3s - loss: 0.0333 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0320 - accuracy: 0.98 - ETA: 3s - loss: 0.0313 - accuracy: 0.98 - ETA: 3s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0307 - accuracy: 0.98 - ETA: 3s - loss: 0.0312 - accuracy: 0.98 - ETA: 3s - loss: 0.0304 - accuracy: 0.98 - ETA: 3s - loss: 0.0311 - accuracy: 0.98 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0308 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0300 - accuracy: 0.98 - ETA: 2s - loss: 0.0295 - accuracy: 0.98 - ETA: 2s - loss: 0.0291 - accuracy: 0.98 - ETA: 2s - loss: 0.0291 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0321 - accuracy: 0.98 - ETA: 2s - loss: 0.0316 - accuracy: 0.98 - ETA: 2s - loss: 0.0315 - accuracy: 0.98 - ETA: 2s - loss: 0.0332 - accuracy: 0.98 - ETA: 1s - loss: 0.0330 - accuracy: 0.98 - ETA: 1s - loss: 0.0324 - accuracy: 0.98 - ETA: 1s - loss: 0.0321 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0329 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0324 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0325 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0327 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0319 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0312 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0311 - accuracy: 0.98 - ETA: 0s - loss: 0.0310 - accuracy: 0.98 - ETA: 0s - loss: 0.0307 - accuracy: 0.98 - ETA: 0s - loss: 0.0307 - accuracy: 0.98 - ETA: 0s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0340 - accuracy: 0.98 - 6s 559us/step - loss: 0.0342 - accuracy: 0.9872 - val_loss: 0.0541 - val_accuracy: 0.9806\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01416\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0517 - accuracy: 0.97 - ETA: 5s - loss: 0.0302 - accuracy: 0.98 - ETA: 5s - loss: 0.0338 - accuracy: 0.98 - ETA: 5s - loss: 0.0315 - accuracy: 0.98 - ETA: 5s - loss: 0.0355 - accuracy: 0.98 - ETA: 5s - loss: 0.0365 - accuracy: 0.98 - ETA: 5s - loss: 0.0325 - accuracy: 0.98 - ETA: 5s - loss: 0.0300 - accuracy: 0.98 - ETA: 5s - loss: 0.0284 - accuracy: 0.98 - ETA: 4s - loss: 0.0296 - accuracy: 0.98 - ETA: 5s - loss: 0.0296 - accuracy: 0.98 - ETA: 4s - loss: 0.0275 - accuracy: 0.98 - ETA: 4s - loss: 0.0257 - accuracy: 0.98 - ETA: 4s - loss: 0.0251 - accuracy: 0.99 - ETA: 4s - loss: 0.0243 - accuracy: 0.99 - ETA: 4s - loss: 0.0250 - accuracy: 0.98 - ETA: 4s - loss: 0.0254 - accuracy: 0.98 - ETA: 4s - loss: 0.0247 - accuracy: 0.99 - ETA: 4s - loss: 0.0259 - accuracy: 0.98 - ETA: 4s - loss: 0.0259 - accuracy: 0.98 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0257 - accuracy: 0.98 - ETA: 4s - loss: 0.0289 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.98 - ETA: 3s - loss: 0.0298 - accuracy: 0.98 - ETA: 3s - loss: 0.0305 - accuracy: 0.98 - ETA: 3s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0298 - accuracy: 0.98 - ETA: 3s - loss: 0.0292 - accuracy: 0.98 - ETA: 3s - loss: 0.0303 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0353 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0339 - accuracy: 0.98 - ETA: 2s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0373 - accuracy: 0.98 - ETA: 2s - loss: 0.0369 - accuracy: 0.98 - ETA: 2s - loss: 0.0361 - accuracy: 0.98 - ETA: 2s - loss: 0.0362 - accuracy: 0.98 - ETA: 2s - loss: 0.0355 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0344 - accuracy: 0.98 - ETA: 2s - loss: 0.0341 - accuracy: 0.98 - ETA: 2s - loss: 0.0336 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 1s - loss: 0.0335 - accuracy: 0.98 - ETA: 1s - loss: 0.0335 - accuracy: 0.98 - ETA: 1s - loss: 0.0335 - accuracy: 0.98 - ETA: 1s - loss: 0.0343 - accuracy: 0.98 - ETA: 1s - loss: 0.0342 - accuracy: 0.98 - ETA: 1s - loss: 0.0343 - accuracy: 0.98 - ETA: 1s - loss: 0.0348 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 1s - loss: 0.0343 - accuracy: 0.98 - ETA: 1s - loss: 0.0339 - accuracy: 0.98 - ETA: 1s - loss: 0.0339 - accuracy: 0.98 - ETA: 1s - loss: 0.0341 - accuracy: 0.98 - ETA: 1s - loss: 0.0337 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0322 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0325 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0345 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - 6s 572us/step - loss: 0.0347 - accuracy: 0.9877 - val_loss: 0.0584 - val_accuracy: 0.9840\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01416\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1968 - accuracy: 0.97 - ETA: 5s - loss: 0.1232 - accuracy: 0.97 - ETA: 5s - loss: 0.1117 - accuracy: 0.97 - ETA: 5s - loss: 0.0860 - accuracy: 0.97 - ETA: 5s - loss: 0.0718 - accuracy: 0.98 - ETA: 5s - loss: 0.0611 - accuracy: 0.98 - ETA: 4s - loss: 0.0555 - accuracy: 0.98 - ETA: 4s - loss: 0.0496 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 4s - loss: 0.0418 - accuracy: 0.98 - ETA: 4s - loss: 0.0402 - accuracy: 0.98 - ETA: 4s - loss: 0.0381 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0378 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0362 - accuracy: 0.98 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0340 - accuracy: 0.99 - ETA: 4s - loss: 0.0334 - accuracy: 0.99 - ETA: 4s - loss: 0.0319 - accuracy: 0.99 - ETA: 4s - loss: 0.0308 - accuracy: 0.99 - ETA: 3s - loss: 0.0297 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0326 - accuracy: 0.98 - ETA: 3s - loss: 0.0339 - accuracy: 0.98 - ETA: 3s - loss: 0.0339 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0335 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0322 - accuracy: 0.98 - ETA: 3s - loss: 0.0316 - accuracy: 0.99 - ETA: 3s - loss: 0.0309 - accuracy: 0.99 - ETA: 3s - loss: 0.0301 - accuracy: 0.99 - ETA: 3s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0297 - accuracy: 0.99 - ETA: 2s - loss: 0.0289 - accuracy: 0.99 - ETA: 2s - loss: 0.0288 - accuracy: 0.99 - ETA: 2s - loss: 0.0286 - accuracy: 0.99 - ETA: 2s - loss: 0.0295 - accuracy: 0.99 - ETA: 2s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0304 - accuracy: 0.99 - ETA: 2s - loss: 0.0304 - accuracy: 0.99 - ETA: 2s - loss: 0.0309 - accuracy: 0.98 - ETA: 2s - loss: 0.0314 - accuracy: 0.98 - ETA: 2s - loss: 0.0324 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 1s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0310 - accuracy: 0.98 - ETA: 1s - loss: 0.0306 - accuracy: 0.98 - ETA: 1s - loss: 0.0304 - accuracy: 0.98 - ETA: 1s - loss: 0.0299 - accuracy: 0.98 - ETA: 1s - loss: 0.0303 - accuracy: 0.98 - ETA: 1s - loss: 0.0303 - accuracy: 0.98 - ETA: 1s - loss: 0.0300 - accuracy: 0.98 - ETA: 1s - loss: 0.0301 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0304 - accuracy: 0.98 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0328 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - 6s 556us/step - loss: 0.0338 - accuracy: 0.9880 - val_loss: 0.0256 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01416\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0090 - accuracy: 1.00 - ETA: 5s - loss: 0.0146 - accuracy: 0.99 - ETA: 5s - loss: 0.0183 - accuracy: 0.99 - ETA: 5s - loss: 0.0230 - accuracy: 0.99 - ETA: 5s - loss: 0.0209 - accuracy: 0.99 - ETA: 5s - loss: 0.0217 - accuracy: 0.99 - ETA: 5s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 4s - loss: 0.0207 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0199 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0224 - accuracy: 0.99 - ETA: 4s - loss: 0.0216 - accuracy: 0.99 - ETA: 4s - loss: 0.0210 - accuracy: 0.99 - ETA: 4s - loss: 0.0220 - accuracy: 0.99 - ETA: 4s - loss: 0.0241 - accuracy: 0.99 - ETA: 3s - loss: 0.0256 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0283 - accuracy: 0.99 - ETA: 3s - loss: 0.0293 - accuracy: 0.99 - ETA: 3s - loss: 0.0292 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.98 - ETA: 3s - loss: 0.0299 - accuracy: 0.98 - ETA: 3s - loss: 0.0296 - accuracy: 0.98 - ETA: 3s - loss: 0.0290 - accuracy: 0.98 - ETA: 3s - loss: 0.0287 - accuracy: 0.98 - ETA: 3s - loss: 0.0284 - accuracy: 0.98 - ETA: 2s - loss: 0.0281 - accuracy: 0.98 - ETA: 2s - loss: 0.0279 - accuracy: 0.98 - ETA: 2s - loss: 0.0277 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0258 - accuracy: 0.99 - ETA: 2s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.98 - ETA: 1s - loss: 0.0288 - accuracy: 0.98 - ETA: 1s - loss: 0.0287 - accuracy: 0.98 - ETA: 1s - loss: 0.0285 - accuracy: 0.98 - ETA: 1s - loss: 0.0281 - accuracy: 0.99 - ETA: 1s - loss: 0.0289 - accuracy: 0.98 - ETA: 1s - loss: 0.0299 - accuracy: 0.98 - ETA: 1s - loss: 0.0310 - accuracy: 0.98 - ETA: 1s - loss: 0.0310 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 1s - loss: 0.0319 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0313 - accuracy: 0.98 - ETA: 0s - loss: 0.0313 - accuracy: 0.98 - ETA: 0s - loss: 0.0309 - accuracy: 0.98 - ETA: 0s - loss: 0.0309 - accuracy: 0.98 - ETA: 0s - loss: 0.0305 - accuracy: 0.98 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0306 - accuracy: 0.98 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0302 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.98 - 6s 565us/step - loss: 0.0303 - accuracy: 0.9892 - val_loss: 0.2367 - val_accuracy: 0.9402\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01416\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.3807 - accuracy: 0.88 - ETA: 5s - loss: 0.1983 - accuracy: 0.94 - ETA: 5s - loss: 0.1526 - accuracy: 0.95 - ETA: 5s - loss: 0.1207 - accuracy: 0.96 - ETA: 5s - loss: 0.0981 - accuracy: 0.97 - ETA: 5s - loss: 0.0843 - accuracy: 0.97 - ETA: 5s - loss: 0.0728 - accuracy: 0.97 - ETA: 4s - loss: 0.0645 - accuracy: 0.98 - ETA: 4s - loss: 0.0596 - accuracy: 0.98 - ETA: 4s - loss: 0.0540 - accuracy: 0.98 - ETA: 4s - loss: 0.0505 - accuracy: 0.98 - ETA: 4s - loss: 0.0479 - accuracy: 0.98 - ETA: 4s - loss: 0.0453 - accuracy: 0.98 - ETA: 4s - loss: 0.0462 - accuracy: 0.98 - ETA: 4s - loss: 0.0452 - accuracy: 0.98 - ETA: 4s - loss: 0.0426 - accuracy: 0.98 - ETA: 4s - loss: 0.0450 - accuracy: 0.98 - ETA: 4s - loss: 0.0512 - accuracy: 0.98 - ETA: 4s - loss: 0.0498 - accuracy: 0.98 - ETA: 4s - loss: 0.0485 - accuracy: 0.98 - ETA: 4s - loss: 0.0470 - accuracy: 0.98 - ETA: 3s - loss: 0.0463 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0438 - accuracy: 0.98 - ETA: 3s - loss: 0.0431 - accuracy: 0.98 - ETA: 3s - loss: 0.0416 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0400 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 2s - loss: 0.0385 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0369 - accuracy: 0.98 - ETA: 2s - loss: 0.0370 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0407 - accuracy: 0.98 - ETA: 2s - loss: 0.0401 - accuracy: 0.98 - ETA: 2s - loss: 0.0402 - accuracy: 0.98 - ETA: 2s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0395 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 1s - loss: 0.0388 - accuracy: 0.98 - ETA: 1s - loss: 0.0382 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0364 - accuracy: 0.98 - ETA: 0s - loss: 0.0360 - accuracy: 0.98 - ETA: 0s - loss: 0.0357 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0353 - accuracy: 0.98 - ETA: 0s - loss: 0.0349 - accuracy: 0.98 - 6s 594us/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.0267 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01416\n",
      "Epoch 00020: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 208us/step\n",
      "[2020-05-18 16:24:49 RAM67.2% 1.23GB] Val Score : [0.03592658517726007, 0.985595166683197]\n",
      "[2020-05-18 16:24:49 RAM67.2% 1.23GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:24:49 RAM67.2% 1.23GB] Training on Fold : 4\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 22s - loss: 1.1332 - accuracy: 0.503 - ETA: 13s - loss: 1.0966 - accuracy: 0.496 - ETA: 10s - loss: 1.6210 - accuracy: 0.514 - ETA: 9s - loss: 1.3856 - accuracy: 0.537 - ETA: 8s - loss: 1.2504 - accuracy: 0.55 - ETA: 7s - loss: 1.1588 - accuracy: 0.55 - ETA: 7s - loss: 1.0886 - accuracy: 0.57 - ETA: 7s - loss: 1.0375 - accuracy: 0.57 - ETA: 6s - loss: 0.9925 - accuracy: 0.58 - ETA: 6s - loss: 0.9572 - accuracy: 0.59 - ETA: 6s - loss: 0.9256 - accuracy: 0.60 - ETA: 6s - loss: 0.9001 - accuracy: 0.60 - ETA: 5s - loss: 0.8792 - accuracy: 0.61 - ETA: 5s - loss: 0.8597 - accuracy: 0.61 - ETA: 5s - loss: 0.8413 - accuracy: 0.62 - ETA: 5s - loss: 0.8418 - accuracy: 0.62 - ETA: 5s - loss: 0.8338 - accuracy: 0.61 - ETA: 5s - loss: 0.8241 - accuracy: 0.61 - ETA: 4s - loss: 0.8112 - accuracy: 0.62 - ETA: 4s - loss: 0.7964 - accuracy: 0.63 - ETA: 4s - loss: 0.7852 - accuracy: 0.63 - ETA: 4s - loss: 0.7739 - accuracy: 0.64 - ETA: 4s - loss: 0.7660 - accuracy: 0.64 - ETA: 4s - loss: 0.7567 - accuracy: 0.64 - ETA: 4s - loss: 0.7502 - accuracy: 0.64 - ETA: 4s - loss: 0.7428 - accuracy: 0.65 - ETA: 4s - loss: 0.7389 - accuracy: 0.65 - ETA: 4s - loss: 0.7328 - accuracy: 0.65 - ETA: 3s - loss: 0.7237 - accuracy: 0.65 - ETA: 3s - loss: 0.7146 - accuracy: 0.66 - ETA: 3s - loss: 0.7081 - accuracy: 0.66 - ETA: 3s - loss: 0.6977 - accuracy: 0.67 - ETA: 3s - loss: 0.6921 - accuracy: 0.67 - ETA: 3s - loss: 0.6891 - accuracy: 0.67 - ETA: 3s - loss: 0.6843 - accuracy: 0.67 - ETA: 3s - loss: 0.6792 - accuracy: 0.67 - ETA: 3s - loss: 0.6750 - accuracy: 0.67 - ETA: 3s - loss: 0.6689 - accuracy: 0.67 - ETA: 2s - loss: 0.6625 - accuracy: 0.68 - ETA: 2s - loss: 0.6562 - accuracy: 0.68 - ETA: 2s - loss: 0.6505 - accuracy: 0.68 - ETA: 2s - loss: 0.6458 - accuracy: 0.68 - ETA: 2s - loss: 0.6399 - accuracy: 0.69 - ETA: 2s - loss: 0.6354 - accuracy: 0.69 - ETA: 2s - loss: 0.6332 - accuracy: 0.69 - ETA: 2s - loss: 0.6305 - accuracy: 0.69 - ETA: 2s - loss: 0.6274 - accuracy: 0.69 - ETA: 2s - loss: 0.6241 - accuracy: 0.69 - ETA: 2s - loss: 0.6194 - accuracy: 0.70 - ETA: 2s - loss: 0.6140 - accuracy: 0.70 - ETA: 1s - loss: 0.6096 - accuracy: 0.70 - ETA: 1s - loss: 0.6050 - accuracy: 0.70 - ETA: 1s - loss: 0.6004 - accuracy: 0.71 - ETA: 1s - loss: 0.5963 - accuracy: 0.71 - ETA: 1s - loss: 0.5929 - accuracy: 0.71 - ETA: 1s - loss: 0.5921 - accuracy: 0.71 - ETA: 1s - loss: 0.5917 - accuracy: 0.71 - ETA: 1s - loss: 0.5892 - accuracy: 0.71 - ETA: 1s - loss: 0.5857 - accuracy: 0.72 - ETA: 1s - loss: 0.5809 - accuracy: 0.72 - ETA: 1s - loss: 0.5773 - accuracy: 0.72 - ETA: 1s - loss: 0.5727 - accuracy: 0.72 - ETA: 0s - loss: 0.5682 - accuracy: 0.72 - ETA: 0s - loss: 0.5644 - accuracy: 0.73 - ETA: 0s - loss: 0.5611 - accuracy: 0.73 - ETA: 0s - loss: 0.5572 - accuracy: 0.73 - ETA: 0s - loss: 0.5535 - accuracy: 0.73 - ETA: 0s - loss: 0.5522 - accuracy: 0.73 - ETA: 0s - loss: 0.5501 - accuracy: 0.73 - ETA: 0s - loss: 0.5465 - accuracy: 0.74 - ETA: 0s - loss: 0.5424 - accuracy: 0.74 - ETA: 0s - loss: 0.5393 - accuracy: 0.74 - ETA: 0s - loss: 0.5389 - accuracy: 0.74 - 7s 621us/step - loss: 0.5379 - accuracy: 0.7456 - val_loss: 0.2634 - val_accuracy: 0.9014\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.01416\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.3110 - accuracy: 0.88 - ETA: 5s - loss: 0.3034 - accuracy: 0.88 - ETA: 5s - loss: 0.2749 - accuracy: 0.90 - ETA: 5s - loss: 0.2895 - accuracy: 0.88 - ETA: 5s - loss: 0.2838 - accuracy: 0.89 - ETA: 5s - loss: 0.2731 - accuracy: 0.89 - ETA: 5s - loss: 0.2680 - accuracy: 0.89 - ETA: 5s - loss: 0.2698 - accuracy: 0.89 - ETA: 5s - loss: 0.2650 - accuracy: 0.89 - ETA: 5s - loss: 0.2744 - accuracy: 0.88 - ETA: 5s - loss: 0.2767 - accuracy: 0.88 - ETA: 5s - loss: 0.2757 - accuracy: 0.88 - ETA: 4s - loss: 0.2763 - accuracy: 0.88 - ETA: 4s - loss: 0.2698 - accuracy: 0.88 - ETA: 4s - loss: 0.2696 - accuracy: 0.89 - ETA: 4s - loss: 0.2639 - accuracy: 0.89 - ETA: 4s - loss: 0.2631 - accuracy: 0.89 - ETA: 4s - loss: 0.2607 - accuracy: 0.89 - ETA: 4s - loss: 0.2577 - accuracy: 0.89 - ETA: 4s - loss: 0.2568 - accuracy: 0.89 - ETA: 4s - loss: 0.2534 - accuracy: 0.89 - ETA: 4s - loss: 0.2538 - accuracy: 0.89 - ETA: 4s - loss: 0.2585 - accuracy: 0.89 - ETA: 4s - loss: 0.2645 - accuracy: 0.89 - ETA: 3s - loss: 0.2643 - accuracy: 0.89 - ETA: 3s - loss: 0.2636 - accuracy: 0.89 - ETA: 3s - loss: 0.2632 - accuracy: 0.89 - ETA: 3s - loss: 0.2614 - accuracy: 0.89 - ETA: 3s - loss: 0.2613 - accuracy: 0.89 - ETA: 3s - loss: 0.2584 - accuracy: 0.89 - ETA: 3s - loss: 0.2571 - accuracy: 0.89 - ETA: 3s - loss: 0.2554 - accuracy: 0.89 - ETA: 3s - loss: 0.2558 - accuracy: 0.89 - ETA: 3s - loss: 0.2635 - accuracy: 0.89 - ETA: 3s - loss: 0.2662 - accuracy: 0.89 - ETA: 3s - loss: 0.2647 - accuracy: 0.89 - ETA: 3s - loss: 0.2639 - accuracy: 0.89 - ETA: 2s - loss: 0.2622 - accuracy: 0.89 - ETA: 2s - loss: 0.2607 - accuracy: 0.89 - ETA: 2s - loss: 0.2586 - accuracy: 0.89 - ETA: 2s - loss: 0.2565 - accuracy: 0.89 - ETA: 2s - loss: 0.2539 - accuracy: 0.89 - ETA: 2s - loss: 0.2513 - accuracy: 0.89 - ETA: 2s - loss: 0.2497 - accuracy: 0.89 - ETA: 2s - loss: 0.2493 - accuracy: 0.89 - ETA: 2s - loss: 0.2503 - accuracy: 0.89 - ETA: 2s - loss: 0.2522 - accuracy: 0.89 - ETA: 2s - loss: 0.2521 - accuracy: 0.89 - ETA: 2s - loss: 0.2515 - accuracy: 0.89 - ETA: 1s - loss: 0.2516 - accuracy: 0.89 - ETA: 1s - loss: 0.2507 - accuracy: 0.89 - ETA: 1s - loss: 0.2489 - accuracy: 0.89 - ETA: 1s - loss: 0.2470 - accuracy: 0.89 - ETA: 1s - loss: 0.2451 - accuracy: 0.89 - ETA: 1s - loss: 0.2464 - accuracy: 0.89 - ETA: 1s - loss: 0.2477 - accuracy: 0.89 - ETA: 1s - loss: 0.2475 - accuracy: 0.89 - ETA: 1s - loss: 0.2470 - accuracy: 0.89 - ETA: 1s - loss: 0.2450 - accuracy: 0.89 - ETA: 1s - loss: 0.2435 - accuracy: 0.90 - ETA: 1s - loss: 0.2418 - accuracy: 0.90 - ETA: 0s - loss: 0.2411 - accuracy: 0.90 - ETA: 0s - loss: 0.2404 - accuracy: 0.90 - ETA: 0s - loss: 0.2390 - accuracy: 0.90 - ETA: 0s - loss: 0.2375 - accuracy: 0.90 - ETA: 0s - loss: 0.2363 - accuracy: 0.90 - ETA: 0s - loss: 0.2348 - accuracy: 0.90 - ETA: 0s - loss: 0.2341 - accuracy: 0.90 - ETA: 0s - loss: 0.2351 - accuracy: 0.90 - ETA: 0s - loss: 0.2397 - accuracy: 0.90 - ETA: 0s - loss: 0.2427 - accuracy: 0.90 - ETA: 0s - loss: 0.2424 - accuracy: 0.90 - ETA: 0s - loss: 0.2412 - accuracy: 0.90 - 6s 589us/step - loss: 0.2410 - accuracy: 0.9016 - val_loss: 0.1767 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.01416\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.3048 - accuracy: 0.87 - ETA: 5s - loss: 0.2523 - accuracy: 0.89 - ETA: 5s - loss: 0.2422 - accuracy: 0.89 - ETA: 5s - loss: 0.2345 - accuracy: 0.89 - ETA: 5s - loss: 0.2318 - accuracy: 0.90 - ETA: 5s - loss: 0.2158 - accuracy: 0.91 - ETA: 5s - loss: 0.2029 - accuracy: 0.91 - ETA: 5s - loss: 0.2026 - accuracy: 0.91 - ETA: 5s - loss: 0.1922 - accuracy: 0.92 - ETA: 4s - loss: 0.1914 - accuracy: 0.92 - ETA: 4s - loss: 0.1889 - accuracy: 0.92 - ETA: 4s - loss: 0.1838 - accuracy: 0.92 - ETA: 4s - loss: 0.1823 - accuracy: 0.93 - ETA: 4s - loss: 0.1786 - accuracy: 0.93 - ETA: 4s - loss: 0.1789 - accuracy: 0.93 - ETA: 4s - loss: 0.1775 - accuracy: 0.93 - ETA: 4s - loss: 0.1827 - accuracy: 0.92 - ETA: 4s - loss: 0.1906 - accuracy: 0.92 - ETA: 4s - loss: 0.1938 - accuracy: 0.91 - ETA: 4s - loss: 0.1944 - accuracy: 0.91 - ETA: 4s - loss: 0.1912 - accuracy: 0.92 - ETA: 4s - loss: 0.1892 - accuracy: 0.92 - ETA: 4s - loss: 0.1887 - accuracy: 0.92 - ETA: 4s - loss: 0.1876 - accuracy: 0.92 - ETA: 4s - loss: 0.1860 - accuracy: 0.92 - ETA: 3s - loss: 0.1837 - accuracy: 0.92 - ETA: 3s - loss: 0.1821 - accuracy: 0.92 - ETA: 3s - loss: 0.1817 - accuracy: 0.92 - ETA: 3s - loss: 0.1831 - accuracy: 0.92 - ETA: 3s - loss: 0.1810 - accuracy: 0.92 - ETA: 3s - loss: 0.1773 - accuracy: 0.92 - ETA: 3s - loss: 0.1755 - accuracy: 0.92 - ETA: 3s - loss: 0.1729 - accuracy: 0.92 - ETA: 3s - loss: 0.1738 - accuracy: 0.92 - ETA: 3s - loss: 0.1764 - accuracy: 0.92 - ETA: 3s - loss: 0.1823 - accuracy: 0.92 - ETA: 3s - loss: 0.1868 - accuracy: 0.91 - ETA: 2s - loss: 0.1853 - accuracy: 0.92 - ETA: 2s - loss: 0.1832 - accuracy: 0.92 - ETA: 2s - loss: 0.1830 - accuracy: 0.92 - ETA: 2s - loss: 0.1818 - accuracy: 0.92 - ETA: 2s - loss: 0.1815 - accuracy: 0.92 - ETA: 2s - loss: 0.1842 - accuracy: 0.92 - ETA: 2s - loss: 0.1860 - accuracy: 0.92 - ETA: 2s - loss: 0.1859 - accuracy: 0.92 - ETA: 2s - loss: 0.1851 - accuracy: 0.92 - ETA: 2s - loss: 0.1828 - accuracy: 0.92 - ETA: 2s - loss: 0.1828 - accuracy: 0.92 - ETA: 2s - loss: 0.1808 - accuracy: 0.92 - ETA: 1s - loss: 0.1804 - accuracy: 0.92 - ETA: 1s - loss: 0.1786 - accuracy: 0.92 - ETA: 1s - loss: 0.1766 - accuracy: 0.92 - ETA: 1s - loss: 0.1750 - accuracy: 0.92 - ETA: 1s - loss: 0.1753 - accuracy: 0.92 - ETA: 1s - loss: 0.1765 - accuracy: 0.92 - ETA: 1s - loss: 0.1755 - accuracy: 0.92 - ETA: 1s - loss: 0.1740 - accuracy: 0.92 - ETA: 1s - loss: 0.1732 - accuracy: 0.92 - ETA: 1s - loss: 0.1720 - accuracy: 0.92 - ETA: 1s - loss: 0.1711 - accuracy: 0.92 - ETA: 1s - loss: 0.1708 - accuracy: 0.92 - ETA: 0s - loss: 0.1705 - accuracy: 0.92 - ETA: 0s - loss: 0.1720 - accuracy: 0.92 - ETA: 0s - loss: 0.1761 - accuracy: 0.92 - ETA: 0s - loss: 0.1757 - accuracy: 0.92 - ETA: 0s - loss: 0.1746 - accuracy: 0.92 - ETA: 0s - loss: 0.1738 - accuracy: 0.92 - ETA: 0s - loss: 0.1728 - accuracy: 0.92 - ETA: 0s - loss: 0.1717 - accuracy: 0.92 - ETA: 0s - loss: 0.1710 - accuracy: 0.92 - ETA: 0s - loss: 0.1705 - accuracy: 0.93 - ETA: 0s - loss: 0.1710 - accuracy: 0.92 - ETA: 0s - loss: 0.1697 - accuracy: 0.92 - 6s 585us/step - loss: 0.1693 - accuracy: 0.9299 - val_loss: 0.0717 - val_accuracy: 0.9840\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01416\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0692 - accuracy: 0.97 - ETA: 6s - loss: 0.0780 - accuracy: 0.97 - ETA: 6s - loss: 0.0798 - accuracy: 0.97 - ETA: 5s - loss: 0.0827 - accuracy: 0.96 - ETA: 5s - loss: 0.0941 - accuracy: 0.96 - ETA: 5s - loss: 0.0873 - accuracy: 0.96 - ETA: 5s - loss: 0.0836 - accuracy: 0.96 - ETA: 5s - loss: 0.0833 - accuracy: 0.96 - ETA: 5s - loss: 0.0853 - accuracy: 0.96 - ETA: 5s - loss: 0.0970 - accuracy: 0.96 - ETA: 5s - loss: 0.0998 - accuracy: 0.96 - ETA: 4s - loss: 0.1039 - accuracy: 0.96 - ETA: 4s - loss: 0.1269 - accuracy: 0.95 - ETA: 4s - loss: 0.1360 - accuracy: 0.95 - ETA: 4s - loss: 0.1369 - accuracy: 0.94 - ETA: 4s - loss: 0.1341 - accuracy: 0.95 - ETA: 4s - loss: 0.1347 - accuracy: 0.94 - ETA: 4s - loss: 0.1343 - accuracy: 0.94 - ETA: 4s - loss: 0.1327 - accuracy: 0.94 - ETA: 4s - loss: 0.1318 - accuracy: 0.95 - ETA: 4s - loss: 0.1343 - accuracy: 0.94 - ETA: 4s - loss: 0.1348 - accuracy: 0.94 - ETA: 4s - loss: 0.1336 - accuracy: 0.95 - ETA: 4s - loss: 0.1309 - accuracy: 0.95 - ETA: 4s - loss: 0.1289 - accuracy: 0.95 - ETA: 3s - loss: 0.1276 - accuracy: 0.95 - ETA: 3s - loss: 0.1264 - accuracy: 0.95 - ETA: 3s - loss: 0.1253 - accuracy: 0.95 - ETA: 3s - loss: 0.1240 - accuracy: 0.95 - ETA: 3s - loss: 0.1227 - accuracy: 0.95 - ETA: 3s - loss: 0.1224 - accuracy: 0.95 - ETA: 3s - loss: 0.1211 - accuracy: 0.95 - ETA: 3s - loss: 0.1202 - accuracy: 0.95 - ETA: 3s - loss: 0.1190 - accuracy: 0.95 - ETA: 3s - loss: 0.1169 - accuracy: 0.95 - ETA: 3s - loss: 0.1157 - accuracy: 0.95 - ETA: 2s - loss: 0.1141 - accuracy: 0.95 - ETA: 2s - loss: 0.1139 - accuracy: 0.95 - ETA: 2s - loss: 0.1121 - accuracy: 0.95 - ETA: 2s - loss: 0.1109 - accuracy: 0.95 - ETA: 2s - loss: 0.1104 - accuracy: 0.95 - ETA: 2s - loss: 0.1098 - accuracy: 0.95 - ETA: 2s - loss: 0.1095 - accuracy: 0.95 - ETA: 2s - loss: 0.1090 - accuracy: 0.95 - ETA: 2s - loss: 0.1090 - accuracy: 0.96 - ETA: 2s - loss: 0.1113 - accuracy: 0.95 - ETA: 2s - loss: 0.1113 - accuracy: 0.95 - ETA: 2s - loss: 0.1113 - accuracy: 0.95 - ETA: 1s - loss: 0.1133 - accuracy: 0.95 - ETA: 1s - loss: 0.1199 - accuracy: 0.95 - ETA: 1s - loss: 0.1372 - accuracy: 0.94 - ETA: 1s - loss: 0.1405 - accuracy: 0.94 - ETA: 1s - loss: 0.1394 - accuracy: 0.94 - ETA: 1s - loss: 0.1391 - accuracy: 0.94 - ETA: 1s - loss: 0.1388 - accuracy: 0.94 - ETA: 1s - loss: 0.1378 - accuracy: 0.94 - ETA: 1s - loss: 0.1369 - accuracy: 0.95 - ETA: 1s - loss: 0.1358 - accuracy: 0.95 - ETA: 1s - loss: 0.1346 - accuracy: 0.95 - ETA: 1s - loss: 0.1339 - accuracy: 0.95 - ETA: 1s - loss: 0.1334 - accuracy: 0.95 - ETA: 0s - loss: 0.1331 - accuracy: 0.95 - ETA: 0s - loss: 0.1326 - accuracy: 0.95 - ETA: 0s - loss: 0.1328 - accuracy: 0.95 - ETA: 0s - loss: 0.1323 - accuracy: 0.95 - ETA: 0s - loss: 0.1311 - accuracy: 0.95 - ETA: 0s - loss: 0.1304 - accuracy: 0.95 - ETA: 0s - loss: 0.1291 - accuracy: 0.95 - ETA: 0s - loss: 0.1284 - accuracy: 0.95 - ETA: 0s - loss: 0.1285 - accuracy: 0.95 - ETA: 0s - loss: 0.1275 - accuracy: 0.95 - ETA: 0s - loss: 0.1269 - accuracy: 0.95 - ETA: 0s - loss: 0.1270 - accuracy: 0.95 - 6s 565us/step - loss: 0.1271 - accuracy: 0.9544 - val_loss: 0.0759 - val_accuracy: 0.9806\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01416\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1347 - accuracy: 0.95 - ETA: 5s - loss: 0.1001 - accuracy: 0.96 - ETA: 5s - loss: 0.1073 - accuracy: 0.97 - ETA: 5s - loss: 0.1032 - accuracy: 0.96 - ETA: 5s - loss: 0.1049 - accuracy: 0.96 - ETA: 5s - loss: 0.0996 - accuracy: 0.96 - ETA: 5s - loss: 0.0926 - accuracy: 0.96 - ETA: 4s - loss: 0.0856 - accuracy: 0.96 - ETA: 4s - loss: 0.0828 - accuracy: 0.96 - ETA: 4s - loss: 0.0840 - accuracy: 0.96 - ETA: 4s - loss: 0.0811 - accuracy: 0.96 - ETA: 4s - loss: 0.0813 - accuracy: 0.96 - ETA: 4s - loss: 0.0795 - accuracy: 0.96 - ETA: 4s - loss: 0.0771 - accuracy: 0.96 - ETA: 4s - loss: 0.0792 - accuracy: 0.96 - ETA: 4s - loss: 0.0770 - accuracy: 0.97 - ETA: 4s - loss: 0.0748 - accuracy: 0.97 - ETA: 4s - loss: 0.0737 - accuracy: 0.97 - ETA: 4s - loss: 0.0777 - accuracy: 0.96 - ETA: 4s - loss: 0.0855 - accuracy: 0.96 - ETA: 4s - loss: 0.0873 - accuracy: 0.96 - ETA: 3s - loss: 0.0868 - accuracy: 0.96 - ETA: 3s - loss: 0.0879 - accuracy: 0.96 - ETA: 3s - loss: 0.0866 - accuracy: 0.96 - ETA: 3s - loss: 0.0851 - accuracy: 0.96 - ETA: 3s - loss: 0.0852 - accuracy: 0.96 - ETA: 3s - loss: 0.0857 - accuracy: 0.96 - ETA: 3s - loss: 0.0845 - accuracy: 0.96 - ETA: 3s - loss: 0.0832 - accuracy: 0.96 - ETA: 3s - loss: 0.0818 - accuracy: 0.96 - ETA: 3s - loss: 0.0817 - accuracy: 0.96 - ETA: 3s - loss: 0.0815 - accuracy: 0.96 - ETA: 3s - loss: 0.0818 - accuracy: 0.96 - ETA: 3s - loss: 0.0882 - accuracy: 0.96 - ETA: 2s - loss: 0.0890 - accuracy: 0.96 - ETA: 2s - loss: 0.0908 - accuracy: 0.96 - ETA: 2s - loss: 0.0926 - accuracy: 0.96 - ETA: 2s - loss: 0.0927 - accuracy: 0.96 - ETA: 2s - loss: 0.0924 - accuracy: 0.96 - ETA: 2s - loss: 0.0915 - accuracy: 0.96 - ETA: 2s - loss: 0.0906 - accuracy: 0.96 - ETA: 2s - loss: 0.0911 - accuracy: 0.96 - ETA: 2s - loss: 0.0912 - accuracy: 0.96 - ETA: 2s - loss: 0.0897 - accuracy: 0.96 - ETA: 2s - loss: 0.0902 - accuracy: 0.96 - ETA: 2s - loss: 0.0888 - accuracy: 0.96 - ETA: 2s - loss: 0.0875 - accuracy: 0.96 - ETA: 1s - loss: 0.0877 - accuracy: 0.96 - ETA: 1s - loss: 0.0925 - accuracy: 0.96 - ETA: 1s - loss: 0.0966 - accuracy: 0.96 - ETA: 1s - loss: 0.0957 - accuracy: 0.96 - ETA: 1s - loss: 0.0948 - accuracy: 0.96 - ETA: 1s - loss: 0.0946 - accuracy: 0.96 - ETA: 1s - loss: 0.0938 - accuracy: 0.96 - ETA: 1s - loss: 0.0934 - accuracy: 0.96 - ETA: 1s - loss: 0.0933 - accuracy: 0.96 - ETA: 1s - loss: 0.0927 - accuracy: 0.96 - ETA: 1s - loss: 0.0918 - accuracy: 0.96 - ETA: 1s - loss: 0.0912 - accuracy: 0.96 - ETA: 1s - loss: 0.0907 - accuracy: 0.96 - ETA: 0s - loss: 0.0904 - accuracy: 0.96 - ETA: 0s - loss: 0.0899 - accuracy: 0.96 - ETA: 0s - loss: 0.0895 - accuracy: 0.96 - ETA: 0s - loss: 0.0890 - accuracy: 0.96 - ETA: 0s - loss: 0.0881 - accuracy: 0.96 - ETA: 0s - loss: 0.0882 - accuracy: 0.96 - ETA: 0s - loss: 0.0872 - accuracy: 0.96 - ETA: 0s - loss: 0.0872 - accuracy: 0.96 - ETA: 0s - loss: 0.0866 - accuracy: 0.96 - ETA: 0s - loss: 0.0865 - accuracy: 0.96 - ETA: 0s - loss: 0.0865 - accuracy: 0.96 - ETA: 0s - loss: 0.0871 - accuracy: 0.96 - ETA: 0s - loss: 0.0884 - accuracy: 0.96 - 6s 564us/step - loss: 0.0883 - accuracy: 0.9683 - val_loss: 0.0462 - val_accuracy: 0.9832\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01416\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0781 - accuracy: 0.96 - ETA: 6s - loss: 0.0681 - accuracy: 0.97 - ETA: 6s - loss: 0.0553 - accuracy: 0.97 - ETA: 5s - loss: 0.0507 - accuracy: 0.97 - ETA: 5s - loss: 0.0658 - accuracy: 0.97 - ETA: 5s - loss: 0.1017 - accuracy: 0.96 - ETA: 5s - loss: 0.1197 - accuracy: 0.95 - ETA: 5s - loss: 0.1451 - accuracy: 0.94 - ETA: 5s - loss: 0.1605 - accuracy: 0.94 - ETA: 5s - loss: 0.1506 - accuracy: 0.94 - ETA: 5s - loss: 0.1417 - accuracy: 0.94 - ETA: 4s - loss: 0.1317 - accuracy: 0.95 - ETA: 4s - loss: 0.1261 - accuracy: 0.95 - ETA: 4s - loss: 0.1184 - accuracy: 0.95 - ETA: 4s - loss: 0.1134 - accuracy: 0.95 - ETA: 4s - loss: 0.1098 - accuracy: 0.95 - ETA: 4s - loss: 0.1047 - accuracy: 0.96 - ETA: 4s - loss: 0.1005 - accuracy: 0.96 - ETA: 4s - loss: 0.0979 - accuracy: 0.96 - ETA: 4s - loss: 0.0941 - accuracy: 0.96 - ETA: 4s - loss: 0.0922 - accuracy: 0.96 - ETA: 4s - loss: 0.0906 - accuracy: 0.96 - ETA: 4s - loss: 0.0913 - accuracy: 0.96 - ETA: 3s - loss: 0.0919 - accuracy: 0.96 - ETA: 3s - loss: 0.0904 - accuracy: 0.96 - ETA: 3s - loss: 0.0877 - accuracy: 0.96 - ETA: 3s - loss: 0.0850 - accuracy: 0.96 - ETA: 3s - loss: 0.0827 - accuracy: 0.96 - ETA: 3s - loss: 0.0810 - accuracy: 0.97 - ETA: 3s - loss: 0.0791 - accuracy: 0.97 - ETA: 3s - loss: 0.0781 - accuracy: 0.97 - ETA: 3s - loss: 0.0768 - accuracy: 0.97 - ETA: 3s - loss: 0.0757 - accuracy: 0.97 - ETA: 3s - loss: 0.0785 - accuracy: 0.97 - ETA: 3s - loss: 0.0819 - accuracy: 0.97 - ETA: 3s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0815 - accuracy: 0.97 - ETA: 2s - loss: 0.0807 - accuracy: 0.97 - ETA: 2s - loss: 0.0799 - accuracy: 0.97 - ETA: 2s - loss: 0.0793 - accuracy: 0.97 - ETA: 2s - loss: 0.0784 - accuracy: 0.97 - ETA: 2s - loss: 0.0786 - accuracy: 0.97 - ETA: 2s - loss: 0.0799 - accuracy: 0.97 - ETA: 2s - loss: 0.0850 - accuracy: 0.97 - ETA: 2s - loss: 0.0854 - accuracy: 0.96 - ETA: 2s - loss: 0.0844 - accuracy: 0.97 - ETA: 2s - loss: 0.0834 - accuracy: 0.97 - ETA: 2s - loss: 0.0827 - accuracy: 0.97 - ETA: 2s - loss: 0.0819 - accuracy: 0.97 - ETA: 1s - loss: 0.0817 - accuracy: 0.97 - ETA: 1s - loss: 0.0815 - accuracy: 0.97 - ETA: 1s - loss: 0.0814 - accuracy: 0.97 - ETA: 1s - loss: 0.0808 - accuracy: 0.97 - ETA: 1s - loss: 0.0798 - accuracy: 0.97 - ETA: 1s - loss: 0.0790 - accuracy: 0.97 - ETA: 1s - loss: 0.0782 - accuracy: 0.97 - ETA: 1s - loss: 0.0777 - accuracy: 0.97 - ETA: 1s - loss: 0.0769 - accuracy: 0.97 - ETA: 1s - loss: 0.0767 - accuracy: 0.97 - ETA: 1s - loss: 0.0770 - accuracy: 0.97 - ETA: 1s - loss: 0.0784 - accuracy: 0.97 - ETA: 0s - loss: 0.0784 - accuracy: 0.97 - ETA: 0s - loss: 0.0778 - accuracy: 0.97 - ETA: 0s - loss: 0.0774 - accuracy: 0.97 - ETA: 0s - loss: 0.0765 - accuracy: 0.97 - ETA: 0s - loss: 0.0756 - accuracy: 0.97 - ETA: 0s - loss: 0.0748 - accuracy: 0.97 - ETA: 0s - loss: 0.0742 - accuracy: 0.97 - ETA: 0s - loss: 0.0740 - accuracy: 0.97 - ETA: 0s - loss: 0.0732 - accuracy: 0.97 - ETA: 0s - loss: 0.0730 - accuracy: 0.97 - ETA: 0s - loss: 0.0740 - accuracy: 0.97 - ETA: 0s - loss: 0.0738 - accuracy: 0.97 - 6s 592us/step - loss: 0.0734 - accuracy: 0.9740 - val_loss: 0.0722 - val_accuracy: 0.9671\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01416\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0298 - accuracy: 1.00 - ETA: 5s - loss: 0.0446 - accuracy: 0.98 - ETA: 5s - loss: 0.0934 - accuracy: 0.97 - ETA: 5s - loss: 0.1483 - accuracy: 0.96 - ETA: 5s - loss: 0.1428 - accuracy: 0.95 - ETA: 5s - loss: 0.1363 - accuracy: 0.96 - ETA: 5s - loss: 0.1229 - accuracy: 0.96 - ETA: 5s - loss: 0.1122 - accuracy: 0.96 - ETA: 5s - loss: 0.1040 - accuracy: 0.96 - ETA: 5s - loss: 0.1027 - accuracy: 0.96 - ETA: 5s - loss: 0.0968 - accuracy: 0.96 - ETA: 4s - loss: 0.0938 - accuracy: 0.97 - ETA: 4s - loss: 0.0899 - accuracy: 0.97 - ETA: 4s - loss: 0.0848 - accuracy: 0.97 - ETA: 4s - loss: 0.0811 - accuracy: 0.97 - ETA: 4s - loss: 0.0809 - accuracy: 0.97 - ETA: 4s - loss: 0.0850 - accuracy: 0.97 - ETA: 4s - loss: 0.0938 - accuracy: 0.97 - ETA: 4s - loss: 0.1007 - accuracy: 0.96 - ETA: 4s - loss: 0.0984 - accuracy: 0.96 - ETA: 4s - loss: 0.0941 - accuracy: 0.96 - ETA: 4s - loss: 0.0916 - accuracy: 0.96 - ETA: 4s - loss: 0.0893 - accuracy: 0.96 - ETA: 4s - loss: 0.0863 - accuracy: 0.97 - ETA: 3s - loss: 0.0847 - accuracy: 0.97 - ETA: 3s - loss: 0.0834 - accuracy: 0.97 - ETA: 3s - loss: 0.0831 - accuracy: 0.97 - ETA: 3s - loss: 0.0814 - accuracy: 0.97 - ETA: 3s - loss: 0.0796 - accuracy: 0.97 - ETA: 3s - loss: 0.0783 - accuracy: 0.97 - ETA: 3s - loss: 0.0798 - accuracy: 0.97 - ETA: 3s - loss: 0.0805 - accuracy: 0.97 - ETA: 3s - loss: 0.0851 - accuracy: 0.97 - ETA: 3s - loss: 0.0843 - accuracy: 0.97 - ETA: 3s - loss: 0.0832 - accuracy: 0.97 - ETA: 3s - loss: 0.0836 - accuracy: 0.97 - ETA: 2s - loss: 0.0820 - accuracy: 0.97 - ETA: 2s - loss: 0.0811 - accuracy: 0.97 - ETA: 2s - loss: 0.0808 - accuracy: 0.97 - ETA: 2s - loss: 0.0798 - accuracy: 0.97 - ETA: 2s - loss: 0.0791 - accuracy: 0.97 - ETA: 2s - loss: 0.0787 - accuracy: 0.97 - ETA: 2s - loss: 0.0777 - accuracy: 0.97 - ETA: 2s - loss: 0.0765 - accuracy: 0.97 - ETA: 2s - loss: 0.0759 - accuracy: 0.97 - ETA: 2s - loss: 0.0765 - accuracy: 0.97 - ETA: 2s - loss: 0.0781 - accuracy: 0.97 - ETA: 2s - loss: 0.0772 - accuracy: 0.97 - ETA: 1s - loss: 0.0761 - accuracy: 0.97 - ETA: 1s - loss: 0.0758 - accuracy: 0.97 - ETA: 1s - loss: 0.0750 - accuracy: 0.97 - ETA: 1s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0730 - accuracy: 0.97 - ETA: 1s - loss: 0.0729 - accuracy: 0.97 - ETA: 1s - loss: 0.0732 - accuracy: 0.97 - ETA: 1s - loss: 0.0733 - accuracy: 0.97 - ETA: 1s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0754 - accuracy: 0.97 - ETA: 1s - loss: 0.0750 - accuracy: 0.97 - ETA: 1s - loss: 0.0746 - accuracy: 0.97 - ETA: 1s - loss: 0.0736 - accuracy: 0.97 - ETA: 0s - loss: 0.0730 - accuracy: 0.97 - ETA: 0s - loss: 0.0723 - accuracy: 0.97 - ETA: 0s - loss: 0.0723 - accuracy: 0.97 - ETA: 0s - loss: 0.0719 - accuracy: 0.97 - ETA: 0s - loss: 0.0718 - accuracy: 0.97 - ETA: 0s - loss: 0.0720 - accuracy: 0.97 - ETA: 0s - loss: 0.0713 - accuracy: 0.97 - ETA: 0s - loss: 0.0713 - accuracy: 0.97 - ETA: 0s - loss: 0.0724 - accuracy: 0.97 - ETA: 0s - loss: 0.0720 - accuracy: 0.97 - ETA: 0s - loss: 0.0716 - accuracy: 0.97 - ETA: 0s - loss: 0.0718 - accuracy: 0.97 - 6s 577us/step - loss: 0.0721 - accuracy: 0.9755 - val_loss: 0.1305 - val_accuracy: 0.9469\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01416\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1457 - accuracy: 0.94 - ETA: 5s - loss: 0.1430 - accuracy: 0.94 - ETA: 5s - loss: 0.1167 - accuracy: 0.96 - ETA: 5s - loss: 0.1037 - accuracy: 0.96 - ETA: 5s - loss: 0.0879 - accuracy: 0.96 - ETA: 5s - loss: 0.0754 - accuracy: 0.97 - ETA: 5s - loss: 0.0670 - accuracy: 0.97 - ETA: 5s - loss: 0.0661 - accuracy: 0.97 - ETA: 5s - loss: 0.0656 - accuracy: 0.97 - ETA: 5s - loss: 0.0638 - accuracy: 0.97 - ETA: 5s - loss: 0.0620 - accuracy: 0.97 - ETA: 5s - loss: 0.0633 - accuracy: 0.97 - ETA: 4s - loss: 0.0623 - accuracy: 0.97 - ETA: 4s - loss: 0.0606 - accuracy: 0.97 - ETA: 4s - loss: 0.0615 - accuracy: 0.97 - ETA: 4s - loss: 0.0594 - accuracy: 0.97 - ETA: 4s - loss: 0.0589 - accuracy: 0.97 - ETA: 4s - loss: 0.0574 - accuracy: 0.97 - ETA: 4s - loss: 0.0562 - accuracy: 0.97 - ETA: 4s - loss: 0.0572 - accuracy: 0.97 - ETA: 4s - loss: 0.0580 - accuracy: 0.97 - ETA: 4s - loss: 0.0576 - accuracy: 0.97 - ETA: 4s - loss: 0.0573 - accuracy: 0.97 - ETA: 4s - loss: 0.0588 - accuracy: 0.97 - ETA: 4s - loss: 0.0587 - accuracy: 0.97 - ETA: 3s - loss: 0.0598 - accuracy: 0.97 - ETA: 3s - loss: 0.0633 - accuracy: 0.97 - ETA: 3s - loss: 0.0625 - accuracy: 0.97 - ETA: 3s - loss: 0.0609 - accuracy: 0.97 - ETA: 3s - loss: 0.0602 - accuracy: 0.97 - ETA: 3s - loss: 0.0592 - accuracy: 0.97 - ETA: 3s - loss: 0.0580 - accuracy: 0.97 - ETA: 3s - loss: 0.0571 - accuracy: 0.97 - ETA: 3s - loss: 0.0564 - accuracy: 0.97 - ETA: 3s - loss: 0.0556 - accuracy: 0.97 - ETA: 3s - loss: 0.0551 - accuracy: 0.97 - ETA: 3s - loss: 0.0554 - accuracy: 0.97 - ETA: 2s - loss: 0.0554 - accuracy: 0.97 - ETA: 2s - loss: 0.0555 - accuracy: 0.97 - ETA: 2s - loss: 0.0583 - accuracy: 0.97 - ETA: 2s - loss: 0.0609 - accuracy: 0.97 - ETA: 2s - loss: 0.0657 - accuracy: 0.97 - ETA: 2s - loss: 0.0700 - accuracy: 0.97 - ETA: 2s - loss: 0.0700 - accuracy: 0.97 - ETA: 2s - loss: 0.0688 - accuracy: 0.97 - ETA: 2s - loss: 0.0678 - accuracy: 0.97 - ETA: 2s - loss: 0.0672 - accuracy: 0.97 - ETA: 2s - loss: 0.0665 - accuracy: 0.97 - ETA: 2s - loss: 0.0653 - accuracy: 0.97 - ETA: 1s - loss: 0.0647 - accuracy: 0.97 - ETA: 1s - loss: 0.0653 - accuracy: 0.97 - ETA: 1s - loss: 0.0659 - accuracy: 0.97 - ETA: 1s - loss: 0.0654 - accuracy: 0.97 - ETA: 1s - loss: 0.0654 - accuracy: 0.97 - ETA: 1s - loss: 0.0645 - accuracy: 0.97 - ETA: 1s - loss: 0.0639 - accuracy: 0.97 - ETA: 1s - loss: 0.0633 - accuracy: 0.97 - ETA: 1s - loss: 0.0631 - accuracy: 0.97 - ETA: 1s - loss: 0.0630 - accuracy: 0.97 - ETA: 1s - loss: 0.0623 - accuracy: 0.97 - ETA: 1s - loss: 0.0621 - accuracy: 0.97 - ETA: 0s - loss: 0.0618 - accuracy: 0.97 - ETA: 0s - loss: 0.0613 - accuracy: 0.97 - ETA: 0s - loss: 0.0612 - accuracy: 0.97 - ETA: 0s - loss: 0.0615 - accuracy: 0.97 - ETA: 0s - loss: 0.0628 - accuracy: 0.97 - ETA: 0s - loss: 0.0629 - accuracy: 0.97 - ETA: 0s - loss: 0.0628 - accuracy: 0.97 - ETA: 0s - loss: 0.0631 - accuracy: 0.97 - ETA: 0s - loss: 0.0631 - accuracy: 0.97 - ETA: 0s - loss: 0.0629 - accuracy: 0.97 - ETA: 0s - loss: 0.0624 - accuracy: 0.97 - ETA: 0s - loss: 0.0630 - accuracy: 0.97 - 6s 568us/step - loss: 0.0626 - accuracy: 0.9777 - val_loss: 0.0216 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01416\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0407 - accuracy: 0.98 - ETA: 6s - loss: 0.0663 - accuracy: 0.97 - ETA: 5s - loss: 0.0863 - accuracy: 0.96 - ETA: 5s - loss: 0.0948 - accuracy: 0.96 - ETA: 5s - loss: 0.0790 - accuracy: 0.97 - ETA: 5s - loss: 0.0708 - accuracy: 0.97 - ETA: 5s - loss: 0.0650 - accuracy: 0.97 - ETA: 5s - loss: 0.0602 - accuracy: 0.97 - ETA: 5s - loss: 0.0548 - accuracy: 0.98 - ETA: 4s - loss: 0.0511 - accuracy: 0.98 - ETA: 4s - loss: 0.0481 - accuracy: 0.98 - ETA: 4s - loss: 0.0462 - accuracy: 0.98 - ETA: 4s - loss: 0.0451 - accuracy: 0.98 - ETA: 4s - loss: 0.0431 - accuracy: 0.98 - ETA: 4s - loss: 0.0414 - accuracy: 0.98 - ETA: 4s - loss: 0.0392 - accuracy: 0.98 - ETA: 4s - loss: 0.0397 - accuracy: 0.98 - ETA: 4s - loss: 0.0512 - accuracy: 0.98 - ETA: 4s - loss: 0.0735 - accuracy: 0.97 - ETA: 4s - loss: 0.0771 - accuracy: 0.97 - ETA: 4s - loss: 0.0754 - accuracy: 0.97 - ETA: 4s - loss: 0.0743 - accuracy: 0.97 - ETA: 3s - loss: 0.0742 - accuracy: 0.97 - ETA: 3s - loss: 0.0728 - accuracy: 0.97 - ETA: 3s - loss: 0.0732 - accuracy: 0.97 - ETA: 3s - loss: 0.0717 - accuracy: 0.97 - ETA: 3s - loss: 0.0696 - accuracy: 0.97 - ETA: 3s - loss: 0.0687 - accuracy: 0.97 - ETA: 3s - loss: 0.0689 - accuracy: 0.97 - ETA: 3s - loss: 0.0708 - accuracy: 0.97 - ETA: 3s - loss: 0.0688 - accuracy: 0.97 - ETA: 3s - loss: 0.0678 - accuracy: 0.97 - ETA: 3s - loss: 0.0681 - accuracy: 0.97 - ETA: 3s - loss: 0.0680 - accuracy: 0.97 - ETA: 3s - loss: 0.0671 - accuracy: 0.97 - ETA: 2s - loss: 0.0662 - accuracy: 0.97 - ETA: 2s - loss: 0.0653 - accuracy: 0.97 - ETA: 2s - loss: 0.0638 - accuracy: 0.97 - ETA: 2s - loss: 0.0626 - accuracy: 0.98 - ETA: 2s - loss: 0.0615 - accuracy: 0.98 - ETA: 2s - loss: 0.0605 - accuracy: 0.98 - ETA: 2s - loss: 0.0596 - accuracy: 0.98 - ETA: 2s - loss: 0.0598 - accuracy: 0.98 - ETA: 2s - loss: 0.0602 - accuracy: 0.98 - ETA: 2s - loss: 0.0597 - accuracy: 0.98 - ETA: 2s - loss: 0.0590 - accuracy: 0.98 - ETA: 2s - loss: 0.0581 - accuracy: 0.98 - ETA: 2s - loss: 0.0572 - accuracy: 0.98 - ETA: 1s - loss: 0.0570 - accuracy: 0.98 - ETA: 1s - loss: 0.0575 - accuracy: 0.98 - ETA: 1s - loss: 0.0570 - accuracy: 0.98 - ETA: 1s - loss: 0.0560 - accuracy: 0.98 - ETA: 1s - loss: 0.0554 - accuracy: 0.98 - ETA: 1s - loss: 0.0551 - accuracy: 0.98 - ETA: 1s - loss: 0.0559 - accuracy: 0.98 - ETA: 1s - loss: 0.0552 - accuracy: 0.98 - ETA: 1s - loss: 0.0550 - accuracy: 0.98 - ETA: 1s - loss: 0.0555 - accuracy: 0.98 - ETA: 1s - loss: 0.0567 - accuracy: 0.98 - ETA: 1s - loss: 0.0561 - accuracy: 0.98 - ETA: 1s - loss: 0.0570 - accuracy: 0.98 - ETA: 0s - loss: 0.0567 - accuracy: 0.98 - ETA: 0s - loss: 0.0572 - accuracy: 0.98 - ETA: 0s - loss: 0.0572 - accuracy: 0.98 - ETA: 0s - loss: 0.0573 - accuracy: 0.98 - ETA: 0s - loss: 0.0569 - accuracy: 0.98 - ETA: 0s - loss: 0.0564 - accuracy: 0.98 - ETA: 0s - loss: 0.0562 - accuracy: 0.98 - ETA: 0s - loss: 0.0556 - accuracy: 0.98 - ETA: 0s - loss: 0.0551 - accuracy: 0.98 - ETA: 0s - loss: 0.0548 - accuracy: 0.98 - ETA: 0s - loss: 0.0546 - accuracy: 0.98 - ETA: 0s - loss: 0.0543 - accuracy: 0.98 - 6s 571us/step - loss: 0.0543 - accuracy: 0.9820 - val_loss: 0.0384 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01416\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0247 - accuracy: 0.98 - ETA: 5s - loss: 0.0214 - accuracy: 0.98 - ETA: 5s - loss: 0.0365 - accuracy: 0.98 - ETA: 5s - loss: 0.0340 - accuracy: 0.98 - ETA: 5s - loss: 0.0313 - accuracy: 0.98 - ETA: 5s - loss: 0.0482 - accuracy: 0.98 - ETA: 5s - loss: 0.0683 - accuracy: 0.97 - ETA: 5s - loss: 0.0633 - accuracy: 0.97 - ETA: 5s - loss: 0.0659 - accuracy: 0.97 - ETA: 5s - loss: 0.0634 - accuracy: 0.97 - ETA: 5s - loss: 0.0645 - accuracy: 0.97 - ETA: 4s - loss: 0.0612 - accuracy: 0.97 - ETA: 4s - loss: 0.0586 - accuracy: 0.97 - ETA: 4s - loss: 0.0561 - accuracy: 0.97 - ETA: 4s - loss: 0.0536 - accuracy: 0.97 - ETA: 4s - loss: 0.0534 - accuracy: 0.97 - ETA: 4s - loss: 0.0507 - accuracy: 0.98 - ETA: 4s - loss: 0.0495 - accuracy: 0.98 - ETA: 4s - loss: 0.0477 - accuracy: 0.98 - ETA: 4s - loss: 0.0469 - accuracy: 0.98 - ETA: 4s - loss: 0.0480 - accuracy: 0.98 - ETA: 4s - loss: 0.0496 - accuracy: 0.98 - ETA: 4s - loss: 0.0481 - accuracy: 0.98 - ETA: 3s - loss: 0.0478 - accuracy: 0.98 - ETA: 3s - loss: 0.0474 - accuracy: 0.98 - ETA: 3s - loss: 0.0474 - accuracy: 0.98 - ETA: 3s - loss: 0.0514 - accuracy: 0.98 - ETA: 3s - loss: 0.0520 - accuracy: 0.98 - ETA: 3s - loss: 0.0507 - accuracy: 0.98 - ETA: 3s - loss: 0.0496 - accuracy: 0.98 - ETA: 3s - loss: 0.0490 - accuracy: 0.98 - ETA: 3s - loss: 0.0477 - accuracy: 0.98 - ETA: 3s - loss: 0.0476 - accuracy: 0.98 - ETA: 3s - loss: 0.0508 - accuracy: 0.98 - ETA: 3s - loss: 0.0516 - accuracy: 0.98 - ETA: 3s - loss: 0.0515 - accuracy: 0.98 - ETA: 2s - loss: 0.0516 - accuracy: 0.98 - ETA: 2s - loss: 0.0509 - accuracy: 0.98 - ETA: 2s - loss: 0.0507 - accuracy: 0.98 - ETA: 2s - loss: 0.0506 - accuracy: 0.98 - ETA: 2s - loss: 0.0501 - accuracy: 0.98 - ETA: 2s - loss: 0.0505 - accuracy: 0.98 - ETA: 2s - loss: 0.0533 - accuracy: 0.98 - ETA: 2s - loss: 0.0523 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 2s - loss: 0.0515 - accuracy: 0.98 - ETA: 2s - loss: 0.0509 - accuracy: 0.98 - ETA: 2s - loss: 0.0508 - accuracy: 0.98 - ETA: 1s - loss: 0.0502 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0510 - accuracy: 0.98 - ETA: 1s - loss: 0.0507 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 1s - loss: 0.0492 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0492 - accuracy: 0.98 - ETA: 0s - loss: 0.0496 - accuracy: 0.98 - ETA: 0s - loss: 0.0493 - accuracy: 0.98 - ETA: 0s - loss: 0.0497 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0483 - accuracy: 0.98 - ETA: 0s - loss: 0.0487 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0539 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - 6s 568us/step - loss: 0.0534 - accuracy: 0.9811 - val_loss: 0.0305 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01416\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0341 - accuracy: 0.98 - ETA: 5s - loss: 0.0236 - accuracy: 0.99 - ETA: 5s - loss: 0.0221 - accuracy: 0.99 - ETA: 5s - loss: 0.0370 - accuracy: 0.98 - ETA: 5s - loss: 0.0389 - accuracy: 0.98 - ETA: 5s - loss: 0.0459 - accuracy: 0.98 - ETA: 5s - loss: 0.0423 - accuracy: 0.98 - ETA: 5s - loss: 0.0391 - accuracy: 0.98 - ETA: 5s - loss: 0.0384 - accuracy: 0.98 - ETA: 5s - loss: 0.0358 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0315 - accuracy: 0.98 - ETA: 4s - loss: 0.0319 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0320 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0324 - accuracy: 0.98 - ETA: 4s - loss: 0.0318 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0307 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 4s - loss: 0.0344 - accuracy: 0.98 - ETA: 3s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0371 - accuracy: 0.98 - ETA: 3s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0373 - accuracy: 0.98 - ETA: 3s - loss: 0.0371 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0369 - accuracy: 0.98 - ETA: 3s - loss: 0.0362 - accuracy: 0.98 - ETA: 3s - loss: 0.0358 - accuracy: 0.98 - ETA: 3s - loss: 0.0352 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0352 - accuracy: 0.98 - ETA: 2s - loss: 0.0354 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0352 - accuracy: 0.98 - ETA: 2s - loss: 0.0351 - accuracy: 0.98 - ETA: 2s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0361 - accuracy: 0.98 - ETA: 1s - loss: 0.0359 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 1s - loss: 0.0364 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0362 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0376 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - 6s 582us/step - loss: 0.0379 - accuracy: 0.9866 - val_loss: 0.0545 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01416\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1268 - accuracy: 0.97 - ETA: 5s - loss: 0.0701 - accuracy: 0.98 - ETA: 5s - loss: 0.0536 - accuracy: 0.98 - ETA: 5s - loss: 0.0433 - accuracy: 0.98 - ETA: 5s - loss: 0.0398 - accuracy: 0.98 - ETA: 5s - loss: 0.0340 - accuracy: 0.99 - ETA: 5s - loss: 0.0315 - accuracy: 0.99 - ETA: 5s - loss: 0.0322 - accuracy: 0.98 - ETA: 5s - loss: 0.0374 - accuracy: 0.98 - ETA: 5s - loss: 0.0362 - accuracy: 0.98 - ETA: 4s - loss: 0.0340 - accuracy: 0.98 - ETA: 4s - loss: 0.0346 - accuracy: 0.98 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0369 - accuracy: 0.98 - ETA: 4s - loss: 0.0348 - accuracy: 0.98 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0324 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0344 - accuracy: 0.98 - ETA: 4s - loss: 0.0401 - accuracy: 0.98 - ETA: 4s - loss: 0.0452 - accuracy: 0.98 - ETA: 4s - loss: 0.0457 - accuracy: 0.98 - ETA: 4s - loss: 0.0473 - accuracy: 0.98 - ETA: 3s - loss: 0.0468 - accuracy: 0.98 - ETA: 3s - loss: 0.0456 - accuracy: 0.98 - ETA: 3s - loss: 0.0442 - accuracy: 0.98 - ETA: 3s - loss: 0.0437 - accuracy: 0.98 - ETA: 3s - loss: 0.0424 - accuracy: 0.98 - ETA: 3s - loss: 0.0433 - accuracy: 0.98 - ETA: 3s - loss: 0.0421 - accuracy: 0.98 - ETA: 3s - loss: 0.0414 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0388 - accuracy: 0.98 - ETA: 3s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0421 - accuracy: 0.98 - ETA: 2s - loss: 0.0415 - accuracy: 0.98 - ETA: 2s - loss: 0.0407 - accuracy: 0.98 - ETA: 2s - loss: 0.0402 - accuracy: 0.98 - ETA: 2s - loss: 0.0393 - accuracy: 0.98 - ETA: 2s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0364 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0387 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0394 - accuracy: 0.98 - ETA: 1s - loss: 0.0394 - accuracy: 0.98 - ETA: 1s - loss: 0.0408 - accuracy: 0.98 - ETA: 1s - loss: 0.0421 - accuracy: 0.98 - ETA: 0s - loss: 0.0427 - accuracy: 0.98 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0421 - accuracy: 0.98 - ETA: 0s - loss: 0.0418 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - ETA: 0s - loss: 0.0411 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0404 - accuracy: 0.98 - ETA: 0s - loss: 0.0412 - accuracy: 0.98 - 6s 578us/step - loss: 0.0413 - accuracy: 0.9853 - val_loss: 0.0244 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01416\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0114 - accuracy: 1.00 - ETA: 5s - loss: 0.0209 - accuracy: 0.99 - ETA: 5s - loss: 0.0302 - accuracy: 0.99 - ETA: 5s - loss: 0.0284 - accuracy: 0.99 - ETA: 5s - loss: 0.0316 - accuracy: 0.98 - ETA: 5s - loss: 0.0311 - accuracy: 0.98 - ETA: 5s - loss: 0.0336 - accuracy: 0.99 - ETA: 5s - loss: 0.0421 - accuracy: 0.98 - ETA: 5s - loss: 0.0422 - accuracy: 0.98 - ETA: 5s - loss: 0.0389 - accuracy: 0.98 - ETA: 5s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0349 - accuracy: 0.98 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0320 - accuracy: 0.99 - ETA: 4s - loss: 0.0306 - accuracy: 0.99 - ETA: 4s - loss: 0.0307 - accuracy: 0.99 - ETA: 4s - loss: 0.0302 - accuracy: 0.98 - ETA: 4s - loss: 0.0289 - accuracy: 0.99 - ETA: 4s - loss: 0.0290 - accuracy: 0.99 - ETA: 4s - loss: 0.0280 - accuracy: 0.99 - ETA: 4s - loss: 0.0271 - accuracy: 0.99 - ETA: 4s - loss: 0.0265 - accuracy: 0.99 - ETA: 4s - loss: 0.0261 - accuracy: 0.99 - ETA: 4s - loss: 0.0254 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0292 - accuracy: 0.99 - ETA: 3s - loss: 0.0289 - accuracy: 0.99 - ETA: 3s - loss: 0.0293 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0414 - accuracy: 0.98 - ETA: 3s - loss: 0.0407 - accuracy: 0.98 - ETA: 3s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 2s - loss: 0.0378 - accuracy: 0.98 - ETA: 2s - loss: 0.0388 - accuracy: 0.98 - ETA: 2s - loss: 0.0383 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0390 - accuracy: 0.98 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0411 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0375 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0362 - accuracy: 0.98 - ETA: 0s - loss: 0.0360 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - 6s 597us/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 0.0164 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01416\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0144 - accuracy: 0.99 - ETA: 6s - loss: 0.0105 - accuracy: 0.99 - ETA: 6s - loss: 0.0072 - accuracy: 0.99 - ETA: 6s - loss: 0.0067 - accuracy: 0.99 - ETA: 6s - loss: 0.0059 - accuracy: 0.99 - ETA: 6s - loss: 0.0096 - accuracy: 0.99 - ETA: 6s - loss: 0.0130 - accuracy: 0.99 - ETA: 5s - loss: 0.0154 - accuracy: 0.99 - ETA: 5s - loss: 0.0163 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 5s - loss: 0.0290 - accuracy: 0.98 - ETA: 5s - loss: 0.0290 - accuracy: 0.98 - ETA: 5s - loss: 0.0345 - accuracy: 0.98 - ETA: 5s - loss: 0.0342 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0328 - accuracy: 0.98 - ETA: 4s - loss: 0.0317 - accuracy: 0.98 - ETA: 4s - loss: 0.0319 - accuracy: 0.98 - ETA: 4s - loss: 0.0365 - accuracy: 0.98 - ETA: 4s - loss: 0.0387 - accuracy: 0.98 - ETA: 4s - loss: 0.0385 - accuracy: 0.98 - ETA: 4s - loss: 0.0372 - accuracy: 0.98 - ETA: 4s - loss: 0.0371 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0370 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 3s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0417 - accuracy: 0.98 - ETA: 2s - loss: 0.0424 - accuracy: 0.98 - ETA: 2s - loss: 0.0417 - accuracy: 0.98 - ETA: 2s - loss: 0.0410 - accuracy: 0.98 - ETA: 2s - loss: 0.0410 - accuracy: 0.98 - ETA: 2s - loss: 0.0410 - accuracy: 0.98 - ETA: 2s - loss: 0.0408 - accuracy: 0.98 - ETA: 2s - loss: 0.0408 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0416 - accuracy: 0.98 - ETA: 2s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0370 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0362 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0350 - accuracy: 0.98 - 6s 569us/step - loss: 0.0348 - accuracy: 0.9879 - val_loss: 0.0156 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01416\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0432 - accuracy: 0.98 - ETA: 5s - loss: 0.0255 - accuracy: 0.99 - ETA: 5s - loss: 0.0251 - accuracy: 0.99 - ETA: 5s - loss: 0.0249 - accuracy: 0.99 - ETA: 5s - loss: 0.0215 - accuracy: 0.99 - ETA: 5s - loss: 0.0191 - accuracy: 0.99 - ETA: 5s - loss: 0.0168 - accuracy: 0.99 - ETA: 5s - loss: 0.0156 - accuracy: 0.99 - ETA: 5s - loss: 0.0231 - accuracy: 0.99 - ETA: 4s - loss: 0.0230 - accuracy: 0.99 - ETA: 4s - loss: 0.0269 - accuracy: 0.99 - ETA: 4s - loss: 0.0286 - accuracy: 0.99 - ETA: 4s - loss: 0.0512 - accuracy: 0.98 - ETA: 4s - loss: 0.0540 - accuracy: 0.98 - ETA: 4s - loss: 0.0520 - accuracy: 0.98 - ETA: 4s - loss: 0.0520 - accuracy: 0.98 - ETA: 4s - loss: 0.0506 - accuracy: 0.98 - ETA: 4s - loss: 0.0497 - accuracy: 0.98 - ETA: 4s - loss: 0.0498 - accuracy: 0.98 - ETA: 4s - loss: 0.0516 - accuracy: 0.98 - ETA: 4s - loss: 0.0518 - accuracy: 0.98 - ETA: 4s - loss: 0.0498 - accuracy: 0.98 - ETA: 3s - loss: 0.0486 - accuracy: 0.98 - ETA: 3s - loss: 0.0469 - accuracy: 0.98 - ETA: 3s - loss: 0.0460 - accuracy: 0.98 - ETA: 3s - loss: 0.0465 - accuracy: 0.98 - ETA: 3s - loss: 0.0450 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0441 - accuracy: 0.98 - ETA: 3s - loss: 0.0435 - accuracy: 0.98 - ETA: 3s - loss: 0.0427 - accuracy: 0.98 - ETA: 3s - loss: 0.0415 - accuracy: 0.98 - ETA: 3s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0429 - accuracy: 0.98 - ETA: 2s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0433 - accuracy: 0.98 - ETA: 2s - loss: 0.0431 - accuracy: 0.98 - ETA: 2s - loss: 0.0434 - accuracy: 0.98 - ETA: 2s - loss: 0.0432 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0421 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0403 - accuracy: 0.98 - ETA: 2s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 1s - loss: 0.0395 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0422 - accuracy: 0.98 - ETA: 1s - loss: 0.0422 - accuracy: 0.98 - ETA: 1s - loss: 0.0418 - accuracy: 0.98 - ETA: 1s - loss: 0.0418 - accuracy: 0.98 - ETA: 1s - loss: 0.0442 - accuracy: 0.98 - ETA: 1s - loss: 0.0454 - accuracy: 0.98 - ETA: 0s - loss: 0.0449 - accuracy: 0.98 - ETA: 0s - loss: 0.0443 - accuracy: 0.98 - ETA: 0s - loss: 0.0440 - accuracy: 0.98 - ETA: 0s - loss: 0.0436 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0428 - accuracy: 0.98 - ETA: 0s - loss: 0.0425 - accuracy: 0.98 - ETA: 0s - loss: 0.0423 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0414 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0407 - accuracy: 0.98 - 6s 579us/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.0158 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01416\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0022 - accuracy: 1.00 - ETA: 5s - loss: 0.0113 - accuracy: 0.99 - ETA: 5s - loss: 0.0118 - accuracy: 0.99 - ETA: 5s - loss: 0.0154 - accuracy: 0.99 - ETA: 5s - loss: 0.0150 - accuracy: 0.99 - ETA: 5s - loss: 0.0168 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0322 - accuracy: 0.98 - ETA: 4s - loss: 0.0383 - accuracy: 0.98 - ETA: 4s - loss: 0.0358 - accuracy: 0.98 - ETA: 4s - loss: 0.0348 - accuracy: 0.98 - ETA: 4s - loss: 0.0332 - accuracy: 0.98 - ETA: 4s - loss: 0.0315 - accuracy: 0.98 - ETA: 4s - loss: 0.0335 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0340 - accuracy: 0.98 - ETA: 4s - loss: 0.0327 - accuracy: 0.98 - ETA: 4s - loss: 0.0320 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0386 - accuracy: 0.98 - ETA: 3s - loss: 0.0370 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0353 - accuracy: 0.98 - ETA: 3s - loss: 0.0353 - accuracy: 0.98 - ETA: 3s - loss: 0.0350 - accuracy: 0.98 - ETA: 3s - loss: 0.0344 - accuracy: 0.98 - ETA: 3s - loss: 0.0333 - accuracy: 0.98 - ETA: 3s - loss: 0.0360 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0351 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0357 - accuracy: 0.98 - ETA: 2s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0369 - accuracy: 0.98 - ETA: 2s - loss: 0.0365 - accuracy: 0.98 - ETA: 2s - loss: 0.0361 - accuracy: 0.98 - ETA: 2s - loss: 0.0357 - accuracy: 0.98 - ETA: 2s - loss: 0.0355 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0344 - accuracy: 0.98 - ETA: 2s - loss: 0.0342 - accuracy: 0.98 - ETA: 2s - loss: 0.0335 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 2s - loss: 0.0327 - accuracy: 0.98 - ETA: 1s - loss: 0.0325 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 1s - loss: 0.0319 - accuracy: 0.98 - ETA: 1s - loss: 0.0325 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 1s - loss: 0.0325 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0329 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0327 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0362 - accuracy: 0.98 - ETA: 0s - loss: 0.0358 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0358 - accuracy: 0.98 - 6s 554us/step - loss: 0.0355 - accuracy: 0.9882 - val_loss: 0.0115 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01416 to 0.01154, saving model to cnn_bee_1.h5\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0277 - accuracy: 0.98 - ETA: 5s - loss: 0.0317 - accuracy: 0.99 - ETA: 5s - loss: 0.0253 - accuracy: 0.99 - ETA: 5s - loss: 0.0230 - accuracy: 0.99 - ETA: 5s - loss: 0.0204 - accuracy: 0.99 - ETA: 5s - loss: 0.0212 - accuracy: 0.99 - ETA: 5s - loss: 0.0402 - accuracy: 0.98 - ETA: 5s - loss: 0.0582 - accuracy: 0.98 - ETA: 5s - loss: 0.0539 - accuracy: 0.98 - ETA: 4s - loss: 0.0497 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0414 - accuracy: 0.98 - ETA: 4s - loss: 0.0404 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0349 - accuracy: 0.98 - ETA: 4s - loss: 0.0363 - accuracy: 0.98 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.99 - ETA: 4s - loss: 0.0327 - accuracy: 0.99 - ETA: 3s - loss: 0.0325 - accuracy: 0.98 - ETA: 3s - loss: 0.0345 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0346 - accuracy: 0.98 - ETA: 3s - loss: 0.0334 - accuracy: 0.98 - ETA: 3s - loss: 0.0336 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0344 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0346 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0358 - accuracy: 0.98 - ETA: 2s - loss: 0.0352 - accuracy: 0.98 - ETA: 2s - loss: 0.0352 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0345 - accuracy: 0.98 - ETA: 2s - loss: 0.0341 - accuracy: 0.98 - ETA: 2s - loss: 0.0334 - accuracy: 0.98 - ETA: 2s - loss: 0.0338 - accuracy: 0.98 - ETA: 2s - loss: 0.0340 - accuracy: 0.98 - ETA: 2s - loss: 0.0335 - accuracy: 0.98 - ETA: 2s - loss: 0.0331 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 2s - loss: 0.0330 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 1s - loss: 0.0337 - accuracy: 0.98 - ETA: 1s - loss: 0.0340 - accuracy: 0.98 - ETA: 1s - loss: 0.0342 - accuracy: 0.98 - ETA: 1s - loss: 0.0338 - accuracy: 0.98 - ETA: 1s - loss: 0.0338 - accuracy: 0.98 - ETA: 1s - loss: 0.0335 - accuracy: 0.98 - ETA: 1s - loss: 0.0329 - accuracy: 0.98 - ETA: 1s - loss: 0.0324 - accuracy: 0.98 - ETA: 1s - loss: 0.0319 - accuracy: 0.98 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0337 - accuracy: 0.98 - ETA: 1s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0345 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0337 - accuracy: 0.98 - ETA: 0s - loss: 0.0336 - accuracy: 0.98 - ETA: 0s - loss: 0.0339 - accuracy: 0.98 - 6s 557us/step - loss: 0.0336 - accuracy: 0.9895 - val_loss: 0.0099 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01154 to 0.00992, saving model to cnn_bee_1.h5\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0041 - accuracy: 1.00 - ETA: 5s - loss: 0.0124 - accuracy: 0.99 - ETA: 5s - loss: 0.0138 - accuracy: 0.99 - ETA: 5s - loss: 0.0157 - accuracy: 0.99 - ETA: 5s - loss: 0.0147 - accuracy: 0.99 - ETA: 5s - loss: 0.0128 - accuracy: 0.99 - ETA: 4s - loss: 0.0123 - accuracy: 0.99 - ETA: 4s - loss: 0.0111 - accuracy: 0.99 - ETA: 4s - loss: 0.0112 - accuracy: 0.99 - ETA: 4s - loss: 0.0124 - accuracy: 0.99 - ETA: 4s - loss: 0.0114 - accuracy: 0.99 - ETA: 4s - loss: 0.0136 - accuracy: 0.99 - ETA: 4s - loss: 0.0263 - accuracy: 0.99 - ETA: 4s - loss: 0.0246 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0263 - accuracy: 0.99 - ETA: 4s - loss: 0.0345 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.99 - ETA: 4s - loss: 0.0334 - accuracy: 0.98 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0386 - accuracy: 0.98 - ETA: 4s - loss: 0.0371 - accuracy: 0.98 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 3s - loss: 0.0338 - accuracy: 0.98 - ETA: 3s - loss: 0.0333 - accuracy: 0.98 - ETA: 3s - loss: 0.0324 - accuracy: 0.99 - ETA: 3s - loss: 0.0322 - accuracy: 0.99 - ETA: 3s - loss: 0.0314 - accuracy: 0.99 - ETA: 3s - loss: 0.0318 - accuracy: 0.99 - ETA: 3s - loss: 0.0326 - accuracy: 0.99 - ETA: 3s - loss: 0.0319 - accuracy: 0.99 - ETA: 3s - loss: 0.0310 - accuracy: 0.99 - ETA: 3s - loss: 0.0307 - accuracy: 0.99 - ETA: 3s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0297 - accuracy: 0.99 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0332 - accuracy: 0.98 - ETA: 2s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0385 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 2s - loss: 0.0376 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0361 - accuracy: 0.98 - ETA: 1s - loss: 0.0361 - accuracy: 0.98 - ETA: 1s - loss: 0.0363 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.98 - ETA: 1s - loss: 0.0355 - accuracy: 0.98 - ETA: 1s - loss: 0.0349 - accuracy: 0.98 - ETA: 1s - loss: 0.0348 - accuracy: 0.98 - ETA: 1s - loss: 0.0342 - accuracy: 0.98 - ETA: 1s - loss: 0.0343 - accuracy: 0.98 - ETA: 1s - loss: 0.0338 - accuracy: 0.98 - ETA: 1s - loss: 0.0336 - accuracy: 0.98 - ETA: 1s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0327 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0328 - accuracy: 0.98 - ETA: 0s - loss: 0.0327 - accuracy: 0.98 - ETA: 0s - loss: 0.0325 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0317 - accuracy: 0.98 - ETA: 0s - loss: 0.0313 - accuracy: 0.98 - 6s 555us/step - loss: 0.0311 - accuracy: 0.9895 - val_loss: 0.0111 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00992\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0441 - accuracy: 0.97 - ETA: 5s - loss: 0.0284 - accuracy: 0.98 - ETA: 5s - loss: 0.0218 - accuracy: 0.99 - ETA: 5s - loss: 0.0188 - accuracy: 0.99 - ETA: 5s - loss: 0.0192 - accuracy: 0.99 - ETA: 5s - loss: 0.0172 - accuracy: 0.99 - ETA: 5s - loss: 0.0157 - accuracy: 0.99 - ETA: 5s - loss: 0.0143 - accuracy: 0.99 - ETA: 5s - loss: 0.0143 - accuracy: 0.99 - ETA: 5s - loss: 0.0444 - accuracy: 0.98 - ETA: 5s - loss: 0.0949 - accuracy: 0.97 - ETA: 4s - loss: 0.0900 - accuracy: 0.97 - ETA: 4s - loss: 0.0858 - accuracy: 0.97 - ETA: 4s - loss: 0.0809 - accuracy: 0.97 - ETA: 4s - loss: 0.0757 - accuracy: 0.97 - ETA: 4s - loss: 0.0714 - accuracy: 0.97 - ETA: 4s - loss: 0.0685 - accuracy: 0.97 - ETA: 4s - loss: 0.0655 - accuracy: 0.98 - ETA: 4s - loss: 0.0627 - accuracy: 0.98 - ETA: 4s - loss: 0.0596 - accuracy: 0.98 - ETA: 4s - loss: 0.0570 - accuracy: 0.98 - ETA: 4s - loss: 0.0554 - accuracy: 0.98 - ETA: 4s - loss: 0.0536 - accuracy: 0.98 - ETA: 4s - loss: 0.0516 - accuracy: 0.98 - ETA: 3s - loss: 0.0503 - accuracy: 0.98 - ETA: 3s - loss: 0.0489 - accuracy: 0.98 - ETA: 3s - loss: 0.0474 - accuracy: 0.98 - ETA: 3s - loss: 0.0473 - accuracy: 0.98 - ETA: 3s - loss: 0.0466 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0441 - accuracy: 0.98 - ETA: 3s - loss: 0.0433 - accuracy: 0.98 - ETA: 3s - loss: 0.0426 - accuracy: 0.98 - ETA: 3s - loss: 0.0422 - accuracy: 0.98 - ETA: 3s - loss: 0.0431 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0463 - accuracy: 0.98 - ETA: 3s - loss: 0.0457 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0429 - accuracy: 0.98 - ETA: 2s - loss: 0.0420 - accuracy: 0.98 - ETA: 2s - loss: 0.0411 - accuracy: 0.98 - ETA: 2s - loss: 0.0415 - accuracy: 0.98 - ETA: 2s - loss: 0.0422 - accuracy: 0.98 - ETA: 2s - loss: 0.0415 - accuracy: 0.98 - ETA: 2s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0402 - accuracy: 0.98 - ETA: 2s - loss: 0.0399 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 1s - loss: 0.0399 - accuracy: 0.98 - ETA: 1s - loss: 0.0395 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0370 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0371 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0381 - accuracy: 0.98 - ETA: 0s - loss: 0.0384 - accuracy: 0.98 - ETA: 0s - loss: 0.0382 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - 6s 601us/step - loss: 0.0378 - accuracy: 0.9877 - val_loss: 0.0215 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00992\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0081 - accuracy: 1.00 - ETA: 5s - loss: 0.0120 - accuracy: 0.99 - ETA: 5s - loss: 0.0149 - accuracy: 0.99 - ETA: 5s - loss: 0.0141 - accuracy: 0.99 - ETA: 5s - loss: 0.0138 - accuracy: 0.99 - ETA: 5s - loss: 0.0169 - accuracy: 0.99 - ETA: 5s - loss: 0.0263 - accuracy: 0.99 - ETA: 5s - loss: 0.0244 - accuracy: 0.99 - ETA: 5s - loss: 0.0229 - accuracy: 0.99 - ETA: 5s - loss: 0.0231 - accuracy: 0.99 - ETA: 5s - loss: 0.0232 - accuracy: 0.99 - ETA: 5s - loss: 0.0215 - accuracy: 0.99 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0188 - accuracy: 0.99 - ETA: 4s - loss: 0.0189 - accuracy: 0.99 - ETA: 4s - loss: 0.0199 - accuracy: 0.99 - ETA: 4s - loss: 0.0277 - accuracy: 0.99 - ETA: 4s - loss: 0.0307 - accuracy: 0.99 - ETA: 4s - loss: 0.0305 - accuracy: 0.99 - ETA: 4s - loss: 0.0291 - accuracy: 0.99 - ETA: 4s - loss: 0.0284 - accuracy: 0.99 - ETA: 4s - loss: 0.0274 - accuracy: 0.99 - ETA: 4s - loss: 0.0266 - accuracy: 0.99 - ETA: 4s - loss: 0.0264 - accuracy: 0.99 - ETA: 4s - loss: 0.0256 - accuracy: 0.99 - ETA: 3s - loss: 0.0256 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0266 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0283 - accuracy: 0.99 - ETA: 3s - loss: 0.0284 - accuracy: 0.99 - ETA: 3s - loss: 0.0277 - accuracy: 0.99 - ETA: 3s - loss: 0.0270 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0263 - accuracy: 0.99 - ETA: 3s - loss: 0.0257 - accuracy: 0.99 - ETA: 2s - loss: 0.0251 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0237 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.98 - ETA: 1s - loss: 0.0274 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0268 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0280 - accuracy: 0.98 - ETA: 0s - loss: 0.0293 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0295 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0295 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0291 - accuracy: 0.98 - ETA: 0s - loss: 0.0288 - accuracy: 0.98 - ETA: 0s - loss: 0.0286 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - 6s 595us/step - loss: 0.0288 - accuracy: 0.9895 - val_loss: 0.0079 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00992 to 0.00794, saving model to cnn_bee_1.h5\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0105 - accuracy: 0.99 - ETA: 5s - loss: 0.0128 - accuracy: 0.98 - ETA: 5s - loss: 0.0410 - accuracy: 0.97 - ETA: 5s - loss: 0.0918 - accuracy: 0.97 - ETA: 5s - loss: 0.0828 - accuracy: 0.97 - ETA: 5s - loss: 0.0712 - accuracy: 0.97 - ETA: 5s - loss: 0.0630 - accuracy: 0.98 - ETA: 5s - loss: 0.0571 - accuracy: 0.98 - ETA: 5s - loss: 0.0522 - accuracy: 0.98 - ETA: 5s - loss: 0.0486 - accuracy: 0.98 - ETA: 5s - loss: 0.0447 - accuracy: 0.98 - ETA: 5s - loss: 0.0410 - accuracy: 0.98 - ETA: 5s - loss: 0.0382 - accuracy: 0.98 - ETA: 5s - loss: 0.0359 - accuracy: 0.98 - ETA: 5s - loss: 0.0336 - accuracy: 0.98 - ETA: 5s - loss: 0.0317 - accuracy: 0.98 - ETA: 5s - loss: 0.0299 - accuracy: 0.99 - ETA: 4s - loss: 0.0291 - accuracy: 0.99 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0361 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0344 - accuracy: 0.98 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0326 - accuracy: 0.98 - ETA: 4s - loss: 0.0319 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0306 - accuracy: 0.98 - ETA: 4s - loss: 0.0305 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0347 - accuracy: 0.98 - ETA: 3s - loss: 0.0337 - accuracy: 0.98 - ETA: 3s - loss: 0.0333 - accuracy: 0.98 - ETA: 3s - loss: 0.0336 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0321 - accuracy: 0.98 - ETA: 3s - loss: 0.0313 - accuracy: 0.98 - ETA: 3s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0308 - accuracy: 0.98 - ETA: 2s - loss: 0.0308 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0309 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0301 - accuracy: 0.98 - ETA: 1s - loss: 0.0296 - accuracy: 0.98 - ETA: 1s - loss: 0.0296 - accuracy: 0.98 - ETA: 1s - loss: 0.0295 - accuracy: 0.98 - ETA: 1s - loss: 0.0296 - accuracy: 0.98 - ETA: 1s - loss: 0.0294 - accuracy: 0.98 - ETA: 1s - loss: 0.0291 - accuracy: 0.98 - ETA: 1s - loss: 0.0288 - accuracy: 0.98 - ETA: 1s - loss: 0.0291 - accuracy: 0.98 - ETA: 0s - loss: 0.0289 - accuracy: 0.98 - ETA: 0s - loss: 0.0287 - accuracy: 0.98 - ETA: 0s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0285 - accuracy: 0.98 - ETA: 0s - loss: 0.0287 - accuracy: 0.98 - ETA: 0s - loss: 0.0283 - accuracy: 0.98 - ETA: 0s - loss: 0.0280 - accuracy: 0.98 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0280 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - 7s 632us/step - loss: 0.0278 - accuracy: 0.9901 - val_loss: 0.0123 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00794\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0357 - accuracy: 0.98 - ETA: 6s - loss: 0.0738 - accuracy: 0.97 - ETA: 6s - loss: 0.0635 - accuracy: 0.97 - ETA: 5s - loss: 0.0487 - accuracy: 0.98 - ETA: 5s - loss: 0.0410 - accuracy: 0.98 - ETA: 5s - loss: 0.0352 - accuracy: 0.98 - ETA: 5s - loss: 0.0313 - accuracy: 0.98 - ETA: 5s - loss: 0.0284 - accuracy: 0.99 - ETA: 5s - loss: 0.0260 - accuracy: 0.99 - ETA: 5s - loss: 0.0240 - accuracy: 0.99 - ETA: 5s - loss: 0.0232 - accuracy: 0.99 - ETA: 5s - loss: 0.0220 - accuracy: 0.99 - ETA: 5s - loss: 0.0209 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0206 - accuracy: 0.99 - ETA: 4s - loss: 0.0243 - accuracy: 0.99 - ETA: 4s - loss: 0.0307 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0305 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0288 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.98 - ETA: 4s - loss: 0.0291 - accuracy: 0.98 - ETA: 4s - loss: 0.0283 - accuracy: 0.98 - ETA: 3s - loss: 0.0293 - accuracy: 0.98 - ETA: 3s - loss: 0.0286 - accuracy: 0.98 - ETA: 3s - loss: 0.0292 - accuracy: 0.98 - ETA: 3s - loss: 0.0288 - accuracy: 0.98 - ETA: 3s - loss: 0.0280 - accuracy: 0.98 - ETA: 3s - loss: 0.0275 - accuracy: 0.98 - ETA: 3s - loss: 0.0270 - accuracy: 0.98 - ETA: 3s - loss: 0.0266 - accuracy: 0.98 - ETA: 3s - loss: 0.0274 - accuracy: 0.98 - ETA: 3s - loss: 0.0272 - accuracy: 0.98 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0256 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0286 - accuracy: 0.99 - ETA: 2s - loss: 0.0280 - accuracy: 0.99 - ETA: 2s - loss: 0.0288 - accuracy: 0.99 - ETA: 2s - loss: 0.0296 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.98 - ETA: 1s - loss: 0.0295 - accuracy: 0.98 - ETA: 1s - loss: 0.0290 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0285 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0284 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0284 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0285 - accuracy: 0.98 - ETA: 0s - loss: 0.0282 - accuracy: 0.98 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - 6s 582us/step - loss: 0.0280 - accuracy: 0.9901 - val_loss: 0.0128 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00794\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0139 - accuracy: 0.99 - ETA: 5s - loss: 0.0165 - accuracy: 0.98 - ETA: 5s - loss: 0.0226 - accuracy: 0.98 - ETA: 5s - loss: 0.0226 - accuracy: 0.98 - ETA: 6s - loss: 0.0208 - accuracy: 0.98 - ETA: 6s - loss: 0.0180 - accuracy: 0.98 - ETA: 5s - loss: 0.0198 - accuracy: 0.98 - ETA: 5s - loss: 0.0224 - accuracy: 0.98 - ETA: 5s - loss: 0.0454 - accuracy: 0.98 - ETA: 5s - loss: 0.0461 - accuracy: 0.98 - ETA: 5s - loss: 0.0426 - accuracy: 0.98 - ETA: 5s - loss: 0.0396 - accuracy: 0.98 - ETA: 5s - loss: 0.0379 - accuracy: 0.98 - ETA: 5s - loss: 0.0354 - accuracy: 0.98 - ETA: 5s - loss: 0.0334 - accuracy: 0.98 - ETA: 5s - loss: 0.0315 - accuracy: 0.98 - ETA: 4s - loss: 0.0297 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.99 - ETA: 4s - loss: 0.0269 - accuracy: 0.99 - ETA: 4s - loss: 0.0258 - accuracy: 0.99 - ETA: 4s - loss: 0.0264 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0247 - accuracy: 0.99 - ETA: 4s - loss: 0.0258 - accuracy: 0.99 - ETA: 4s - loss: 0.0293 - accuracy: 0.99 - ETA: 4s - loss: 0.0285 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0267 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0269 - accuracy: 0.99 - ETA: 3s - loss: 0.0262 - accuracy: 0.99 - ETA: 3s - loss: 0.0255 - accuracy: 0.99 - ETA: 3s - loss: 0.0255 - accuracy: 0.99 - ETA: 3s - loss: 0.0254 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0250 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.99 - ETA: 2s - loss: 0.0238 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.99 - ETA: 2s - loss: 0.0257 - accuracy: 0.99 - ETA: 2s - loss: 0.0323 - accuracy: 0.99 - ETA: 2s - loss: 0.0325 - accuracy: 0.99 - ETA: 1s - loss: 0.0323 - accuracy: 0.99 - ETA: 1s - loss: 0.0319 - accuracy: 0.99 - ETA: 1s - loss: 0.0314 - accuracy: 0.99 - ETA: 1s - loss: 0.0310 - accuracy: 0.99 - ETA: 1s - loss: 0.0306 - accuracy: 0.99 - ETA: 1s - loss: 0.0301 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 1s - loss: 0.0291 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0285 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0289 - accuracy: 0.99 - ETA: 0s - loss: 0.0298 - accuracy: 0.99 - ETA: 0s - loss: 0.0297 - accuracy: 0.99 - ETA: 0s - loss: 0.0298 - accuracy: 0.99 - ETA: 0s - loss: 0.0296 - accuracy: 0.99 - ETA: 0s - loss: 0.0292 - accuracy: 0.99 - ETA: 0s - loss: 0.0289 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - 7s 613us/step - loss: 0.0284 - accuracy: 0.9912 - val_loss: 0.0122 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00794\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0456 - accuracy: 0.98 - ETA: 6s - loss: 0.0286 - accuracy: 0.98 - ETA: 5s - loss: 0.0358 - accuracy: 0.98 - ETA: 5s - loss: 0.0285 - accuracy: 0.98 - ETA: 5s - loss: 0.0306 - accuracy: 0.98 - ETA: 5s - loss: 0.0255 - accuracy: 0.99 - ETA: 5s - loss: 0.0269 - accuracy: 0.99 - ETA: 5s - loss: 0.0348 - accuracy: 0.98 - ETA: 5s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0309 - accuracy: 0.98 - ETA: 4s - loss: 0.0323 - accuracy: 0.98 - ETA: 4s - loss: 0.0301 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0286 - accuracy: 0.98 - ETA: 4s - loss: 0.0271 - accuracy: 0.98 - ETA: 4s - loss: 0.0264 - accuracy: 0.98 - ETA: 4s - loss: 0.0261 - accuracy: 0.98 - ETA: 4s - loss: 0.0271 - accuracy: 0.98 - ETA: 4s - loss: 0.0260 - accuracy: 0.99 - ETA: 4s - loss: 0.0254 - accuracy: 0.99 - ETA: 4s - loss: 0.0275 - accuracy: 0.99 - ETA: 4s - loss: 0.0304 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.98 - ETA: 3s - loss: 0.0292 - accuracy: 0.98 - ETA: 3s - loss: 0.0282 - accuracy: 0.98 - ETA: 3s - loss: 0.0277 - accuracy: 0.98 - ETA: 3s - loss: 0.0268 - accuracy: 0.98 - ETA: 3s - loss: 0.0272 - accuracy: 0.98 - ETA: 3s - loss: 0.0269 - accuracy: 0.98 - ETA: 3s - loss: 0.0263 - accuracy: 0.98 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 3s - loss: 0.0261 - accuracy: 0.98 - ETA: 3s - loss: 0.0270 - accuracy: 0.98 - ETA: 3s - loss: 0.0268 - accuracy: 0.98 - ETA: 3s - loss: 0.0263 - accuracy: 0.98 - ETA: 3s - loss: 0.0258 - accuracy: 0.98 - ETA: 2s - loss: 0.0255 - accuracy: 0.98 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0230 - accuracy: 0.99 - ETA: 2s - loss: 0.0230 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0237 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0256 - accuracy: 0.99 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0268 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0281 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0274 - accuracy: 0.99 - ETA: 0s - loss: 0.0274 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0263 - accuracy: 0.99 - ETA: 0s - loss: 0.0259 - accuracy: 0.99 - 6s 585us/step - loss: 0.0258 - accuracy: 0.9908 - val_loss: 0.0151 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00794\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0174 - accuracy: 0.99 - ETA: 5s - loss: 0.0156 - accuracy: 0.99 - ETA: 5s - loss: 0.0107 - accuracy: 0.99 - ETA: 5s - loss: 0.0121 - accuracy: 0.99 - ETA: 5s - loss: 0.0107 - accuracy: 0.99 - ETA: 5s - loss: 0.0140 - accuracy: 0.99 - ETA: 5s - loss: 0.0123 - accuracy: 0.99 - ETA: 5s - loss: 0.0155 - accuracy: 0.99 - ETA: 5s - loss: 0.0220 - accuracy: 0.98 - ETA: 5s - loss: 0.0418 - accuracy: 0.98 - ETA: 5s - loss: 0.0462 - accuracy: 0.98 - ETA: 5s - loss: 0.0447 - accuracy: 0.98 - ETA: 5s - loss: 0.0428 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0411 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0370 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0362 - accuracy: 0.98 - ETA: 4s - loss: 0.0359 - accuracy: 0.98 - ETA: 4s - loss: 0.0343 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0320 - accuracy: 0.98 - ETA: 4s - loss: 0.0323 - accuracy: 0.98 - ETA: 4s - loss: 0.0317 - accuracy: 0.98 - ETA: 3s - loss: 0.0311 - accuracy: 0.98 - ETA: 3s - loss: 0.0316 - accuracy: 0.98 - ETA: 3s - loss: 0.0307 - accuracy: 0.98 - ETA: 3s - loss: 0.0304 - accuracy: 0.98 - ETA: 3s - loss: 0.0296 - accuracy: 0.99 - ETA: 3s - loss: 0.0288 - accuracy: 0.99 - ETA: 3s - loss: 0.0290 - accuracy: 0.99 - ETA: 3s - loss: 0.0282 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0271 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.98 - ETA: 2s - loss: 0.0285 - accuracy: 0.98 - ETA: 2s - loss: 0.0284 - accuracy: 0.98 - ETA: 2s - loss: 0.0282 - accuracy: 0.98 - ETA: 2s - loss: 0.0277 - accuracy: 0.98 - ETA: 2s - loss: 0.0275 - accuracy: 0.98 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0281 - accuracy: 0.98 - ETA: 1s - loss: 0.0291 - accuracy: 0.98 - ETA: 1s - loss: 0.0287 - accuracy: 0.98 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0280 - accuracy: 0.98 - ETA: 1s - loss: 0.0277 - accuracy: 0.98 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0268 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0284 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - ETA: 0s - loss: 0.0286 - accuracy: 0.98 - ETA: 0s - loss: 0.0284 - accuracy: 0.98 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0274 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0268 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - 6s 601us/step - loss: 0.0265 - accuracy: 0.9907 - val_loss: 0.0347 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00794\n",
      "Epoch 00025: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 215us/step\n",
      "[2020-05-18 16:27:26 RAM69.2% 1.32GB] Val Score : [0.016043218696656567, 0.9939348101615906]\n",
      "[2020-05-18 16:27:26 RAM69.2% 1.32GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:27:26 RAM69.2% 1.32GB] Training on Fold : 5\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 21s - loss: 3.3813 - accuracy: 0.517 - ETA: 13s - loss: 3.0075 - accuracy: 0.572 - ETA: 10s - loss: 3.9475 - accuracy: 0.524 - ETA: 9s - loss: 3.3541 - accuracy: 0.536 - ETA: 8s - loss: 2.9527 - accuracy: 0.52 - ETA: 8s - loss: 2.6349 - accuracy: 0.53 - ETA: 7s - loss: 2.3980 - accuracy: 0.53 - ETA: 7s - loss: 2.2132 - accuracy: 0.53 - ETA: 6s - loss: 2.0564 - accuracy: 0.54 - ETA: 6s - loss: 1.9526 - accuracy: 0.53 - ETA: 6s - loss: 1.8532 - accuracy: 0.53 - ETA: 6s - loss: 1.7718 - accuracy: 0.53 - ETA: 6s - loss: 1.6959 - accuracy: 0.54 - ETA: 5s - loss: 1.6368 - accuracy: 0.54 - ETA: 5s - loss: 1.5757 - accuracy: 0.54 - ETA: 5s - loss: 1.5287 - accuracy: 0.54 - ETA: 5s - loss: 1.4825 - accuracy: 0.54 - ETA: 5s - loss: 1.4433 - accuracy: 0.55 - ETA: 5s - loss: 1.4083 - accuracy: 0.55 - ETA: 5s - loss: 1.3751 - accuracy: 0.55 - ETA: 4s - loss: 1.3411 - accuracy: 0.55 - ETA: 4s - loss: 1.3101 - accuracy: 0.56 - ETA: 4s - loss: 1.2826 - accuracy: 0.56 - ETA: 4s - loss: 1.2617 - accuracy: 0.56 - ETA: 4s - loss: 1.2376 - accuracy: 0.56 - ETA: 4s - loss: 1.2159 - accuracy: 0.57 - ETA: 4s - loss: 1.1951 - accuracy: 0.57 - ETA: 4s - loss: 1.1777 - accuracy: 0.57 - ETA: 4s - loss: 1.1614 - accuracy: 0.57 - ETA: 3s - loss: 1.1433 - accuracy: 0.57 - ETA: 3s - loss: 1.1283 - accuracy: 0.57 - ETA: 3s - loss: 1.1125 - accuracy: 0.58 - ETA: 3s - loss: 1.0977 - accuracy: 0.58 - ETA: 3s - loss: 1.0823 - accuracy: 0.58 - ETA: 3s - loss: 1.0701 - accuracy: 0.58 - ETA: 3s - loss: 1.0600 - accuracy: 0.58 - ETA: 3s - loss: 1.0531 - accuracy: 0.58 - ETA: 3s - loss: 1.0439 - accuracy: 0.58 - ETA: 3s - loss: 1.0340 - accuracy: 0.58 - ETA: 2s - loss: 1.0211 - accuracy: 0.59 - ETA: 2s - loss: 1.0101 - accuracy: 0.59 - ETA: 2s - loss: 1.0049 - accuracy: 0.59 - ETA: 2s - loss: 0.9956 - accuracy: 0.59 - ETA: 2s - loss: 0.9858 - accuracy: 0.59 - ETA: 2s - loss: 0.9793 - accuracy: 0.59 - ETA: 2s - loss: 0.9720 - accuracy: 0.59 - ETA: 2s - loss: 0.9628 - accuracy: 0.60 - ETA: 2s - loss: 0.9536 - accuracy: 0.60 - ETA: 2s - loss: 0.9435 - accuracy: 0.60 - ETA: 2s - loss: 0.9336 - accuracy: 0.61 - ETA: 1s - loss: 0.9255 - accuracy: 0.61 - ETA: 1s - loss: 0.9185 - accuracy: 0.61 - ETA: 1s - loss: 0.9110 - accuracy: 0.61 - ETA: 1s - loss: 0.9037 - accuracy: 0.62 - ETA: 1s - loss: 0.8952 - accuracy: 0.62 - ETA: 1s - loss: 0.8870 - accuracy: 0.62 - ETA: 1s - loss: 0.8784 - accuracy: 0.62 - ETA: 1s - loss: 0.8708 - accuracy: 0.63 - ETA: 1s - loss: 0.8645 - accuracy: 0.63 - ETA: 1s - loss: 0.8584 - accuracy: 0.63 - ETA: 1s - loss: 0.8506 - accuracy: 0.63 - ETA: 1s - loss: 0.8416 - accuracy: 0.64 - ETA: 0s - loss: 0.8348 - accuracy: 0.64 - ETA: 0s - loss: 0.8287 - accuracy: 0.64 - ETA: 0s - loss: 0.8209 - accuracy: 0.65 - ETA: 0s - loss: 0.8142 - accuracy: 0.65 - ETA: 0s - loss: 0.8067 - accuracy: 0.65 - ETA: 0s - loss: 0.7990 - accuracy: 0.66 - ETA: 0s - loss: 0.7922 - accuracy: 0.66 - ETA: 0s - loss: 0.7852 - accuracy: 0.66 - ETA: 0s - loss: 0.7823 - accuracy: 0.66 - ETA: 0s - loss: 0.7805 - accuracy: 0.66 - ETA: 0s - loss: 0.7756 - accuracy: 0.66 - 7s 618us/step - loss: 0.7718 - accuracy: 0.6714 - val_loss: 0.4361 - val_accuracy: 0.8037\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00794\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.5104 - accuracy: 0.74 - ETA: 5s - loss: 0.5088 - accuracy: 0.72 - ETA: 5s - loss: 0.4704 - accuracy: 0.75 - ETA: 5s - loss: 0.4428 - accuracy: 0.76 - ETA: 5s - loss: 0.4113 - accuracy: 0.78 - ETA: 5s - loss: 0.3882 - accuracy: 0.80 - ETA: 5s - loss: 0.3716 - accuracy: 0.81 - ETA: 5s - loss: 0.3844 - accuracy: 0.80 - ETA: 5s - loss: 0.3860 - accuracy: 0.80 - ETA: 5s - loss: 0.3841 - accuracy: 0.81 - ETA: 5s - loss: 0.3699 - accuracy: 0.82 - ETA: 5s - loss: 0.3633 - accuracy: 0.82 - ETA: 5s - loss: 0.3532 - accuracy: 0.83 - ETA: 5s - loss: 0.3496 - accuracy: 0.83 - ETA: 5s - loss: 0.3463 - accuracy: 0.83 - ETA: 4s - loss: 0.3451 - accuracy: 0.83 - ETA: 4s - loss: 0.3430 - accuracy: 0.84 - ETA: 4s - loss: 0.3372 - accuracy: 0.84 - ETA: 4s - loss: 0.3322 - accuracy: 0.84 - ETA: 4s - loss: 0.3305 - accuracy: 0.84 - ETA: 4s - loss: 0.3321 - accuracy: 0.84 - ETA: 4s - loss: 0.3369 - accuracy: 0.84 - ETA: 4s - loss: 0.3313 - accuracy: 0.84 - ETA: 4s - loss: 0.3295 - accuracy: 0.84 - ETA: 4s - loss: 0.3258 - accuracy: 0.84 - ETA: 4s - loss: 0.3235 - accuracy: 0.85 - ETA: 4s - loss: 0.3206 - accuracy: 0.85 - ETA: 3s - loss: 0.3177 - accuracy: 0.85 - ETA: 3s - loss: 0.3165 - accuracy: 0.85 - ETA: 3s - loss: 0.3133 - accuracy: 0.85 - ETA: 3s - loss: 0.3131 - accuracy: 0.85 - ETA: 3s - loss: 0.3126 - accuracy: 0.85 - ETA: 3s - loss: 0.3094 - accuracy: 0.85 - ETA: 3s - loss: 0.3079 - accuracy: 0.85 - ETA: 3s - loss: 0.3067 - accuracy: 0.85 - ETA: 3s - loss: 0.3048 - accuracy: 0.86 - ETA: 3s - loss: 0.3044 - accuracy: 0.86 - ETA: 3s - loss: 0.3030 - accuracy: 0.86 - ETA: 3s - loss: 0.3005 - accuracy: 0.86 - ETA: 2s - loss: 0.2982 - accuracy: 0.86 - ETA: 2s - loss: 0.2947 - accuracy: 0.86 - ETA: 2s - loss: 0.2911 - accuracy: 0.86 - ETA: 2s - loss: 0.2883 - accuracy: 0.87 - ETA: 2s - loss: 0.2874 - accuracy: 0.87 - ETA: 2s - loss: 0.2864 - accuracy: 0.87 - ETA: 2s - loss: 0.2830 - accuracy: 0.87 - ETA: 2s - loss: 0.2818 - accuracy: 0.87 - ETA: 2s - loss: 0.2798 - accuracy: 0.87 - ETA: 2s - loss: 0.2780 - accuracy: 0.87 - ETA: 2s - loss: 0.2781 - accuracy: 0.87 - ETA: 1s - loss: 0.2780 - accuracy: 0.87 - ETA: 1s - loss: 0.2774 - accuracy: 0.87 - ETA: 1s - loss: 0.2767 - accuracy: 0.87 - ETA: 1s - loss: 0.2769 - accuracy: 0.87 - ETA: 1s - loss: 0.2753 - accuracy: 0.87 - ETA: 1s - loss: 0.2732 - accuracy: 0.87 - ETA: 1s - loss: 0.2719 - accuracy: 0.87 - ETA: 1s - loss: 0.2716 - accuracy: 0.87 - ETA: 1s - loss: 0.2707 - accuracy: 0.87 - ETA: 1s - loss: 0.2691 - accuracy: 0.88 - ETA: 1s - loss: 0.2672 - accuracy: 0.88 - ETA: 0s - loss: 0.2648 - accuracy: 0.88 - ETA: 0s - loss: 0.2625 - accuracy: 0.88 - ETA: 0s - loss: 0.2621 - accuracy: 0.88 - ETA: 0s - loss: 0.2620 - accuracy: 0.88 - ETA: 0s - loss: 0.2641 - accuracy: 0.88 - ETA: 0s - loss: 0.2714 - accuracy: 0.88 - ETA: 0s - loss: 0.2738 - accuracy: 0.87 - ETA: 0s - loss: 0.2753 - accuracy: 0.87 - ETA: 0s - loss: 0.2746 - accuracy: 0.87 - ETA: 0s - loss: 0.2738 - accuracy: 0.87 - ETA: 0s - loss: 0.2730 - accuracy: 0.87 - ETA: 0s - loss: 0.2714 - accuracy: 0.88 - 7s 609us/step - loss: 0.2709 - accuracy: 0.8809 - val_loss: 0.1363 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00794\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.2206 - accuracy: 0.92 - ETA: 5s - loss: 0.2073 - accuracy: 0.91 - ETA: 5s - loss: 0.2039 - accuracy: 0.91 - ETA: 5s - loss: 0.1940 - accuracy: 0.91 - ETA: 5s - loss: 0.1867 - accuracy: 0.91 - ETA: 5s - loss: 0.1864 - accuracy: 0.91 - ETA: 5s - loss: 0.1810 - accuracy: 0.92 - ETA: 5s - loss: 0.1784 - accuracy: 0.92 - ETA: 5s - loss: 0.1773 - accuracy: 0.92 - ETA: 5s - loss: 0.1727 - accuracy: 0.93 - ETA: 5s - loss: 0.1760 - accuracy: 0.92 - ETA: 5s - loss: 0.1779 - accuracy: 0.92 - ETA: 5s - loss: 0.1770 - accuracy: 0.92 - ETA: 5s - loss: 0.1742 - accuracy: 0.92 - ETA: 5s - loss: 0.1735 - accuracy: 0.92 - ETA: 5s - loss: 0.1742 - accuracy: 0.92 - ETA: 4s - loss: 0.1751 - accuracy: 0.92 - ETA: 4s - loss: 0.1759 - accuracy: 0.92 - ETA: 4s - loss: 0.1797 - accuracy: 0.92 - ETA: 4s - loss: 0.1760 - accuracy: 0.92 - ETA: 4s - loss: 0.1755 - accuracy: 0.92 - ETA: 4s - loss: 0.1773 - accuracy: 0.92 - ETA: 4s - loss: 0.1809 - accuracy: 0.91 - ETA: 4s - loss: 0.1811 - accuracy: 0.91 - ETA: 4s - loss: 0.1809 - accuracy: 0.91 - ETA: 4s - loss: 0.1808 - accuracy: 0.91 - ETA: 4s - loss: 0.1816 - accuracy: 0.91 - ETA: 3s - loss: 0.1799 - accuracy: 0.92 - ETA: 3s - loss: 0.1778 - accuracy: 0.92 - ETA: 3s - loss: 0.1747 - accuracy: 0.92 - ETA: 3s - loss: 0.1726 - accuracy: 0.92 - ETA: 3s - loss: 0.1712 - accuracy: 0.92 - ETA: 3s - loss: 0.1702 - accuracy: 0.92 - ETA: 3s - loss: 0.1729 - accuracy: 0.92 - ETA: 3s - loss: 0.1740 - accuracy: 0.92 - ETA: 3s - loss: 0.1720 - accuracy: 0.92 - ETA: 3s - loss: 0.1708 - accuracy: 0.92 - ETA: 3s - loss: 0.1694 - accuracy: 0.92 - ETA: 2s - loss: 0.1684 - accuracy: 0.92 - ETA: 2s - loss: 0.1694 - accuracy: 0.92 - ETA: 2s - loss: 0.1684 - accuracy: 0.92 - ETA: 2s - loss: 0.1665 - accuracy: 0.92 - ETA: 2s - loss: 0.1657 - accuracy: 0.92 - ETA: 2s - loss: 0.1667 - accuracy: 0.92 - ETA: 2s - loss: 0.1713 - accuracy: 0.92 - ETA: 2s - loss: 0.1860 - accuracy: 0.92 - ETA: 2s - loss: 0.1888 - accuracy: 0.92 - ETA: 2s - loss: 0.1892 - accuracy: 0.91 - ETA: 2s - loss: 0.1878 - accuracy: 0.92 - ETA: 1s - loss: 0.1880 - accuracy: 0.92 - ETA: 1s - loss: 0.1881 - accuracy: 0.92 - ETA: 1s - loss: 0.1877 - accuracy: 0.92 - ETA: 1s - loss: 0.1865 - accuracy: 0.92 - ETA: 1s - loss: 0.1856 - accuracy: 0.92 - ETA: 1s - loss: 0.1846 - accuracy: 0.92 - ETA: 1s - loss: 0.1842 - accuracy: 0.92 - ETA: 1s - loss: 0.1833 - accuracy: 0.92 - ETA: 1s - loss: 0.1823 - accuracy: 0.92 - ETA: 1s - loss: 0.1808 - accuracy: 0.92 - ETA: 1s - loss: 0.1797 - accuracy: 0.92 - ETA: 1s - loss: 0.1782 - accuracy: 0.92 - ETA: 0s - loss: 0.1771 - accuracy: 0.92 - ETA: 0s - loss: 0.1768 - accuracy: 0.92 - ETA: 0s - loss: 0.1772 - accuracy: 0.92 - ETA: 0s - loss: 0.1775 - accuracy: 0.92 - ETA: 0s - loss: 0.1778 - accuracy: 0.92 - ETA: 0s - loss: 0.1773 - accuracy: 0.92 - ETA: 0s - loss: 0.1771 - accuracy: 0.92 - ETA: 0s - loss: 0.1772 - accuracy: 0.92 - ETA: 0s - loss: 0.1783 - accuracy: 0.92 - ETA: 0s - loss: 0.1782 - accuracy: 0.92 - ETA: 0s - loss: 0.1781 - accuracy: 0.92 - ETA: 0s - loss: 0.1779 - accuracy: 0.92 - 6s 608us/step - loss: 0.1778 - accuracy: 0.9247 - val_loss: 0.1158 - val_accuracy: 0.9646\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00794\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1679 - accuracy: 0.95 - ETA: 5s - loss: 0.1745 - accuracy: 0.92 - ETA: 5s - loss: 0.1704 - accuracy: 0.92 - ETA: 5s - loss: 0.1605 - accuracy: 0.93 - ETA: 5s - loss: 0.1586 - accuracy: 0.93 - ETA: 5s - loss: 0.1640 - accuracy: 0.92 - ETA: 5s - loss: 0.1575 - accuracy: 0.92 - ETA: 5s - loss: 0.1499 - accuracy: 0.93 - ETA: 5s - loss: 0.1560 - accuracy: 0.93 - ETA: 5s - loss: 0.1703 - accuracy: 0.92 - ETA: 5s - loss: 0.1683 - accuracy: 0.92 - ETA: 5s - loss: 0.1682 - accuracy: 0.92 - ETA: 4s - loss: 0.1613 - accuracy: 0.92 - ETA: 4s - loss: 0.1563 - accuracy: 0.92 - ETA: 4s - loss: 0.1509 - accuracy: 0.93 - ETA: 4s - loss: 0.1509 - accuracy: 0.93 - ETA: 4s - loss: 0.1507 - accuracy: 0.93 - ETA: 4s - loss: 0.1482 - accuracy: 0.93 - ETA: 4s - loss: 0.1462 - accuracy: 0.93 - ETA: 4s - loss: 0.1483 - accuracy: 0.93 - ETA: 4s - loss: 0.1501 - accuracy: 0.93 - ETA: 4s - loss: 0.1512 - accuracy: 0.93 - ETA: 4s - loss: 0.1511 - accuracy: 0.93 - ETA: 4s - loss: 0.1511 - accuracy: 0.93 - ETA: 4s - loss: 0.1483 - accuracy: 0.93 - ETA: 3s - loss: 0.1500 - accuracy: 0.93 - ETA: 3s - loss: 0.1513 - accuracy: 0.93 - ETA: 3s - loss: 0.1542 - accuracy: 0.93 - ETA: 3s - loss: 0.1551 - accuracy: 0.93 - ETA: 3s - loss: 0.1552 - accuracy: 0.93 - ETA: 3s - loss: 0.1543 - accuracy: 0.93 - ETA: 3s - loss: 0.1525 - accuracy: 0.93 - ETA: 3s - loss: 0.1510 - accuracy: 0.93 - ETA: 3s - loss: 0.1496 - accuracy: 0.93 - ETA: 3s - loss: 0.1487 - accuracy: 0.93 - ETA: 3s - loss: 0.1494 - accuracy: 0.93 - ETA: 3s - loss: 0.1503 - accuracy: 0.93 - ETA: 2s - loss: 0.1530 - accuracy: 0.93 - ETA: 2s - loss: 0.1524 - accuracy: 0.93 - ETA: 2s - loss: 0.1501 - accuracy: 0.93 - ETA: 2s - loss: 0.1506 - accuracy: 0.93 - ETA: 2s - loss: 0.1510 - accuracy: 0.93 - ETA: 2s - loss: 0.1501 - accuracy: 0.93 - ETA: 2s - loss: 0.1485 - accuracy: 0.93 - ETA: 2s - loss: 0.1466 - accuracy: 0.93 - ETA: 2s - loss: 0.1461 - accuracy: 0.93 - ETA: 2s - loss: 0.1447 - accuracy: 0.93 - ETA: 2s - loss: 0.1439 - accuracy: 0.93 - ETA: 2s - loss: 0.1429 - accuracy: 0.93 - ETA: 1s - loss: 0.1419 - accuracy: 0.93 - ETA: 1s - loss: 0.1421 - accuracy: 0.93 - ETA: 1s - loss: 0.1437 - accuracy: 0.93 - ETA: 1s - loss: 0.1441 - accuracy: 0.93 - ETA: 1s - loss: 0.1430 - accuracy: 0.93 - ETA: 1s - loss: 0.1418 - accuracy: 0.93 - ETA: 1s - loss: 0.1419 - accuracy: 0.93 - ETA: 1s - loss: 0.1424 - accuracy: 0.93 - ETA: 1s - loss: 0.1419 - accuracy: 0.93 - ETA: 1s - loss: 0.1411 - accuracy: 0.94 - ETA: 1s - loss: 0.1400 - accuracy: 0.94 - ETA: 1s - loss: 0.1401 - accuracy: 0.94 - ETA: 0s - loss: 0.1402 - accuracy: 0.94 - ETA: 0s - loss: 0.1431 - accuracy: 0.93 - ETA: 0s - loss: 0.1458 - accuracy: 0.93 - ETA: 0s - loss: 0.1453 - accuracy: 0.93 - ETA: 0s - loss: 0.1444 - accuracy: 0.93 - ETA: 0s - loss: 0.1437 - accuracy: 0.93 - ETA: 0s - loss: 0.1426 - accuracy: 0.93 - ETA: 0s - loss: 0.1416 - accuracy: 0.94 - ETA: 0s - loss: 0.1407 - accuracy: 0.94 - ETA: 0s - loss: 0.1402 - accuracy: 0.94 - ETA: 0s - loss: 0.1402 - accuracy: 0.94 - ETA: 0s - loss: 0.1396 - accuracy: 0.94 - 6s 593us/step - loss: 0.1390 - accuracy: 0.9411 - val_loss: 0.1163 - val_accuracy: 0.9385\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00794\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1404 - accuracy: 0.93 - ETA: 5s - loss: 0.1296 - accuracy: 0.93 - ETA: 5s - loss: 0.1205 - accuracy: 0.94 - ETA: 5s - loss: 0.1159 - accuracy: 0.95 - ETA: 5s - loss: 0.1084 - accuracy: 0.95 - ETA: 5s - loss: 0.1112 - accuracy: 0.95 - ETA: 5s - loss: 0.1381 - accuracy: 0.94 - ETA: 4s - loss: 0.1740 - accuracy: 0.93 - ETA: 5s - loss: 0.1721 - accuracy: 0.93 - ETA: 5s - loss: 0.1653 - accuracy: 0.93 - ETA: 4s - loss: 0.1564 - accuracy: 0.94 - ETA: 4s - loss: 0.1495 - accuracy: 0.94 - ETA: 4s - loss: 0.1475 - accuracy: 0.94 - ETA: 4s - loss: 0.1423 - accuracy: 0.94 - ETA: 4s - loss: 0.1370 - accuracy: 0.94 - ETA: 4s - loss: 0.1321 - accuracy: 0.95 - ETA: 4s - loss: 0.1285 - accuracy: 0.95 - ETA: 4s - loss: 0.1245 - accuracy: 0.95 - ETA: 4s - loss: 0.1218 - accuracy: 0.95 - ETA: 4s - loss: 0.1178 - accuracy: 0.95 - ETA: 4s - loss: 0.1154 - accuracy: 0.95 - ETA: 4s - loss: 0.1138 - accuracy: 0.95 - ETA: 4s - loss: 0.1140 - accuracy: 0.95 - ETA: 3s - loss: 0.1125 - accuracy: 0.95 - ETA: 3s - loss: 0.1121 - accuracy: 0.95 - ETA: 3s - loss: 0.1111 - accuracy: 0.95 - ETA: 3s - loss: 0.1131 - accuracy: 0.95 - ETA: 3s - loss: 0.1287 - accuracy: 0.95 - ETA: 3s - loss: 0.1579 - accuracy: 0.94 - ETA: 3s - loss: 0.1555 - accuracy: 0.94 - ETA: 3s - loss: 0.1534 - accuracy: 0.94 - ETA: 3s - loss: 0.1526 - accuracy: 0.94 - ETA: 3s - loss: 0.1532 - accuracy: 0.94 - ETA: 3s - loss: 0.1512 - accuracy: 0.94 - ETA: 3s - loss: 0.1500 - accuracy: 0.94 - ETA: 2s - loss: 0.1486 - accuracy: 0.94 - ETA: 2s - loss: 0.1466 - accuracy: 0.94 - ETA: 2s - loss: 0.1440 - accuracy: 0.94 - ETA: 2s - loss: 0.1421 - accuracy: 0.94 - ETA: 2s - loss: 0.1418 - accuracy: 0.94 - ETA: 2s - loss: 0.1397 - accuracy: 0.94 - ETA: 2s - loss: 0.1375 - accuracy: 0.94 - ETA: 2s - loss: 0.1361 - accuracy: 0.95 - ETA: 2s - loss: 0.1351 - accuracy: 0.95 - ETA: 2s - loss: 0.1343 - accuracy: 0.95 - ETA: 2s - loss: 0.1332 - accuracy: 0.95 - ETA: 2s - loss: 0.1316 - accuracy: 0.95 - ETA: 2s - loss: 0.1317 - accuracy: 0.95 - ETA: 1s - loss: 0.1313 - accuracy: 0.95 - ETA: 1s - loss: 0.1315 - accuracy: 0.95 - ETA: 1s - loss: 0.1318 - accuracy: 0.95 - ETA: 1s - loss: 0.1304 - accuracy: 0.95 - ETA: 1s - loss: 0.1304 - accuracy: 0.95 - ETA: 1s - loss: 0.1304 - accuracy: 0.95 - ETA: 1s - loss: 0.1298 - accuracy: 0.95 - ETA: 1s - loss: 0.1289 - accuracy: 0.95 - ETA: 1s - loss: 0.1293 - accuracy: 0.95 - ETA: 1s - loss: 0.1288 - accuracy: 0.95 - ETA: 1s - loss: 0.1282 - accuracy: 0.95 - ETA: 1s - loss: 0.1278 - accuracy: 0.95 - ETA: 0s - loss: 0.1264 - accuracy: 0.95 - ETA: 0s - loss: 0.1249 - accuracy: 0.95 - ETA: 0s - loss: 0.1239 - accuracy: 0.95 - ETA: 0s - loss: 0.1230 - accuracy: 0.95 - ETA: 0s - loss: 0.1221 - accuracy: 0.95 - ETA: 0s - loss: 0.1210 - accuracy: 0.95 - ETA: 0s - loss: 0.1211 - accuracy: 0.95 - ETA: 0s - loss: 0.1210 - accuracy: 0.95 - ETA: 0s - loss: 0.1205 - accuracy: 0.95 - ETA: 0s - loss: 0.1212 - accuracy: 0.95 - ETA: 0s - loss: 0.1209 - accuracy: 0.95 - ETA: 0s - loss: 0.1202 - accuracy: 0.95 - ETA: 0s - loss: 0.1196 - accuracy: 0.95 - 6s 557us/step - loss: 0.1191 - accuracy: 0.9565 - val_loss: 0.0519 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00794\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0362 - accuracy: 0.98 - ETA: 5s - loss: 0.0568 - accuracy: 0.97 - ETA: 5s - loss: 0.0697 - accuracy: 0.97 - ETA: 5s - loss: 0.0652 - accuracy: 0.97 - ETA: 5s - loss: 0.0680 - accuracy: 0.97 - ETA: 5s - loss: 0.0628 - accuracy: 0.97 - ETA: 5s - loss: 0.0617 - accuracy: 0.97 - ETA: 5s - loss: 0.0631 - accuracy: 0.97 - ETA: 5s - loss: 0.0781 - accuracy: 0.97 - ETA: 4s - loss: 0.0928 - accuracy: 0.96 - ETA: 4s - loss: 0.0952 - accuracy: 0.96 - ETA: 4s - loss: 0.0996 - accuracy: 0.96 - ETA: 4s - loss: 0.1007 - accuracy: 0.96 - ETA: 4s - loss: 0.0953 - accuracy: 0.96 - ETA: 4s - loss: 0.0920 - accuracy: 0.96 - ETA: 4s - loss: 0.0912 - accuracy: 0.96 - ETA: 4s - loss: 0.0887 - accuracy: 0.97 - ETA: 4s - loss: 0.0874 - accuracy: 0.97 - ETA: 4s - loss: 0.0927 - accuracy: 0.96 - ETA: 4s - loss: 0.0953 - accuracy: 0.96 - ETA: 4s - loss: 0.0949 - accuracy: 0.96 - ETA: 4s - loss: 0.0926 - accuracy: 0.96 - ETA: 4s - loss: 0.0908 - accuracy: 0.96 - ETA: 3s - loss: 0.0903 - accuracy: 0.96 - ETA: 3s - loss: 0.0905 - accuracy: 0.96 - ETA: 3s - loss: 0.0898 - accuracy: 0.96 - ETA: 3s - loss: 0.0876 - accuracy: 0.96 - ETA: 3s - loss: 0.0873 - accuracy: 0.96 - ETA: 3s - loss: 0.0877 - accuracy: 0.96 - ETA: 3s - loss: 0.0865 - accuracy: 0.96 - ETA: 3s - loss: 0.0862 - accuracy: 0.96 - ETA: 3s - loss: 0.0863 - accuracy: 0.96 - ETA: 3s - loss: 0.0962 - accuracy: 0.96 - ETA: 3s - loss: 0.1024 - accuracy: 0.96 - ETA: 3s - loss: 0.1020 - accuracy: 0.96 - ETA: 2s - loss: 0.1011 - accuracy: 0.96 - ETA: 2s - loss: 0.0999 - accuracy: 0.96 - ETA: 2s - loss: 0.0986 - accuracy: 0.96 - ETA: 2s - loss: 0.0974 - accuracy: 0.96 - ETA: 2s - loss: 0.0974 - accuracy: 0.96 - ETA: 2s - loss: 0.0963 - accuracy: 0.96 - ETA: 2s - loss: 0.0954 - accuracy: 0.96 - ETA: 2s - loss: 0.0949 - accuracy: 0.96 - ETA: 2s - loss: 0.0953 - accuracy: 0.96 - ETA: 2s - loss: 0.0941 - accuracy: 0.96 - ETA: 2s - loss: 0.0930 - accuracy: 0.96 - ETA: 2s - loss: 0.0935 - accuracy: 0.96 - ETA: 2s - loss: 0.0922 - accuracy: 0.96 - ETA: 1s - loss: 0.0914 - accuracy: 0.96 - ETA: 1s - loss: 0.0903 - accuracy: 0.96 - ETA: 1s - loss: 0.0892 - accuracy: 0.96 - ETA: 1s - loss: 0.0882 - accuracy: 0.96 - ETA: 1s - loss: 0.0882 - accuracy: 0.96 - ETA: 1s - loss: 0.0884 - accuracy: 0.96 - ETA: 1s - loss: 0.0894 - accuracy: 0.96 - ETA: 1s - loss: 0.0896 - accuracy: 0.96 - ETA: 1s - loss: 0.0891 - accuracy: 0.96 - ETA: 1s - loss: 0.0890 - accuracy: 0.96 - ETA: 1s - loss: 0.0891 - accuracy: 0.96 - ETA: 1s - loss: 0.0886 - accuracy: 0.96 - ETA: 0s - loss: 0.0884 - accuracy: 0.96 - ETA: 0s - loss: 0.0881 - accuracy: 0.96 - ETA: 0s - loss: 0.0885 - accuracy: 0.96 - ETA: 0s - loss: 0.0881 - accuracy: 0.96 - ETA: 0s - loss: 0.0873 - accuracy: 0.96 - ETA: 0s - loss: 0.0867 - accuracy: 0.96 - ETA: 0s - loss: 0.0866 - accuracy: 0.96 - ETA: 0s - loss: 0.0862 - accuracy: 0.96 - ETA: 0s - loss: 0.0861 - accuracy: 0.96 - ETA: 0s - loss: 0.0854 - accuracy: 0.96 - ETA: 0s - loss: 0.0854 - accuracy: 0.96 - ETA: 0s - loss: 0.0849 - accuracy: 0.96 - ETA: 0s - loss: 0.0858 - accuracy: 0.96 - 6s 563us/step - loss: 0.0866 - accuracy: 0.9679 - val_loss: 0.1048 - val_accuracy: 0.9646\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00794\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1620 - accuracy: 0.94 - ETA: 5s - loss: 0.1217 - accuracy: 0.95 - ETA: 5s - loss: 0.1199 - accuracy: 0.95 - ETA: 5s - loss: 0.1045 - accuracy: 0.96 - ETA: 5s - loss: 0.0934 - accuracy: 0.96 - ETA: 5s - loss: 0.0858 - accuracy: 0.96 - ETA: 5s - loss: 0.0826 - accuracy: 0.97 - ETA: 5s - loss: 0.0846 - accuracy: 0.96 - ETA: 5s - loss: 0.0831 - accuracy: 0.97 - ETA: 5s - loss: 0.0786 - accuracy: 0.97 - ETA: 5s - loss: 0.0766 - accuracy: 0.97 - ETA: 5s - loss: 0.0771 - accuracy: 0.97 - ETA: 5s - loss: 0.0910 - accuracy: 0.96 - ETA: 4s - loss: 0.1030 - accuracy: 0.96 - ETA: 4s - loss: 0.1107 - accuracy: 0.96 - ETA: 4s - loss: 0.1135 - accuracy: 0.96 - ETA: 4s - loss: 0.1115 - accuracy: 0.96 - ETA: 4s - loss: 0.1096 - accuracy: 0.96 - ETA: 4s - loss: 0.1063 - accuracy: 0.96 - ETA: 4s - loss: 0.1032 - accuracy: 0.96 - ETA: 4s - loss: 0.1007 - accuracy: 0.96 - ETA: 4s - loss: 0.0992 - accuracy: 0.96 - ETA: 4s - loss: 0.0975 - accuracy: 0.96 - ETA: 3s - loss: 0.0948 - accuracy: 0.96 - ETA: 3s - loss: 0.0934 - accuracy: 0.96 - ETA: 3s - loss: 0.0937 - accuracy: 0.96 - ETA: 3s - loss: 0.0923 - accuracy: 0.96 - ETA: 3s - loss: 0.0902 - accuracy: 0.96 - ETA: 3s - loss: 0.0890 - accuracy: 0.96 - ETA: 3s - loss: 0.0894 - accuracy: 0.96 - ETA: 3s - loss: 0.0902 - accuracy: 0.96 - ETA: 3s - loss: 0.0918 - accuracy: 0.96 - ETA: 3s - loss: 0.0903 - accuracy: 0.96 - ETA: 3s - loss: 0.0889 - accuracy: 0.96 - ETA: 3s - loss: 0.0887 - accuracy: 0.96 - ETA: 2s - loss: 0.0879 - accuracy: 0.97 - ETA: 2s - loss: 0.0865 - accuracy: 0.97 - ETA: 2s - loss: 0.0850 - accuracy: 0.97 - ETA: 2s - loss: 0.0842 - accuracy: 0.97 - ETA: 2s - loss: 0.0850 - accuracy: 0.97 - ETA: 2s - loss: 0.0866 - accuracy: 0.97 - ETA: 2s - loss: 0.0853 - accuracy: 0.97 - ETA: 2s - loss: 0.0849 - accuracy: 0.97 - ETA: 2s - loss: 0.0847 - accuracy: 0.97 - ETA: 2s - loss: 0.0842 - accuracy: 0.97 - ETA: 2s - loss: 0.0829 - accuracy: 0.97 - ETA: 2s - loss: 0.0818 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 1s - loss: 0.0796 - accuracy: 0.97 - ETA: 1s - loss: 0.0788 - accuracy: 0.97 - ETA: 1s - loss: 0.0780 - accuracy: 0.97 - ETA: 1s - loss: 0.0778 - accuracy: 0.97 - ETA: 1s - loss: 0.0770 - accuracy: 0.97 - ETA: 1s - loss: 0.0772 - accuracy: 0.97 - ETA: 1s - loss: 0.0800 - accuracy: 0.97 - ETA: 1s - loss: 0.0811 - accuracy: 0.97 - ETA: 1s - loss: 0.0816 - accuracy: 0.97 - ETA: 1s - loss: 0.0809 - accuracy: 0.97 - ETA: 1s - loss: 0.0808 - accuracy: 0.97 - ETA: 0s - loss: 0.0807 - accuracy: 0.97 - ETA: 0s - loss: 0.0806 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0799 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0790 - accuracy: 0.97 - ETA: 0s - loss: 0.0787 - accuracy: 0.97 - ETA: 0s - loss: 0.0786 - accuracy: 0.97 - ETA: 0s - loss: 0.0784 - accuracy: 0.97 - ETA: 0s - loss: 0.0784 - accuracy: 0.97 - ETA: 0s - loss: 0.0777 - accuracy: 0.97 - ETA: 0s - loss: 0.0771 - accuracy: 0.97 - ETA: 0s - loss: 0.0765 - accuracy: 0.97 - 6s 555us/step - loss: 0.0762 - accuracy: 0.9729 - val_loss: 0.0688 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00794\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0322 - accuracy: 0.98 - ETA: 5s - loss: 0.0564 - accuracy: 0.97 - ETA: 5s - loss: 0.1247 - accuracy: 0.94 - ETA: 5s - loss: 0.1359 - accuracy: 0.95 - ETA: 5s - loss: 0.1444 - accuracy: 0.94 - ETA: 5s - loss: 0.1282 - accuracy: 0.95 - ETA: 5s - loss: 0.1144 - accuracy: 0.95 - ETA: 5s - loss: 0.1038 - accuracy: 0.96 - ETA: 5s - loss: 0.0986 - accuracy: 0.96 - ETA: 4s - loss: 0.0949 - accuracy: 0.96 - ETA: 4s - loss: 0.0916 - accuracy: 0.96 - ETA: 4s - loss: 0.0861 - accuracy: 0.96 - ETA: 4s - loss: 0.0832 - accuracy: 0.96 - ETA: 4s - loss: 0.0848 - accuracy: 0.96 - ETA: 4s - loss: 0.0826 - accuracy: 0.96 - ETA: 4s - loss: 0.0839 - accuracy: 0.96 - ETA: 4s - loss: 0.0817 - accuracy: 0.96 - ETA: 4s - loss: 0.0799 - accuracy: 0.96 - ETA: 4s - loss: 0.0765 - accuracy: 0.97 - ETA: 4s - loss: 0.0762 - accuracy: 0.97 - ETA: 4s - loss: 0.0753 - accuracy: 0.97 - ETA: 4s - loss: 0.0732 - accuracy: 0.97 - ETA: 3s - loss: 0.0739 - accuracy: 0.97 - ETA: 3s - loss: 0.0766 - accuracy: 0.97 - ETA: 3s - loss: 0.0767 - accuracy: 0.97 - ETA: 3s - loss: 0.0749 - accuracy: 0.97 - ETA: 3s - loss: 0.0745 - accuracy: 0.97 - ETA: 3s - loss: 0.0733 - accuracy: 0.97 - ETA: 3s - loss: 0.0748 - accuracy: 0.97 - ETA: 3s - loss: 0.0735 - accuracy: 0.97 - ETA: 3s - loss: 0.0723 - accuracy: 0.97 - ETA: 3s - loss: 0.0709 - accuracy: 0.97 - ETA: 3s - loss: 0.0692 - accuracy: 0.97 - ETA: 3s - loss: 0.0684 - accuracy: 0.97 - ETA: 2s - loss: 0.0686 - accuracy: 0.97 - ETA: 2s - loss: 0.0691 - accuracy: 0.97 - ETA: 2s - loss: 0.0684 - accuracy: 0.97 - ETA: 2s - loss: 0.0728 - accuracy: 0.97 - ETA: 2s - loss: 0.0747 - accuracy: 0.97 - ETA: 2s - loss: 0.0737 - accuracy: 0.97 - ETA: 2s - loss: 0.0736 - accuracy: 0.97 - ETA: 2s - loss: 0.0738 - accuracy: 0.97 - ETA: 2s - loss: 0.0731 - accuracy: 0.97 - ETA: 2s - loss: 0.0724 - accuracy: 0.97 - ETA: 2s - loss: 0.0716 - accuracy: 0.97 - ETA: 2s - loss: 0.0708 - accuracy: 0.97 - ETA: 2s - loss: 0.0697 - accuracy: 0.97 - ETA: 1s - loss: 0.0695 - accuracy: 0.97 - ETA: 1s - loss: 0.0683 - accuracy: 0.97 - ETA: 1s - loss: 0.0674 - accuracy: 0.97 - ETA: 1s - loss: 0.0665 - accuracy: 0.97 - ETA: 1s - loss: 0.0669 - accuracy: 0.97 - ETA: 1s - loss: 0.0661 - accuracy: 0.97 - ETA: 1s - loss: 0.0653 - accuracy: 0.97 - ETA: 1s - loss: 0.0652 - accuracy: 0.97 - ETA: 1s - loss: 0.0653 - accuracy: 0.97 - ETA: 1s - loss: 0.0648 - accuracy: 0.97 - ETA: 1s - loss: 0.0643 - accuracy: 0.97 - ETA: 1s - loss: 0.0641 - accuracy: 0.97 - ETA: 1s - loss: 0.0654 - accuracy: 0.97 - ETA: 0s - loss: 0.0668 - accuracy: 0.97 - ETA: 0s - loss: 0.0663 - accuracy: 0.97 - ETA: 0s - loss: 0.0657 - accuracy: 0.97 - ETA: 0s - loss: 0.0650 - accuracy: 0.97 - ETA: 0s - loss: 0.0648 - accuracy: 0.97 - ETA: 0s - loss: 0.0642 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - ETA: 0s - loss: 0.0644 - accuracy: 0.97 - ETA: 0s - loss: 0.0638 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - ETA: 0s - loss: 0.0632 - accuracy: 0.97 - ETA: 0s - loss: 0.0640 - accuracy: 0.97 - 6s 563us/step - loss: 0.0639 - accuracy: 0.9770 - val_loss: 0.0211 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00794\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0153 - accuracy: 0.99 - ETA: 6s - loss: 0.0287 - accuracy: 0.99 - ETA: 6s - loss: 0.0241 - accuracy: 0.99 - ETA: 5s - loss: 0.0346 - accuracy: 0.98 - ETA: 5s - loss: 0.0310 - accuracy: 0.99 - ETA: 5s - loss: 0.0291 - accuracy: 0.99 - ETA: 5s - loss: 0.0304 - accuracy: 0.99 - ETA: 5s - loss: 0.0391 - accuracy: 0.98 - ETA: 5s - loss: 0.0465 - accuracy: 0.98 - ETA: 5s - loss: 0.0515 - accuracy: 0.98 - ETA: 5s - loss: 0.0512 - accuracy: 0.98 - ETA: 5s - loss: 0.0639 - accuracy: 0.97 - ETA: 5s - loss: 0.0679 - accuracy: 0.97 - ETA: 4s - loss: 0.0649 - accuracy: 0.97 - ETA: 4s - loss: 0.0649 - accuracy: 0.97 - ETA: 4s - loss: 0.0665 - accuracy: 0.97 - ETA: 4s - loss: 0.0655 - accuracy: 0.97 - ETA: 4s - loss: 0.0641 - accuracy: 0.97 - ETA: 4s - loss: 0.0660 - accuracy: 0.97 - ETA: 4s - loss: 0.0657 - accuracy: 0.97 - ETA: 4s - loss: 0.0678 - accuracy: 0.97 - ETA: 4s - loss: 0.0697 - accuracy: 0.97 - ETA: 4s - loss: 0.0694 - accuracy: 0.97 - ETA: 4s - loss: 0.0682 - accuracy: 0.97 - ETA: 4s - loss: 0.0661 - accuracy: 0.97 - ETA: 3s - loss: 0.0648 - accuracy: 0.97 - ETA: 3s - loss: 0.0631 - accuracy: 0.97 - ETA: 3s - loss: 0.0618 - accuracy: 0.97 - ETA: 3s - loss: 0.0616 - accuracy: 0.97 - ETA: 3s - loss: 0.0601 - accuracy: 0.98 - ETA: 3s - loss: 0.0595 - accuracy: 0.98 - ETA: 3s - loss: 0.0588 - accuracy: 0.98 - ETA: 3s - loss: 0.0586 - accuracy: 0.98 - ETA: 3s - loss: 0.0588 - accuracy: 0.98 - ETA: 3s - loss: 0.0594 - accuracy: 0.98 - ETA: 3s - loss: 0.0592 - accuracy: 0.98 - ETA: 3s - loss: 0.0595 - accuracy: 0.98 - ETA: 2s - loss: 0.0591 - accuracy: 0.98 - ETA: 2s - loss: 0.0591 - accuracy: 0.98 - ETA: 2s - loss: 0.0586 - accuracy: 0.98 - ETA: 2s - loss: 0.0595 - accuracy: 0.97 - ETA: 2s - loss: 0.0595 - accuracy: 0.97 - ETA: 2s - loss: 0.0589 - accuracy: 0.98 - ETA: 2s - loss: 0.0588 - accuracy: 0.97 - ETA: 2s - loss: 0.0586 - accuracy: 0.97 - ETA: 2s - loss: 0.0581 - accuracy: 0.97 - ETA: 2s - loss: 0.0577 - accuracy: 0.97 - ETA: 2s - loss: 0.0568 - accuracy: 0.98 - ETA: 2s - loss: 0.0563 - accuracy: 0.98 - ETA: 1s - loss: 0.0559 - accuracy: 0.98 - ETA: 1s - loss: 0.0550 - accuracy: 0.98 - ETA: 1s - loss: 0.0546 - accuracy: 0.98 - ETA: 1s - loss: 0.0541 - accuracy: 0.98 - ETA: 1s - loss: 0.0535 - accuracy: 0.98 - ETA: 1s - loss: 0.0529 - accuracy: 0.98 - ETA: 1s - loss: 0.0527 - accuracy: 0.98 - ETA: 1s - loss: 0.0530 - accuracy: 0.98 - ETA: 1s - loss: 0.0547 - accuracy: 0.98 - ETA: 1s - loss: 0.0600 - accuracy: 0.97 - ETA: 1s - loss: 0.0676 - accuracy: 0.97 - ETA: 1s - loss: 0.0675 - accuracy: 0.97 - ETA: 0s - loss: 0.0675 - accuracy: 0.97 - ETA: 0s - loss: 0.0667 - accuracy: 0.97 - ETA: 0s - loss: 0.0664 - accuracy: 0.97 - ETA: 0s - loss: 0.0657 - accuracy: 0.97 - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - ETA: 0s - loss: 0.0641 - accuracy: 0.97 - ETA: 0s - loss: 0.0637 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - ETA: 0s - loss: 0.0633 - accuracy: 0.97 - ETA: 0s - loss: 0.0632 - accuracy: 0.97 - ETA: 0s - loss: 0.0627 - accuracy: 0.97 - ETA: 0s - loss: 0.0628 - accuracy: 0.97 - 6s 590us/step - loss: 0.0625 - accuracy: 0.9786 - val_loss: 0.0272 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00794\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0187 - accuracy: 1.00 - ETA: 7s - loss: 0.0530 - accuracy: 0.98 - ETA: 6s - loss: 0.0485 - accuracy: 0.98 - ETA: 6s - loss: 0.0448 - accuracy: 0.98 - ETA: 6s - loss: 0.0393 - accuracy: 0.98 - ETA: 6s - loss: 0.0350 - accuracy: 0.99 - ETA: 6s - loss: 0.0349 - accuracy: 0.99 - ETA: 6s - loss: 0.0348 - accuracy: 0.98 - ETA: 6s - loss: 0.0396 - accuracy: 0.98 - ETA: 6s - loss: 0.0435 - accuracy: 0.98 - ETA: 6s - loss: 0.0416 - accuracy: 0.98 - ETA: 6s - loss: 0.0416 - accuracy: 0.98 - ETA: 6s - loss: 0.0413 - accuracy: 0.98 - ETA: 6s - loss: 0.0414 - accuracy: 0.98 - ETA: 5s - loss: 0.0492 - accuracy: 0.98 - ETA: 5s - loss: 0.0546 - accuracy: 0.98 - ETA: 5s - loss: 0.0542 - accuracy: 0.98 - ETA: 5s - loss: 0.0545 - accuracy: 0.98 - ETA: 5s - loss: 0.0563 - accuracy: 0.98 - ETA: 5s - loss: 0.0572 - accuracy: 0.98 - ETA: 5s - loss: 0.0556 - accuracy: 0.98 - ETA: 4s - loss: 0.0541 - accuracy: 0.98 - ETA: 4s - loss: 0.0530 - accuracy: 0.98 - ETA: 4s - loss: 0.0516 - accuracy: 0.98 - ETA: 4s - loss: 0.0536 - accuracy: 0.98 - ETA: 4s - loss: 0.0544 - accuracy: 0.98 - ETA: 4s - loss: 0.0535 - accuracy: 0.98 - ETA: 4s - loss: 0.0520 - accuracy: 0.98 - ETA: 4s - loss: 0.0514 - accuracy: 0.98 - ETA: 3s - loss: 0.0539 - accuracy: 0.98 - ETA: 3s - loss: 0.0566 - accuracy: 0.98 - ETA: 3s - loss: 0.0590 - accuracy: 0.98 - ETA: 3s - loss: 0.0583 - accuracy: 0.98 - ETA: 3s - loss: 0.0581 - accuracy: 0.98 - ETA: 3s - loss: 0.0574 - accuracy: 0.98 - ETA: 3s - loss: 0.0560 - accuracy: 0.98 - ETA: 3s - loss: 0.0552 - accuracy: 0.98 - ETA: 3s - loss: 0.0544 - accuracy: 0.98 - ETA: 3s - loss: 0.0533 - accuracy: 0.98 - ETA: 2s - loss: 0.0531 - accuracy: 0.98 - ETA: 2s - loss: 0.0542 - accuracy: 0.98 - ETA: 2s - loss: 0.0557 - accuracy: 0.98 - ETA: 2s - loss: 0.0559 - accuracy: 0.98 - ETA: 2s - loss: 0.0550 - accuracy: 0.98 - ETA: 2s - loss: 0.0542 - accuracy: 0.98 - ETA: 2s - loss: 0.0534 - accuracy: 0.98 - ETA: 2s - loss: 0.0529 - accuracy: 0.98 - ETA: 2s - loss: 0.0527 - accuracy: 0.98 - ETA: 2s - loss: 0.0523 - accuracy: 0.98 - ETA: 2s - loss: 0.0516 - accuracy: 0.98 - ETA: 1s - loss: 0.0510 - accuracy: 0.98 - ETA: 1s - loss: 0.0508 - accuracy: 0.98 - ETA: 1s - loss: 0.0524 - accuracy: 0.98 - ETA: 1s - loss: 0.0531 - accuracy: 0.98 - ETA: 1s - loss: 0.0529 - accuracy: 0.98 - ETA: 1s - loss: 0.0522 - accuracy: 0.98 - ETA: 1s - loss: 0.0516 - accuracy: 0.98 - ETA: 1s - loss: 0.0516 - accuracy: 0.98 - ETA: 1s - loss: 0.0508 - accuracy: 0.98 - ETA: 1s - loss: 0.0502 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - ETA: 0s - loss: 0.0516 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.98 - ETA: 0s - loss: 0.0530 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0519 - accuracy: 0.98 - ETA: 0s - loss: 0.0521 - accuracy: 0.98 - ETA: 0s - loss: 0.0520 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0515 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - 6s 592us/step - loss: 0.0519 - accuracy: 0.9814 - val_loss: 0.1129 - val_accuracy: 0.9562\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00794\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1194 - accuracy: 0.95 - ETA: 5s - loss: 0.0821 - accuracy: 0.96 - ETA: 5s - loss: 0.0677 - accuracy: 0.97 - ETA: 5s - loss: 0.0529 - accuracy: 0.98 - ETA: 5s - loss: 0.0540 - accuracy: 0.97 - ETA: 5s - loss: 0.0534 - accuracy: 0.97 - ETA: 5s - loss: 0.0492 - accuracy: 0.98 - ETA: 5s - loss: 0.0473 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0432 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0420 - accuracy: 0.98 - ETA: 4s - loss: 0.0403 - accuracy: 0.98 - ETA: 4s - loss: 0.0398 - accuracy: 0.98 - ETA: 4s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0426 - accuracy: 0.98 - ETA: 4s - loss: 0.0406 - accuracy: 0.98 - ETA: 4s - loss: 0.0400 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0380 - accuracy: 0.98 - ETA: 3s - loss: 0.0423 - accuracy: 0.98 - ETA: 3s - loss: 0.0429 - accuracy: 0.98 - ETA: 3s - loss: 0.0423 - accuracy: 0.98 - ETA: 3s - loss: 0.0435 - accuracy: 0.98 - ETA: 3s - loss: 0.0482 - accuracy: 0.98 - ETA: 3s - loss: 0.0483 - accuracy: 0.98 - ETA: 3s - loss: 0.0491 - accuracy: 0.98 - ETA: 3s - loss: 0.0491 - accuracy: 0.98 - ETA: 3s - loss: 0.0488 - accuracy: 0.98 - ETA: 3s - loss: 0.0506 - accuracy: 0.98 - ETA: 2s - loss: 0.0520 - accuracy: 0.98 - ETA: 2s - loss: 0.0510 - accuracy: 0.98 - ETA: 2s - loss: 0.0507 - accuracy: 0.98 - ETA: 2s - loss: 0.0497 - accuracy: 0.98 - ETA: 2s - loss: 0.0495 - accuracy: 0.98 - ETA: 2s - loss: 0.0491 - accuracy: 0.98 - ETA: 2s - loss: 0.0482 - accuracy: 0.98 - ETA: 2s - loss: 0.0475 - accuracy: 0.98 - ETA: 2s - loss: 0.0467 - accuracy: 0.98 - ETA: 2s - loss: 0.0460 - accuracy: 0.98 - ETA: 2s - loss: 0.0451 - accuracy: 0.98 - ETA: 2s - loss: 0.0465 - accuracy: 0.98 - ETA: 2s - loss: 0.0465 - accuracy: 0.98 - ETA: 2s - loss: 0.0463 - accuracy: 0.98 - ETA: 1s - loss: 0.0462 - accuracy: 0.98 - ETA: 1s - loss: 0.0457 - accuracy: 0.98 - ETA: 1s - loss: 0.0451 - accuracy: 0.98 - ETA: 1s - loss: 0.0448 - accuracy: 0.98 - ETA: 1s - loss: 0.0444 - accuracy: 0.98 - ETA: 1s - loss: 0.0444 - accuracy: 0.98 - ETA: 1s - loss: 0.0441 - accuracy: 0.98 - ETA: 1s - loss: 0.0440 - accuracy: 0.98 - ETA: 1s - loss: 0.0437 - accuracy: 0.98 - ETA: 1s - loss: 0.0431 - accuracy: 0.98 - ETA: 1s - loss: 0.0429 - accuracy: 0.98 - ETA: 1s - loss: 0.0424 - accuracy: 0.98 - ETA: 1s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0420 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0515 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0503 - accuracy: 0.98 - ETA: 0s - loss: 0.0498 - accuracy: 0.98 - ETA: 0s - loss: 0.0498 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - 6s 538us/step - loss: 0.0487 - accuracy: 0.9839 - val_loss: 0.0311 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00794\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0439 - accuracy: 0.98 - ETA: 5s - loss: 0.0519 - accuracy: 0.98 - ETA: 5s - loss: 0.0505 - accuracy: 0.98 - ETA: 5s - loss: 0.0488 - accuracy: 0.98 - ETA: 5s - loss: 0.0416 - accuracy: 0.98 - ETA: 5s - loss: 0.0414 - accuracy: 0.98 - ETA: 5s - loss: 0.0391 - accuracy: 0.98 - ETA: 4s - loss: 0.0357 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0318 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0334 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0331 - accuracy: 0.99 - ETA: 4s - loss: 0.0322 - accuracy: 0.99 - ETA: 4s - loss: 0.0310 - accuracy: 0.99 - ETA: 3s - loss: 0.0299 - accuracy: 0.99 - ETA: 3s - loss: 0.0288 - accuracy: 0.99 - ETA: 3s - loss: 0.0284 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 3s - loss: 0.0292 - accuracy: 0.99 - ETA: 3s - loss: 0.0306 - accuracy: 0.99 - ETA: 3s - loss: 0.0325 - accuracy: 0.99 - ETA: 3s - loss: 0.0318 - accuracy: 0.99 - ETA: 3s - loss: 0.0314 - accuracy: 0.99 - ETA: 3s - loss: 0.0308 - accuracy: 0.99 - ETA: 3s - loss: 0.0300 - accuracy: 0.99 - ETA: 3s - loss: 0.0299 - accuracy: 0.99 - ETA: 3s - loss: 0.0303 - accuracy: 0.99 - ETA: 3s - loss: 0.0304 - accuracy: 0.99 - ETA: 2s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0295 - accuracy: 0.99 - ETA: 2s - loss: 0.0294 - accuracy: 0.99 - ETA: 2s - loss: 0.0303 - accuracy: 0.99 - ETA: 2s - loss: 0.0352 - accuracy: 0.99 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0396 - accuracy: 0.98 - ETA: 2s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0382 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0403 - accuracy: 0.98 - ETA: 2s - loss: 0.0401 - accuracy: 0.98 - ETA: 2s - loss: 0.0401 - accuracy: 0.98 - ETA: 2s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0391 - accuracy: 0.98 - ETA: 1s - loss: 0.0387 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0398 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0388 - accuracy: 0.98 - ETA: 1s - loss: 0.0388 - accuracy: 0.98 - ETA: 1s - loss: 0.0382 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0382 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0389 - accuracy: 0.98 - ETA: 0s - loss: 0.0395 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0392 - accuracy: 0.98 - ETA: 0s - loss: 0.0392 - accuracy: 0.98 - ETA: 0s - loss: 0.0389 - accuracy: 0.98 - ETA: 0s - loss: 0.0390 - accuracy: 0.98 - ETA: 0s - loss: 0.0391 - accuracy: 0.98 - ETA: 0s - loss: 0.0389 - accuracy: 0.98 - ETA: 0s - loss: 0.0384 - accuracy: 0.98 - 6s 598us/step - loss: 0.0382 - accuracy: 0.9881 - val_loss: 0.0200 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00794\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0738 - accuracy: 0.97 - ETA: 5s - loss: 0.0825 - accuracy: 0.96 - ETA: 5s - loss: 0.0847 - accuracy: 0.95 - ETA: 5s - loss: 0.0989 - accuracy: 0.95 - ETA: 5s - loss: 0.0898 - accuracy: 0.96 - ETA: 5s - loss: 0.0793 - accuracy: 0.96 - ETA: 5s - loss: 0.0710 - accuracy: 0.97 - ETA: 5s - loss: 0.0658 - accuracy: 0.97 - ETA: 5s - loss: 0.0648 - accuracy: 0.97 - ETA: 5s - loss: 0.0595 - accuracy: 0.97 - ETA: 5s - loss: 0.0569 - accuracy: 0.97 - ETA: 4s - loss: 0.0555 - accuracy: 0.97 - ETA: 4s - loss: 0.0530 - accuracy: 0.97 - ETA: 4s - loss: 0.0533 - accuracy: 0.98 - ETA: 4s - loss: 0.0533 - accuracy: 0.98 - ETA: 4s - loss: 0.0542 - accuracy: 0.97 - ETA: 4s - loss: 0.0538 - accuracy: 0.97 - ETA: 4s - loss: 0.0511 - accuracy: 0.98 - ETA: 4s - loss: 0.0506 - accuracy: 0.98 - ETA: 4s - loss: 0.0486 - accuracy: 0.98 - ETA: 4s - loss: 0.0492 - accuracy: 0.98 - ETA: 4s - loss: 0.0481 - accuracy: 0.98 - ETA: 4s - loss: 0.0483 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0471 - accuracy: 0.98 - ETA: 4s - loss: 0.0471 - accuracy: 0.98 - ETA: 4s - loss: 0.0460 - accuracy: 0.98 - ETA: 4s - loss: 0.0462 - accuracy: 0.98 - ETA: 4s - loss: 0.0466 - accuracy: 0.98 - ETA: 4s - loss: 0.0524 - accuracy: 0.98 - ETA: 3s - loss: 0.0532 - accuracy: 0.98 - ETA: 3s - loss: 0.0521 - accuracy: 0.98 - ETA: 3s - loss: 0.0508 - accuracy: 0.98 - ETA: 3s - loss: 0.0514 - accuracy: 0.98 - ETA: 3s - loss: 0.0505 - accuracy: 0.98 - ETA: 3s - loss: 0.0493 - accuracy: 0.98 - ETA: 3s - loss: 0.0492 - accuracy: 0.98 - ETA: 3s - loss: 0.0490 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 3s - loss: 0.0475 - accuracy: 0.98 - ETA: 3s - loss: 0.0468 - accuracy: 0.98 - ETA: 3s - loss: 0.0460 - accuracy: 0.98 - ETA: 2s - loss: 0.0463 - accuracy: 0.98 - ETA: 2s - loss: 0.0459 - accuracy: 0.98 - ETA: 2s - loss: 0.0452 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0429 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0417 - accuracy: 0.98 - ETA: 2s - loss: 0.0422 - accuracy: 0.98 - ETA: 2s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0430 - accuracy: 0.98 - ETA: 1s - loss: 0.0424 - accuracy: 0.98 - ETA: 1s - loss: 0.0419 - accuracy: 0.98 - ETA: 1s - loss: 0.0419 - accuracy: 0.98 - ETA: 1s - loss: 0.0413 - accuracy: 0.98 - ETA: 1s - loss: 0.0409 - accuracy: 0.98 - ETA: 1s - loss: 0.0408 - accuracy: 0.98 - ETA: 1s - loss: 0.0423 - accuracy: 0.98 - ETA: 1s - loss: 0.0433 - accuracy: 0.98 - ETA: 1s - loss: 0.0437 - accuracy: 0.98 - ETA: 1s - loss: 0.0436 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0427 - accuracy: 0.98 - ETA: 0s - loss: 0.0426 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0435 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0429 - accuracy: 0.98 - ETA: 0s - loss: 0.0436 - accuracy: 0.98 - 8s 712us/step - loss: 0.0453 - accuracy: 0.9836 - val_loss: 0.0512 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00794\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 8s - loss: 0.0454 - accuracy: 0.99 - ETA: 8s - loss: 0.0547 - accuracy: 0.98 - ETA: 7s - loss: 0.0433 - accuracy: 0.98 - ETA: 7s - loss: 0.0350 - accuracy: 0.99 - ETA: 7s - loss: 0.0302 - accuracy: 0.99 - ETA: 7s - loss: 0.0300 - accuracy: 0.99 - ETA: 7s - loss: 0.0273 - accuracy: 0.99 - ETA: 7s - loss: 0.0255 - accuracy: 0.99 - ETA: 6s - loss: 0.0232 - accuracy: 0.99 - ETA: 6s - loss: 0.0210 - accuracy: 0.99 - ETA: 6s - loss: 0.0221 - accuracy: 0.99 - ETA: 6s - loss: 0.0222 - accuracy: 0.99 - ETA: 6s - loss: 0.0209 - accuracy: 0.99 - ETA: 6s - loss: 0.0199 - accuracy: 0.99 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 5s - loss: 0.0193 - accuracy: 0.99 - ETA: 5s - loss: 0.0192 - accuracy: 0.99 - ETA: 5s - loss: 0.0200 - accuracy: 0.99 - ETA: 5s - loss: 0.0225 - accuracy: 0.99 - ETA: 5s - loss: 0.0285 - accuracy: 0.99 - ETA: 5s - loss: 0.0279 - accuracy: 0.99 - ETA: 5s - loss: 0.0277 - accuracy: 0.99 - ETA: 4s - loss: 0.0268 - accuracy: 0.99 - ETA: 4s - loss: 0.0279 - accuracy: 0.99 - ETA: 4s - loss: 0.0281 - accuracy: 0.99 - ETA: 4s - loss: 0.0273 - accuracy: 0.99 - ETA: 4s - loss: 0.0275 - accuracy: 0.99 - ETA: 4s - loss: 0.0271 - accuracy: 0.99 - ETA: 4s - loss: 0.0263 - accuracy: 0.99 - ETA: 4s - loss: 0.0264 - accuracy: 0.99 - ETA: 4s - loss: 0.0257 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0261 - accuracy: 0.99 - ETA: 3s - loss: 0.0301 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0301 - accuracy: 0.99 - ETA: 3s - loss: 0.0304 - accuracy: 0.98 - ETA: 3s - loss: 0.0303 - accuracy: 0.98 - ETA: 3s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0298 - accuracy: 0.99 - ETA: 3s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0307 - accuracy: 0.98 - ETA: 3s - loss: 0.0315 - accuracy: 0.98 - ETA: 2s - loss: 0.0314 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0317 - accuracy: 0.98 - ETA: 2s - loss: 0.0312 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0304 - accuracy: 0.98 - ETA: 2s - loss: 0.0305 - accuracy: 0.98 - ETA: 2s - loss: 0.0302 - accuracy: 0.98 - ETA: 2s - loss: 0.0299 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0315 - accuracy: 0.98 - ETA: 1s - loss: 0.0319 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 1s - loss: 0.0336 - accuracy: 0.98 - ETA: 1s - loss: 0.0338 - accuracy: 0.98 - ETA: 1s - loss: 0.0334 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 1s - loss: 0.0333 - accuracy: 0.98 - ETA: 1s - loss: 0.0333 - accuracy: 0.98 - ETA: 1s - loss: 0.0334 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0339 - accuracy: 0.98 - ETA: 0s - loss: 0.0337 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0362 - accuracy: 0.98 - ETA: 0s - loss: 0.0358 - accuracy: 0.98 - ETA: 0s - loss: 0.0357 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - 7s 683us/step - loss: 0.0356 - accuracy: 0.9874 - val_loss: 0.0235 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00794\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0279 - accuracy: 0.98 - ETA: 5s - loss: 0.0375 - accuracy: 0.98 - ETA: 5s - loss: 0.0306 - accuracy: 0.98 - ETA: 5s - loss: 0.0311 - accuracy: 0.98 - ETA: 5s - loss: 0.0301 - accuracy: 0.98 - ETA: 5s - loss: 0.0296 - accuracy: 0.98 - ETA: 5s - loss: 0.0277 - accuracy: 0.98 - ETA: 5s - loss: 0.0287 - accuracy: 0.98 - ETA: 5s - loss: 0.0293 - accuracy: 0.98 - ETA: 5s - loss: 0.0286 - accuracy: 0.98 - ETA: 5s - loss: 0.0295 - accuracy: 0.98 - ETA: 5s - loss: 0.0280 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.98 - ETA: 4s - loss: 0.0286 - accuracy: 0.98 - ETA: 4s - loss: 0.0301 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0371 - accuracy: 0.98 - ETA: 4s - loss: 0.0406 - accuracy: 0.98 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 4s - loss: 0.0389 - accuracy: 0.98 - ETA: 4s - loss: 0.0396 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0388 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0349 - accuracy: 0.98 - ETA: 3s - loss: 0.0346 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0369 - accuracy: 0.98 - ETA: 3s - loss: 0.0376 - accuracy: 0.98 - ETA: 3s - loss: 0.0388 - accuracy: 0.98 - ETA: 3s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0364 - accuracy: 0.98 - ETA: 2s - loss: 0.0355 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0352 - accuracy: 0.98 - ETA: 2s - loss: 0.0345 - accuracy: 0.98 - ETA: 2s - loss: 0.0341 - accuracy: 0.98 - ETA: 2s - loss: 0.0339 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 2s - loss: 0.0327 - accuracy: 0.98 - ETA: 2s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0330 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0337 - accuracy: 0.98 - ETA: 1s - loss: 0.0346 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0343 - accuracy: 0.98 - ETA: 1s - loss: 0.0342 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 1s - loss: 0.0353 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0346 - accuracy: 0.98 - ETA: 0s - loss: 0.0343 - accuracy: 0.98 - ETA: 0s - loss: 0.0340 - accuracy: 0.98 - ETA: 0s - loss: 0.0343 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0336 - accuracy: 0.98 - ETA: 0s - loss: 0.0349 - accuracy: 0.98 - ETA: 0s - loss: 0.0355 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0360 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - 6s 583us/step - loss: 0.0360 - accuracy: 0.9866 - val_loss: 0.0332 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00794\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0926 - accuracy: 0.95 - ETA: 7s - loss: 0.0876 - accuracy: 0.96 - ETA: 7s - loss: 0.0772 - accuracy: 0.96 - ETA: 6s - loss: 0.0918 - accuracy: 0.96 - ETA: 6s - loss: 0.0780 - accuracy: 0.97 - ETA: 6s - loss: 0.0697 - accuracy: 0.97 - ETA: 6s - loss: 0.0612 - accuracy: 0.98 - ETA: 5s - loss: 0.0553 - accuracy: 0.98 - ETA: 5s - loss: 0.0495 - accuracy: 0.98 - ETA: 5s - loss: 0.0478 - accuracy: 0.98 - ETA: 5s - loss: 0.0444 - accuracy: 0.98 - ETA: 5s - loss: 0.0411 - accuracy: 0.98 - ETA: 5s - loss: 0.0385 - accuracy: 0.98 - ETA: 5s - loss: 0.0368 - accuracy: 0.98 - ETA: 5s - loss: 0.0351 - accuracy: 0.98 - ETA: 5s - loss: 0.0362 - accuracy: 0.98 - ETA: 4s - loss: 0.0359 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0344 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0318 - accuracy: 0.98 - ETA: 4s - loss: 0.0307 - accuracy: 0.99 - ETA: 4s - loss: 0.0306 - accuracy: 0.99 - ETA: 4s - loss: 0.0332 - accuracy: 0.98 - ETA: 4s - loss: 0.0327 - accuracy: 0.98 - ETA: 4s - loss: 0.0324 - accuracy: 0.98 - ETA: 3s - loss: 0.0320 - accuracy: 0.98 - ETA: 3s - loss: 0.0325 - accuracy: 0.98 - ETA: 3s - loss: 0.0325 - accuracy: 0.98 - ETA: 3s - loss: 0.0320 - accuracy: 0.98 - ETA: 3s - loss: 0.0316 - accuracy: 0.99 - ETA: 3s - loss: 0.0319 - accuracy: 0.99 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 2s - loss: 0.0383 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0370 - accuracy: 0.98 - ETA: 2s - loss: 0.0370 - accuracy: 0.98 - ETA: 2s - loss: 0.0364 - accuracy: 0.98 - ETA: 2s - loss: 0.0359 - accuracy: 0.98 - ETA: 2s - loss: 0.0359 - accuracy: 0.98 - ETA: 2s - loss: 0.0359 - accuracy: 0.98 - ETA: 2s - loss: 0.0368 - accuracy: 0.98 - ETA: 2s - loss: 0.0370 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 1s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0364 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0363 - accuracy: 0.98 - ETA: 1s - loss: 0.0358 - accuracy: 0.98 - ETA: 1s - loss: 0.0355 - accuracy: 0.98 - ETA: 1s - loss: 0.0358 - accuracy: 0.98 - ETA: 1s - loss: 0.0361 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 0s - loss: 0.0351 - accuracy: 0.98 - ETA: 0s - loss: 0.0350 - accuracy: 0.98 - ETA: 0s - loss: 0.0344 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0353 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0350 - accuracy: 0.98 - ETA: 0s - loss: 0.0346 - accuracy: 0.98 - ETA: 0s - loss: 0.0343 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0340 - accuracy: 0.98 - ETA: 0s - loss: 0.0340 - accuracy: 0.98 - 6s 595us/step - loss: 0.0346 - accuracy: 0.9887 - val_loss: 0.0280 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00794\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0267 - accuracy: 0.98 - ETA: 5s - loss: 0.0169 - accuracy: 0.99 - ETA: 5s - loss: 0.0211 - accuracy: 0.99 - ETA: 5s - loss: 0.0228 - accuracy: 0.99 - ETA: 5s - loss: 0.0193 - accuracy: 0.99 - ETA: 5s - loss: 0.0214 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0172 - accuracy: 0.99 - ETA: 5s - loss: 0.0161 - accuracy: 0.99 - ETA: 5s - loss: 0.0167 - accuracy: 0.99 - ETA: 5s - loss: 0.0171 - accuracy: 0.99 - ETA: 5s - loss: 0.0167 - accuracy: 0.99 - ETA: 5s - loss: 0.0155 - accuracy: 0.99 - ETA: 4s - loss: 0.0156 - accuracy: 0.99 - ETA: 4s - loss: 0.0152 - accuracy: 0.99 - ETA: 4s - loss: 0.0168 - accuracy: 0.99 - ETA: 4s - loss: 0.0217 - accuracy: 0.99 - ETA: 4s - loss: 0.0233 - accuracy: 0.99 - ETA: 4s - loss: 0.0260 - accuracy: 0.99 - ETA: 4s - loss: 0.0394 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0368 - accuracy: 0.99 - ETA: 4s - loss: 0.0386 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0370 - accuracy: 0.99 - ETA: 4s - loss: 0.0389 - accuracy: 0.98 - ETA: 3s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0373 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0360 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0354 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0350 - accuracy: 0.98 - ETA: 3s - loss: 0.0342 - accuracy: 0.98 - ETA: 2s - loss: 0.0337 - accuracy: 0.98 - ETA: 2s - loss: 0.0330 - accuracy: 0.98 - ETA: 2s - loss: 0.0323 - accuracy: 0.99 - ETA: 2s - loss: 0.0345 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 2s - loss: 0.0337 - accuracy: 0.99 - ETA: 2s - loss: 0.0345 - accuracy: 0.98 - ETA: 2s - loss: 0.0345 - accuracy: 0.98 - ETA: 2s - loss: 0.0344 - accuracy: 0.98 - ETA: 2s - loss: 0.0342 - accuracy: 0.98 - ETA: 2s - loss: 0.0342 - accuracy: 0.98 - ETA: 1s - loss: 0.0340 - accuracy: 0.98 - ETA: 1s - loss: 0.0336 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 1s - loss: 0.0332 - accuracy: 0.98 - ETA: 1s - loss: 0.0327 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.99 - ETA: 1s - loss: 0.0313 - accuracy: 0.99 - ETA: 1s - loss: 0.0312 - accuracy: 0.99 - ETA: 0s - loss: 0.0310 - accuracy: 0.99 - ETA: 0s - loss: 0.0312 - accuracy: 0.99 - ETA: 0s - loss: 0.0316 - accuracy: 0.99 - ETA: 0s - loss: 0.0313 - accuracy: 0.99 - ETA: 0s - loss: 0.0309 - accuracy: 0.99 - ETA: 0s - loss: 0.0306 - accuracy: 0.99 - ETA: 0s - loss: 0.0308 - accuracy: 0.99 - ETA: 0s - loss: 0.0308 - accuracy: 0.99 - ETA: 0s - loss: 0.0305 - accuracy: 0.99 - ETA: 0s - loss: 0.0303 - accuracy: 0.99 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - 6s 590us/step - loss: 0.0339 - accuracy: 0.9891 - val_loss: 0.0276 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00794\n",
      "Epoch 00017: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 215us/step\n",
      "[2020-05-18 16:29:16 RAM72.4% 1.41GB] Val Score : [0.033767592687219086, 0.9871114492416382]\n",
      "[2020-05-18 16:29:16 RAM72.4% 1.41GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:29:16 RAM72.4% 1.41GB] Training on Fold : 6\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 21s - loss: 7.0239 - accuracy: 0.427 - ETA: 13s - loss: 9.0989 - accuracy: 0.520 - ETA: 10s - loss: 6.8674 - accuracy: 0.528 - ETA: 9s - loss: 5.6741 - accuracy: 0.515 - ETA: 8s - loss: 4.9737 - accuracy: 0.53 - ETA: 7s - loss: 4.4951 - accuracy: 0.52 - ETA: 7s - loss: 4.1208 - accuracy: 0.53 - ETA: 6s - loss: 3.7779 - accuracy: 0.52 - ETA: 6s - loss: 3.4860 - accuracy: 0.53 - ETA: 6s - loss: 3.2554 - accuracy: 0.53 - ETA: 6s - loss: 3.0648 - accuracy: 0.53 - ETA: 5s - loss: 2.9002 - accuracy: 0.52 - ETA: 5s - loss: 2.7579 - accuracy: 0.53 - ETA: 5s - loss: 2.6220 - accuracy: 0.53 - ETA: 5s - loss: 2.5020 - accuracy: 0.53 - ETA: 5s - loss: 2.4050 - accuracy: 0.54 - ETA: 5s - loss: 2.3191 - accuracy: 0.54 - ETA: 4s - loss: 2.2375 - accuracy: 0.54 - ETA: 4s - loss: 2.1642 - accuracy: 0.54 - ETA: 4s - loss: 2.1051 - accuracy: 0.55 - ETA: 4s - loss: 2.0400 - accuracy: 0.55 - ETA: 4s - loss: 1.9836 - accuracy: 0.55 - ETA: 4s - loss: 1.9310 - accuracy: 0.55 - ETA: 4s - loss: 1.8757 - accuracy: 0.56 - ETA: 4s - loss: 1.8326 - accuracy: 0.56 - ETA: 4s - loss: 1.7916 - accuracy: 0.56 - ETA: 3s - loss: 1.7487 - accuracy: 0.56 - ETA: 3s - loss: 1.7114 - accuracy: 0.57 - ETA: 3s - loss: 1.6763 - accuracy: 0.57 - ETA: 3s - loss: 1.6416 - accuracy: 0.57 - ETA: 3s - loss: 1.6076 - accuracy: 0.58 - ETA: 3s - loss: 1.5782 - accuracy: 0.58 - ETA: 3s - loss: 1.5515 - accuracy: 0.58 - ETA: 3s - loss: 1.5249 - accuracy: 0.58 - ETA: 3s - loss: 1.5035 - accuracy: 0.58 - ETA: 3s - loss: 1.4817 - accuracy: 0.58 - ETA: 3s - loss: 1.4603 - accuracy: 0.58 - ETA: 2s - loss: 1.4374 - accuracy: 0.58 - ETA: 2s - loss: 1.4151 - accuracy: 0.59 - ETA: 2s - loss: 1.3938 - accuracy: 0.59 - ETA: 2s - loss: 1.3721 - accuracy: 0.59 - ETA: 2s - loss: 1.3509 - accuracy: 0.60 - ETA: 2s - loss: 1.3307 - accuracy: 0.60 - ETA: 2s - loss: 1.3128 - accuracy: 0.60 - ETA: 2s - loss: 1.2938 - accuracy: 0.60 - ETA: 2s - loss: 1.2764 - accuracy: 0.61 - ETA: 2s - loss: 1.2588 - accuracy: 0.61 - ETA: 2s - loss: 1.2428 - accuracy: 0.61 - ETA: 2s - loss: 1.2269 - accuracy: 0.62 - ETA: 1s - loss: 1.2125 - accuracy: 0.62 - ETA: 1s - loss: 1.1972 - accuracy: 0.62 - ETA: 1s - loss: 1.1820 - accuracy: 0.63 - ETA: 1s - loss: 1.1679 - accuracy: 0.63 - ETA: 1s - loss: 1.1537 - accuracy: 0.63 - ETA: 1s - loss: 1.1421 - accuracy: 0.63 - ETA: 1s - loss: 1.1308 - accuracy: 0.64 - ETA: 1s - loss: 1.1224 - accuracy: 0.64 - ETA: 1s - loss: 1.1117 - accuracy: 0.64 - ETA: 1s - loss: 1.1013 - accuracy: 0.64 - ETA: 1s - loss: 1.0901 - accuracy: 0.64 - ETA: 1s - loss: 1.0800 - accuracy: 0.64 - ETA: 0s - loss: 1.0689 - accuracy: 0.65 - ETA: 0s - loss: 1.0569 - accuracy: 0.65 - ETA: 0s - loss: 1.0456 - accuracy: 0.65 - ETA: 0s - loss: 1.0352 - accuracy: 0.66 - ETA: 0s - loss: 1.0266 - accuracy: 0.66 - ETA: 0s - loss: 1.0182 - accuracy: 0.66 - ETA: 0s - loss: 1.0112 - accuracy: 0.66 - ETA: 0s - loss: 1.0025 - accuracy: 0.66 - ETA: 0s - loss: 0.9942 - accuracy: 0.66 - ETA: 0s - loss: 0.9856 - accuracy: 0.67 - ETA: 0s - loss: 0.9773 - accuracy: 0.67 - ETA: 0s - loss: 0.9694 - accuracy: 0.67 - 6s 582us/step - loss: 0.9651 - accuracy: 0.6746 - val_loss: 0.3333 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00794\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.3586 - accuracy: 0.82 - ETA: 5s - loss: 0.3390 - accuracy: 0.85 - ETA: 5s - loss: 0.3298 - accuracy: 0.87 - ETA: 5s - loss: 0.3183 - accuracy: 0.87 - ETA: 5s - loss: 0.3241 - accuracy: 0.87 - ETA: 5s - loss: 0.3245 - accuracy: 0.87 - ETA: 4s - loss: 0.3211 - accuracy: 0.87 - ETA: 4s - loss: 0.3200 - accuracy: 0.87 - ETA: 4s - loss: 0.3345 - accuracy: 0.86 - ETA: 4s - loss: 0.3680 - accuracy: 0.84 - ETA: 4s - loss: 0.3772 - accuracy: 0.83 - ETA: 4s - loss: 0.3833 - accuracy: 0.82 - ETA: 4s - loss: 0.3837 - accuracy: 0.82 - ETA: 4s - loss: 0.3773 - accuracy: 0.82 - ETA: 4s - loss: 0.3701 - accuracy: 0.83 - ETA: 4s - loss: 0.3658 - accuracy: 0.83 - ETA: 4s - loss: 0.3593 - accuracy: 0.83 - ETA: 4s - loss: 0.3548 - accuracy: 0.84 - ETA: 4s - loss: 0.3609 - accuracy: 0.83 - ETA: 4s - loss: 0.3715 - accuracy: 0.83 - ETA: 4s - loss: 0.3772 - accuracy: 0.82 - ETA: 3s - loss: 0.3749 - accuracy: 0.83 - ETA: 3s - loss: 0.3695 - accuracy: 0.83 - ETA: 3s - loss: 0.3658 - accuracy: 0.83 - ETA: 3s - loss: 0.3620 - accuracy: 0.83 - ETA: 3s - loss: 0.3579 - accuracy: 0.84 - ETA: 3s - loss: 0.3531 - accuracy: 0.84 - ETA: 3s - loss: 0.3506 - accuracy: 0.84 - ETA: 3s - loss: 0.3494 - accuracy: 0.84 - ETA: 3s - loss: 0.3479 - accuracy: 0.84 - ETA: 3s - loss: 0.3522 - accuracy: 0.84 - ETA: 3s - loss: 0.3513 - accuracy: 0.84 - ETA: 3s - loss: 0.3478 - accuracy: 0.84 - ETA: 3s - loss: 0.3481 - accuracy: 0.84 - ETA: 3s - loss: 0.3485 - accuracy: 0.84 - ETA: 2s - loss: 0.3471 - accuracy: 0.84 - ETA: 2s - loss: 0.3436 - accuracy: 0.84 - ETA: 2s - loss: 0.3416 - accuracy: 0.84 - ETA: 2s - loss: 0.3396 - accuracy: 0.84 - ETA: 2s - loss: 0.3408 - accuracy: 0.84 - ETA: 2s - loss: 0.3384 - accuracy: 0.84 - ETA: 2s - loss: 0.3386 - accuracy: 0.85 - ETA: 2s - loss: 0.3359 - accuracy: 0.85 - ETA: 2s - loss: 0.3331 - accuracy: 0.85 - ETA: 2s - loss: 0.3297 - accuracy: 0.85 - ETA: 2s - loss: 0.3277 - accuracy: 0.85 - ETA: 2s - loss: 0.3248 - accuracy: 0.85 - ETA: 2s - loss: 0.3243 - accuracy: 0.85 - ETA: 1s - loss: 0.3236 - accuracy: 0.85 - ETA: 1s - loss: 0.3214 - accuracy: 0.85 - ETA: 1s - loss: 0.3199 - accuracy: 0.85 - ETA: 1s - loss: 0.3185 - accuracy: 0.85 - ETA: 1s - loss: 0.3169 - accuracy: 0.85 - ETA: 1s - loss: 0.3159 - accuracy: 0.86 - ETA: 1s - loss: 0.3167 - accuracy: 0.86 - ETA: 1s - loss: 0.3169 - accuracy: 0.86 - ETA: 1s - loss: 0.3151 - accuracy: 0.86 - ETA: 1s - loss: 0.3133 - accuracy: 0.86 - ETA: 1s - loss: 0.3108 - accuracy: 0.86 - ETA: 1s - loss: 0.3108 - accuracy: 0.86 - ETA: 0s - loss: 0.3098 - accuracy: 0.86 - ETA: 0s - loss: 0.3096 - accuracy: 0.86 - ETA: 0s - loss: 0.3077 - accuracy: 0.86 - ETA: 0s - loss: 0.3057 - accuracy: 0.86 - ETA: 0s - loss: 0.3041 - accuracy: 0.86 - ETA: 0s - loss: 0.3037 - accuracy: 0.86 - ETA: 0s - loss: 0.3031 - accuracy: 0.86 - ETA: 0s - loss: 0.3057 - accuracy: 0.86 - ETA: 0s - loss: 0.3084 - accuracy: 0.86 - ETA: 0s - loss: 0.3098 - accuracy: 0.86 - ETA: 0s - loss: 0.3093 - accuracy: 0.86 - ETA: 0s - loss: 0.3071 - accuracy: 0.86 - ETA: 0s - loss: 0.3051 - accuracy: 0.86 - 6s 555us/step - loss: 0.3038 - accuracy: 0.8671 - val_loss: 0.1451 - val_accuracy: 0.9436\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00794\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.1672 - accuracy: 0.93 - ETA: 6s - loss: 0.1714 - accuracy: 0.93 - ETA: 5s - loss: 0.2193 - accuracy: 0.91 - ETA: 5s - loss: 0.2191 - accuracy: 0.91 - ETA: 5s - loss: 0.2126 - accuracy: 0.91 - ETA: 5s - loss: 0.2139 - accuracy: 0.91 - ETA: 5s - loss: 0.2113 - accuracy: 0.91 - ETA: 5s - loss: 0.2064 - accuracy: 0.91 - ETA: 5s - loss: 0.2052 - accuracy: 0.92 - ETA: 5s - loss: 0.2066 - accuracy: 0.91 - ETA: 4s - loss: 0.2092 - accuracy: 0.91 - ETA: 4s - loss: 0.2065 - accuracy: 0.91 - ETA: 4s - loss: 0.2054 - accuracy: 0.91 - ETA: 4s - loss: 0.2065 - accuracy: 0.91 - ETA: 4s - loss: 0.2116 - accuracy: 0.91 - ETA: 4s - loss: 0.2228 - accuracy: 0.90 - ETA: 4s - loss: 0.2354 - accuracy: 0.90 - ETA: 4s - loss: 0.2328 - accuracy: 0.90 - ETA: 4s - loss: 0.2286 - accuracy: 0.90 - ETA: 4s - loss: 0.2278 - accuracy: 0.90 - ETA: 4s - loss: 0.2242 - accuracy: 0.90 - ETA: 4s - loss: 0.2212 - accuracy: 0.90 - ETA: 4s - loss: 0.2169 - accuracy: 0.91 - ETA: 4s - loss: 0.2142 - accuracy: 0.91 - ETA: 3s - loss: 0.2117 - accuracy: 0.91 - ETA: 3s - loss: 0.2120 - accuracy: 0.91 - ETA: 3s - loss: 0.2131 - accuracy: 0.91 - ETA: 3s - loss: 0.2118 - accuracy: 0.91 - ETA: 3s - loss: 0.2126 - accuracy: 0.91 - ETA: 3s - loss: 0.2128 - accuracy: 0.91 - ETA: 3s - loss: 0.2155 - accuracy: 0.90 - ETA: 3s - loss: 0.2214 - accuracy: 0.90 - ETA: 3s - loss: 0.2347 - accuracy: 0.89 - ETA: 3s - loss: 0.2374 - accuracy: 0.89 - ETA: 3s - loss: 0.2370 - accuracy: 0.89 - ETA: 3s - loss: 0.2349 - accuracy: 0.89 - ETA: 2s - loss: 0.2327 - accuracy: 0.89 - ETA: 2s - loss: 0.2314 - accuracy: 0.90 - ETA: 2s - loss: 0.2294 - accuracy: 0.90 - ETA: 2s - loss: 0.2277 - accuracy: 0.90 - ETA: 2s - loss: 0.2260 - accuracy: 0.90 - ETA: 2s - loss: 0.2244 - accuracy: 0.90 - ETA: 2s - loss: 0.2216 - accuracy: 0.90 - ETA: 2s - loss: 0.2198 - accuracy: 0.90 - ETA: 2s - loss: 0.2191 - accuracy: 0.90 - ETA: 2s - loss: 0.2204 - accuracy: 0.90 - ETA: 2s - loss: 0.2224 - accuracy: 0.90 - ETA: 2s - loss: 0.2206 - accuracy: 0.90 - ETA: 1s - loss: 0.2181 - accuracy: 0.90 - ETA: 1s - loss: 0.2164 - accuracy: 0.90 - ETA: 1s - loss: 0.2144 - accuracy: 0.90 - ETA: 1s - loss: 0.2126 - accuracy: 0.90 - ETA: 1s - loss: 0.2118 - accuracy: 0.90 - ETA: 1s - loss: 0.2099 - accuracy: 0.90 - ETA: 1s - loss: 0.2094 - accuracy: 0.90 - ETA: 1s - loss: 0.2096 - accuracy: 0.90 - ETA: 1s - loss: 0.2091 - accuracy: 0.90 - ETA: 1s - loss: 0.2091 - accuracy: 0.90 - ETA: 1s - loss: 0.2103 - accuracy: 0.90 - ETA: 1s - loss: 0.2109 - accuracy: 0.90 - ETA: 1s - loss: 0.2103 - accuracy: 0.90 - ETA: 0s - loss: 0.2090 - accuracy: 0.91 - ETA: 0s - loss: 0.2083 - accuracy: 0.91 - ETA: 0s - loss: 0.2080 - accuracy: 0.91 - ETA: 0s - loss: 0.2088 - accuracy: 0.91 - ETA: 0s - loss: 0.2082 - accuracy: 0.91 - ETA: 0s - loss: 0.2086 - accuracy: 0.91 - ETA: 0s - loss: 0.2078 - accuracy: 0.91 - ETA: 0s - loss: 0.2066 - accuracy: 0.91 - ETA: 0s - loss: 0.2053 - accuracy: 0.91 - ETA: 0s - loss: 0.2049 - accuracy: 0.91 - ETA: 0s - loss: 0.2039 - accuracy: 0.91 - ETA: 0s - loss: 0.2041 - accuracy: 0.91 - 6s 562us/step - loss: 0.2046 - accuracy: 0.9118 - val_loss: 0.1460 - val_accuracy: 0.9545\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00794\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0985 - accuracy: 0.98 - ETA: 5s - loss: 0.1326 - accuracy: 0.95 - ETA: 5s - loss: 0.1380 - accuracy: 0.95 - ETA: 5s - loss: 0.1253 - accuracy: 0.95 - ETA: 5s - loss: 0.1192 - accuracy: 0.95 - ETA: 5s - loss: 0.1177 - accuracy: 0.95 - ETA: 5s - loss: 0.1168 - accuracy: 0.95 - ETA: 4s - loss: 0.1225 - accuracy: 0.94 - ETA: 4s - loss: 0.1375 - accuracy: 0.94 - ETA: 4s - loss: 0.1602 - accuracy: 0.93 - ETA: 4s - loss: 0.1690 - accuracy: 0.92 - ETA: 4s - loss: 0.1644 - accuracy: 0.92 - ETA: 4s - loss: 0.1572 - accuracy: 0.93 - ETA: 4s - loss: 0.1558 - accuracy: 0.93 - ETA: 4s - loss: 0.1532 - accuracy: 0.93 - ETA: 4s - loss: 0.1496 - accuracy: 0.93 - ETA: 4s - loss: 0.1494 - accuracy: 0.93 - ETA: 4s - loss: 0.1475 - accuracy: 0.93 - ETA: 4s - loss: 0.1444 - accuracy: 0.93 - ETA: 4s - loss: 0.1465 - accuracy: 0.93 - ETA: 4s - loss: 0.1453 - accuracy: 0.93 - ETA: 3s - loss: 0.1444 - accuracy: 0.93 - ETA: 3s - loss: 0.1488 - accuracy: 0.93 - ETA: 3s - loss: 0.1580 - accuracy: 0.93 - ETA: 3s - loss: 0.1622 - accuracy: 0.93 - ETA: 3s - loss: 0.1630 - accuracy: 0.93 - ETA: 3s - loss: 0.1616 - accuracy: 0.93 - ETA: 3s - loss: 0.1627 - accuracy: 0.93 - ETA: 3s - loss: 0.1606 - accuracy: 0.93 - ETA: 3s - loss: 0.1576 - accuracy: 0.93 - ETA: 3s - loss: 0.1567 - accuracy: 0.93 - ETA: 3s - loss: 0.1578 - accuracy: 0.93 - ETA: 3s - loss: 0.1605 - accuracy: 0.93 - ETA: 3s - loss: 0.1609 - accuracy: 0.93 - ETA: 3s - loss: 0.1615 - accuracy: 0.93 - ETA: 3s - loss: 0.1589 - accuracy: 0.93 - ETA: 2s - loss: 0.1570 - accuracy: 0.93 - ETA: 2s - loss: 0.1559 - accuracy: 0.93 - ETA: 2s - loss: 0.1563 - accuracy: 0.93 - ETA: 2s - loss: 0.1545 - accuracy: 0.93 - ETA: 2s - loss: 0.1541 - accuracy: 0.93 - ETA: 2s - loss: 0.1548 - accuracy: 0.93 - ETA: 2s - loss: 0.1556 - accuracy: 0.93 - ETA: 2s - loss: 0.1558 - accuracy: 0.93 - ETA: 2s - loss: 0.1563 - accuracy: 0.93 - ETA: 2s - loss: 0.1564 - accuracy: 0.93 - ETA: 2s - loss: 0.1580 - accuracy: 0.93 - ETA: 2s - loss: 0.1569 - accuracy: 0.93 - ETA: 2s - loss: 0.1553 - accuracy: 0.93 - ETA: 1s - loss: 0.1537 - accuracy: 0.93 - ETA: 1s - loss: 0.1525 - accuracy: 0.93 - ETA: 1s - loss: 0.1511 - accuracy: 0.93 - ETA: 1s - loss: 0.1519 - accuracy: 0.93 - ETA: 1s - loss: 0.1519 - accuracy: 0.93 - ETA: 1s - loss: 0.1523 - accuracy: 0.93 - ETA: 1s - loss: 0.1518 - accuracy: 0.93 - ETA: 1s - loss: 0.1530 - accuracy: 0.93 - ETA: 1s - loss: 0.1523 - accuracy: 0.93 - ETA: 1s - loss: 0.1519 - accuracy: 0.93 - ETA: 1s - loss: 0.1529 - accuracy: 0.93 - ETA: 1s - loss: 0.1518 - accuracy: 0.93 - ETA: 0s - loss: 0.1517 - accuracy: 0.93 - ETA: 0s - loss: 0.1507 - accuracy: 0.93 - ETA: 0s - loss: 0.1497 - accuracy: 0.93 - ETA: 0s - loss: 0.1489 - accuracy: 0.93 - ETA: 0s - loss: 0.1478 - accuracy: 0.93 - ETA: 0s - loss: 0.1463 - accuracy: 0.93 - ETA: 0s - loss: 0.1467 - accuracy: 0.93 - ETA: 0s - loss: 0.1473 - accuracy: 0.93 - ETA: 0s - loss: 0.1463 - accuracy: 0.93 - ETA: 0s - loss: 0.1462 - accuracy: 0.93 - ETA: 0s - loss: 0.1460 - accuracy: 0.93 - ETA: 0s - loss: 0.1461 - accuracy: 0.93 - 6s 604us/step - loss: 0.1462 - accuracy: 0.9393 - val_loss: 0.1297 - val_accuracy: 0.9436\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00794\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1939 - accuracy: 0.92 - ETA: 5s - loss: 0.1773 - accuracy: 0.92 - ETA: 5s - loss: 0.1516 - accuracy: 0.93 - ETA: 5s - loss: 0.1632 - accuracy: 0.92 - ETA: 5s - loss: 0.1613 - accuracy: 0.92 - ETA: 5s - loss: 0.1624 - accuracy: 0.93 - ETA: 5s - loss: 0.1553 - accuracy: 0.93 - ETA: 5s - loss: 0.1558 - accuracy: 0.93 - ETA: 4s - loss: 0.1463 - accuracy: 0.93 - ETA: 4s - loss: 0.1395 - accuracy: 0.94 - ETA: 4s - loss: 0.1422 - accuracy: 0.94 - ETA: 4s - loss: 0.1400 - accuracy: 0.94 - ETA: 4s - loss: 0.1350 - accuracy: 0.94 - ETA: 4s - loss: 0.1318 - accuracy: 0.94 - ETA: 4s - loss: 0.1342 - accuracy: 0.94 - ETA: 4s - loss: 0.1321 - accuracy: 0.94 - ETA: 4s - loss: 0.1296 - accuracy: 0.94 - ETA: 4s - loss: 0.1280 - accuracy: 0.94 - ETA: 4s - loss: 0.1251 - accuracy: 0.94 - ETA: 4s - loss: 0.1254 - accuracy: 0.94 - ETA: 4s - loss: 0.1226 - accuracy: 0.94 - ETA: 4s - loss: 0.1218 - accuracy: 0.94 - ETA: 4s - loss: 0.1220 - accuracy: 0.94 - ETA: 4s - loss: 0.1245 - accuracy: 0.94 - ETA: 3s - loss: 0.1246 - accuracy: 0.94 - ETA: 3s - loss: 0.1233 - accuracy: 0.94 - ETA: 3s - loss: 0.1205 - accuracy: 0.94 - ETA: 3s - loss: 0.1204 - accuracy: 0.94 - ETA: 3s - loss: 0.1227 - accuracy: 0.94 - ETA: 3s - loss: 0.1316 - accuracy: 0.94 - ETA: 3s - loss: 0.1584 - accuracy: 0.94 - ETA: 3s - loss: 0.1672 - accuracy: 0.93 - ETA: 3s - loss: 0.1665 - accuracy: 0.93 - ETA: 3s - loss: 0.1644 - accuracy: 0.93 - ETA: 3s - loss: 0.1626 - accuracy: 0.93 - ETA: 3s - loss: 0.1611 - accuracy: 0.93 - ETA: 3s - loss: 0.1594 - accuracy: 0.93 - ETA: 2s - loss: 0.1581 - accuracy: 0.94 - ETA: 2s - loss: 0.1565 - accuracy: 0.94 - ETA: 2s - loss: 0.1571 - accuracy: 0.94 - ETA: 2s - loss: 0.1572 - accuracy: 0.93 - ETA: 2s - loss: 0.1554 - accuracy: 0.94 - ETA: 2s - loss: 0.1542 - accuracy: 0.94 - ETA: 2s - loss: 0.1548 - accuracy: 0.94 - ETA: 2s - loss: 0.1535 - accuracy: 0.94 - ETA: 2s - loss: 0.1520 - accuracy: 0.94 - ETA: 2s - loss: 0.1497 - accuracy: 0.94 - ETA: 2s - loss: 0.1492 - accuracy: 0.94 - ETA: 2s - loss: 0.1481 - accuracy: 0.94 - ETA: 1s - loss: 0.1464 - accuracy: 0.94 - ETA: 1s - loss: 0.1450 - accuracy: 0.94 - ETA: 1s - loss: 0.1441 - accuracy: 0.94 - ETA: 1s - loss: 0.1435 - accuracy: 0.94 - ETA: 1s - loss: 0.1427 - accuracy: 0.94 - ETA: 1s - loss: 0.1415 - accuracy: 0.94 - ETA: 1s - loss: 0.1400 - accuracy: 0.94 - ETA: 1s - loss: 0.1389 - accuracy: 0.94 - ETA: 1s - loss: 0.1397 - accuracy: 0.94 - ETA: 1s - loss: 0.1405 - accuracy: 0.94 - ETA: 1s - loss: 0.1420 - accuracy: 0.94 - ETA: 1s - loss: 0.1442 - accuracy: 0.94 - ETA: 0s - loss: 0.1467 - accuracy: 0.94 - ETA: 0s - loss: 0.1483 - accuracy: 0.94 - ETA: 0s - loss: 0.1479 - accuracy: 0.94 - ETA: 0s - loss: 0.1469 - accuracy: 0.94 - ETA: 0s - loss: 0.1467 - accuracy: 0.94 - ETA: 0s - loss: 0.1460 - accuracy: 0.94 - ETA: 0s - loss: 0.1449 - accuracy: 0.94 - ETA: 0s - loss: 0.1435 - accuracy: 0.94 - ETA: 0s - loss: 0.1425 - accuracy: 0.94 - ETA: 0s - loss: 0.1422 - accuracy: 0.94 - ETA: 0s - loss: 0.1411 - accuracy: 0.94 - ETA: 0s - loss: 0.1401 - accuracy: 0.94 - 6s 593us/step - loss: 0.1394 - accuracy: 0.9448 - val_loss: 0.0788 - val_accuracy: 0.9697\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00794\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 7s - loss: 0.1105 - accuracy: 0.94 - ETA: 6s - loss: 0.1608 - accuracy: 0.93 - ETA: 6s - loss: 0.1496 - accuracy: 0.93 - ETA: 6s - loss: 0.1378 - accuracy: 0.94 - ETA: 6s - loss: 0.1300 - accuracy: 0.94 - ETA: 6s - loss: 0.1160 - accuracy: 0.95 - ETA: 6s - loss: 0.1109 - accuracy: 0.95 - ETA: 5s - loss: 0.1080 - accuracy: 0.95 - ETA: 5s - loss: 0.1036 - accuracy: 0.95 - ETA: 5s - loss: 0.1032 - accuracy: 0.95 - ETA: 5s - loss: 0.0989 - accuracy: 0.95 - ETA: 5s - loss: 0.0960 - accuracy: 0.95 - ETA: 5s - loss: 0.0964 - accuracy: 0.95 - ETA: 5s - loss: 0.0952 - accuracy: 0.96 - ETA: 5s - loss: 0.0950 - accuracy: 0.96 - ETA: 5s - loss: 0.0970 - accuracy: 0.95 - ETA: 4s - loss: 0.1025 - accuracy: 0.95 - ETA: 4s - loss: 0.1068 - accuracy: 0.95 - ETA: 4s - loss: 0.1150 - accuracy: 0.94 - ETA: 4s - loss: 0.1248 - accuracy: 0.94 - ETA: 4s - loss: 0.1250 - accuracy: 0.94 - ETA: 4s - loss: 0.1210 - accuracy: 0.94 - ETA: 4s - loss: 0.1216 - accuracy: 0.94 - ETA: 4s - loss: 0.1192 - accuracy: 0.94 - ETA: 4s - loss: 0.1192 - accuracy: 0.94 - ETA: 4s - loss: 0.1180 - accuracy: 0.95 - ETA: 4s - loss: 0.1176 - accuracy: 0.95 - ETA: 3s - loss: 0.1164 - accuracy: 0.95 - ETA: 3s - loss: 0.1149 - accuracy: 0.95 - ETA: 3s - loss: 0.1125 - accuracy: 0.95 - ETA: 3s - loss: 0.1112 - accuracy: 0.95 - ETA: 3s - loss: 0.1119 - accuracy: 0.95 - ETA: 3s - loss: 0.1116 - accuracy: 0.95 - ETA: 3s - loss: 0.1121 - accuracy: 0.95 - ETA: 3s - loss: 0.1146 - accuracy: 0.95 - ETA: 3s - loss: 0.1142 - accuracy: 0.95 - ETA: 3s - loss: 0.1134 - accuracy: 0.95 - ETA: 3s - loss: 0.1117 - accuracy: 0.95 - ETA: 2s - loss: 0.1100 - accuracy: 0.95 - ETA: 2s - loss: 0.1098 - accuracy: 0.95 - ETA: 2s - loss: 0.1083 - accuracy: 0.95 - ETA: 2s - loss: 0.1075 - accuracy: 0.95 - ETA: 2s - loss: 0.1065 - accuracy: 0.95 - ETA: 2s - loss: 0.1053 - accuracy: 0.95 - ETA: 2s - loss: 0.1067 - accuracy: 0.95 - ETA: 2s - loss: 0.1128 - accuracy: 0.95 - ETA: 2s - loss: 0.1242 - accuracy: 0.95 - ETA: 2s - loss: 0.1281 - accuracy: 0.94 - ETA: 2s - loss: 0.1272 - accuracy: 0.95 - ETA: 2s - loss: 0.1261 - accuracy: 0.95 - ETA: 1s - loss: 0.1247 - accuracy: 0.95 - ETA: 1s - loss: 0.1244 - accuracy: 0.95 - ETA: 1s - loss: 0.1228 - accuracy: 0.95 - ETA: 1s - loss: 0.1219 - accuracy: 0.95 - ETA: 1s - loss: 0.1207 - accuracy: 0.95 - ETA: 1s - loss: 0.1199 - accuracy: 0.95 - ETA: 1s - loss: 0.1185 - accuracy: 0.95 - ETA: 1s - loss: 0.1174 - accuracy: 0.95 - ETA: 1s - loss: 0.1174 - accuracy: 0.95 - ETA: 1s - loss: 0.1174 - accuracy: 0.95 - ETA: 1s - loss: 0.1165 - accuracy: 0.95 - ETA: 0s - loss: 0.1157 - accuracy: 0.95 - ETA: 0s - loss: 0.1147 - accuracy: 0.95 - ETA: 0s - loss: 0.1142 - accuracy: 0.95 - ETA: 0s - loss: 0.1135 - accuracy: 0.95 - ETA: 0s - loss: 0.1132 - accuracy: 0.95 - ETA: 0s - loss: 0.1146 - accuracy: 0.95 - ETA: 0s - loss: 0.1198 - accuracy: 0.95 - ETA: 0s - loss: 0.1210 - accuracy: 0.95 - ETA: 0s - loss: 0.1206 - accuracy: 0.95 - ETA: 0s - loss: 0.1199 - accuracy: 0.95 - ETA: 0s - loss: 0.1190 - accuracy: 0.95 - ETA: 0s - loss: 0.1179 - accuracy: 0.95 - 7s 609us/step - loss: 0.1172 - accuracy: 0.9543 - val_loss: 0.0535 - val_accuracy: 0.9840\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00794\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0562 - accuracy: 0.97 - ETA: 5s - loss: 0.0606 - accuracy: 0.97 - ETA: 5s - loss: 0.0764 - accuracy: 0.96 - ETA: 5s - loss: 0.0733 - accuracy: 0.96 - ETA: 5s - loss: 0.0758 - accuracy: 0.96 - ETA: 5s - loss: 0.0720 - accuracy: 0.96 - ETA: 5s - loss: 0.0671 - accuracy: 0.97 - ETA: 5s - loss: 0.0617 - accuracy: 0.97 - ETA: 5s - loss: 0.0596 - accuracy: 0.97 - ETA: 5s - loss: 0.0696 - accuracy: 0.97 - ETA: 5s - loss: 0.0785 - accuracy: 0.96 - ETA: 5s - loss: 0.0879 - accuracy: 0.96 - ETA: 5s - loss: 0.0876 - accuracy: 0.96 - ETA: 5s - loss: 0.0888 - accuracy: 0.96 - ETA: 5s - loss: 0.0847 - accuracy: 0.96 - ETA: 4s - loss: 0.0858 - accuracy: 0.96 - ETA: 4s - loss: 0.0857 - accuracy: 0.96 - ETA: 4s - loss: 0.0848 - accuracy: 0.96 - ETA: 4s - loss: 0.0834 - accuracy: 0.96 - ETA: 4s - loss: 0.0836 - accuracy: 0.96 - ETA: 4s - loss: 0.0863 - accuracy: 0.96 - ETA: 4s - loss: 0.0871 - accuracy: 0.96 - ETA: 4s - loss: 0.0861 - accuracy: 0.96 - ETA: 4s - loss: 0.0855 - accuracy: 0.96 - ETA: 4s - loss: 0.0855 - accuracy: 0.96 - ETA: 4s - loss: 0.0875 - accuracy: 0.96 - ETA: 3s - loss: 0.0863 - accuracy: 0.96 - ETA: 3s - loss: 0.0852 - accuracy: 0.96 - ETA: 3s - loss: 0.0842 - accuracy: 0.96 - ETA: 3s - loss: 0.0840 - accuracy: 0.96 - ETA: 3s - loss: 0.0836 - accuracy: 0.96 - ETA: 3s - loss: 0.0823 - accuracy: 0.96 - ETA: 3s - loss: 0.0819 - accuracy: 0.96 - ETA: 3s - loss: 0.0839 - accuracy: 0.96 - ETA: 3s - loss: 0.0845 - accuracy: 0.96 - ETA: 3s - loss: 0.0851 - accuracy: 0.96 - ETA: 3s - loss: 0.0902 - accuracy: 0.96 - ETA: 3s - loss: 0.0926 - accuracy: 0.96 - ETA: 2s - loss: 0.0953 - accuracy: 0.96 - ETA: 2s - loss: 0.0953 - accuracy: 0.96 - ETA: 2s - loss: 0.0952 - accuracy: 0.96 - ETA: 2s - loss: 0.0949 - accuracy: 0.96 - ETA: 2s - loss: 0.0946 - accuracy: 0.96 - ETA: 2s - loss: 0.0944 - accuracy: 0.96 - ETA: 2s - loss: 0.0942 - accuracy: 0.96 - ETA: 2s - loss: 0.0950 - accuracy: 0.96 - ETA: 2s - loss: 0.0945 - accuracy: 0.96 - ETA: 2s - loss: 0.0937 - accuracy: 0.96 - ETA: 2s - loss: 0.0927 - accuracy: 0.96 - ETA: 2s - loss: 0.0916 - accuracy: 0.96 - ETA: 1s - loss: 0.0909 - accuracy: 0.96 - ETA: 1s - loss: 0.0904 - accuracy: 0.96 - ETA: 1s - loss: 0.0901 - accuracy: 0.96 - ETA: 1s - loss: 0.0901 - accuracy: 0.96 - ETA: 1s - loss: 0.0911 - accuracy: 0.96 - ETA: 1s - loss: 0.0904 - accuracy: 0.96 - ETA: 1s - loss: 0.0895 - accuracy: 0.96 - ETA: 1s - loss: 0.0901 - accuracy: 0.96 - ETA: 1s - loss: 0.0924 - accuracy: 0.96 - ETA: 1s - loss: 0.0922 - accuracy: 0.96 - ETA: 1s - loss: 0.0920 - accuracy: 0.96 - ETA: 1s - loss: 0.0920 - accuracy: 0.96 - ETA: 0s - loss: 0.0925 - accuracy: 0.96 - ETA: 0s - loss: 0.0952 - accuracy: 0.96 - ETA: 0s - loss: 0.0960 - accuracy: 0.96 - ETA: 0s - loss: 0.0966 - accuracy: 0.96 - ETA: 0s - loss: 0.0964 - accuracy: 0.96 - ETA: 0s - loss: 0.0960 - accuracy: 0.96 - ETA: 0s - loss: 0.0952 - accuracy: 0.96 - ETA: 0s - loss: 0.0943 - accuracy: 0.96 - ETA: 0s - loss: 0.0938 - accuracy: 0.96 - ETA: 0s - loss: 0.0935 - accuracy: 0.96 - ETA: 0s - loss: 0.0932 - accuracy: 0.96 - 7s 617us/step - loss: 0.0932 - accuracy: 0.9644 - val_loss: 0.0716 - val_accuracy: 0.9739\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00794\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0689 - accuracy: 0.96 - ETA: 5s - loss: 0.0641 - accuracy: 0.96 - ETA: 6s - loss: 0.0790 - accuracy: 0.96 - ETA: 5s - loss: 0.0815 - accuracy: 0.96 - ETA: 5s - loss: 0.0737 - accuracy: 0.97 - ETA: 5s - loss: 0.0720 - accuracy: 0.97 - ETA: 5s - loss: 0.0665 - accuracy: 0.97 - ETA: 5s - loss: 0.0654 - accuracy: 0.97 - ETA: 5s - loss: 0.0643 - accuracy: 0.97 - ETA: 5s - loss: 0.0609 - accuracy: 0.97 - ETA: 5s - loss: 0.0631 - accuracy: 0.97 - ETA: 5s - loss: 0.0635 - accuracy: 0.97 - ETA: 5s - loss: 0.0642 - accuracy: 0.97 - ETA: 5s - loss: 0.0649 - accuracy: 0.97 - ETA: 5s - loss: 0.0635 - accuracy: 0.97 - ETA: 4s - loss: 0.0628 - accuracy: 0.97 - ETA: 4s - loss: 0.0627 - accuracy: 0.97 - ETA: 4s - loss: 0.0635 - accuracy: 0.97 - ETA: 4s - loss: 0.0616 - accuracy: 0.97 - ETA: 4s - loss: 0.0624 - accuracy: 0.97 - ETA: 4s - loss: 0.0628 - accuracy: 0.97 - ETA: 4s - loss: 0.0681 - accuracy: 0.97 - ETA: 4s - loss: 0.0760 - accuracy: 0.97 - ETA: 4s - loss: 0.0785 - accuracy: 0.97 - ETA: 4s - loss: 0.0885 - accuracy: 0.96 - ETA: 4s - loss: 0.0877 - accuracy: 0.96 - ETA: 4s - loss: 0.0861 - accuracy: 0.96 - ETA: 3s - loss: 0.0839 - accuracy: 0.96 - ETA: 3s - loss: 0.0838 - accuracy: 0.96 - ETA: 3s - loss: 0.0846 - accuracy: 0.96 - ETA: 3s - loss: 0.0845 - accuracy: 0.96 - ETA: 3s - loss: 0.0840 - accuracy: 0.97 - ETA: 3s - loss: 0.0842 - accuracy: 0.96 - ETA: 3s - loss: 0.0842 - accuracy: 0.96 - ETA: 3s - loss: 0.0843 - accuracy: 0.97 - ETA: 3s - loss: 0.0839 - accuracy: 0.97 - ETA: 3s - loss: 0.0838 - accuracy: 0.97 - ETA: 3s - loss: 0.0826 - accuracy: 0.97 - ETA: 2s - loss: 0.0820 - accuracy: 0.97 - ETA: 2s - loss: 0.0817 - accuracy: 0.97 - ETA: 2s - loss: 0.0816 - accuracy: 0.97 - ETA: 2s - loss: 0.0809 - accuracy: 0.97 - ETA: 2s - loss: 0.0809 - accuracy: 0.97 - ETA: 2s - loss: 0.0801 - accuracy: 0.97 - ETA: 2s - loss: 0.0799 - accuracy: 0.97 - ETA: 2s - loss: 0.0786 - accuracy: 0.97 - ETA: 2s - loss: 0.0787 - accuracy: 0.97 - ETA: 2s - loss: 0.0789 - accuracy: 0.97 - ETA: 2s - loss: 0.0799 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 1s - loss: 0.0784 - accuracy: 0.97 - ETA: 1s - loss: 0.0781 - accuracy: 0.97 - ETA: 1s - loss: 0.0791 - accuracy: 0.97 - ETA: 1s - loss: 0.0808 - accuracy: 0.97 - ETA: 1s - loss: 0.0814 - accuracy: 0.97 - ETA: 1s - loss: 0.0810 - accuracy: 0.97 - ETA: 1s - loss: 0.0805 - accuracy: 0.97 - ETA: 1s - loss: 0.0798 - accuracy: 0.97 - ETA: 1s - loss: 0.0794 - accuracy: 0.97 - ETA: 1s - loss: 0.0786 - accuracy: 0.97 - ETA: 1s - loss: 0.0780 - accuracy: 0.97 - ETA: 1s - loss: 0.0771 - accuracy: 0.97 - ETA: 0s - loss: 0.0766 - accuracy: 0.97 - ETA: 0s - loss: 0.0761 - accuracy: 0.97 - ETA: 0s - loss: 0.0764 - accuracy: 0.97 - ETA: 0s - loss: 0.0761 - accuracy: 0.97 - ETA: 0s - loss: 0.0756 - accuracy: 0.97 - ETA: 0s - loss: 0.0759 - accuracy: 0.97 - ETA: 0s - loss: 0.0769 - accuracy: 0.97 - ETA: 0s - loss: 0.0780 - accuracy: 0.97 - ETA: 0s - loss: 0.0777 - accuracy: 0.97 - ETA: 0s - loss: 0.0774 - accuracy: 0.97 - ETA: 0s - loss: 0.0774 - accuracy: 0.97 - 7s 611us/step - loss: 0.0769 - accuracy: 0.9727 - val_loss: 0.0319 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00794\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0458 - accuracy: 0.98 - ETA: 5s - loss: 0.0398 - accuracy: 0.98 - ETA: 5s - loss: 0.0492 - accuracy: 0.98 - ETA: 5s - loss: 0.0644 - accuracy: 0.97 - ETA: 5s - loss: 0.0844 - accuracy: 0.96 - ETA: 5s - loss: 0.1221 - accuracy: 0.95 - ETA: 5s - loss: 0.1242 - accuracy: 0.95 - ETA: 5s - loss: 0.1213 - accuracy: 0.95 - ETA: 5s - loss: 0.1117 - accuracy: 0.95 - ETA: 5s - loss: 0.1043 - accuracy: 0.95 - ETA: 5s - loss: 0.0986 - accuracy: 0.96 - ETA: 5s - loss: 0.0933 - accuracy: 0.96 - ETA: 5s - loss: 0.0894 - accuracy: 0.96 - ETA: 4s - loss: 0.0865 - accuracy: 0.96 - ETA: 4s - loss: 0.0872 - accuracy: 0.96 - ETA: 4s - loss: 0.0854 - accuracy: 0.96 - ETA: 4s - loss: 0.0832 - accuracy: 0.96 - ETA: 4s - loss: 0.0803 - accuracy: 0.97 - ETA: 4s - loss: 0.0789 - accuracy: 0.97 - ETA: 4s - loss: 0.0774 - accuracy: 0.97 - ETA: 4s - loss: 0.0817 - accuracy: 0.96 - ETA: 4s - loss: 0.0846 - accuracy: 0.96 - ETA: 4s - loss: 0.0827 - accuracy: 0.96 - ETA: 4s - loss: 0.0803 - accuracy: 0.97 - ETA: 4s - loss: 0.0786 - accuracy: 0.97 - ETA: 3s - loss: 0.0779 - accuracy: 0.97 - ETA: 3s - loss: 0.0776 - accuracy: 0.97 - ETA: 3s - loss: 0.0785 - accuracy: 0.97 - ETA: 3s - loss: 0.0829 - accuracy: 0.96 - ETA: 3s - loss: 0.0844 - accuracy: 0.96 - ETA: 3s - loss: 0.0878 - accuracy: 0.96 - ETA: 3s - loss: 0.0905 - accuracy: 0.96 - ETA: 3s - loss: 0.0901 - accuracy: 0.96 - ETA: 3s - loss: 0.0884 - accuracy: 0.96 - ETA: 3s - loss: 0.0866 - accuracy: 0.96 - ETA: 3s - loss: 0.0857 - accuracy: 0.96 - ETA: 3s - loss: 0.0841 - accuracy: 0.96 - ETA: 2s - loss: 0.0828 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0803 - accuracy: 0.97 - ETA: 2s - loss: 0.0797 - accuracy: 0.97 - ETA: 2s - loss: 0.0795 - accuracy: 0.97 - ETA: 2s - loss: 0.0785 - accuracy: 0.97 - ETA: 2s - loss: 0.0798 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 2s - loss: 0.0778 - accuracy: 0.97 - ETA: 2s - loss: 0.0773 - accuracy: 0.97 - ETA: 2s - loss: 0.0766 - accuracy: 0.97 - ETA: 2s - loss: 0.0755 - accuracy: 0.97 - ETA: 1s - loss: 0.0749 - accuracy: 0.97 - ETA: 1s - loss: 0.0743 - accuracy: 0.97 - ETA: 1s - loss: 0.0742 - accuracy: 0.97 - ETA: 1s - loss: 0.0734 - accuracy: 0.97 - ETA: 1s - loss: 0.0733 - accuracy: 0.97 - ETA: 1s - loss: 0.0763 - accuracy: 0.97 - ETA: 1s - loss: 0.0782 - accuracy: 0.97 - ETA: 1s - loss: 0.0795 - accuracy: 0.97 - ETA: 1s - loss: 0.0809 - accuracy: 0.97 - ETA: 1s - loss: 0.0804 - accuracy: 0.97 - ETA: 1s - loss: 0.0795 - accuracy: 0.97 - ETA: 1s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0783 - accuracy: 0.97 - ETA: 0s - loss: 0.0777 - accuracy: 0.97 - ETA: 0s - loss: 0.0775 - accuracy: 0.97 - ETA: 0s - loss: 0.0771 - accuracy: 0.97 - ETA: 0s - loss: 0.0762 - accuracy: 0.97 - ETA: 0s - loss: 0.0759 - accuracy: 0.97 - ETA: 0s - loss: 0.0752 - accuracy: 0.97 - ETA: 0s - loss: 0.0746 - accuracy: 0.97 - ETA: 0s - loss: 0.0746 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0733 - accuracy: 0.97 - ETA: 0s - loss: 0.0733 - accuracy: 0.97 - 6s 591us/step - loss: 0.0736 - accuracy: 0.9742 - val_loss: 0.1685 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00794\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1798 - accuracy: 0.94 - ETA: 5s - loss: 0.1213 - accuracy: 0.95 - ETA: 5s - loss: 0.0896 - accuracy: 0.97 - ETA: 5s - loss: 0.0829 - accuracy: 0.97 - ETA: 5s - loss: 0.0772 - accuracy: 0.97 - ETA: 5s - loss: 0.0934 - accuracy: 0.96 - ETA: 5s - loss: 0.0862 - accuracy: 0.96 - ETA: 5s - loss: 0.0781 - accuracy: 0.97 - ETA: 5s - loss: 0.0768 - accuracy: 0.97 - ETA: 5s - loss: 0.0727 - accuracy: 0.97 - ETA: 5s - loss: 0.0711 - accuracy: 0.97 - ETA: 4s - loss: 0.0703 - accuracy: 0.97 - ETA: 4s - loss: 0.0658 - accuracy: 0.97 - ETA: 4s - loss: 0.0623 - accuracy: 0.97 - ETA: 4s - loss: 0.0593 - accuracy: 0.97 - ETA: 4s - loss: 0.0605 - accuracy: 0.97 - ETA: 4s - loss: 0.0673 - accuracy: 0.97 - ETA: 4s - loss: 0.0709 - accuracy: 0.97 - ETA: 4s - loss: 0.0714 - accuracy: 0.97 - ETA: 4s - loss: 0.0704 - accuracy: 0.97 - ETA: 4s - loss: 0.0694 - accuracy: 0.97 - ETA: 4s - loss: 0.0679 - accuracy: 0.97 - ETA: 4s - loss: 0.0664 - accuracy: 0.97 - ETA: 4s - loss: 0.0644 - accuracy: 0.97 - ETA: 4s - loss: 0.0638 - accuracy: 0.97 - ETA: 3s - loss: 0.0626 - accuracy: 0.97 - ETA: 3s - loss: 0.0650 - accuracy: 0.97 - ETA: 3s - loss: 0.0671 - accuracy: 0.97 - ETA: 3s - loss: 0.0713 - accuracy: 0.97 - ETA: 3s - loss: 0.0711 - accuracy: 0.97 - ETA: 3s - loss: 0.0696 - accuracy: 0.97 - ETA: 3s - loss: 0.0690 - accuracy: 0.97 - ETA: 3s - loss: 0.0684 - accuracy: 0.97 - ETA: 3s - loss: 0.0666 - accuracy: 0.97 - ETA: 3s - loss: 0.0651 - accuracy: 0.97 - ETA: 3s - loss: 0.0644 - accuracy: 0.97 - ETA: 3s - loss: 0.0632 - accuracy: 0.97 - ETA: 3s - loss: 0.0630 - accuracy: 0.97 - ETA: 2s - loss: 0.0626 - accuracy: 0.97 - ETA: 2s - loss: 0.0620 - accuracy: 0.97 - ETA: 2s - loss: 0.0616 - accuracy: 0.97 - ETA: 2s - loss: 0.0634 - accuracy: 0.97 - ETA: 2s - loss: 0.0658 - accuracy: 0.97 - ETA: 2s - loss: 0.0660 - accuracy: 0.97 - ETA: 2s - loss: 0.0661 - accuracy: 0.97 - ETA: 2s - loss: 0.0659 - accuracy: 0.97 - ETA: 2s - loss: 0.0663 - accuracy: 0.97 - ETA: 2s - loss: 0.0661 - accuracy: 0.97 - ETA: 2s - loss: 0.0658 - accuracy: 0.97 - ETA: 2s - loss: 0.0652 - accuracy: 0.97 - ETA: 1s - loss: 0.0643 - accuracy: 0.97 - ETA: 1s - loss: 0.0640 - accuracy: 0.97 - ETA: 1s - loss: 0.0638 - accuracy: 0.97 - ETA: 1s - loss: 0.0629 - accuracy: 0.97 - ETA: 1s - loss: 0.0629 - accuracy: 0.97 - ETA: 1s - loss: 0.0624 - accuracy: 0.97 - ETA: 1s - loss: 0.0619 - accuracy: 0.97 - ETA: 1s - loss: 0.0613 - accuracy: 0.97 - ETA: 1s - loss: 0.0610 - accuracy: 0.97 - ETA: 1s - loss: 0.0635 - accuracy: 0.97 - ETA: 1s - loss: 0.0660 - accuracy: 0.97 - ETA: 0s - loss: 0.0706 - accuracy: 0.97 - ETA: 0s - loss: 0.0793 - accuracy: 0.97 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - ETA: 0s - loss: 0.0785 - accuracy: 0.97 - ETA: 0s - loss: 0.0781 - accuracy: 0.97 - ETA: 0s - loss: 0.0772 - accuracy: 0.97 - ETA: 0s - loss: 0.0767 - accuracy: 0.97 - ETA: 0s - loss: 0.0768 - accuracy: 0.97 - ETA: 0s - loss: 0.0766 - accuracy: 0.97 - ETA: 0s - loss: 0.0761 - accuracy: 0.97 - ETA: 0s - loss: 0.0753 - accuracy: 0.97 - ETA: 0s - loss: 0.0753 - accuracy: 0.97 - 6s 607us/step - loss: 0.0750 - accuracy: 0.9745 - val_loss: 0.0477 - val_accuracy: 0.9840\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00794\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0407 - accuracy: 0.97 - ETA: 6s - loss: 0.0295 - accuracy: 0.98 - ETA: 6s - loss: 0.0446 - accuracy: 0.97 - ETA: 5s - loss: 0.0412 - accuracy: 0.97 - ETA: 5s - loss: 0.0478 - accuracy: 0.97 - ETA: 5s - loss: 0.0447 - accuracy: 0.97 - ETA: 5s - loss: 0.0409 - accuracy: 0.98 - ETA: 5s - loss: 0.0366 - accuracy: 0.98 - ETA: 5s - loss: 0.0395 - accuracy: 0.98 - ETA: 5s - loss: 0.0432 - accuracy: 0.98 - ETA: 5s - loss: 0.0455 - accuracy: 0.97 - ETA: 5s - loss: 0.0425 - accuracy: 0.98 - ETA: 5s - loss: 0.0415 - accuracy: 0.98 - ETA: 5s - loss: 0.0419 - accuracy: 0.98 - ETA: 5s - loss: 0.0403 - accuracy: 0.98 - ETA: 4s - loss: 0.0393 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0404 - accuracy: 0.98 - ETA: 4s - loss: 0.0392 - accuracy: 0.98 - ETA: 4s - loss: 0.0398 - accuracy: 0.98 - ETA: 4s - loss: 0.0389 - accuracy: 0.98 - ETA: 4s - loss: 0.0379 - accuracy: 0.98 - ETA: 4s - loss: 0.0372 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 4s - loss: 0.0361 - accuracy: 0.98 - ETA: 4s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0386 - accuracy: 0.98 - ETA: 3s - loss: 0.0423 - accuracy: 0.98 - ETA: 3s - loss: 0.0428 - accuracy: 0.98 - ETA: 3s - loss: 0.0455 - accuracy: 0.98 - ETA: 3s - loss: 0.0450 - accuracy: 0.98 - ETA: 3s - loss: 0.0464 - accuracy: 0.98 - ETA: 3s - loss: 0.0455 - accuracy: 0.98 - ETA: 3s - loss: 0.0455 - accuracy: 0.98 - ETA: 3s - loss: 0.0450 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 2s - loss: 0.0453 - accuracy: 0.98 - ETA: 2s - loss: 0.0448 - accuracy: 0.98 - ETA: 2s - loss: 0.0453 - accuracy: 0.98 - ETA: 2s - loss: 0.0458 - accuracy: 0.98 - ETA: 2s - loss: 0.0462 - accuracy: 0.98 - ETA: 2s - loss: 0.0467 - accuracy: 0.98 - ETA: 2s - loss: 0.0478 - accuracy: 0.98 - ETA: 2s - loss: 0.0492 - accuracy: 0.98 - ETA: 2s - loss: 0.0494 - accuracy: 0.98 - ETA: 2s - loss: 0.0493 - accuracy: 0.98 - ETA: 2s - loss: 0.0489 - accuracy: 0.98 - ETA: 2s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0487 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 1s - loss: 0.0503 - accuracy: 0.98 - ETA: 1s - loss: 0.0504 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0493 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.97 - ETA: 0s - loss: 0.0509 - accuracy: 0.97 - ETA: 0s - loss: 0.0504 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0496 - accuracy: 0.98 - ETA: 0s - loss: 0.0493 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - 6s 604us/step - loss: 0.0494 - accuracy: 0.9808 - val_loss: 0.0237 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00794\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0353 - accuracy: 0.98 - ETA: 5s - loss: 0.0383 - accuracy: 0.98 - ETA: 5s - loss: 0.0360 - accuracy: 0.98 - ETA: 5s - loss: 0.0329 - accuracy: 0.98 - ETA: 5s - loss: 0.0353 - accuracy: 0.98 - ETA: 5s - loss: 0.0332 - accuracy: 0.98 - ETA: 5s - loss: 0.0363 - accuracy: 0.98 - ETA: 5s - loss: 0.0352 - accuracy: 0.98 - ETA: 5s - loss: 0.0341 - accuracy: 0.98 - ETA: 5s - loss: 0.0343 - accuracy: 0.98 - ETA: 5s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0370 - accuracy: 0.98 - ETA: 4s - loss: 0.0383 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0423 - accuracy: 0.98 - ETA: 4s - loss: 0.0413 - accuracy: 0.98 - ETA: 4s - loss: 0.0417 - accuracy: 0.98 - ETA: 4s - loss: 0.0465 - accuracy: 0.98 - ETA: 4s - loss: 0.0466 - accuracy: 0.97 - ETA: 4s - loss: 0.0502 - accuracy: 0.97 - ETA: 4s - loss: 0.0511 - accuracy: 0.97 - ETA: 4s - loss: 0.0539 - accuracy: 0.97 - ETA: 3s - loss: 0.0565 - accuracy: 0.97 - ETA: 3s - loss: 0.0566 - accuracy: 0.97 - ETA: 3s - loss: 0.0549 - accuracy: 0.97 - ETA: 3s - loss: 0.0546 - accuracy: 0.97 - ETA: 3s - loss: 0.0542 - accuracy: 0.97 - ETA: 3s - loss: 0.0537 - accuracy: 0.97 - ETA: 3s - loss: 0.0531 - accuracy: 0.97 - ETA: 3s - loss: 0.0521 - accuracy: 0.97 - ETA: 3s - loss: 0.0514 - accuracy: 0.97 - ETA: 3s - loss: 0.0508 - accuracy: 0.97 - ETA: 3s - loss: 0.0502 - accuracy: 0.97 - ETA: 3s - loss: 0.0495 - accuracy: 0.97 - ETA: 2s - loss: 0.0489 - accuracy: 0.97 - ETA: 2s - loss: 0.0496 - accuracy: 0.98 - ETA: 2s - loss: 0.0494 - accuracy: 0.98 - ETA: 2s - loss: 0.0487 - accuracy: 0.98 - ETA: 2s - loss: 0.0482 - accuracy: 0.98 - ETA: 2s - loss: 0.0481 - accuracy: 0.98 - ETA: 2s - loss: 0.0494 - accuracy: 0.97 - ETA: 2s - loss: 0.0520 - accuracy: 0.97 - ETA: 2s - loss: 0.0522 - accuracy: 0.97 - ETA: 2s - loss: 0.0518 - accuracy: 0.97 - ETA: 2s - loss: 0.0521 - accuracy: 0.97 - ETA: 2s - loss: 0.0538 - accuracy: 0.97 - ETA: 1s - loss: 0.0534 - accuracy: 0.97 - ETA: 1s - loss: 0.0527 - accuracy: 0.98 - ETA: 1s - loss: 0.0519 - accuracy: 0.98 - ETA: 1s - loss: 0.0515 - accuracy: 0.98 - ETA: 1s - loss: 0.0509 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0491 - accuracy: 0.98 - ETA: 1s - loss: 0.0489 - accuracy: 0.98 - ETA: 1s - loss: 0.0484 - accuracy: 0.98 - ETA: 1s - loss: 0.0488 - accuracy: 0.98 - ETA: 0s - loss: 0.0502 - accuracy: 0.98 - ETA: 0s - loss: 0.0502 - accuracy: 0.98 - ETA: 0s - loss: 0.0497 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0482 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0498 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - 6s 592us/step - loss: 0.0497 - accuracy: 0.9811 - val_loss: 0.0226 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00794\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0204 - accuracy: 0.99 - ETA: 5s - loss: 0.0145 - accuracy: 0.99 - ETA: 5s - loss: 0.0426 - accuracy: 0.98 - ETA: 5s - loss: 0.0514 - accuracy: 0.97 - ETA: 5s - loss: 0.0513 - accuracy: 0.98 - ETA: 5s - loss: 0.0498 - accuracy: 0.98 - ETA: 5s - loss: 0.0485 - accuracy: 0.98 - ETA: 5s - loss: 0.0543 - accuracy: 0.98 - ETA: 5s - loss: 0.0574 - accuracy: 0.97 - ETA: 5s - loss: 0.0595 - accuracy: 0.97 - ETA: 5s - loss: 0.0568 - accuracy: 0.97 - ETA: 4s - loss: 0.0530 - accuracy: 0.97 - ETA: 4s - loss: 0.0508 - accuracy: 0.98 - ETA: 4s - loss: 0.0508 - accuracy: 0.97 - ETA: 4s - loss: 0.0498 - accuracy: 0.98 - ETA: 4s - loss: 0.0489 - accuracy: 0.98 - ETA: 4s - loss: 0.0471 - accuracy: 0.98 - ETA: 4s - loss: 0.0451 - accuracy: 0.98 - ETA: 4s - loss: 0.0431 - accuracy: 0.98 - ETA: 4s - loss: 0.0420 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 4s - loss: 0.0414 - accuracy: 0.98 - ETA: 4s - loss: 0.0437 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 3s - loss: 0.0441 - accuracy: 0.98 - ETA: 3s - loss: 0.0441 - accuracy: 0.98 - ETA: 3s - loss: 0.0447 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0442 - accuracy: 0.98 - ETA: 3s - loss: 0.0431 - accuracy: 0.98 - ETA: 3s - loss: 0.0437 - accuracy: 0.98 - ETA: 3s - loss: 0.0436 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0448 - accuracy: 0.98 - ETA: 3s - loss: 0.0458 - accuracy: 0.98 - ETA: 2s - loss: 0.0462 - accuracy: 0.98 - ETA: 2s - loss: 0.0460 - accuracy: 0.98 - ETA: 2s - loss: 0.0455 - accuracy: 0.98 - ETA: 2s - loss: 0.0451 - accuracy: 0.98 - ETA: 2s - loss: 0.0449 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0441 - accuracy: 0.98 - ETA: 2s - loss: 0.0481 - accuracy: 0.98 - ETA: 2s - loss: 0.0513 - accuracy: 0.98 - ETA: 2s - loss: 0.0518 - accuracy: 0.98 - ETA: 2s - loss: 0.0514 - accuracy: 0.98 - ETA: 2s - loss: 0.0513 - accuracy: 0.98 - ETA: 2s - loss: 0.0506 - accuracy: 0.98 - ETA: 1s - loss: 0.0505 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0488 - accuracy: 0.98 - ETA: 1s - loss: 0.0481 - accuracy: 0.98 - ETA: 1s - loss: 0.0478 - accuracy: 0.98 - ETA: 1s - loss: 0.0480 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0467 - accuracy: 0.98 - ETA: 1s - loss: 0.0465 - accuracy: 0.98 - ETA: 0s - loss: 0.0494 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0533 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0517 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0506 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - 6s 583us/step - loss: 0.0507 - accuracy: 0.9817 - val_loss: 0.0509 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00794\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0409 - accuracy: 0.98 - ETA: 5s - loss: 0.0361 - accuracy: 0.98 - ETA: 5s - loss: 0.0346 - accuracy: 0.98 - ETA: 5s - loss: 0.0521 - accuracy: 0.98 - ETA: 5s - loss: 0.0499 - accuracy: 0.98 - ETA: 5s - loss: 0.0524 - accuracy: 0.98 - ETA: 5s - loss: 0.0576 - accuracy: 0.97 - ETA: 5s - loss: 0.0843 - accuracy: 0.97 - ETA: 5s - loss: 0.0772 - accuracy: 0.97 - ETA: 5s - loss: 0.0705 - accuracy: 0.97 - ETA: 5s - loss: 0.0707 - accuracy: 0.97 - ETA: 4s - loss: 0.0681 - accuracy: 0.97 - ETA: 4s - loss: 0.0661 - accuracy: 0.97 - ETA: 4s - loss: 0.0625 - accuracy: 0.98 - ETA: 4s - loss: 0.0594 - accuracy: 0.98 - ETA: 4s - loss: 0.0568 - accuracy: 0.98 - ETA: 4s - loss: 0.0537 - accuracy: 0.98 - ETA: 4s - loss: 0.0521 - accuracy: 0.98 - ETA: 4s - loss: 0.0511 - accuracy: 0.98 - ETA: 4s - loss: 0.0509 - accuracy: 0.98 - ETA: 4s - loss: 0.0513 - accuracy: 0.98 - ETA: 4s - loss: 0.0528 - accuracy: 0.98 - ETA: 4s - loss: 0.0512 - accuracy: 0.98 - ETA: 4s - loss: 0.0510 - accuracy: 0.98 - ETA: 3s - loss: 0.0534 - accuracy: 0.98 - ETA: 3s - loss: 0.0609 - accuracy: 0.97 - ETA: 3s - loss: 0.0663 - accuracy: 0.97 - ETA: 3s - loss: 0.0646 - accuracy: 0.97 - ETA: 3s - loss: 0.0633 - accuracy: 0.97 - ETA: 3s - loss: 0.0624 - accuracy: 0.97 - ETA: 3s - loss: 0.0616 - accuracy: 0.97 - ETA: 3s - loss: 0.0613 - accuracy: 0.97 - ETA: 3s - loss: 0.0598 - accuracy: 0.98 - ETA: 3s - loss: 0.0595 - accuracy: 0.98 - ETA: 3s - loss: 0.0588 - accuracy: 0.98 - ETA: 3s - loss: 0.0584 - accuracy: 0.98 - ETA: 2s - loss: 0.0575 - accuracy: 0.98 - ETA: 2s - loss: 0.0570 - accuracy: 0.98 - ETA: 2s - loss: 0.0570 - accuracy: 0.98 - ETA: 2s - loss: 0.0566 - accuracy: 0.98 - ETA: 2s - loss: 0.0562 - accuracy: 0.98 - ETA: 2s - loss: 0.0556 - accuracy: 0.98 - ETA: 2s - loss: 0.0555 - accuracy: 0.98 - ETA: 2s - loss: 0.0546 - accuracy: 0.98 - ETA: 2s - loss: 0.0539 - accuracy: 0.98 - ETA: 2s - loss: 0.0535 - accuracy: 0.98 - ETA: 2s - loss: 0.0531 - accuracy: 0.98 - ETA: 2s - loss: 0.0525 - accuracy: 0.98 - ETA: 2s - loss: 0.0520 - accuracy: 0.98 - ETA: 1s - loss: 0.0516 - accuracy: 0.98 - ETA: 1s - loss: 0.0507 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0492 - accuracy: 0.98 - ETA: 1s - loss: 0.0491 - accuracy: 0.98 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 1s - loss: 0.0489 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0504 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0501 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0482 - accuracy: 0.98 - ETA: 0s - loss: 0.0478 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0467 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - 6s 597us/step - loss: 0.0468 - accuracy: 0.9843 - val_loss: 0.0602 - val_accuracy: 0.9840\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00794\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0910 - accuracy: 0.96 - ETA: 6s - loss: 0.0711 - accuracy: 0.96 - ETA: 6s - loss: 0.0696 - accuracy: 0.96 - ETA: 5s - loss: 0.0586 - accuracy: 0.96 - ETA: 5s - loss: 0.0552 - accuracy: 0.97 - ETA: 5s - loss: 0.0496 - accuracy: 0.97 - ETA: 5s - loss: 0.0450 - accuracy: 0.97 - ETA: 5s - loss: 0.0461 - accuracy: 0.97 - ETA: 5s - loss: 0.0488 - accuracy: 0.97 - ETA: 5s - loss: 0.0480 - accuracy: 0.97 - ETA: 5s - loss: 0.0445 - accuracy: 0.97 - ETA: 5s - loss: 0.0440 - accuracy: 0.97 - ETA: 5s - loss: 0.0418 - accuracy: 0.98 - ETA: 5s - loss: 0.0411 - accuracy: 0.98 - ETA: 5s - loss: 0.0397 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0376 - accuracy: 0.98 - ETA: 4s - loss: 0.0363 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0359 - accuracy: 0.98 - ETA: 4s - loss: 0.0394 - accuracy: 0.98 - ETA: 4s - loss: 0.0457 - accuracy: 0.98 - ETA: 4s - loss: 0.0475 - accuracy: 0.98 - ETA: 4s - loss: 0.0465 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 4s - loss: 0.0453 - accuracy: 0.98 - ETA: 3s - loss: 0.0439 - accuracy: 0.98 - ETA: 3s - loss: 0.0428 - accuracy: 0.98 - ETA: 3s - loss: 0.0418 - accuracy: 0.98 - ETA: 3s - loss: 0.0409 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 3s - loss: 0.0388 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 3s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0410 - accuracy: 0.98 - ETA: 2s - loss: 0.0453 - accuracy: 0.98 - ETA: 2s - loss: 0.0457 - accuracy: 0.98 - ETA: 2s - loss: 0.0450 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0438 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0438 - accuracy: 0.98 - ETA: 2s - loss: 0.0448 - accuracy: 0.98 - ETA: 2s - loss: 0.0445 - accuracy: 0.98 - ETA: 2s - loss: 0.0439 - accuracy: 0.98 - ETA: 1s - loss: 0.0434 - accuracy: 0.98 - ETA: 1s - loss: 0.0428 - accuracy: 0.98 - ETA: 1s - loss: 0.0426 - accuracy: 0.98 - ETA: 1s - loss: 0.0420 - accuracy: 0.98 - ETA: 1s - loss: 0.0414 - accuracy: 0.98 - ETA: 1s - loss: 0.0407 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 0s - loss: 0.0402 - accuracy: 0.98 - ETA: 0s - loss: 0.0402 - accuracy: 0.98 - ETA: 0s - loss: 0.0414 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0433 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.98 - ETA: 0s - loss: 0.0425 - accuracy: 0.98 - ETA: 0s - loss: 0.0428 - accuracy: 0.98 - 7s 613us/step - loss: 0.0426 - accuracy: 0.9844 - val_loss: 0.0469 - val_accuracy: 0.9857\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00794\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0120 - accuracy: 0.99 - ETA: 6s - loss: 0.0252 - accuracy: 0.98 - ETA: 6s - loss: 0.0246 - accuracy: 0.98 - ETA: 6s - loss: 0.0364 - accuracy: 0.98 - ETA: 5s - loss: 0.0365 - accuracy: 0.98 - ETA: 5s - loss: 0.0362 - accuracy: 0.98 - ETA: 5s - loss: 0.0383 - accuracy: 0.98 - ETA: 5s - loss: 0.0396 - accuracy: 0.98 - ETA: 5s - loss: 0.0365 - accuracy: 0.98 - ETA: 5s - loss: 0.0376 - accuracy: 0.98 - ETA: 5s - loss: 0.0414 - accuracy: 0.98 - ETA: 5s - loss: 0.0420 - accuracy: 0.98 - ETA: 5s - loss: 0.0435 - accuracy: 0.98 - ETA: 5s - loss: 0.0428 - accuracy: 0.98 - ETA: 5s - loss: 0.0408 - accuracy: 0.98 - ETA: 5s - loss: 0.0400 - accuracy: 0.98 - ETA: 5s - loss: 0.0418 - accuracy: 0.98 - ETA: 4s - loss: 0.0428 - accuracy: 0.98 - ETA: 4s - loss: 0.0408 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0374 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0349 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0386 - accuracy: 0.98 - ETA: 4s - loss: 0.0395 - accuracy: 0.98 - ETA: 4s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0402 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0400 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0390 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 3s - loss: 0.0376 - accuracy: 0.98 - ETA: 3s - loss: 0.0370 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 2s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 2s - loss: 0.0339 - accuracy: 0.98 - ETA: 2s - loss: 0.0344 - accuracy: 0.98 - ETA: 2s - loss: 0.0338 - accuracy: 0.98 - ETA: 2s - loss: 0.0334 - accuracy: 0.98 - ETA: 2s - loss: 0.0333 - accuracy: 0.98 - ETA: 2s - loss: 0.0346 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0346 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0407 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0391 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0395 - accuracy: 0.98 - ETA: 1s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0398 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0396 - accuracy: 0.98 - ETA: 0s - loss: 0.0399 - accuracy: 0.98 - ETA: 0s - loss: 0.0399 - accuracy: 0.98 - ETA: 0s - loss: 0.0395 - accuracy: 0.98 - ETA: 0s - loss: 0.0396 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0391 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0400 - accuracy: 0.98 - 7s 624us/step - loss: 0.0401 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00794\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0315 - accuracy: 0.98 - ETA: 6s - loss: 0.0189 - accuracy: 0.99 - ETA: 5s - loss: 0.0134 - accuracy: 0.99 - ETA: 5s - loss: 0.0133 - accuracy: 0.99 - ETA: 5s - loss: 0.0165 - accuracy: 0.99 - ETA: 5s - loss: 0.0168 - accuracy: 0.99 - ETA: 5s - loss: 0.0176 - accuracy: 0.99 - ETA: 5s - loss: 0.0194 - accuracy: 0.99 - ETA: 5s - loss: 0.0198 - accuracy: 0.99 - ETA: 5s - loss: 0.0240 - accuracy: 0.99 - ETA: 5s - loss: 0.0263 - accuracy: 0.99 - ETA: 5s - loss: 0.0276 - accuracy: 0.99 - ETA: 5s - loss: 0.0259 - accuracy: 0.99 - ETA: 5s - loss: 0.0254 - accuracy: 0.99 - ETA: 5s - loss: 0.0260 - accuracy: 0.99 - ETA: 5s - loss: 0.0296 - accuracy: 0.99 - ETA: 5s - loss: 0.0304 - accuracy: 0.98 - ETA: 5s - loss: 0.0305 - accuracy: 0.99 - ETA: 5s - loss: 0.0329 - accuracy: 0.98 - ETA: 5s - loss: 0.0397 - accuracy: 0.98 - ETA: 4s - loss: 0.0410 - accuracy: 0.98 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 4s - loss: 0.0391 - accuracy: 0.98 - ETA: 4s - loss: 0.0389 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 4s - loss: 0.0410 - accuracy: 0.98 - ETA: 4s - loss: 0.0404 - accuracy: 0.98 - ETA: 4s - loss: 0.0404 - accuracy: 0.98 - ETA: 4s - loss: 0.0413 - accuracy: 0.98 - ETA: 4s - loss: 0.0409 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0415 - accuracy: 0.98 - ETA: 3s - loss: 0.0408 - accuracy: 0.98 - ETA: 3s - loss: 0.0418 - accuracy: 0.98 - ETA: 3s - loss: 0.0413 - accuracy: 0.98 - ETA: 3s - loss: 0.0408 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 2s - loss: 0.0388 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 2s - loss: 0.0390 - accuracy: 0.98 - ETA: 2s - loss: 0.0394 - accuracy: 0.98 - ETA: 2s - loss: 0.0394 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 2s - loss: 0.0394 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 2s - loss: 0.0388 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0389 - accuracy: 0.98 - ETA: 1s - loss: 0.0387 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0379 - accuracy: 0.98 - ETA: 0s - loss: 0.0414 - accuracy: 0.98 - ETA: 0s - loss: 0.0425 - accuracy: 0.98 - ETA: 0s - loss: 0.0426 - accuracy: 0.98 - ETA: 0s - loss: 0.0423 - accuracy: 0.98 - ETA: 0s - loss: 0.0418 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - ETA: 0s - loss: 0.0413 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0408 - accuracy: 0.98 - 7s 617us/step - loss: 0.0405 - accuracy: 0.9859 - val_loss: 0.0433 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00794\n",
      "Epoch 00017: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 243us/step\n",
      "[2020-05-18 16:31:05 RAM72.3% 1.45GB] Val Score : [0.03444436848674808, 0.9878696203231812]\n",
      "[2020-05-18 16:31:05 RAM72.3% 1.45GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:31:05 RAM72.3% 1.45GB] Training on Fold : 7\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 21s - loss: 8.4809 - accuracy: 0.434 - ETA: 13s - loss: 10.2204 - accuracy: 0.48 - ETA: 10s - loss: 7.3619 - accuracy: 0.5195 - ETA: 9s - loss: 6.0426 - accuracy: 0.522 - ETA: 8s - loss: 5.2033 - accuracy: 0.52 - ETA: 8s - loss: 4.6235 - accuracy: 0.52 - ETA: 7s - loss: 4.1643 - accuracy: 0.52 - ETA: 7s - loss: 3.7930 - accuracy: 0.53 - ETA: 6s - loss: 3.4861 - accuracy: 0.54 - ETA: 6s - loss: 3.2548 - accuracy: 0.54 - ETA: 6s - loss: 3.0639 - accuracy: 0.53 - ETA: 6s - loss: 2.9128 - accuracy: 0.53 - ETA: 6s - loss: 2.7699 - accuracy: 0.53 - ETA: 5s - loss: 2.6439 - accuracy: 0.53 - ETA: 5s - loss: 2.5361 - accuracy: 0.53 - ETA: 5s - loss: 2.4440 - accuracy: 0.52 - ETA: 5s - loss: 2.3472 - accuracy: 0.52 - ETA: 5s - loss: 2.2662 - accuracy: 0.53 - ETA: 5s - loss: 2.1890 - accuracy: 0.53 - ETA: 4s - loss: 2.1228 - accuracy: 0.53 - ETA: 4s - loss: 2.0622 - accuracy: 0.53 - ETA: 4s - loss: 2.0009 - accuracy: 0.53 - ETA: 4s - loss: 1.9454 - accuracy: 0.54 - ETA: 4s - loss: 1.8987 - accuracy: 0.54 - ETA: 4s - loss: 1.8527 - accuracy: 0.54 - ETA: 4s - loss: 1.8120 - accuracy: 0.54 - ETA: 4s - loss: 1.7716 - accuracy: 0.54 - ETA: 4s - loss: 1.7337 - accuracy: 0.54 - ETA: 4s - loss: 1.7013 - accuracy: 0.54 - ETA: 3s - loss: 1.6726 - accuracy: 0.54 - ETA: 3s - loss: 1.6405 - accuracy: 0.55 - ETA: 3s - loss: 1.6134 - accuracy: 0.55 - ETA: 3s - loss: 1.5831 - accuracy: 0.55 - ETA: 3s - loss: 1.5571 - accuracy: 0.55 - ETA: 3s - loss: 1.5318 - accuracy: 0.56 - ETA: 3s - loss: 1.5060 - accuracy: 0.56 - ETA: 3s - loss: 1.4833 - accuracy: 0.56 - ETA: 3s - loss: 1.4618 - accuracy: 0.56 - ETA: 3s - loss: 1.4420 - accuracy: 0.56 - ETA: 2s - loss: 1.4251 - accuracy: 0.56 - ETA: 2s - loss: 1.4082 - accuracy: 0.57 - ETA: 2s - loss: 1.3920 - accuracy: 0.57 - ETA: 2s - loss: 1.3739 - accuracy: 0.57 - ETA: 2s - loss: 1.3552 - accuracy: 0.57 - ETA: 2s - loss: 1.3370 - accuracy: 0.58 - ETA: 2s - loss: 1.3191 - accuracy: 0.58 - ETA: 2s - loss: 1.3026 - accuracy: 0.58 - ETA: 2s - loss: 1.2876 - accuracy: 0.58 - ETA: 2s - loss: 1.2739 - accuracy: 0.59 - ETA: 2s - loss: 1.2642 - accuracy: 0.59 - ETA: 1s - loss: 1.2538 - accuracy: 0.59 - ETA: 1s - loss: 1.2403 - accuracy: 0.59 - ETA: 1s - loss: 1.2269 - accuracy: 0.59 - ETA: 1s - loss: 1.2136 - accuracy: 0.59 - ETA: 1s - loss: 1.2020 - accuracy: 0.60 - ETA: 1s - loss: 1.1914 - accuracy: 0.60 - ETA: 1s - loss: 1.1828 - accuracy: 0.60 - ETA: 1s - loss: 1.1734 - accuracy: 0.60 - ETA: 1s - loss: 1.1634 - accuracy: 0.60 - ETA: 1s - loss: 1.1538 - accuracy: 0.60 - ETA: 1s - loss: 1.1430 - accuracy: 0.61 - ETA: 0s - loss: 1.1319 - accuracy: 0.61 - ETA: 0s - loss: 1.1207 - accuracy: 0.61 - ETA: 0s - loss: 1.1116 - accuracy: 0.61 - ETA: 0s - loss: 1.1042 - accuracy: 0.61 - ETA: 0s - loss: 1.0934 - accuracy: 0.62 - ETA: 0s - loss: 1.0844 - accuracy: 0.62 - ETA: 0s - loss: 1.0743 - accuracy: 0.62 - ETA: 0s - loss: 1.0650 - accuracy: 0.62 - ETA: 0s - loss: 1.0578 - accuracy: 0.62 - ETA: 0s - loss: 1.0511 - accuracy: 0.63 - ETA: 0s - loss: 1.0453 - accuracy: 0.63 - ETA: 0s - loss: 1.0372 - accuracy: 0.63 - 7s 611us/step - loss: 1.0331 - accuracy: 0.6341 - val_loss: 0.5544 - val_accuracy: 0.6723\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00794\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.5651 - accuracy: 0.68 - ETA: 5s - loss: 0.5018 - accuracy: 0.75 - ETA: 5s - loss: 0.4637 - accuracy: 0.78 - ETA: 5s - loss: 0.4403 - accuracy: 0.80 - ETA: 5s - loss: 0.4122 - accuracy: 0.81 - ETA: 5s - loss: 0.4005 - accuracy: 0.82 - ETA: 5s - loss: 0.4022 - accuracy: 0.82 - ETA: 5s - loss: 0.3998 - accuracy: 0.82 - ETA: 5s - loss: 0.4068 - accuracy: 0.81 - ETA: 5s - loss: 0.4200 - accuracy: 0.80 - ETA: 4s - loss: 0.4312 - accuracy: 0.79 - ETA: 4s - loss: 0.4311 - accuracy: 0.79 - ETA: 4s - loss: 0.4240 - accuracy: 0.80 - ETA: 4s - loss: 0.4148 - accuracy: 0.80 - ETA: 4s - loss: 0.4072 - accuracy: 0.81 - ETA: 4s - loss: 0.4041 - accuracy: 0.81 - ETA: 4s - loss: 0.3977 - accuracy: 0.81 - ETA: 4s - loss: 0.3990 - accuracy: 0.81 - ETA: 4s - loss: 0.3918 - accuracy: 0.82 - ETA: 4s - loss: 0.3900 - accuracy: 0.82 - ETA: 4s - loss: 0.3848 - accuracy: 0.82 - ETA: 4s - loss: 0.3819 - accuracy: 0.82 - ETA: 4s - loss: 0.3826 - accuracy: 0.82 - ETA: 3s - loss: 0.3846 - accuracy: 0.82 - ETA: 3s - loss: 0.3986 - accuracy: 0.81 - ETA: 3s - loss: 0.4052 - accuracy: 0.81 - ETA: 3s - loss: 0.4027 - accuracy: 0.81 - ETA: 3s - loss: 0.3986 - accuracy: 0.81 - ETA: 3s - loss: 0.3960 - accuracy: 0.82 - ETA: 3s - loss: 0.3897 - accuracy: 0.82 - ETA: 3s - loss: 0.3847 - accuracy: 0.82 - ETA: 3s - loss: 0.3825 - accuracy: 0.82 - ETA: 3s - loss: 0.3769 - accuracy: 0.83 - ETA: 3s - loss: 0.3737 - accuracy: 0.83 - ETA: 3s - loss: 0.3690 - accuracy: 0.83 - ETA: 3s - loss: 0.3676 - accuracy: 0.83 - ETA: 2s - loss: 0.3640 - accuracy: 0.84 - ETA: 2s - loss: 0.3605 - accuracy: 0.84 - ETA: 2s - loss: 0.3591 - accuracy: 0.84 - ETA: 2s - loss: 0.3563 - accuracy: 0.84 - ETA: 2s - loss: 0.3533 - accuracy: 0.84 - ETA: 2s - loss: 0.3486 - accuracy: 0.84 - ETA: 2s - loss: 0.3441 - accuracy: 0.85 - ETA: 2s - loss: 0.3410 - accuracy: 0.85 - ETA: 2s - loss: 0.3406 - accuracy: 0.85 - ETA: 2s - loss: 0.3386 - accuracy: 0.85 - ETA: 2s - loss: 0.3389 - accuracy: 0.85 - ETA: 2s - loss: 0.3415 - accuracy: 0.85 - ETA: 2s - loss: 0.3423 - accuracy: 0.85 - ETA: 1s - loss: 0.3413 - accuracy: 0.85 - ETA: 1s - loss: 0.3390 - accuracy: 0.85 - ETA: 1s - loss: 0.3378 - accuracy: 0.85 - ETA: 1s - loss: 0.3367 - accuracy: 0.85 - ETA: 1s - loss: 0.3351 - accuracy: 0.85 - ETA: 1s - loss: 0.3337 - accuracy: 0.85 - ETA: 1s - loss: 0.3322 - accuracy: 0.85 - ETA: 1s - loss: 0.3304 - accuracy: 0.85 - ETA: 1s - loss: 0.3304 - accuracy: 0.85 - ETA: 1s - loss: 0.3281 - accuracy: 0.85 - ETA: 1s - loss: 0.3262 - accuracy: 0.85 - ETA: 1s - loss: 0.3237 - accuracy: 0.86 - ETA: 0s - loss: 0.3224 - accuracy: 0.86 - ETA: 0s - loss: 0.3225 - accuracy: 0.86 - ETA: 0s - loss: 0.3229 - accuracy: 0.86 - ETA: 0s - loss: 0.3219 - accuracy: 0.86 - ETA: 0s - loss: 0.3228 - accuracy: 0.86 - ETA: 0s - loss: 0.3230 - accuracy: 0.85 - ETA: 0s - loss: 0.3222 - accuracy: 0.86 - ETA: 0s - loss: 0.3209 - accuracy: 0.86 - ETA: 0s - loss: 0.3186 - accuracy: 0.86 - ETA: 0s - loss: 0.3166 - accuracy: 0.86 - ETA: 0s - loss: 0.3148 - accuracy: 0.86 - ETA: 0s - loss: 0.3135 - accuracy: 0.86 - 6s 582us/step - loss: 0.3126 - accuracy: 0.8648 - val_loss: 0.1850 - val_accuracy: 0.9495\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00794\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.2143 - accuracy: 0.89 - ETA: 5s - loss: 0.1992 - accuracy: 0.91 - ETA: 5s - loss: 0.1795 - accuracy: 0.92 - ETA: 5s - loss: 0.1692 - accuracy: 0.93 - ETA: 5s - loss: 0.1675 - accuracy: 0.93 - ETA: 5s - loss: 0.1693 - accuracy: 0.93 - ETA: 5s - loss: 0.1746 - accuracy: 0.92 - ETA: 5s - loss: 0.2206 - accuracy: 0.90 - ETA: 5s - loss: 0.2686 - accuracy: 0.88 - ETA: 5s - loss: 0.2719 - accuracy: 0.88 - ETA: 5s - loss: 0.2684 - accuracy: 0.88 - ETA: 5s - loss: 0.2571 - accuracy: 0.89 - ETA: 4s - loss: 0.2522 - accuracy: 0.89 - ETA: 4s - loss: 0.2496 - accuracy: 0.89 - ETA: 4s - loss: 0.2458 - accuracy: 0.89 - ETA: 4s - loss: 0.2431 - accuracy: 0.89 - ETA: 4s - loss: 0.2396 - accuracy: 0.90 - ETA: 4s - loss: 0.2331 - accuracy: 0.90 - ETA: 4s - loss: 0.2287 - accuracy: 0.90 - ETA: 4s - loss: 0.2259 - accuracy: 0.90 - ETA: 4s - loss: 0.2258 - accuracy: 0.90 - ETA: 4s - loss: 0.2239 - accuracy: 0.90 - ETA: 4s - loss: 0.2221 - accuracy: 0.90 - ETA: 4s - loss: 0.2179 - accuracy: 0.91 - ETA: 3s - loss: 0.2167 - accuracy: 0.91 - ETA: 3s - loss: 0.2152 - accuracy: 0.91 - ETA: 3s - loss: 0.2135 - accuracy: 0.91 - ETA: 3s - loss: 0.2140 - accuracy: 0.91 - ETA: 3s - loss: 0.2195 - accuracy: 0.90 - ETA: 3s - loss: 0.2215 - accuracy: 0.90 - ETA: 3s - loss: 0.2243 - accuracy: 0.90 - ETA: 3s - loss: 0.2220 - accuracy: 0.90 - ETA: 3s - loss: 0.2196 - accuracy: 0.90 - ETA: 3s - loss: 0.2196 - accuracy: 0.90 - ETA: 3s - loss: 0.2178 - accuracy: 0.90 - ETA: 3s - loss: 0.2197 - accuracy: 0.90 - ETA: 2s - loss: 0.2182 - accuracy: 0.90 - ETA: 2s - loss: 0.2177 - accuracy: 0.90 - ETA: 2s - loss: 0.2154 - accuracy: 0.90 - ETA: 2s - loss: 0.2144 - accuracy: 0.91 - ETA: 2s - loss: 0.2139 - accuracy: 0.91 - ETA: 2s - loss: 0.2159 - accuracy: 0.90 - ETA: 2s - loss: 0.2161 - accuracy: 0.90 - ETA: 2s - loss: 0.2178 - accuracy: 0.90 - ETA: 2s - loss: 0.2202 - accuracy: 0.90 - ETA: 2s - loss: 0.2227 - accuracy: 0.90 - ETA: 2s - loss: 0.2219 - accuracy: 0.90 - ETA: 2s - loss: 0.2209 - accuracy: 0.90 - ETA: 2s - loss: 0.2195 - accuracy: 0.90 - ETA: 1s - loss: 0.2178 - accuracy: 0.90 - ETA: 1s - loss: 0.2167 - accuracy: 0.90 - ETA: 1s - loss: 0.2160 - accuracy: 0.90 - ETA: 1s - loss: 0.2146 - accuracy: 0.90 - ETA: 1s - loss: 0.2152 - accuracy: 0.90 - ETA: 1s - loss: 0.2145 - accuracy: 0.90 - ETA: 1s - loss: 0.2145 - accuracy: 0.90 - ETA: 1s - loss: 0.2146 - accuracy: 0.90 - ETA: 1s - loss: 0.2148 - accuracy: 0.90 - ETA: 1s - loss: 0.2161 - accuracy: 0.90 - ETA: 1s - loss: 0.2148 - accuracy: 0.90 - ETA: 1s - loss: 0.2138 - accuracy: 0.90 - ETA: 0s - loss: 0.2126 - accuracy: 0.91 - ETA: 0s - loss: 0.2115 - accuracy: 0.91 - ETA: 0s - loss: 0.2109 - accuracy: 0.91 - ETA: 0s - loss: 0.2095 - accuracy: 0.91 - ETA: 0s - loss: 0.2085 - accuracy: 0.91 - ETA: 0s - loss: 0.2078 - accuracy: 0.91 - ETA: 0s - loss: 0.2078 - accuracy: 0.91 - ETA: 0s - loss: 0.2072 - accuracy: 0.91 - ETA: 0s - loss: 0.2066 - accuracy: 0.91 - ETA: 0s - loss: 0.2065 - accuracy: 0.91 - ETA: 0s - loss: 0.2083 - accuracy: 0.91 - ETA: 0s - loss: 0.2093 - accuracy: 0.91 - 6s 579us/step - loss: 0.2094 - accuracy: 0.9113 - val_loss: 0.1217 - val_accuracy: 0.9511\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00794\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1351 - accuracy: 0.93 - ETA: 5s - loss: 0.1375 - accuracy: 0.94 - ETA: 5s - loss: 0.1264 - accuracy: 0.95 - ETA: 5s - loss: 0.1322 - accuracy: 0.95 - ETA: 5s - loss: 0.1472 - accuracy: 0.93 - ETA: 5s - loss: 0.1756 - accuracy: 0.92 - ETA: 5s - loss: 0.2339 - accuracy: 0.90 - ETA: 5s - loss: 0.2315 - accuracy: 0.91 - ETA: 5s - loss: 0.2174 - accuracy: 0.91 - ETA: 5s - loss: 0.2083 - accuracy: 0.92 - ETA: 5s - loss: 0.2005 - accuracy: 0.92 - ETA: 5s - loss: 0.1940 - accuracy: 0.92 - ETA: 4s - loss: 0.2033 - accuracy: 0.92 - ETA: 4s - loss: 0.2028 - accuracy: 0.92 - ETA: 4s - loss: 0.2026 - accuracy: 0.92 - ETA: 4s - loss: 0.1993 - accuracy: 0.92 - ETA: 4s - loss: 0.1977 - accuracy: 0.92 - ETA: 4s - loss: 0.1946 - accuracy: 0.92 - ETA: 4s - loss: 0.1893 - accuracy: 0.92 - ETA: 4s - loss: 0.1853 - accuracy: 0.92 - ETA: 4s - loss: 0.1839 - accuracy: 0.92 - ETA: 4s - loss: 0.1801 - accuracy: 0.93 - ETA: 4s - loss: 0.1777 - accuracy: 0.93 - ETA: 4s - loss: 0.1753 - accuracy: 0.93 - ETA: 4s - loss: 0.1718 - accuracy: 0.93 - ETA: 3s - loss: 0.1681 - accuracy: 0.93 - ETA: 3s - loss: 0.1653 - accuracy: 0.93 - ETA: 3s - loss: 0.1679 - accuracy: 0.93 - ETA: 3s - loss: 0.1719 - accuracy: 0.93 - ETA: 3s - loss: 0.1713 - accuracy: 0.93 - ETA: 3s - loss: 0.1698 - accuracy: 0.93 - ETA: 3s - loss: 0.1690 - accuracy: 0.93 - ETA: 3s - loss: 0.1680 - accuracy: 0.93 - ETA: 3s - loss: 0.1665 - accuracy: 0.93 - ETA: 3s - loss: 0.1652 - accuracy: 0.93 - ETA: 3s - loss: 0.1632 - accuracy: 0.93 - ETA: 3s - loss: 0.1608 - accuracy: 0.93 - ETA: 2s - loss: 0.1598 - accuracy: 0.93 - ETA: 2s - loss: 0.1612 - accuracy: 0.93 - ETA: 2s - loss: 0.1619 - accuracy: 0.93 - ETA: 2s - loss: 0.1611 - accuracy: 0.93 - ETA: 2s - loss: 0.1612 - accuracy: 0.93 - ETA: 2s - loss: 0.1602 - accuracy: 0.93 - ETA: 2s - loss: 0.1604 - accuracy: 0.93 - ETA: 2s - loss: 0.1611 - accuracy: 0.93 - ETA: 2s - loss: 0.1613 - accuracy: 0.93 - ETA: 2s - loss: 0.1602 - accuracy: 0.93 - ETA: 2s - loss: 0.1612 - accuracy: 0.93 - ETA: 2s - loss: 0.1634 - accuracy: 0.93 - ETA: 1s - loss: 0.1679 - accuracy: 0.93 - ETA: 1s - loss: 0.1679 - accuracy: 0.93 - ETA: 1s - loss: 0.1672 - accuracy: 0.93 - ETA: 1s - loss: 0.1660 - accuracy: 0.93 - ETA: 1s - loss: 0.1656 - accuracy: 0.93 - ETA: 1s - loss: 0.1644 - accuracy: 0.93 - ETA: 1s - loss: 0.1650 - accuracy: 0.93 - ETA: 1s - loss: 0.1647 - accuracy: 0.93 - ETA: 1s - loss: 0.1643 - accuracy: 0.93 - ETA: 1s - loss: 0.1634 - accuracy: 0.93 - ETA: 1s - loss: 0.1622 - accuracy: 0.93 - ETA: 1s - loss: 0.1608 - accuracy: 0.93 - ETA: 0s - loss: 0.1590 - accuracy: 0.93 - ETA: 0s - loss: 0.1582 - accuracy: 0.93 - ETA: 0s - loss: 0.1597 - accuracy: 0.93 - ETA: 0s - loss: 0.1620 - accuracy: 0.93 - ETA: 0s - loss: 0.1624 - accuracy: 0.93 - ETA: 0s - loss: 0.1618 - accuracy: 0.93 - ETA: 0s - loss: 0.1611 - accuracy: 0.93 - ETA: 0s - loss: 0.1630 - accuracy: 0.93 - ETA: 0s - loss: 0.1669 - accuracy: 0.93 - ETA: 0s - loss: 0.1661 - accuracy: 0.93 - ETA: 0s - loss: 0.1649 - accuracy: 0.93 - ETA: 0s - loss: 0.1641 - accuracy: 0.93 - 6s 585us/step - loss: 0.1642 - accuracy: 0.9364 - val_loss: 0.1062 - val_accuracy: 0.9688\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00794\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1790 - accuracy: 0.91 - ETA: 5s - loss: 0.1547 - accuracy: 0.92 - ETA: 5s - loss: 0.1365 - accuracy: 0.93 - ETA: 5s - loss: 0.1345 - accuracy: 0.93 - ETA: 5s - loss: 0.1255 - accuracy: 0.94 - ETA: 5s - loss: 0.1145 - accuracy: 0.94 - ETA: 5s - loss: 0.1226 - accuracy: 0.94 - ETA: 5s - loss: 0.1214 - accuracy: 0.94 - ETA: 5s - loss: 0.1182 - accuracy: 0.94 - ETA: 5s - loss: 0.1143 - accuracy: 0.95 - ETA: 5s - loss: 0.1090 - accuracy: 0.95 - ETA: 4s - loss: 0.1056 - accuracy: 0.95 - ETA: 4s - loss: 0.1048 - accuracy: 0.95 - ETA: 4s - loss: 0.1049 - accuracy: 0.95 - ETA: 4s - loss: 0.1288 - accuracy: 0.94 - ETA: 4s - loss: 0.1382 - accuracy: 0.94 - ETA: 4s - loss: 0.1384 - accuracy: 0.94 - ETA: 4s - loss: 0.1390 - accuracy: 0.94 - ETA: 4s - loss: 0.1363 - accuracy: 0.94 - ETA: 4s - loss: 0.1383 - accuracy: 0.94 - ETA: 4s - loss: 0.1443 - accuracy: 0.94 - ETA: 4s - loss: 0.1427 - accuracy: 0.94 - ETA: 4s - loss: 0.1396 - accuracy: 0.94 - ETA: 4s - loss: 0.1365 - accuracy: 0.94 - ETA: 3s - loss: 0.1349 - accuracy: 0.94 - ETA: 3s - loss: 0.1329 - accuracy: 0.94 - ETA: 3s - loss: 0.1305 - accuracy: 0.94 - ETA: 3s - loss: 0.1273 - accuracy: 0.95 - ETA: 3s - loss: 0.1251 - accuracy: 0.95 - ETA: 3s - loss: 0.1231 - accuracy: 0.95 - ETA: 3s - loss: 0.1232 - accuracy: 0.95 - ETA: 3s - loss: 0.1256 - accuracy: 0.95 - ETA: 3s - loss: 0.1299 - accuracy: 0.94 - ETA: 3s - loss: 0.1290 - accuracy: 0.94 - ETA: 3s - loss: 0.1274 - accuracy: 0.95 - ETA: 3s - loss: 0.1266 - accuracy: 0.95 - ETA: 3s - loss: 0.1253 - accuracy: 0.95 - ETA: 2s - loss: 0.1240 - accuracy: 0.95 - ETA: 2s - loss: 0.1227 - accuracy: 0.95 - ETA: 2s - loss: 0.1211 - accuracy: 0.95 - ETA: 2s - loss: 0.1202 - accuracy: 0.95 - ETA: 2s - loss: 0.1201 - accuracy: 0.95 - ETA: 2s - loss: 0.1202 - accuracy: 0.95 - ETA: 2s - loss: 0.1203 - accuracy: 0.95 - ETA: 2s - loss: 0.1231 - accuracy: 0.95 - ETA: 2s - loss: 0.1250 - accuracy: 0.95 - ETA: 2s - loss: 0.1252 - accuracy: 0.95 - ETA: 2s - loss: 0.1244 - accuracy: 0.95 - ETA: 2s - loss: 0.1231 - accuracy: 0.95 - ETA: 1s - loss: 0.1224 - accuracy: 0.95 - ETA: 1s - loss: 0.1226 - accuracy: 0.95 - ETA: 1s - loss: 0.1230 - accuracy: 0.95 - ETA: 1s - loss: 0.1228 - accuracy: 0.95 - ETA: 1s - loss: 0.1230 - accuracy: 0.95 - ETA: 1s - loss: 0.1234 - accuracy: 0.95 - ETA: 1s - loss: 0.1226 - accuracy: 0.95 - ETA: 1s - loss: 0.1219 - accuracy: 0.95 - ETA: 1s - loss: 0.1216 - accuracy: 0.95 - ETA: 1s - loss: 0.1211 - accuracy: 0.95 - ETA: 1s - loss: 0.1199 - accuracy: 0.95 - ETA: 1s - loss: 0.1193 - accuracy: 0.95 - ETA: 0s - loss: 0.1193 - accuracy: 0.95 - ETA: 0s - loss: 0.1187 - accuracy: 0.95 - ETA: 0s - loss: 0.1181 - accuracy: 0.95 - ETA: 0s - loss: 0.1180 - accuracy: 0.95 - ETA: 0s - loss: 0.1176 - accuracy: 0.95 - ETA: 0s - loss: 0.1170 - accuracy: 0.95 - ETA: 0s - loss: 0.1161 - accuracy: 0.95 - ETA: 0s - loss: 0.1156 - accuracy: 0.95 - ETA: 0s - loss: 0.1148 - accuracy: 0.95 - ETA: 0s - loss: 0.1143 - accuracy: 0.95 - ETA: 0s - loss: 0.1141 - accuracy: 0.95 - ETA: 0s - loss: 0.1142 - accuracy: 0.95 - 6s 586us/step - loss: 0.1148 - accuracy: 0.9552 - val_loss: 0.1044 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00794\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0670 - accuracy: 0.98 - ETA: 5s - loss: 0.0598 - accuracy: 0.98 - ETA: 5s - loss: 0.0611 - accuracy: 0.98 - ETA: 5s - loss: 0.0628 - accuracy: 0.98 - ETA: 5s - loss: 0.0750 - accuracy: 0.97 - ETA: 5s - loss: 0.0840 - accuracy: 0.97 - ETA: 5s - loss: 0.0984 - accuracy: 0.96 - ETA: 5s - loss: 0.1081 - accuracy: 0.95 - ETA: 5s - loss: 0.1126 - accuracy: 0.95 - ETA: 5s - loss: 0.1054 - accuracy: 0.95 - ETA: 5s - loss: 0.1045 - accuracy: 0.96 - ETA: 4s - loss: 0.1045 - accuracy: 0.96 - ETA: 4s - loss: 0.1070 - accuracy: 0.96 - ETA: 4s - loss: 0.1094 - accuracy: 0.96 - ETA: 4s - loss: 0.1062 - accuracy: 0.96 - ETA: 4s - loss: 0.1023 - accuracy: 0.96 - ETA: 4s - loss: 0.1035 - accuracy: 0.96 - ETA: 4s - loss: 0.1073 - accuracy: 0.95 - ETA: 4s - loss: 0.1101 - accuracy: 0.95 - ETA: 4s - loss: 0.1127 - accuracy: 0.95 - ETA: 4s - loss: 0.1158 - accuracy: 0.95 - ETA: 4s - loss: 0.1139 - accuracy: 0.95 - ETA: 4s - loss: 0.1111 - accuracy: 0.95 - ETA: 4s - loss: 0.1093 - accuracy: 0.96 - ETA: 3s - loss: 0.1087 - accuracy: 0.96 - ETA: 3s - loss: 0.1084 - accuracy: 0.96 - ETA: 3s - loss: 0.1082 - accuracy: 0.96 - ETA: 3s - loss: 0.1071 - accuracy: 0.96 - ETA: 3s - loss: 0.1058 - accuracy: 0.96 - ETA: 3s - loss: 0.1036 - accuracy: 0.96 - ETA: 3s - loss: 0.1026 - accuracy: 0.96 - ETA: 3s - loss: 0.1034 - accuracy: 0.96 - ETA: 3s - loss: 0.1049 - accuracy: 0.96 - ETA: 3s - loss: 0.1039 - accuracy: 0.96 - ETA: 3s - loss: 0.1033 - accuracy: 0.96 - ETA: 3s - loss: 0.1025 - accuracy: 0.96 - ETA: 2s - loss: 0.1019 - accuracy: 0.96 - ETA: 2s - loss: 0.1008 - accuracy: 0.96 - ETA: 2s - loss: 0.0992 - accuracy: 0.96 - ETA: 2s - loss: 0.0976 - accuracy: 0.96 - ETA: 2s - loss: 0.0971 - accuracy: 0.96 - ETA: 2s - loss: 0.0986 - accuracy: 0.96 - ETA: 2s - loss: 0.0984 - accuracy: 0.96 - ETA: 2s - loss: 0.0977 - accuracy: 0.96 - ETA: 2s - loss: 0.0966 - accuracy: 0.96 - ETA: 2s - loss: 0.0959 - accuracy: 0.96 - ETA: 2s - loss: 0.0951 - accuracy: 0.96 - ETA: 2s - loss: 0.0947 - accuracy: 0.96 - ETA: 2s - loss: 0.0954 - accuracy: 0.96 - ETA: 1s - loss: 0.0951 - accuracy: 0.96 - ETA: 1s - loss: 0.0947 - accuracy: 0.96 - ETA: 1s - loss: 0.0959 - accuracy: 0.96 - ETA: 1s - loss: 0.0975 - accuracy: 0.96 - ETA: 1s - loss: 0.1053 - accuracy: 0.96 - ETA: 1s - loss: 0.1228 - accuracy: 0.95 - ETA: 1s - loss: 0.1238 - accuracy: 0.95 - ETA: 1s - loss: 0.1230 - accuracy: 0.95 - ETA: 1s - loss: 0.1214 - accuracy: 0.95 - ETA: 1s - loss: 0.1200 - accuracy: 0.95 - ETA: 1s - loss: 0.1187 - accuracy: 0.95 - ETA: 1s - loss: 0.1172 - accuracy: 0.95 - ETA: 0s - loss: 0.1163 - accuracy: 0.95 - ETA: 0s - loss: 0.1150 - accuracy: 0.96 - ETA: 0s - loss: 0.1137 - accuracy: 0.96 - ETA: 0s - loss: 0.1128 - accuracy: 0.96 - ETA: 0s - loss: 0.1119 - accuracy: 0.96 - ETA: 0s - loss: 0.1108 - accuracy: 0.96 - ETA: 0s - loss: 0.1115 - accuracy: 0.96 - ETA: 0s - loss: 0.1120 - accuracy: 0.96 - ETA: 0s - loss: 0.1118 - accuracy: 0.96 - ETA: 0s - loss: 0.1109 - accuracy: 0.96 - ETA: 0s - loss: 0.1106 - accuracy: 0.96 - ETA: 0s - loss: 0.1109 - accuracy: 0.96 - 6s 588us/step - loss: 0.1108 - accuracy: 0.9614 - val_loss: 0.1016 - val_accuracy: 0.9629\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00794\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1251 - accuracy: 0.95 - ETA: 5s - loss: 0.1026 - accuracy: 0.95 - ETA: 5s - loss: 0.0875 - accuracy: 0.96 - ETA: 5s - loss: 0.0825 - accuracy: 0.96 - ETA: 5s - loss: 0.0810 - accuracy: 0.96 - ETA: 5s - loss: 0.0773 - accuracy: 0.96 - ETA: 5s - loss: 0.0733 - accuracy: 0.97 - ETA: 5s - loss: 0.0750 - accuracy: 0.97 - ETA: 5s - loss: 0.0746 - accuracy: 0.97 - ETA: 5s - loss: 0.0728 - accuracy: 0.97 - ETA: 5s - loss: 0.0723 - accuracy: 0.97 - ETA: 5s - loss: 0.0725 - accuracy: 0.97 - ETA: 5s - loss: 0.0726 - accuracy: 0.97 - ETA: 4s - loss: 0.0743 - accuracy: 0.97 - ETA: 4s - loss: 0.0759 - accuracy: 0.97 - ETA: 4s - loss: 0.0738 - accuracy: 0.97 - ETA: 4s - loss: 0.0749 - accuracy: 0.97 - ETA: 4s - loss: 0.0735 - accuracy: 0.97 - ETA: 4s - loss: 0.0723 - accuracy: 0.97 - ETA: 4s - loss: 0.0751 - accuracy: 0.97 - ETA: 4s - loss: 0.0738 - accuracy: 0.97 - ETA: 4s - loss: 0.0721 - accuracy: 0.97 - ETA: 4s - loss: 0.0715 - accuracy: 0.97 - ETA: 4s - loss: 0.0723 - accuracy: 0.97 - ETA: 4s - loss: 0.0726 - accuracy: 0.97 - ETA: 3s - loss: 0.0870 - accuracy: 0.96 - ETA: 3s - loss: 0.0923 - accuracy: 0.96 - ETA: 3s - loss: 0.0936 - accuracy: 0.96 - ETA: 3s - loss: 0.0918 - accuracy: 0.96 - ETA: 3s - loss: 0.0903 - accuracy: 0.96 - ETA: 3s - loss: 0.0923 - accuracy: 0.96 - ETA: 3s - loss: 0.0911 - accuracy: 0.96 - ETA: 3s - loss: 0.0902 - accuracy: 0.96 - ETA: 3s - loss: 0.0889 - accuracy: 0.96 - ETA: 3s - loss: 0.0897 - accuracy: 0.96 - ETA: 3s - loss: 0.0892 - accuracy: 0.96 - ETA: 3s - loss: 0.0899 - accuracy: 0.96 - ETA: 2s - loss: 0.0912 - accuracy: 0.96 - ETA: 2s - loss: 0.0918 - accuracy: 0.96 - ETA: 2s - loss: 0.0907 - accuracy: 0.96 - ETA: 2s - loss: 0.0890 - accuracy: 0.96 - ETA: 2s - loss: 0.0888 - accuracy: 0.96 - ETA: 2s - loss: 0.0874 - accuracy: 0.96 - ETA: 2s - loss: 0.0870 - accuracy: 0.96 - ETA: 2s - loss: 0.0876 - accuracy: 0.96 - ETA: 2s - loss: 0.0868 - accuracy: 0.96 - ETA: 2s - loss: 0.0864 - accuracy: 0.96 - ETA: 2s - loss: 0.0863 - accuracy: 0.96 - ETA: 2s - loss: 0.0862 - accuracy: 0.96 - ETA: 1s - loss: 0.0866 - accuracy: 0.96 - ETA: 1s - loss: 0.0904 - accuracy: 0.96 - ETA: 1s - loss: 0.0929 - accuracy: 0.96 - ETA: 1s - loss: 0.0952 - accuracy: 0.96 - ETA: 1s - loss: 0.0946 - accuracy: 0.96 - ETA: 1s - loss: 0.0940 - accuracy: 0.96 - ETA: 1s - loss: 0.0933 - accuracy: 0.96 - ETA: 1s - loss: 0.0933 - accuracy: 0.96 - ETA: 1s - loss: 0.0922 - accuracy: 0.96 - ETA: 1s - loss: 0.0917 - accuracy: 0.96 - ETA: 1s - loss: 0.0909 - accuracy: 0.96 - ETA: 1s - loss: 0.0902 - accuracy: 0.96 - ETA: 0s - loss: 0.0894 - accuracy: 0.96 - ETA: 0s - loss: 0.0893 - accuracy: 0.96 - ETA: 0s - loss: 0.0885 - accuracy: 0.96 - ETA: 0s - loss: 0.0878 - accuracy: 0.96 - ETA: 0s - loss: 0.0874 - accuracy: 0.96 - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - ETA: 0s - loss: 0.0877 - accuracy: 0.96 - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - ETA: 0s - loss: 0.0871 - accuracy: 0.96 - ETA: 0s - loss: 0.0883 - accuracy: 0.96 - ETA: 0s - loss: 0.0919 - accuracy: 0.96 - ETA: 0s - loss: 0.0918 - accuracy: 0.96 - 6s 591us/step - loss: 0.0915 - accuracy: 0.9669 - val_loss: 0.0473 - val_accuracy: 0.9840\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00794\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0406 - accuracy: 0.97 - ETA: 5s - loss: 0.0461 - accuracy: 0.98 - ETA: 5s - loss: 0.0438 - accuracy: 0.98 - ETA: 5s - loss: 0.0474 - accuracy: 0.97 - ETA: 5s - loss: 0.0527 - accuracy: 0.97 - ETA: 5s - loss: 0.0714 - accuracy: 0.97 - ETA: 5s - loss: 0.0802 - accuracy: 0.96 - ETA: 5s - loss: 0.0875 - accuracy: 0.96 - ETA: 5s - loss: 0.0849 - accuracy: 0.96 - ETA: 5s - loss: 0.0790 - accuracy: 0.97 - ETA: 5s - loss: 0.0747 - accuracy: 0.97 - ETA: 5s - loss: 0.0713 - accuracy: 0.97 - ETA: 4s - loss: 0.0692 - accuracy: 0.97 - ETA: 4s - loss: 0.0692 - accuracy: 0.97 - ETA: 4s - loss: 0.0676 - accuracy: 0.97 - ETA: 4s - loss: 0.0682 - accuracy: 0.97 - ETA: 4s - loss: 0.0684 - accuracy: 0.97 - ETA: 4s - loss: 0.0704 - accuracy: 0.97 - ETA: 4s - loss: 0.0774 - accuracy: 0.97 - ETA: 4s - loss: 0.0895 - accuracy: 0.96 - ETA: 4s - loss: 0.0869 - accuracy: 0.96 - ETA: 4s - loss: 0.0838 - accuracy: 0.96 - ETA: 4s - loss: 0.0833 - accuracy: 0.96 - ETA: 4s - loss: 0.0818 - accuracy: 0.97 - ETA: 3s - loss: 0.0816 - accuracy: 0.97 - ETA: 3s - loss: 0.0813 - accuracy: 0.96 - ETA: 3s - loss: 0.0797 - accuracy: 0.97 - ETA: 3s - loss: 0.0788 - accuracy: 0.97 - ETA: 3s - loss: 0.0778 - accuracy: 0.97 - ETA: 3s - loss: 0.0772 - accuracy: 0.97 - ETA: 3s - loss: 0.0780 - accuracy: 0.97 - ETA: 3s - loss: 0.0782 - accuracy: 0.97 - ETA: 3s - loss: 0.0787 - accuracy: 0.97 - ETA: 3s - loss: 0.0785 - accuracy: 0.97 - ETA: 3s - loss: 0.0775 - accuracy: 0.97 - ETA: 3s - loss: 0.0775 - accuracy: 0.97 - ETA: 3s - loss: 0.0779 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 2s - loss: 0.0786 - accuracy: 0.97 - ETA: 2s - loss: 0.0775 - accuracy: 0.97 - ETA: 2s - loss: 0.0765 - accuracy: 0.97 - ETA: 2s - loss: 0.0766 - accuracy: 0.97 - ETA: 2s - loss: 0.0763 - accuracy: 0.97 - ETA: 2s - loss: 0.0758 - accuracy: 0.97 - ETA: 2s - loss: 0.0748 - accuracy: 0.97 - ETA: 2s - loss: 0.0749 - accuracy: 0.97 - ETA: 2s - loss: 0.0747 - accuracy: 0.97 - ETA: 2s - loss: 0.0738 - accuracy: 0.97 - ETA: 2s - loss: 0.0737 - accuracy: 0.97 - ETA: 1s - loss: 0.0736 - accuracy: 0.97 - ETA: 1s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0736 - accuracy: 0.97 - ETA: 1s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0745 - accuracy: 0.97 - ETA: 1s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0740 - accuracy: 0.97 - ETA: 1s - loss: 0.0730 - accuracy: 0.97 - ETA: 1s - loss: 0.0729 - accuracy: 0.97 - ETA: 1s - loss: 0.0726 - accuracy: 0.97 - ETA: 1s - loss: 0.0720 - accuracy: 0.97 - ETA: 0s - loss: 0.0720 - accuracy: 0.97 - ETA: 0s - loss: 0.0730 - accuracy: 0.97 - ETA: 0s - loss: 0.0736 - accuracy: 0.97 - ETA: 0s - loss: 0.0767 - accuracy: 0.97 - ETA: 0s - loss: 0.0818 - accuracy: 0.97 - ETA: 0s - loss: 0.0816 - accuracy: 0.97 - ETA: 0s - loss: 0.0820 - accuracy: 0.97 - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - ETA: 0s - loss: 0.0811 - accuracy: 0.97 - ETA: 0s - loss: 0.0807 - accuracy: 0.97 - ETA: 0s - loss: 0.0801 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - 6s 584us/step - loss: 0.0792 - accuracy: 0.9712 - val_loss: 0.0515 - val_accuracy: 0.9848\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00794\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0609 - accuracy: 0.97 - ETA: 5s - loss: 0.0664 - accuracy: 0.97 - ETA: 5s - loss: 0.0912 - accuracy: 0.96 - ETA: 5s - loss: 0.0786 - accuracy: 0.96 - ETA: 5s - loss: 0.0727 - accuracy: 0.96 - ETA: 5s - loss: 0.0768 - accuracy: 0.96 - ETA: 5s - loss: 0.0756 - accuracy: 0.96 - ETA: 5s - loss: 0.0747 - accuracy: 0.96 - ETA: 5s - loss: 0.0700 - accuracy: 0.96 - ETA: 5s - loss: 0.0677 - accuracy: 0.97 - ETA: 5s - loss: 0.0678 - accuracy: 0.96 - ETA: 4s - loss: 0.0675 - accuracy: 0.97 - ETA: 4s - loss: 0.0649 - accuracy: 0.97 - ETA: 4s - loss: 0.0633 - accuracy: 0.97 - ETA: 4s - loss: 0.0627 - accuracy: 0.97 - ETA: 4s - loss: 0.0605 - accuracy: 0.97 - ETA: 4s - loss: 0.0579 - accuracy: 0.97 - ETA: 4s - loss: 0.0589 - accuracy: 0.97 - ETA: 4s - loss: 0.0590 - accuracy: 0.97 - ETA: 4s - loss: 0.0585 - accuracy: 0.97 - ETA: 4s - loss: 0.0600 - accuracy: 0.97 - ETA: 4s - loss: 0.0612 - accuracy: 0.97 - ETA: 4s - loss: 0.0627 - accuracy: 0.97 - ETA: 4s - loss: 0.0640 - accuracy: 0.97 - ETA: 3s - loss: 0.0651 - accuracy: 0.97 - ETA: 3s - loss: 0.0661 - accuracy: 0.97 - ETA: 3s - loss: 0.0657 - accuracy: 0.97 - ETA: 3s - loss: 0.0652 - accuracy: 0.97 - ETA: 3s - loss: 0.0638 - accuracy: 0.97 - ETA: 3s - loss: 0.0626 - accuracy: 0.97 - ETA: 3s - loss: 0.0626 - accuracy: 0.97 - ETA: 3s - loss: 0.0641 - accuracy: 0.97 - ETA: 3s - loss: 0.0647 - accuracy: 0.97 - ETA: 3s - loss: 0.0634 - accuracy: 0.97 - ETA: 3s - loss: 0.0634 - accuracy: 0.97 - ETA: 3s - loss: 0.0633 - accuracy: 0.97 - ETA: 3s - loss: 0.0626 - accuracy: 0.97 - ETA: 2s - loss: 0.0619 - accuracy: 0.97 - ETA: 2s - loss: 0.0625 - accuracy: 0.97 - ETA: 2s - loss: 0.0626 - accuracy: 0.97 - ETA: 2s - loss: 0.0615 - accuracy: 0.97 - ETA: 2s - loss: 0.0610 - accuracy: 0.97 - ETA: 2s - loss: 0.0623 - accuracy: 0.97 - ETA: 2s - loss: 0.0620 - accuracy: 0.97 - ETA: 2s - loss: 0.0617 - accuracy: 0.97 - ETA: 2s - loss: 0.0614 - accuracy: 0.97 - ETA: 2s - loss: 0.0618 - accuracy: 0.97 - ETA: 2s - loss: 0.0615 - accuracy: 0.97 - ETA: 2s - loss: 0.0611 - accuracy: 0.97 - ETA: 1s - loss: 0.0617 - accuracy: 0.97 - ETA: 1s - loss: 0.0633 - accuracy: 0.97 - ETA: 1s - loss: 0.0635 - accuracy: 0.97 - ETA: 1s - loss: 0.0634 - accuracy: 0.97 - ETA: 1s - loss: 0.0634 - accuracy: 0.97 - ETA: 1s - loss: 0.0636 - accuracy: 0.97 - ETA: 1s - loss: 0.0632 - accuracy: 0.97 - ETA: 1s - loss: 0.0655 - accuracy: 0.97 - ETA: 1s - loss: 0.0669 - accuracy: 0.97 - ETA: 1s - loss: 0.0664 - accuracy: 0.97 - ETA: 1s - loss: 0.0658 - accuracy: 0.97 - ETA: 1s - loss: 0.0651 - accuracy: 0.97 - ETA: 0s - loss: 0.0648 - accuracy: 0.97 - ETA: 0s - loss: 0.0646 - accuracy: 0.97 - ETA: 0s - loss: 0.0642 - accuracy: 0.97 - ETA: 0s - loss: 0.0642 - accuracy: 0.97 - ETA: 0s - loss: 0.0642 - accuracy: 0.97 - ETA: 0s - loss: 0.0638 - accuracy: 0.97 - ETA: 0s - loss: 0.0635 - accuracy: 0.97 - ETA: 0s - loss: 0.0638 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0669 - accuracy: 0.97 - ETA: 0s - loss: 0.0672 - accuracy: 0.97 - ETA: 0s - loss: 0.0672 - accuracy: 0.97 - 6s 582us/step - loss: 0.0671 - accuracy: 0.9752 - val_loss: 0.0395 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00794\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0293 - accuracy: 0.98 - ETA: 5s - loss: 0.0305 - accuracy: 0.98 - ETA: 6s - loss: 0.0340 - accuracy: 0.98 - ETA: 5s - loss: 0.0361 - accuracy: 0.98 - ETA: 5s - loss: 0.0347 - accuracy: 0.98 - ETA: 5s - loss: 0.0339 - accuracy: 0.98 - ETA: 5s - loss: 0.0327 - accuracy: 0.98 - ETA: 5s - loss: 0.0345 - accuracy: 0.98 - ETA: 5s - loss: 0.0379 - accuracy: 0.98 - ETA: 5s - loss: 0.0421 - accuracy: 0.98 - ETA: 5s - loss: 0.0420 - accuracy: 0.98 - ETA: 5s - loss: 0.0411 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 4s - loss: 0.0445 - accuracy: 0.98 - ETA: 4s - loss: 0.0575 - accuracy: 0.97 - ETA: 4s - loss: 0.0690 - accuracy: 0.97 - ETA: 4s - loss: 0.0686 - accuracy: 0.97 - ETA: 4s - loss: 0.0684 - accuracy: 0.97 - ETA: 4s - loss: 0.0669 - accuracy: 0.97 - ETA: 4s - loss: 0.0643 - accuracy: 0.97 - ETA: 4s - loss: 0.0629 - accuracy: 0.97 - ETA: 4s - loss: 0.0642 - accuracy: 0.97 - ETA: 4s - loss: 0.0695 - accuracy: 0.97 - ETA: 4s - loss: 0.0731 - accuracy: 0.97 - ETA: 3s - loss: 0.0731 - accuracy: 0.97 - ETA: 3s - loss: 0.0719 - accuracy: 0.97 - ETA: 3s - loss: 0.0701 - accuracy: 0.97 - ETA: 3s - loss: 0.0685 - accuracy: 0.97 - ETA: 3s - loss: 0.0674 - accuracy: 0.97 - ETA: 3s - loss: 0.0660 - accuracy: 0.97 - ETA: 3s - loss: 0.0644 - accuracy: 0.97 - ETA: 3s - loss: 0.0637 - accuracy: 0.97 - ETA: 3s - loss: 0.0627 - accuracy: 0.97 - ETA: 3s - loss: 0.0624 - accuracy: 0.97 - ETA: 3s - loss: 0.0634 - accuracy: 0.97 - ETA: 3s - loss: 0.0636 - accuracy: 0.97 - ETA: 3s - loss: 0.0643 - accuracy: 0.97 - ETA: 2s - loss: 0.0636 - accuracy: 0.97 - ETA: 2s - loss: 0.0627 - accuracy: 0.97 - ETA: 2s - loss: 0.0622 - accuracy: 0.97 - ETA: 2s - loss: 0.0623 - accuracy: 0.97 - ETA: 2s - loss: 0.0642 - accuracy: 0.97 - ETA: 2s - loss: 0.0665 - accuracy: 0.97 - ETA: 2s - loss: 0.0665 - accuracy: 0.97 - ETA: 2s - loss: 0.0660 - accuracy: 0.97 - ETA: 2s - loss: 0.0648 - accuracy: 0.97 - ETA: 2s - loss: 0.0649 - accuracy: 0.97 - ETA: 2s - loss: 0.0648 - accuracy: 0.97 - ETA: 2s - loss: 0.0648 - accuracy: 0.97 - ETA: 1s - loss: 0.0661 - accuracy: 0.97 - ETA: 1s - loss: 0.0667 - accuracy: 0.97 - ETA: 1s - loss: 0.0660 - accuracy: 0.97 - ETA: 1s - loss: 0.0659 - accuracy: 0.97 - ETA: 1s - loss: 0.0667 - accuracy: 0.97 - ETA: 1s - loss: 0.0659 - accuracy: 0.97 - ETA: 1s - loss: 0.0656 - accuracy: 0.97 - ETA: 1s - loss: 0.0646 - accuracy: 0.97 - ETA: 1s - loss: 0.0640 - accuracy: 0.97 - ETA: 1s - loss: 0.0632 - accuracy: 0.97 - ETA: 1s - loss: 0.0627 - accuracy: 0.97 - ETA: 1s - loss: 0.0636 - accuracy: 0.97 - ETA: 0s - loss: 0.0634 - accuracy: 0.97 - ETA: 0s - loss: 0.0631 - accuracy: 0.97 - ETA: 0s - loss: 0.0630 - accuracy: 0.97 - ETA: 0s - loss: 0.0626 - accuracy: 0.97 - ETA: 0s - loss: 0.0619 - accuracy: 0.97 - ETA: 0s - loss: 0.0611 - accuracy: 0.97 - ETA: 0s - loss: 0.0606 - accuracy: 0.97 - ETA: 0s - loss: 0.0601 - accuracy: 0.97 - ETA: 0s - loss: 0.0599 - accuracy: 0.97 - ETA: 0s - loss: 0.0601 - accuracy: 0.97 - ETA: 0s - loss: 0.0625 - accuracy: 0.97 - ETA: 0s - loss: 0.0639 - accuracy: 0.97 - 6s 598us/step - loss: 0.0656 - accuracy: 0.9757 - val_loss: 0.0922 - val_accuracy: 0.9680\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00794\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1672 - accuracy: 0.93 - ETA: 5s - loss: 0.1051 - accuracy: 0.95 - ETA: 5s - loss: 0.0778 - accuracy: 0.96 - ETA: 5s - loss: 0.0729 - accuracy: 0.96 - ETA: 5s - loss: 0.0685 - accuracy: 0.97 - ETA: 5s - loss: 0.0704 - accuracy: 0.97 - ETA: 5s - loss: 0.0658 - accuracy: 0.97 - ETA: 5s - loss: 0.0623 - accuracy: 0.97 - ETA: 5s - loss: 0.0589 - accuracy: 0.97 - ETA: 5s - loss: 0.0569 - accuracy: 0.97 - ETA: 5s - loss: 0.0537 - accuracy: 0.97 - ETA: 5s - loss: 0.0499 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0469 - accuracy: 0.98 - ETA: 4s - loss: 0.0452 - accuracy: 0.98 - ETA: 4s - loss: 0.0479 - accuracy: 0.98 - ETA: 4s - loss: 0.0504 - accuracy: 0.98 - ETA: 4s - loss: 0.0504 - accuracy: 0.98 - ETA: 4s - loss: 0.0483 - accuracy: 0.98 - ETA: 4s - loss: 0.0464 - accuracy: 0.98 - ETA: 4s - loss: 0.0450 - accuracy: 0.98 - ETA: 4s - loss: 0.0451 - accuracy: 0.98 - ETA: 4s - loss: 0.0443 - accuracy: 0.98 - ETA: 4s - loss: 0.0444 - accuracy: 0.98 - ETA: 4s - loss: 0.0441 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0495 - accuracy: 0.98 - ETA: 3s - loss: 0.0600 - accuracy: 0.97 - ETA: 3s - loss: 0.0587 - accuracy: 0.98 - ETA: 3s - loss: 0.0594 - accuracy: 0.97 - ETA: 3s - loss: 0.0598 - accuracy: 0.97 - ETA: 3s - loss: 0.0626 - accuracy: 0.97 - ETA: 3s - loss: 0.0624 - accuracy: 0.97 - ETA: 3s - loss: 0.0618 - accuracy: 0.97 - ETA: 3s - loss: 0.0607 - accuracy: 0.97 - ETA: 3s - loss: 0.0603 - accuracy: 0.97 - ETA: 3s - loss: 0.0591 - accuracy: 0.98 - ETA: 2s - loss: 0.0578 - accuracy: 0.98 - ETA: 2s - loss: 0.0581 - accuracy: 0.98 - ETA: 2s - loss: 0.0571 - accuracy: 0.98 - ETA: 2s - loss: 0.0564 - accuracy: 0.98 - ETA: 2s - loss: 0.0573 - accuracy: 0.98 - ETA: 2s - loss: 0.0570 - accuracy: 0.98 - ETA: 2s - loss: 0.0564 - accuracy: 0.98 - ETA: 2s - loss: 0.0559 - accuracy: 0.98 - ETA: 2s - loss: 0.0548 - accuracy: 0.98 - ETA: 2s - loss: 0.0541 - accuracy: 0.98 - ETA: 2s - loss: 0.0536 - accuracy: 0.98 - ETA: 2s - loss: 0.0537 - accuracy: 0.98 - ETA: 1s - loss: 0.0533 - accuracy: 0.98 - ETA: 1s - loss: 0.0556 - accuracy: 0.98 - ETA: 1s - loss: 0.0589 - accuracy: 0.98 - ETA: 1s - loss: 0.0583 - accuracy: 0.98 - ETA: 1s - loss: 0.0583 - accuracy: 0.98 - ETA: 1s - loss: 0.0580 - accuracy: 0.98 - ETA: 1s - loss: 0.0585 - accuracy: 0.98 - ETA: 1s - loss: 0.0582 - accuracy: 0.98 - ETA: 1s - loss: 0.0579 - accuracy: 0.98 - ETA: 1s - loss: 0.0573 - accuracy: 0.98 - ETA: 1s - loss: 0.0565 - accuracy: 0.98 - ETA: 1s - loss: 0.0559 - accuracy: 0.98 - ETA: 0s - loss: 0.0551 - accuracy: 0.98 - ETA: 0s - loss: 0.0544 - accuracy: 0.98 - ETA: 0s - loss: 0.0536 - accuracy: 0.98 - ETA: 0s - loss: 0.0535 - accuracy: 0.98 - ETA: 0s - loss: 0.0541 - accuracy: 0.98 - ETA: 0s - loss: 0.0537 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0532 - accuracy: 0.98 - ETA: 0s - loss: 0.0534 - accuracy: 0.98 - ETA: 0s - loss: 0.0559 - accuracy: 0.98 - ETA: 0s - loss: 0.0574 - accuracy: 0.98 - 6s 601us/step - loss: 0.0572 - accuracy: 0.9808 - val_loss: 0.0327 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00794\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0188 - accuracy: 1.00 - ETA: 5s - loss: 0.0312 - accuracy: 0.99 - ETA: 5s - loss: 0.0301 - accuracy: 0.99 - ETA: 5s - loss: 0.0390 - accuracy: 0.98 - ETA: 5s - loss: 0.0334 - accuracy: 0.99 - ETA: 5s - loss: 0.0368 - accuracy: 0.98 - ETA: 5s - loss: 0.0372 - accuracy: 0.98 - ETA: 5s - loss: 0.0343 - accuracy: 0.99 - ETA: 5s - loss: 0.0322 - accuracy: 0.99 - ETA: 5s - loss: 0.0317 - accuracy: 0.99 - ETA: 5s - loss: 0.0373 - accuracy: 0.98 - ETA: 5s - loss: 0.0402 - accuracy: 0.98 - ETA: 5s - loss: 0.0413 - accuracy: 0.98 - ETA: 5s - loss: 0.0403 - accuracy: 0.98 - ETA: 5s - loss: 0.0393 - accuracy: 0.98 - ETA: 4s - loss: 0.0413 - accuracy: 0.98 - ETA: 4s - loss: 0.0432 - accuracy: 0.98 - ETA: 4s - loss: 0.0542 - accuracy: 0.98 - ETA: 4s - loss: 0.0539 - accuracy: 0.98 - ETA: 4s - loss: 0.0547 - accuracy: 0.97 - ETA: 4s - loss: 0.0533 - accuracy: 0.97 - ETA: 4s - loss: 0.0521 - accuracy: 0.98 - ETA: 4s - loss: 0.0515 - accuracy: 0.98 - ETA: 4s - loss: 0.0518 - accuracy: 0.98 - ETA: 4s - loss: 0.0505 - accuracy: 0.98 - ETA: 4s - loss: 0.0497 - accuracy: 0.98 - ETA: 3s - loss: 0.0498 - accuracy: 0.98 - ETA: 3s - loss: 0.0516 - accuracy: 0.98 - ETA: 3s - loss: 0.0533 - accuracy: 0.98 - ETA: 3s - loss: 0.0552 - accuracy: 0.98 - ETA: 3s - loss: 0.0545 - accuracy: 0.98 - ETA: 3s - loss: 0.0531 - accuracy: 0.98 - ETA: 3s - loss: 0.0517 - accuracy: 0.98 - ETA: 3s - loss: 0.0505 - accuracy: 0.98 - ETA: 3s - loss: 0.0493 - accuracy: 0.98 - ETA: 3s - loss: 0.0485 - accuracy: 0.98 - ETA: 3s - loss: 0.0487 - accuracy: 0.98 - ETA: 3s - loss: 0.0512 - accuracy: 0.98 - ETA: 2s - loss: 0.0527 - accuracy: 0.98 - ETA: 2s - loss: 0.0531 - accuracy: 0.98 - ETA: 2s - loss: 0.0531 - accuracy: 0.98 - ETA: 2s - loss: 0.0539 - accuracy: 0.98 - ETA: 2s - loss: 0.0548 - accuracy: 0.98 - ETA: 2s - loss: 0.0538 - accuracy: 0.98 - ETA: 2s - loss: 0.0533 - accuracy: 0.98 - ETA: 2s - loss: 0.0527 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 2s - loss: 0.0518 - accuracy: 0.98 - ETA: 2s - loss: 0.0514 - accuracy: 0.98 - ETA: 2s - loss: 0.0513 - accuracy: 0.98 - ETA: 1s - loss: 0.0508 - accuracy: 0.98 - ETA: 1s - loss: 0.0506 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0489 - accuracy: 0.98 - ETA: 1s - loss: 0.0484 - accuracy: 0.98 - ETA: 1s - loss: 0.0478 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0489 - accuracy: 0.98 - ETA: 1s - loss: 0.0548 - accuracy: 0.98 - ETA: 1s - loss: 0.0562 - accuracy: 0.98 - ETA: 0s - loss: 0.0563 - accuracy: 0.98 - ETA: 0s - loss: 0.0565 - accuracy: 0.98 - ETA: 0s - loss: 0.0558 - accuracy: 0.98 - ETA: 0s - loss: 0.0552 - accuracy: 0.98 - ETA: 0s - loss: 0.0546 - accuracy: 0.98 - ETA: 0s - loss: 0.0547 - accuracy: 0.98 - ETA: 0s - loss: 0.0543 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0535 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.98 - ETA: 0s - loss: 0.0530 - accuracy: 0.98 - 7s 613us/step - loss: 0.0527 - accuracy: 0.9819 - val_loss: 0.0414 - val_accuracy: 0.9823\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00794\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0475 - accuracy: 0.97 - ETA: 5s - loss: 0.0414 - accuracy: 0.98 - ETA: 6s - loss: 0.0379 - accuracy: 0.98 - ETA: 6s - loss: 0.0324 - accuracy: 0.98 - ETA: 5s - loss: 0.0285 - accuracy: 0.98 - ETA: 5s - loss: 0.0286 - accuracy: 0.98 - ETA: 5s - loss: 0.0295 - accuracy: 0.98 - ETA: 5s - loss: 0.0331 - accuracy: 0.98 - ETA: 5s - loss: 0.0352 - accuracy: 0.98 - ETA: 5s - loss: 0.0348 - accuracy: 0.98 - ETA: 5s - loss: 0.0345 - accuracy: 0.98 - ETA: 5s - loss: 0.0337 - accuracy: 0.98 - ETA: 5s - loss: 0.0337 - accuracy: 0.98 - ETA: 5s - loss: 0.0370 - accuracy: 0.98 - ETA: 5s - loss: 0.0394 - accuracy: 0.98 - ETA: 5s - loss: 0.0445 - accuracy: 0.98 - ETA: 4s - loss: 0.0426 - accuracy: 0.98 - ETA: 4s - loss: 0.0423 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0465 - accuracy: 0.98 - ETA: 4s - loss: 0.0496 - accuracy: 0.98 - ETA: 4s - loss: 0.0537 - accuracy: 0.98 - ETA: 4s - loss: 0.0554 - accuracy: 0.98 - ETA: 4s - loss: 0.0559 - accuracy: 0.98 - ETA: 4s - loss: 0.0557 - accuracy: 0.98 - ETA: 4s - loss: 0.0546 - accuracy: 0.98 - ETA: 4s - loss: 0.0534 - accuracy: 0.98 - ETA: 3s - loss: 0.0549 - accuracy: 0.98 - ETA: 3s - loss: 0.0535 - accuracy: 0.98 - ETA: 3s - loss: 0.0539 - accuracy: 0.98 - ETA: 3s - loss: 0.0565 - accuracy: 0.98 - ETA: 3s - loss: 0.0562 - accuracy: 0.98 - ETA: 3s - loss: 0.0552 - accuracy: 0.98 - ETA: 3s - loss: 0.0550 - accuracy: 0.98 - ETA: 3s - loss: 0.0542 - accuracy: 0.98 - ETA: 3s - loss: 0.0539 - accuracy: 0.98 - ETA: 3s - loss: 0.0534 - accuracy: 0.98 - ETA: 3s - loss: 0.0527 - accuracy: 0.98 - ETA: 2s - loss: 0.0520 - accuracy: 0.98 - ETA: 2s - loss: 0.0512 - accuracy: 0.98 - ETA: 2s - loss: 0.0502 - accuracy: 0.98 - ETA: 2s - loss: 0.0492 - accuracy: 0.98 - ETA: 2s - loss: 0.0483 - accuracy: 0.98 - ETA: 2s - loss: 0.0483 - accuracy: 0.98 - ETA: 2s - loss: 0.0488 - accuracy: 0.98 - ETA: 2s - loss: 0.0481 - accuracy: 0.98 - ETA: 2s - loss: 0.0473 - accuracy: 0.98 - ETA: 2s - loss: 0.0470 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 2s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0472 - accuracy: 0.98 - ETA: 1s - loss: 0.0484 - accuracy: 0.98 - ETA: 1s - loss: 0.0483 - accuracy: 0.98 - ETA: 1s - loss: 0.0482 - accuracy: 0.98 - ETA: 1s - loss: 0.0484 - accuracy: 0.98 - ETA: 1s - loss: 0.0484 - accuracy: 0.98 - ETA: 1s - loss: 0.0481 - accuracy: 0.98 - ETA: 0s - loss: 0.0477 - accuracy: 0.98 - ETA: 0s - loss: 0.0478 - accuracy: 0.98 - ETA: 0s - loss: 0.0487 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0502 - accuracy: 0.98 - ETA: 0s - loss: 0.0502 - accuracy: 0.98 - ETA: 0s - loss: 0.0498 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - 7s 615us/step - loss: 0.0496 - accuracy: 0.9820 - val_loss: 0.0267 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00794\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0174 - accuracy: 0.99 - ETA: 5s - loss: 0.0221 - accuracy: 0.99 - ETA: 5s - loss: 0.0288 - accuracy: 0.99 - ETA: 5s - loss: 0.0323 - accuracy: 0.98 - ETA: 5s - loss: 0.0287 - accuracy: 0.99 - ETA: 5s - loss: 0.0313 - accuracy: 0.99 - ETA: 5s - loss: 0.0329 - accuracy: 0.98 - ETA: 5s - loss: 0.0376 - accuracy: 0.98 - ETA: 5s - loss: 0.0401 - accuracy: 0.98 - ETA: 5s - loss: 0.0368 - accuracy: 0.98 - ETA: 5s - loss: 0.0346 - accuracy: 0.98 - ETA: 5s - loss: 0.0326 - accuracy: 0.98 - ETA: 5s - loss: 0.0304 - accuracy: 0.98 - ETA: 5s - loss: 0.0309 - accuracy: 0.98 - ETA: 5s - loss: 0.0335 - accuracy: 0.98 - ETA: 4s - loss: 0.0427 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0444 - accuracy: 0.98 - ETA: 4s - loss: 0.0425 - accuracy: 0.98 - ETA: 4s - loss: 0.0412 - accuracy: 0.98 - ETA: 4s - loss: 0.0409 - accuracy: 0.98 - ETA: 4s - loss: 0.0416 - accuracy: 0.98 - ETA: 4s - loss: 0.0455 - accuracy: 0.98 - ETA: 4s - loss: 0.0454 - accuracy: 0.98 - ETA: 4s - loss: 0.0457 - accuracy: 0.98 - ETA: 4s - loss: 0.0445 - accuracy: 0.98 - ETA: 3s - loss: 0.0440 - accuracy: 0.98 - ETA: 3s - loss: 0.0431 - accuracy: 0.98 - ETA: 3s - loss: 0.0427 - accuracy: 0.98 - ETA: 3s - loss: 0.0416 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 3s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 2s - loss: 0.0393 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0403 - accuracy: 0.98 - ETA: 2s - loss: 0.0457 - accuracy: 0.98 - ETA: 2s - loss: 0.0513 - accuracy: 0.98 - ETA: 2s - loss: 0.0515 - accuracy: 0.98 - ETA: 2s - loss: 0.0512 - accuracy: 0.98 - ETA: 2s - loss: 0.0507 - accuracy: 0.98 - ETA: 2s - loss: 0.0500 - accuracy: 0.98 - ETA: 2s - loss: 0.0494 - accuracy: 0.98 - ETA: 2s - loss: 0.0489 - accuracy: 0.98 - ETA: 2s - loss: 0.0482 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0467 - accuracy: 0.98 - ETA: 1s - loss: 0.0463 - accuracy: 0.98 - ETA: 1s - loss: 0.0478 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0483 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0477 - accuracy: 0.98 - ETA: 0s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0474 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0466 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0470 - accuracy: 0.98 - 7s 617us/step - loss: 0.0470 - accuracy: 0.9849 - val_loss: 0.0358 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00794\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0229 - accuracy: 0.99 - ETA: 6s - loss: 0.0255 - accuracy: 0.98 - ETA: 6s - loss: 0.0241 - accuracy: 0.98 - ETA: 6s - loss: 0.0234 - accuracy: 0.98 - ETA: 6s - loss: 0.0211 - accuracy: 0.99 - ETA: 6s - loss: 0.0191 - accuracy: 0.99 - ETA: 6s - loss: 0.0177 - accuracy: 0.99 - ETA: 5s - loss: 0.0212 - accuracy: 0.99 - ETA: 5s - loss: 0.0387 - accuracy: 0.98 - ETA: 5s - loss: 0.0458 - accuracy: 0.98 - ETA: 5s - loss: 0.0483 - accuracy: 0.98 - ETA: 5s - loss: 0.0467 - accuracy: 0.98 - ETA: 5s - loss: 0.0468 - accuracy: 0.98 - ETA: 5s - loss: 0.0447 - accuracy: 0.98 - ETA: 5s - loss: 0.0442 - accuracy: 0.98 - ETA: 5s - loss: 0.0444 - accuracy: 0.98 - ETA: 5s - loss: 0.0502 - accuracy: 0.98 - ETA: 4s - loss: 0.0577 - accuracy: 0.98 - ETA: 4s - loss: 0.0687 - accuracy: 0.97 - ETA: 4s - loss: 0.0660 - accuracy: 0.97 - ETA: 4s - loss: 0.0636 - accuracy: 0.98 - ETA: 4s - loss: 0.0612 - accuracy: 0.98 - ETA: 4s - loss: 0.0595 - accuracy: 0.98 - ETA: 4s - loss: 0.0581 - accuracy: 0.98 - ETA: 4s - loss: 0.0578 - accuracy: 0.98 - ETA: 4s - loss: 0.0583 - accuracy: 0.98 - ETA: 4s - loss: 0.0567 - accuracy: 0.98 - ETA: 4s - loss: 0.0553 - accuracy: 0.98 - ETA: 4s - loss: 0.0537 - accuracy: 0.98 - ETA: 3s - loss: 0.0522 - accuracy: 0.98 - ETA: 3s - loss: 0.0509 - accuracy: 0.98 - ETA: 3s - loss: 0.0496 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 3s - loss: 0.0473 - accuracy: 0.98 - ETA: 3s - loss: 0.0466 - accuracy: 0.98 - ETA: 3s - loss: 0.0462 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0455 - accuracy: 0.98 - ETA: 3s - loss: 0.0462 - accuracy: 0.98 - ETA: 2s - loss: 0.0493 - accuracy: 0.98 - ETA: 2s - loss: 0.0491 - accuracy: 0.98 - ETA: 2s - loss: 0.0489 - accuracy: 0.98 - ETA: 2s - loss: 0.0522 - accuracy: 0.98 - ETA: 2s - loss: 0.0549 - accuracy: 0.98 - ETA: 2s - loss: 0.0551 - accuracy: 0.98 - ETA: 2s - loss: 0.0545 - accuracy: 0.98 - ETA: 2s - loss: 0.0540 - accuracy: 0.98 - ETA: 2s - loss: 0.0534 - accuracy: 0.98 - ETA: 2s - loss: 0.0525 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 1s - loss: 0.0512 - accuracy: 0.98 - ETA: 1s - loss: 0.0506 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0491 - accuracy: 0.98 - ETA: 1s - loss: 0.0484 - accuracy: 0.98 - ETA: 1s - loss: 0.0483 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0475 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0477 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0464 - accuracy: 0.98 - ETA: 0s - loss: 0.0467 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - ETA: 0s - loss: 0.0485 - accuracy: 0.98 - ETA: 0s - loss: 0.0493 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0485 - accuracy: 0.98 - 7s 627us/step - loss: 0.0484 - accuracy: 0.9833 - val_loss: 0.0553 - val_accuracy: 0.9848\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00794\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0227 - accuracy: 0.98 - ETA: 6s - loss: 0.0269 - accuracy: 0.98 - ETA: 6s - loss: 0.0231 - accuracy: 0.99 - ETA: 6s - loss: 0.0386 - accuracy: 0.98 - ETA: 5s - loss: 0.0455 - accuracy: 0.98 - ETA: 5s - loss: 0.0432 - accuracy: 0.98 - ETA: 5s - loss: 0.0477 - accuracy: 0.98 - ETA: 5s - loss: 0.0528 - accuracy: 0.97 - ETA: 5s - loss: 0.0507 - accuracy: 0.98 - ETA: 5s - loss: 0.0467 - accuracy: 0.98 - ETA: 5s - loss: 0.0450 - accuracy: 0.98 - ETA: 5s - loss: 0.0465 - accuracy: 0.98 - ETA: 5s - loss: 0.0456 - accuracy: 0.98 - ETA: 5s - loss: 0.0431 - accuracy: 0.98 - ETA: 5s - loss: 0.0424 - accuracy: 0.98 - ETA: 4s - loss: 0.0410 - accuracy: 0.98 - ETA: 4s - loss: 0.0392 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0370 - accuracy: 0.98 - ETA: 4s - loss: 0.0368 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0356 - accuracy: 0.98 - ETA: 4s - loss: 0.0359 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0462 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 3s - loss: 0.0481 - accuracy: 0.98 - ETA: 3s - loss: 0.0472 - accuracy: 0.98 - ETA: 3s - loss: 0.0464 - accuracy: 0.98 - ETA: 3s - loss: 0.0457 - accuracy: 0.98 - ETA: 2s - loss: 0.0455 - accuracy: 0.98 - ETA: 2s - loss: 0.0452 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0461 - accuracy: 0.98 - ETA: 2s - loss: 0.0459 - accuracy: 0.98 - ETA: 2s - loss: 0.0455 - accuracy: 0.98 - ETA: 2s - loss: 0.0448 - accuracy: 0.98 - ETA: 2s - loss: 0.0444 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0429 - accuracy: 0.98 - ETA: 2s - loss: 0.0423 - accuracy: 0.98 - ETA: 2s - loss: 0.0420 - accuracy: 0.98 - ETA: 1s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0408 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0454 - accuracy: 0.98 - ETA: 1s - loss: 0.0488 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0488 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0481 - accuracy: 0.98 - ETA: 0s - loss: 0.0481 - accuracy: 0.98 - ETA: 0s - loss: 0.0478 - accuracy: 0.98 - ETA: 0s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0470 - accuracy: 0.98 - 7s 623us/step - loss: 0.0467 - accuracy: 0.9851 - val_loss: 0.0325 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00794\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0643 - accuracy: 0.98 - ETA: 6s - loss: 0.0736 - accuracy: 0.97 - ETA: 6s - loss: 0.0551 - accuracy: 0.98 - ETA: 5s - loss: 0.0517 - accuracy: 0.98 - ETA: 5s - loss: 0.0452 - accuracy: 0.98 - ETA: 5s - loss: 0.0388 - accuracy: 0.98 - ETA: 5s - loss: 0.0351 - accuracy: 0.98 - ETA: 5s - loss: 0.0336 - accuracy: 0.98 - ETA: 5s - loss: 0.0324 - accuracy: 0.98 - ETA: 5s - loss: 0.0312 - accuracy: 0.98 - ETA: 5s - loss: 0.0299 - accuracy: 0.98 - ETA: 5s - loss: 0.0301 - accuracy: 0.98 - ETA: 5s - loss: 0.0300 - accuracy: 0.98 - ETA: 5s - loss: 0.0303 - accuracy: 0.98 - ETA: 5s - loss: 0.0334 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0452 - accuracy: 0.98 - ETA: 4s - loss: 0.0440 - accuracy: 0.98 - ETA: 4s - loss: 0.0447 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0416 - accuracy: 0.98 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 4s - loss: 0.0401 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0385 - accuracy: 0.98 - ETA: 3s - loss: 0.0389 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0381 - accuracy: 0.98 - ETA: 3s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0421 - accuracy: 0.98 - ETA: 3s - loss: 0.0421 - accuracy: 0.98 - ETA: 3s - loss: 0.0412 - accuracy: 0.98 - ETA: 3s - loss: 0.0404 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0402 - accuracy: 0.98 - ETA: 2s - loss: 0.0399 - accuracy: 0.98 - ETA: 2s - loss: 0.0393 - accuracy: 0.98 - ETA: 2s - loss: 0.0390 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0382 - accuracy: 0.98 - ETA: 2s - loss: 0.0378 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 2s - loss: 0.0369 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0435 - accuracy: 0.98 - ETA: 1s - loss: 0.0462 - accuracy: 0.98 - ETA: 1s - loss: 0.0458 - accuracy: 0.98 - ETA: 1s - loss: 0.0452 - accuracy: 0.98 - ETA: 1s - loss: 0.0453 - accuracy: 0.98 - ETA: 1s - loss: 0.0448 - accuracy: 0.98 - ETA: 1s - loss: 0.0447 - accuracy: 0.98 - ETA: 1s - loss: 0.0443 - accuracy: 0.98 - ETA: 0s - loss: 0.0450 - accuracy: 0.98 - ETA: 0s - loss: 0.0444 - accuracy: 0.98 - ETA: 0s - loss: 0.0443 - accuracy: 0.98 - ETA: 0s - loss: 0.0440 - accuracy: 0.98 - ETA: 0s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0433 - accuracy: 0.98 - ETA: 0s - loss: 0.0428 - accuracy: 0.98 - ETA: 0s - loss: 0.0423 - accuracy: 0.98 - ETA: 0s - loss: 0.0417 - accuracy: 0.98 - ETA: 0s - loss: 0.0412 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - 7s 612us/step - loss: 0.0407 - accuracy: 0.9860 - val_loss: 0.0178 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00794\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0015 - accuracy: 1.00 - ETA: 6s - loss: 0.0175 - accuracy: 0.98 - ETA: 6s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0175 - accuracy: 0.99 - ETA: 5s - loss: 0.0203 - accuracy: 0.99 - ETA: 5s - loss: 0.0317 - accuracy: 0.99 - ETA: 5s - loss: 0.0323 - accuracy: 0.99 - ETA: 5s - loss: 0.0316 - accuracy: 0.99 - ETA: 5s - loss: 0.0323 - accuracy: 0.99 - ETA: 5s - loss: 0.0335 - accuracy: 0.98 - ETA: 5s - loss: 0.0311 - accuracy: 0.99 - ETA: 5s - loss: 0.0298 - accuracy: 0.99 - ETA: 5s - loss: 0.0301 - accuracy: 0.98 - ETA: 5s - loss: 0.0311 - accuracy: 0.99 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0393 - accuracy: 0.98 - ETA: 4s - loss: 0.0428 - accuracy: 0.98 - ETA: 4s - loss: 0.0417 - accuracy: 0.98 - ETA: 4s - loss: 0.0400 - accuracy: 0.98 - ETA: 4s - loss: 0.0388 - accuracy: 0.98 - ETA: 4s - loss: 0.0386 - accuracy: 0.98 - ETA: 4s - loss: 0.0398 - accuracy: 0.98 - ETA: 4s - loss: 0.0427 - accuracy: 0.98 - ETA: 4s - loss: 0.0429 - accuracy: 0.98 - ETA: 4s - loss: 0.0428 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 3s - loss: 0.0408 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0395 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0408 - accuracy: 0.98 - ETA: 3s - loss: 0.0411 - accuracy: 0.98 - ETA: 3s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0401 - accuracy: 0.98 - ETA: 2s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0414 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0420 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0420 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0410 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 1s - loss: 0.0391 - accuracy: 0.98 - ETA: 1s - loss: 0.0391 - accuracy: 0.98 - ETA: 1s - loss: 0.0394 - accuracy: 0.98 - ETA: 1s - loss: 0.0398 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 0s - loss: 0.0404 - accuracy: 0.98 - ETA: 0s - loss: 0.0403 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - ETA: 0s - loss: 0.0438 - accuracy: 0.98 - ETA: 0s - loss: 0.0435 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.98 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - 7s 612us/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.0219 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00794\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0190 - accuracy: 1.00 - ETA: 6s - loss: 0.0165 - accuracy: 0.99 - ETA: 6s - loss: 0.0196 - accuracy: 0.99 - ETA: 6s - loss: 0.0208 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0194 - accuracy: 0.99 - ETA: 5s - loss: 0.0215 - accuracy: 0.99 - ETA: 5s - loss: 0.0196 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0188 - accuracy: 0.99 - ETA: 5s - loss: 0.0182 - accuracy: 0.99 - ETA: 5s - loss: 0.0170 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 5s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0374 - accuracy: 0.98 - ETA: 4s - loss: 0.0358 - accuracy: 0.98 - ETA: 4s - loss: 0.0343 - accuracy: 0.98 - ETA: 4s - loss: 0.0338 - accuracy: 0.99 - ETA: 4s - loss: 0.0343 - accuracy: 0.98 - ETA: 4s - loss: 0.0377 - accuracy: 0.98 - ETA: 4s - loss: 0.0406 - accuracy: 0.98 - ETA: 4s - loss: 0.0394 - accuracy: 0.98 - ETA: 4s - loss: 0.0384 - accuracy: 0.98 - ETA: 4s - loss: 0.0398 - accuracy: 0.98 - ETA: 4s - loss: 0.0418 - accuracy: 0.98 - ETA: 3s - loss: 0.0411 - accuracy: 0.98 - ETA: 3s - loss: 0.0432 - accuracy: 0.98 - ETA: 3s - loss: 0.0430 - accuracy: 0.98 - ETA: 3s - loss: 0.0418 - accuracy: 0.98 - ETA: 3s - loss: 0.0417 - accuracy: 0.98 - ETA: 3s - loss: 0.0414 - accuracy: 0.98 - ETA: 3s - loss: 0.0411 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0388 - accuracy: 0.98 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0390 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0385 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 2s - loss: 0.0370 - accuracy: 0.98 - ETA: 2s - loss: 0.0366 - accuracy: 0.98 - ETA: 2s - loss: 0.0361 - accuracy: 0.98 - ETA: 2s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0351 - accuracy: 0.98 - ETA: 2s - loss: 0.0352 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0362 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0349 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0439 - accuracy: 0.98 - ETA: 0s - loss: 0.0440 - accuracy: 0.98 - ETA: 0s - loss: 0.0439 - accuracy: 0.98 - ETA: 0s - loss: 0.0436 - accuracy: 0.98 - ETA: 0s - loss: 0.0434 - accuracy: 0.98 - 7s 620us/step - loss: 0.0433 - accuracy: 0.9871 - val_loss: 0.0290 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00794\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0523 - accuracy: 0.98 - ETA: 6s - loss: 0.0324 - accuracy: 0.99 - ETA: 6s - loss: 0.0227 - accuracy: 0.99 - ETA: 5s - loss: 0.0175 - accuracy: 0.99 - ETA: 5s - loss: 0.0147 - accuracy: 0.99 - ETA: 5s - loss: 0.0131 - accuracy: 0.99 - ETA: 5s - loss: 0.0158 - accuracy: 0.99 - ETA: 5s - loss: 0.0169 - accuracy: 0.99 - ETA: 5s - loss: 0.0155 - accuracy: 0.99 - ETA: 5s - loss: 0.0173 - accuracy: 0.99 - ETA: 5s - loss: 0.0194 - accuracy: 0.99 - ETA: 5s - loss: 0.0186 - accuracy: 0.99 - ETA: 5s - loss: 0.0233 - accuracy: 0.99 - ETA: 5s - loss: 0.0287 - accuracy: 0.99 - ETA: 5s - loss: 0.0311 - accuracy: 0.98 - ETA: 5s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0326 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0298 - accuracy: 0.98 - ETA: 4s - loss: 0.0288 - accuracy: 0.98 - ETA: 4s - loss: 0.0277 - accuracy: 0.99 - ETA: 4s - loss: 0.0268 - accuracy: 0.99 - ETA: 4s - loss: 0.0278 - accuracy: 0.98 - ETA: 4s - loss: 0.0272 - accuracy: 0.98 - ETA: 4s - loss: 0.0283 - accuracy: 0.98 - ETA: 3s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0300 - accuracy: 0.98 - ETA: 3s - loss: 0.0298 - accuracy: 0.98 - ETA: 3s - loss: 0.0296 - accuracy: 0.98 - ETA: 3s - loss: 0.0290 - accuracy: 0.98 - ETA: 3s - loss: 0.0284 - accuracy: 0.98 - ETA: 3s - loss: 0.0281 - accuracy: 0.98 - ETA: 3s - loss: 0.0294 - accuracy: 0.98 - ETA: 3s - loss: 0.0299 - accuracy: 0.98 - ETA: 3s - loss: 0.0295 - accuracy: 0.98 - ETA: 3s - loss: 0.0290 - accuracy: 0.98 - ETA: 2s - loss: 0.0288 - accuracy: 0.98 - ETA: 2s - loss: 0.0286 - accuracy: 0.98 - ETA: 2s - loss: 0.0300 - accuracy: 0.98 - ETA: 2s - loss: 0.0297 - accuracy: 0.98 - ETA: 2s - loss: 0.0299 - accuracy: 0.98 - ETA: 2s - loss: 0.0299 - accuracy: 0.98 - ETA: 2s - loss: 0.0305 - accuracy: 0.98 - ETA: 2s - loss: 0.0315 - accuracy: 0.98 - ETA: 2s - loss: 0.0312 - accuracy: 0.98 - ETA: 2s - loss: 0.0309 - accuracy: 0.98 - ETA: 2s - loss: 0.0304 - accuracy: 0.98 - ETA: 2s - loss: 0.0300 - accuracy: 0.98 - ETA: 1s - loss: 0.0297 - accuracy: 0.98 - ETA: 1s - loss: 0.0294 - accuracy: 0.98 - ETA: 1s - loss: 0.0301 - accuracy: 0.98 - ETA: 1s - loss: 0.0302 - accuracy: 0.98 - ETA: 1s - loss: 0.0306 - accuracy: 0.98 - ETA: 1s - loss: 0.0304 - accuracy: 0.98 - ETA: 1s - loss: 0.0306 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0301 - accuracy: 0.98 - ETA: 1s - loss: 0.0297 - accuracy: 0.98 - ETA: 1s - loss: 0.0294 - accuracy: 0.98 - ETA: 1s - loss: 0.0295 - accuracy: 0.98 - ETA: 0s - loss: 0.0293 - accuracy: 0.98 - ETA: 0s - loss: 0.0300 - accuracy: 0.98 - ETA: 0s - loss: 0.0302 - accuracy: 0.98 - ETA: 0s - loss: 0.0306 - accuracy: 0.98 - ETA: 0s - loss: 0.0303 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.98 - ETA: 0s - loss: 0.0300 - accuracy: 0.98 - ETA: 0s - loss: 0.0300 - accuracy: 0.98 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0297 - accuracy: 0.98 - ETA: 0s - loss: 0.0296 - accuracy: 0.98 - 7s 615us/step - loss: 0.0296 - accuracy: 0.9896 - val_loss: 0.0196 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00794\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 7s - loss: 0.0516 - accuracy: 0.97 - ETA: 6s - loss: 0.1165 - accuracy: 0.97 - ETA: 6s - loss: 0.1105 - accuracy: 0.97 - ETA: 5s - loss: 0.1062 - accuracy: 0.97 - ETA: 5s - loss: 0.0881 - accuracy: 0.97 - ETA: 5s - loss: 0.0746 - accuracy: 0.98 - ETA: 5s - loss: 0.0672 - accuracy: 0.98 - ETA: 5s - loss: 0.0599 - accuracy: 0.98 - ETA: 5s - loss: 0.0567 - accuracy: 0.98 - ETA: 5s - loss: 0.0565 - accuracy: 0.98 - ETA: 5s - loss: 0.0539 - accuracy: 0.98 - ETA: 5s - loss: 0.0502 - accuracy: 0.98 - ETA: 5s - loss: 0.0472 - accuracy: 0.98 - ETA: 5s - loss: 0.0446 - accuracy: 0.98 - ETA: 5s - loss: 0.0422 - accuracy: 0.98 - ETA: 4s - loss: 0.0401 - accuracy: 0.98 - ETA: 4s - loss: 0.0402 - accuracy: 0.98 - ETA: 4s - loss: 0.0388 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0387 - accuracy: 0.98 - ETA: 4s - loss: 0.0417 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0429 - accuracy: 0.98 - ETA: 4s - loss: 0.0417 - accuracy: 0.98 - ETA: 4s - loss: 0.0405 - accuracy: 0.98 - ETA: 4s - loss: 0.0395 - accuracy: 0.98 - ETA: 4s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0385 - accuracy: 0.98 - ETA: 3s - loss: 0.0377 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0429 - accuracy: 0.98 - ETA: 3s - loss: 0.0460 - accuracy: 0.98 - ETA: 3s - loss: 0.0453 - accuracy: 0.98 - ETA: 3s - loss: 0.0450 - accuracy: 0.98 - ETA: 2s - loss: 0.0441 - accuracy: 0.98 - ETA: 2s - loss: 0.0432 - accuracy: 0.98 - ETA: 2s - loss: 0.0422 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0410 - accuracy: 0.98 - ETA: 2s - loss: 0.0415 - accuracy: 0.98 - ETA: 2s - loss: 0.0408 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0399 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0382 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0370 - accuracy: 0.98 - ETA: 1s - loss: 0.0373 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 1s - loss: 0.0388 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0398 - accuracy: 0.98 - ETA: 0s - loss: 0.0420 - accuracy: 0.98 - ETA: 0s - loss: 0.0421 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0415 - accuracy: 0.98 - ETA: 0s - loss: 0.0413 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0407 - accuracy: 0.98 - ETA: 0s - loss: 0.0402 - accuracy: 0.98 - ETA: 0s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0390 - accuracy: 0.98 - 7s 617us/step - loss: 0.0388 - accuracy: 0.9869 - val_loss: 0.0194 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00794\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0103 - accuracy: 1.00 - ETA: 6s - loss: 0.0152 - accuracy: 0.99 - ETA: 5s - loss: 0.0157 - accuracy: 0.99 - ETA: 5s - loss: 0.0136 - accuracy: 0.99 - ETA: 5s - loss: 0.0159 - accuracy: 0.99 - ETA: 5s - loss: 0.0304 - accuracy: 0.98 - ETA: 5s - loss: 0.0387 - accuracy: 0.98 - ETA: 5s - loss: 0.0361 - accuracy: 0.98 - ETA: 5s - loss: 0.0351 - accuracy: 0.98 - ETA: 5s - loss: 0.0348 - accuracy: 0.98 - ETA: 5s - loss: 0.0370 - accuracy: 0.98 - ETA: 5s - loss: 0.0341 - accuracy: 0.98 - ETA: 5s - loss: 0.0316 - accuracy: 0.98 - ETA: 5s - loss: 0.0332 - accuracy: 0.98 - ETA: 4s - loss: 0.0314 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0298 - accuracy: 0.98 - ETA: 4s - loss: 0.0284 - accuracy: 0.99 - ETA: 4s - loss: 0.0279 - accuracy: 0.99 - ETA: 4s - loss: 0.0272 - accuracy: 0.99 - ETA: 4s - loss: 0.0267 - accuracy: 0.99 - ETA: 4s - loss: 0.0278 - accuracy: 0.99 - ETA: 4s - loss: 0.0293 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0318 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 3s - loss: 0.0304 - accuracy: 0.98 - ETA: 3s - loss: 0.0297 - accuracy: 0.98 - ETA: 3s - loss: 0.0294 - accuracy: 0.98 - ETA: 3s - loss: 0.0288 - accuracy: 0.98 - ETA: 3s - loss: 0.0285 - accuracy: 0.98 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0287 - accuracy: 0.98 - ETA: 3s - loss: 0.0284 - accuracy: 0.99 - ETA: 3s - loss: 0.0283 - accuracy: 0.99 - ETA: 3s - loss: 0.0277 - accuracy: 0.99 - ETA: 3s - loss: 0.0271 - accuracy: 0.99 - ETA: 3s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0304 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0297 - accuracy: 0.98 - ETA: 2s - loss: 0.0292 - accuracy: 0.99 - ETA: 2s - loss: 0.0286 - accuracy: 0.99 - ETA: 2s - loss: 0.0290 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.98 - ETA: 2s - loss: 0.0292 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0327 - accuracy: 0.98 - ETA: 1s - loss: 0.0324 - accuracy: 0.98 - ETA: 1s - loss: 0.0319 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0310 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0302 - accuracy: 0.99 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0327 - accuracy: 0.98 - ETA: 0s - loss: 0.0325 - accuracy: 0.98 - ETA: 0s - loss: 0.0326 - accuracy: 0.98 - ETA: 0s - loss: 0.0328 - accuracy: 0.98 - ETA: 0s - loss: 0.0328 - accuracy: 0.98 - ETA: 0s - loss: 0.0334 - accuracy: 0.98 - 7s 617us/step - loss: 0.0333 - accuracy: 0.9890 - val_loss: 0.0331 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00794\n",
      "Epoch 00022: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 223us/step\n",
      "[2020-05-18 16:33:29 RAM73.3% 1.55GB] Val Score : [0.026386250134372492, 0.9893859028816223]\n",
      "[2020-05-18 16:33:29 RAM73.3% 1.55GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:33:29 RAM73.3% 1.55GB] Training on Fold : 8\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 22s - loss: 4.0924 - accuracy: 0.558 - ETA: 14s - loss: 4.9597 - accuracy: 0.462 - ETA: 11s - loss: 3.6699 - accuracy: 0.508 - ETA: 9s - loss: 2.9699 - accuracy: 0.512 - ETA: 8s - loss: 2.5615 - accuracy: 0.52 - ETA: 8s - loss: 2.2781 - accuracy: 0.52 - ETA: 7s - loss: 2.0576 - accuracy: 0.53 - ETA: 7s - loss: 1.8929 - accuracy: 0.54 - ETA: 7s - loss: 1.7606 - accuracy: 0.55 - ETA: 6s - loss: 1.6567 - accuracy: 0.55 - ETA: 6s - loss: 1.5659 - accuracy: 0.56 - ETA: 6s - loss: 1.4903 - accuracy: 0.56 - ETA: 6s - loss: 1.4284 - accuracy: 0.57 - ETA: 5s - loss: 1.3750 - accuracy: 0.57 - ETA: 5s - loss: 1.3360 - accuracy: 0.57 - ETA: 5s - loss: 1.2960 - accuracy: 0.57 - ETA: 5s - loss: 1.2575 - accuracy: 0.57 - ETA: 5s - loss: 1.2217 - accuracy: 0.58 - ETA: 5s - loss: 1.1878 - accuracy: 0.59 - ETA: 5s - loss: 1.1580 - accuracy: 0.59 - ETA: 4s - loss: 1.1289 - accuracy: 0.59 - ETA: 4s - loss: 1.1054 - accuracy: 0.60 - ETA: 4s - loss: 1.0805 - accuracy: 0.60 - ETA: 4s - loss: 1.0619 - accuracy: 0.61 - ETA: 4s - loss: 1.0458 - accuracy: 0.60 - ETA: 4s - loss: 1.0295 - accuracy: 0.61 - ETA: 4s - loss: 1.0133 - accuracy: 0.61 - ETA: 4s - loss: 0.9969 - accuracy: 0.61 - ETA: 4s - loss: 0.9788 - accuracy: 0.62 - ETA: 3s - loss: 0.9634 - accuracy: 0.62 - ETA: 3s - loss: 0.9477 - accuracy: 0.62 - ETA: 3s - loss: 0.9325 - accuracy: 0.63 - ETA: 3s - loss: 0.9205 - accuracy: 0.63 - ETA: 3s - loss: 0.9166 - accuracy: 0.63 - ETA: 3s - loss: 0.9121 - accuracy: 0.63 - ETA: 3s - loss: 0.9051 - accuracy: 0.63 - ETA: 3s - loss: 0.8934 - accuracy: 0.63 - ETA: 3s - loss: 0.8821 - accuracy: 0.63 - ETA: 3s - loss: 0.8719 - accuracy: 0.64 - ETA: 2s - loss: 0.8610 - accuracy: 0.64 - ETA: 2s - loss: 0.8511 - accuracy: 0.64 - ETA: 2s - loss: 0.8404 - accuracy: 0.65 - ETA: 2s - loss: 0.8324 - accuracy: 0.65 - ETA: 2s - loss: 0.8230 - accuracy: 0.65 - ETA: 2s - loss: 0.8128 - accuracy: 0.66 - ETA: 2s - loss: 0.8036 - accuracy: 0.66 - ETA: 2s - loss: 0.7967 - accuracy: 0.66 - ETA: 2s - loss: 0.7884 - accuracy: 0.67 - ETA: 2s - loss: 0.7829 - accuracy: 0.67 - ETA: 2s - loss: 0.7855 - accuracy: 0.67 - ETA: 1s - loss: 0.7802 - accuracy: 0.67 - ETA: 1s - loss: 0.7727 - accuracy: 0.67 - ETA: 1s - loss: 0.7650 - accuracy: 0.67 - ETA: 1s - loss: 0.7581 - accuracy: 0.68 - ETA: 1s - loss: 0.7509 - accuracy: 0.68 - ETA: 1s - loss: 0.7425 - accuracy: 0.68 - ETA: 1s - loss: 0.7358 - accuracy: 0.69 - ETA: 1s - loss: 0.7298 - accuracy: 0.69 - ETA: 1s - loss: 0.7288 - accuracy: 0.69 - ETA: 1s - loss: 0.7237 - accuracy: 0.69 - ETA: 1s - loss: 0.7186 - accuracy: 0.69 - ETA: 1s - loss: 0.7135 - accuracy: 0.69 - ETA: 0s - loss: 0.7070 - accuracy: 0.70 - ETA: 0s - loss: 0.7019 - accuracy: 0.70 - ETA: 0s - loss: 0.6972 - accuracy: 0.70 - ETA: 0s - loss: 0.6943 - accuracy: 0.70 - ETA: 0s - loss: 0.6907 - accuracy: 0.70 - ETA: 0s - loss: 0.6900 - accuracy: 0.70 - ETA: 0s - loss: 0.6844 - accuracy: 0.70 - ETA: 0s - loss: 0.6782 - accuracy: 0.71 - ETA: 0s - loss: 0.6726 - accuracy: 0.71 - ETA: 0s - loss: 0.6675 - accuracy: 0.71 - ETA: 0s - loss: 0.6628 - accuracy: 0.71 - 7s 614us/step - loss: 0.6598 - accuracy: 0.7199 - val_loss: 0.2659 - val_accuracy: 0.9065\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00794\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.3126 - accuracy: 0.86 - ETA: 5s - loss: 0.2829 - accuracy: 0.88 - ETA: 5s - loss: 0.2713 - accuracy: 0.89 - ETA: 5s - loss: 0.2633 - accuracy: 0.89 - ETA: 5s - loss: 0.2636 - accuracy: 0.89 - ETA: 5s - loss: 0.2784 - accuracy: 0.87 - ETA: 5s - loss: 0.3094 - accuracy: 0.87 - ETA: 5s - loss: 0.3112 - accuracy: 0.86 - ETA: 5s - loss: 0.3007 - accuracy: 0.87 - ETA: 5s - loss: 0.2981 - accuracy: 0.87 - ETA: 5s - loss: 0.2943 - accuracy: 0.87 - ETA: 5s - loss: 0.2930 - accuracy: 0.88 - ETA: 5s - loss: 0.2917 - accuracy: 0.88 - ETA: 4s - loss: 0.2938 - accuracy: 0.88 - ETA: 4s - loss: 0.2894 - accuracy: 0.88 - ETA: 4s - loss: 0.2865 - accuracy: 0.88 - ETA: 4s - loss: 0.2837 - accuracy: 0.88 - ETA: 4s - loss: 0.2861 - accuracy: 0.88 - ETA: 4s - loss: 0.2922 - accuracy: 0.88 - ETA: 4s - loss: 0.2960 - accuracy: 0.88 - ETA: 4s - loss: 0.2922 - accuracy: 0.88 - ETA: 4s - loss: 0.2891 - accuracy: 0.88 - ETA: 4s - loss: 0.2933 - accuracy: 0.88 - ETA: 4s - loss: 0.3076 - accuracy: 0.87 - ETA: 4s - loss: 0.3060 - accuracy: 0.87 - ETA: 3s - loss: 0.3009 - accuracy: 0.88 - ETA: 3s - loss: 0.2985 - accuracy: 0.88 - ETA: 3s - loss: 0.2962 - accuracy: 0.88 - ETA: 3s - loss: 0.2922 - accuracy: 0.88 - ETA: 3s - loss: 0.2905 - accuracy: 0.88 - ETA: 3s - loss: 0.2863 - accuracy: 0.88 - ETA: 3s - loss: 0.2864 - accuracy: 0.88 - ETA: 3s - loss: 0.2836 - accuracy: 0.88 - ETA: 3s - loss: 0.2821 - accuracy: 0.89 - ETA: 3s - loss: 0.2810 - accuracy: 0.89 - ETA: 3s - loss: 0.2807 - accuracy: 0.89 - ETA: 3s - loss: 0.2805 - accuracy: 0.88 - ETA: 3s - loss: 0.2793 - accuracy: 0.88 - ETA: 2s - loss: 0.2797 - accuracy: 0.88 - ETA: 2s - loss: 0.2786 - accuracy: 0.88 - ETA: 2s - loss: 0.2789 - accuracy: 0.88 - ETA: 2s - loss: 0.2837 - accuracy: 0.88 - ETA: 2s - loss: 0.2859 - accuracy: 0.88 - ETA: 2s - loss: 0.2860 - accuracy: 0.88 - ETA: 2s - loss: 0.2842 - accuracy: 0.88 - ETA: 2s - loss: 0.2816 - accuracy: 0.88 - ETA: 2s - loss: 0.2793 - accuracy: 0.88 - ETA: 2s - loss: 0.2767 - accuracy: 0.88 - ETA: 2s - loss: 0.2746 - accuracy: 0.88 - ETA: 1s - loss: 0.2722 - accuracy: 0.89 - ETA: 1s - loss: 0.2708 - accuracy: 0.89 - ETA: 1s - loss: 0.2696 - accuracy: 0.89 - ETA: 1s - loss: 0.2681 - accuracy: 0.89 - ETA: 1s - loss: 0.2665 - accuracy: 0.89 - ETA: 1s - loss: 0.2647 - accuracy: 0.89 - ETA: 1s - loss: 0.2625 - accuracy: 0.89 - ETA: 1s - loss: 0.2604 - accuracy: 0.89 - ETA: 1s - loss: 0.2594 - accuracy: 0.89 - ETA: 1s - loss: 0.2593 - accuracy: 0.89 - ETA: 1s - loss: 0.2601 - accuracy: 0.89 - ETA: 1s - loss: 0.2612 - accuracy: 0.89 - ETA: 0s - loss: 0.2599 - accuracy: 0.89 - ETA: 0s - loss: 0.2577 - accuracy: 0.89 - ETA: 0s - loss: 0.2572 - accuracy: 0.89 - ETA: 0s - loss: 0.2558 - accuracy: 0.89 - ETA: 0s - loss: 0.2538 - accuracy: 0.89 - ETA: 0s - loss: 0.2528 - accuracy: 0.89 - ETA: 0s - loss: 0.2523 - accuracy: 0.89 - ETA: 0s - loss: 0.2506 - accuracy: 0.89 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2512 - accuracy: 0.89 - ETA: 0s - loss: 0.2516 - accuracy: 0.89 - 6s 600us/step - loss: 0.2511 - accuracy: 0.8992 - val_loss: 0.1623 - val_accuracy: 0.9393\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00794\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.1922 - accuracy: 0.89 - ETA: 5s - loss: 0.2489 - accuracy: 0.90 - ETA: 5s - loss: 0.2220 - accuracy: 0.91 - ETA: 5s - loss: 0.2008 - accuracy: 0.92 - ETA: 5s - loss: 0.1806 - accuracy: 0.93 - ETA: 5s - loss: 0.1834 - accuracy: 0.92 - ETA: 5s - loss: 0.1760 - accuracy: 0.93 - ETA: 5s - loss: 0.1773 - accuracy: 0.93 - ETA: 5s - loss: 0.1757 - accuracy: 0.93 - ETA: 5s - loss: 0.1779 - accuracy: 0.93 - ETA: 5s - loss: 0.1708 - accuracy: 0.93 - ETA: 5s - loss: 0.1687 - accuracy: 0.93 - ETA: 5s - loss: 0.1673 - accuracy: 0.93 - ETA: 4s - loss: 0.1701 - accuracy: 0.93 - ETA: 4s - loss: 0.1678 - accuracy: 0.93 - ETA: 4s - loss: 0.1645 - accuracy: 0.93 - ETA: 4s - loss: 0.1659 - accuracy: 0.93 - ETA: 4s - loss: 0.1668 - accuracy: 0.93 - ETA: 4s - loss: 0.1711 - accuracy: 0.93 - ETA: 4s - loss: 0.1757 - accuracy: 0.92 - ETA: 4s - loss: 0.1788 - accuracy: 0.92 - ETA: 4s - loss: 0.1837 - accuracy: 0.92 - ETA: 4s - loss: 0.1886 - accuracy: 0.92 - ETA: 4s - loss: 0.1894 - accuracy: 0.92 - ETA: 4s - loss: 0.1874 - accuracy: 0.92 - ETA: 3s - loss: 0.1854 - accuracy: 0.92 - ETA: 3s - loss: 0.1843 - accuracy: 0.92 - ETA: 3s - loss: 0.1825 - accuracy: 0.92 - ETA: 3s - loss: 0.1820 - accuracy: 0.92 - ETA: 3s - loss: 0.1795 - accuracy: 0.92 - ETA: 3s - loss: 0.1804 - accuracy: 0.92 - ETA: 3s - loss: 0.1802 - accuracy: 0.92 - ETA: 3s - loss: 0.1779 - accuracy: 0.92 - ETA: 3s - loss: 0.1773 - accuracy: 0.92 - ETA: 3s - loss: 0.1756 - accuracy: 0.92 - ETA: 3s - loss: 0.1727 - accuracy: 0.92 - ETA: 3s - loss: 0.1702 - accuracy: 0.92 - ETA: 2s - loss: 0.1673 - accuracy: 0.92 - ETA: 2s - loss: 0.1688 - accuracy: 0.92 - ETA: 2s - loss: 0.1801 - accuracy: 0.92 - ETA: 2s - loss: 0.1890 - accuracy: 0.92 - ETA: 2s - loss: 0.1926 - accuracy: 0.91 - ETA: 2s - loss: 0.1962 - accuracy: 0.91 - ETA: 2s - loss: 0.1962 - accuracy: 0.91 - ETA: 2s - loss: 0.1955 - accuracy: 0.91 - ETA: 2s - loss: 0.1947 - accuracy: 0.91 - ETA: 2s - loss: 0.1924 - accuracy: 0.91 - ETA: 2s - loss: 0.1916 - accuracy: 0.92 - ETA: 2s - loss: 0.1897 - accuracy: 0.92 - ETA: 1s - loss: 0.1873 - accuracy: 0.92 - ETA: 1s - loss: 0.1857 - accuracy: 0.92 - ETA: 1s - loss: 0.1843 - accuracy: 0.92 - ETA: 1s - loss: 0.1837 - accuracy: 0.92 - ETA: 1s - loss: 0.1822 - accuracy: 0.92 - ETA: 1s - loss: 0.1801 - accuracy: 0.92 - ETA: 1s - loss: 0.1794 - accuracy: 0.92 - ETA: 1s - loss: 0.1792 - accuracy: 0.92 - ETA: 1s - loss: 0.1791 - accuracy: 0.92 - ETA: 1s - loss: 0.1784 - accuracy: 0.92 - ETA: 1s - loss: 0.1771 - accuracy: 0.92 - ETA: 1s - loss: 0.1759 - accuracy: 0.92 - ETA: 0s - loss: 0.1751 - accuracy: 0.92 - ETA: 0s - loss: 0.1745 - accuracy: 0.92 - ETA: 0s - loss: 0.1733 - accuracy: 0.92 - ETA: 0s - loss: 0.1717 - accuracy: 0.93 - ETA: 0s - loss: 0.1708 - accuracy: 0.93 - ETA: 0s - loss: 0.1698 - accuracy: 0.93 - ETA: 0s - loss: 0.1707 - accuracy: 0.93 - ETA: 0s - loss: 0.1730 - accuracy: 0.92 - ETA: 0s - loss: 0.1732 - accuracy: 0.92 - ETA: 0s - loss: 0.1726 - accuracy: 0.92 - ETA: 0s - loss: 0.1719 - accuracy: 0.92 - ETA: 0s - loss: 0.1717 - accuracy: 0.92 - 6s 591us/step - loss: 0.1711 - accuracy: 0.9295 - val_loss: 0.1284 - val_accuracy: 0.9579\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00794\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1698 - accuracy: 0.93 - ETA: 6s - loss: 0.1244 - accuracy: 0.95 - ETA: 5s - loss: 0.1105 - accuracy: 0.95 - ETA: 5s - loss: 0.1102 - accuracy: 0.95 - ETA: 5s - loss: 0.1257 - accuracy: 0.94 - ETA: 5s - loss: 0.1264 - accuracy: 0.94 - ETA: 5s - loss: 0.1227 - accuracy: 0.95 - ETA: 5s - loss: 0.1246 - accuracy: 0.95 - ETA: 5s - loss: 0.1241 - accuracy: 0.95 - ETA: 5s - loss: 0.1247 - accuracy: 0.95 - ETA: 5s - loss: 0.1253 - accuracy: 0.95 - ETA: 5s - loss: 0.1246 - accuracy: 0.95 - ETA: 5s - loss: 0.1221 - accuracy: 0.95 - ETA: 5s - loss: 0.1222 - accuracy: 0.95 - ETA: 4s - loss: 0.1237 - accuracy: 0.95 - ETA: 4s - loss: 0.1223 - accuracy: 0.95 - ETA: 4s - loss: 0.1205 - accuracy: 0.95 - ETA: 4s - loss: 0.1207 - accuracy: 0.95 - ETA: 4s - loss: 0.1208 - accuracy: 0.95 - ETA: 4s - loss: 0.1219 - accuracy: 0.95 - ETA: 4s - loss: 0.1226 - accuracy: 0.95 - ETA: 4s - loss: 0.1240 - accuracy: 0.95 - ETA: 4s - loss: 0.1239 - accuracy: 0.95 - ETA: 4s - loss: 0.1225 - accuracy: 0.95 - ETA: 4s - loss: 0.1210 - accuracy: 0.95 - ETA: 3s - loss: 0.1211 - accuracy: 0.95 - ETA: 3s - loss: 0.1302 - accuracy: 0.95 - ETA: 3s - loss: 0.1444 - accuracy: 0.94 - ETA: 3s - loss: 0.1492 - accuracy: 0.94 - ETA: 3s - loss: 0.1482 - accuracy: 0.94 - ETA: 3s - loss: 0.1466 - accuracy: 0.94 - ETA: 3s - loss: 0.1458 - accuracy: 0.94 - ETA: 3s - loss: 0.1443 - accuracy: 0.94 - ETA: 3s - loss: 0.1420 - accuracy: 0.94 - ETA: 3s - loss: 0.1411 - accuracy: 0.94 - ETA: 3s - loss: 0.1393 - accuracy: 0.94 - ETA: 3s - loss: 0.1374 - accuracy: 0.94 - ETA: 2s - loss: 0.1351 - accuracy: 0.94 - ETA: 2s - loss: 0.1338 - accuracy: 0.94 - ETA: 2s - loss: 0.1316 - accuracy: 0.94 - ETA: 2s - loss: 0.1298 - accuracy: 0.94 - ETA: 2s - loss: 0.1311 - accuracy: 0.94 - ETA: 2s - loss: 0.1312 - accuracy: 0.94 - ETA: 2s - loss: 0.1341 - accuracy: 0.94 - ETA: 2s - loss: 0.1330 - accuracy: 0.94 - ETA: 2s - loss: 0.1317 - accuracy: 0.95 - ETA: 2s - loss: 0.1321 - accuracy: 0.95 - ETA: 2s - loss: 0.1315 - accuracy: 0.95 - ETA: 2s - loss: 0.1306 - accuracy: 0.95 - ETA: 1s - loss: 0.1302 - accuracy: 0.95 - ETA: 1s - loss: 0.1289 - accuracy: 0.95 - ETA: 1s - loss: 0.1277 - accuracy: 0.95 - ETA: 1s - loss: 0.1279 - accuracy: 0.95 - ETA: 1s - loss: 0.1302 - accuracy: 0.94 - ETA: 1s - loss: 0.1320 - accuracy: 0.94 - ETA: 1s - loss: 0.1324 - accuracy: 0.94 - ETA: 1s - loss: 0.1311 - accuracy: 0.94 - ETA: 1s - loss: 0.1308 - accuracy: 0.94 - ETA: 1s - loss: 0.1298 - accuracy: 0.94 - ETA: 1s - loss: 0.1289 - accuracy: 0.94 - ETA: 1s - loss: 0.1289 - accuracy: 0.94 - ETA: 0s - loss: 0.1290 - accuracy: 0.94 - ETA: 0s - loss: 0.1281 - accuracy: 0.94 - ETA: 0s - loss: 0.1267 - accuracy: 0.95 - ETA: 0s - loss: 0.1255 - accuracy: 0.95 - ETA: 0s - loss: 0.1257 - accuracy: 0.95 - ETA: 0s - loss: 0.1253 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1249 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1256 - accuracy: 0.95 - ETA: 0s - loss: 0.1253 - accuracy: 0.95 - ETA: 0s - loss: 0.1252 - accuracy: 0.95 - 6s 609us/step - loss: 0.1253 - accuracy: 0.9511 - val_loss: 0.0667 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00794\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0822 - accuracy: 0.95 - ETA: 6s - loss: 0.0781 - accuracy: 0.96 - ETA: 6s - loss: 0.0909 - accuracy: 0.96 - ETA: 5s - loss: 0.1043 - accuracy: 0.95 - ETA: 5s - loss: 0.1031 - accuracy: 0.95 - ETA: 5s - loss: 0.1001 - accuracy: 0.95 - ETA: 5s - loss: 0.0967 - accuracy: 0.95 - ETA: 5s - loss: 0.0969 - accuracy: 0.95 - ETA: 5s - loss: 0.0932 - accuracy: 0.95 - ETA: 5s - loss: 0.0895 - accuracy: 0.96 - ETA: 5s - loss: 0.0860 - accuracy: 0.96 - ETA: 5s - loss: 0.0892 - accuracy: 0.96 - ETA: 5s - loss: 0.0867 - accuracy: 0.96 - ETA: 5s - loss: 0.0855 - accuracy: 0.96 - ETA: 5s - loss: 0.0870 - accuracy: 0.96 - ETA: 4s - loss: 0.0950 - accuracy: 0.96 - ETA: 4s - loss: 0.0976 - accuracy: 0.96 - ETA: 4s - loss: 0.1002 - accuracy: 0.96 - ETA: 4s - loss: 0.0969 - accuracy: 0.96 - ETA: 4s - loss: 0.0940 - accuracy: 0.96 - ETA: 4s - loss: 0.0923 - accuracy: 0.96 - ETA: 4s - loss: 0.0924 - accuracy: 0.96 - ETA: 4s - loss: 0.0904 - accuracy: 0.96 - ETA: 4s - loss: 0.0892 - accuracy: 0.96 - ETA: 4s - loss: 0.0869 - accuracy: 0.96 - ETA: 4s - loss: 0.0897 - accuracy: 0.96 - ETA: 3s - loss: 0.0959 - accuracy: 0.96 - ETA: 3s - loss: 0.1010 - accuracy: 0.96 - ETA: 3s - loss: 0.1039 - accuracy: 0.96 - ETA: 3s - loss: 0.1030 - accuracy: 0.96 - ETA: 3s - loss: 0.1016 - accuracy: 0.96 - ETA: 3s - loss: 0.1002 - accuracy: 0.96 - ETA: 3s - loss: 0.0992 - accuracy: 0.96 - ETA: 3s - loss: 0.0983 - accuracy: 0.96 - ETA: 3s - loss: 0.0967 - accuracy: 0.96 - ETA: 3s - loss: 0.0963 - accuracy: 0.96 - ETA: 3s - loss: 0.0950 - accuracy: 0.96 - ETA: 3s - loss: 0.0932 - accuracy: 0.96 - ETA: 2s - loss: 0.0927 - accuracy: 0.96 - ETA: 2s - loss: 0.0929 - accuracy: 0.96 - ETA: 2s - loss: 0.0919 - accuracy: 0.96 - ETA: 2s - loss: 0.0944 - accuracy: 0.96 - ETA: 2s - loss: 0.0948 - accuracy: 0.96 - ETA: 2s - loss: 0.0936 - accuracy: 0.96 - ETA: 2s - loss: 0.0928 - accuracy: 0.96 - ETA: 2s - loss: 0.0920 - accuracy: 0.96 - ETA: 2s - loss: 0.0920 - accuracy: 0.96 - ETA: 2s - loss: 0.0939 - accuracy: 0.96 - ETA: 2s - loss: 0.0942 - accuracy: 0.96 - ETA: 2s - loss: 0.0941 - accuracy: 0.96 - ETA: 1s - loss: 0.0933 - accuracy: 0.96 - ETA: 1s - loss: 0.0924 - accuracy: 0.96 - ETA: 1s - loss: 0.0916 - accuracy: 0.96 - ETA: 1s - loss: 0.0905 - accuracy: 0.96 - ETA: 1s - loss: 0.0896 - accuracy: 0.96 - ETA: 1s - loss: 0.0894 - accuracy: 0.96 - ETA: 1s - loss: 0.0912 - accuracy: 0.96 - ETA: 1s - loss: 0.0914 - accuracy: 0.96 - ETA: 1s - loss: 0.0915 - accuracy: 0.96 - ETA: 1s - loss: 0.0914 - accuracy: 0.96 - ETA: 1s - loss: 0.0907 - accuracy: 0.96 - ETA: 1s - loss: 0.0897 - accuracy: 0.96 - ETA: 0s - loss: 0.0893 - accuracy: 0.96 - ETA: 0s - loss: 0.0908 - accuracy: 0.96 - ETA: 0s - loss: 0.0916 - accuracy: 0.96 - ETA: 0s - loss: 0.0918 - accuracy: 0.96 - ETA: 0s - loss: 0.0919 - accuracy: 0.96 - ETA: 0s - loss: 0.0926 - accuracy: 0.96 - ETA: 0s - loss: 0.0931 - accuracy: 0.96 - ETA: 0s - loss: 0.0939 - accuracy: 0.96 - ETA: 0s - loss: 0.0934 - accuracy: 0.96 - ETA: 0s - loss: 0.0937 - accuracy: 0.96 - ETA: 0s - loss: 0.0932 - accuracy: 0.96 - 7s 617us/step - loss: 0.0927 - accuracy: 0.9651 - val_loss: 0.0462 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00794\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0378 - accuracy: 0.99 - ETA: 6s - loss: 0.0323 - accuracy: 0.98 - ETA: 6s - loss: 0.0338 - accuracy: 0.98 - ETA: 6s - loss: 0.0422 - accuracy: 0.98 - ETA: 5s - loss: 0.0466 - accuracy: 0.98 - ETA: 5s - loss: 0.0478 - accuracy: 0.98 - ETA: 5s - loss: 0.0460 - accuracy: 0.98 - ETA: 5s - loss: 0.0457 - accuracy: 0.98 - ETA: 5s - loss: 0.0470 - accuracy: 0.98 - ETA: 5s - loss: 0.0475 - accuracy: 0.98 - ETA: 5s - loss: 0.0446 - accuracy: 0.98 - ETA: 5s - loss: 0.0445 - accuracy: 0.98 - ETA: 5s - loss: 0.0457 - accuracy: 0.98 - ETA: 5s - loss: 0.0543 - accuracy: 0.97 - ETA: 5s - loss: 0.0809 - accuracy: 0.97 - ETA: 5s - loss: 0.0918 - accuracy: 0.96 - ETA: 4s - loss: 0.0911 - accuracy: 0.96 - ETA: 4s - loss: 0.0898 - accuracy: 0.96 - ETA: 4s - loss: 0.0871 - accuracy: 0.96 - ETA: 4s - loss: 0.0863 - accuracy: 0.96 - ETA: 4s - loss: 0.0840 - accuracy: 0.97 - ETA: 4s - loss: 0.0831 - accuracy: 0.97 - ETA: 4s - loss: 0.0820 - accuracy: 0.97 - ETA: 4s - loss: 0.0805 - accuracy: 0.97 - ETA: 4s - loss: 0.0797 - accuracy: 0.97 - ETA: 4s - loss: 0.0775 - accuracy: 0.97 - ETA: 4s - loss: 0.0765 - accuracy: 0.97 - ETA: 4s - loss: 0.0747 - accuracy: 0.97 - ETA: 3s - loss: 0.0744 - accuracy: 0.97 - ETA: 3s - loss: 0.0743 - accuracy: 0.97 - ETA: 3s - loss: 0.0741 - accuracy: 0.97 - ETA: 3s - loss: 0.0733 - accuracy: 0.97 - ETA: 3s - loss: 0.0736 - accuracy: 0.97 - ETA: 3s - loss: 0.0737 - accuracy: 0.97 - ETA: 3s - loss: 0.0736 - accuracy: 0.97 - ETA: 3s - loss: 0.0726 - accuracy: 0.97 - ETA: 3s - loss: 0.0718 - accuracy: 0.97 - ETA: 3s - loss: 0.0705 - accuracy: 0.97 - ETA: 3s - loss: 0.0709 - accuracy: 0.97 - ETA: 2s - loss: 0.0694 - accuracy: 0.97 - ETA: 2s - loss: 0.0696 - accuracy: 0.97 - ETA: 2s - loss: 0.0691 - accuracy: 0.97 - ETA: 2s - loss: 0.0687 - accuracy: 0.97 - ETA: 2s - loss: 0.0681 - accuracy: 0.97 - ETA: 2s - loss: 0.0681 - accuracy: 0.97 - ETA: 2s - loss: 0.0675 - accuracy: 0.97 - ETA: 2s - loss: 0.0673 - accuracy: 0.97 - ETA: 2s - loss: 0.0693 - accuracy: 0.97 - ETA: 2s - loss: 0.0725 - accuracy: 0.97 - ETA: 2s - loss: 0.0736 - accuracy: 0.97 - ETA: 1s - loss: 0.0729 - accuracy: 0.97 - ETA: 1s - loss: 0.0720 - accuracy: 0.97 - ETA: 1s - loss: 0.0719 - accuracy: 0.97 - ETA: 1s - loss: 0.0717 - accuracy: 0.97 - ETA: 1s - loss: 0.0710 - accuracy: 0.97 - ETA: 1s - loss: 0.0711 - accuracy: 0.97 - ETA: 1s - loss: 0.0714 - accuracy: 0.97 - ETA: 1s - loss: 0.0712 - accuracy: 0.97 - ETA: 1s - loss: 0.0719 - accuracy: 0.97 - ETA: 1s - loss: 0.0726 - accuracy: 0.97 - ETA: 1s - loss: 0.0734 - accuracy: 0.97 - ETA: 1s - loss: 0.0735 - accuracy: 0.97 - ETA: 0s - loss: 0.0734 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.97 - ETA: 0s - loss: 0.0722 - accuracy: 0.97 - ETA: 0s - loss: 0.0719 - accuracy: 0.97 - ETA: 0s - loss: 0.0720 - accuracy: 0.97 - ETA: 0s - loss: 0.0716 - accuracy: 0.97 - ETA: 0s - loss: 0.0713 - accuracy: 0.97 - ETA: 0s - loss: 0.0713 - accuracy: 0.97 - ETA: 0s - loss: 0.0730 - accuracy: 0.97 - ETA: 0s - loss: 0.0743 - accuracy: 0.97 - 7s 628us/step - loss: 0.0745 - accuracy: 0.9727 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00794\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0505 - accuracy: 0.97 - ETA: 6s - loss: 0.0522 - accuracy: 0.98 - ETA: 6s - loss: 0.0486 - accuracy: 0.98 - ETA: 5s - loss: 0.0506 - accuracy: 0.97 - ETA: 5s - loss: 0.0472 - accuracy: 0.97 - ETA: 5s - loss: 0.0535 - accuracy: 0.97 - ETA: 5s - loss: 0.0619 - accuracy: 0.97 - ETA: 5s - loss: 0.0697 - accuracy: 0.97 - ETA: 5s - loss: 0.0696 - accuracy: 0.97 - ETA: 5s - loss: 0.0686 - accuracy: 0.97 - ETA: 5s - loss: 0.0650 - accuracy: 0.97 - ETA: 5s - loss: 0.0632 - accuracy: 0.97 - ETA: 5s - loss: 0.0598 - accuracy: 0.97 - ETA: 5s - loss: 0.0621 - accuracy: 0.97 - ETA: 5s - loss: 0.0682 - accuracy: 0.97 - ETA: 4s - loss: 0.0680 - accuracy: 0.97 - ETA: 4s - loss: 0.0659 - accuracy: 0.97 - ETA: 4s - loss: 0.0648 - accuracy: 0.97 - ETA: 4s - loss: 0.0629 - accuracy: 0.97 - ETA: 4s - loss: 0.0625 - accuracy: 0.97 - ETA: 4s - loss: 0.0640 - accuracy: 0.97 - ETA: 4s - loss: 0.0666 - accuracy: 0.97 - ETA: 4s - loss: 0.0657 - accuracy: 0.97 - ETA: 4s - loss: 0.0636 - accuracy: 0.97 - ETA: 4s - loss: 0.0623 - accuracy: 0.97 - ETA: 4s - loss: 0.0624 - accuracy: 0.97 - ETA: 4s - loss: 0.0609 - accuracy: 0.97 - ETA: 3s - loss: 0.0608 - accuracy: 0.97 - ETA: 3s - loss: 0.0599 - accuracy: 0.97 - ETA: 3s - loss: 0.0585 - accuracy: 0.97 - ETA: 3s - loss: 0.0570 - accuracy: 0.97 - ETA: 3s - loss: 0.0574 - accuracy: 0.97 - ETA: 3s - loss: 0.0575 - accuracy: 0.97 - ETA: 3s - loss: 0.0576 - accuracy: 0.97 - ETA: 3s - loss: 0.0602 - accuracy: 0.97 - ETA: 3s - loss: 0.0610 - accuracy: 0.97 - ETA: 3s - loss: 0.0602 - accuracy: 0.97 - ETA: 3s - loss: 0.0598 - accuracy: 0.97 - ETA: 3s - loss: 0.0586 - accuracy: 0.97 - ETA: 2s - loss: 0.0576 - accuracy: 0.97 - ETA: 2s - loss: 0.0576 - accuracy: 0.97 - ETA: 2s - loss: 0.0576 - accuracy: 0.97 - ETA: 2s - loss: 0.0587 - accuracy: 0.97 - ETA: 2s - loss: 0.0591 - accuracy: 0.97 - ETA: 2s - loss: 0.0587 - accuracy: 0.97 - ETA: 2s - loss: 0.0579 - accuracy: 0.97 - ETA: 2s - loss: 0.0572 - accuracy: 0.97 - ETA: 2s - loss: 0.0571 - accuracy: 0.97 - ETA: 2s - loss: 0.0566 - accuracy: 0.97 - ETA: 2s - loss: 0.0577 - accuracy: 0.97 - ETA: 1s - loss: 0.0573 - accuracy: 0.97 - ETA: 1s - loss: 0.0564 - accuracy: 0.97 - ETA: 1s - loss: 0.0556 - accuracy: 0.97 - ETA: 1s - loss: 0.0555 - accuracy: 0.97 - ETA: 1s - loss: 0.0554 - accuracy: 0.97 - ETA: 1s - loss: 0.0550 - accuracy: 0.97 - ETA: 1s - loss: 0.0549 - accuracy: 0.97 - ETA: 1s - loss: 0.0545 - accuracy: 0.97 - ETA: 1s - loss: 0.0540 - accuracy: 0.97 - ETA: 1s - loss: 0.0533 - accuracy: 0.97 - ETA: 1s - loss: 0.0527 - accuracy: 0.97 - ETA: 1s - loss: 0.0532 - accuracy: 0.97 - ETA: 0s - loss: 0.0561 - accuracy: 0.97 - ETA: 0s - loss: 0.0562 - accuracy: 0.97 - ETA: 0s - loss: 0.0563 - accuracy: 0.97 - ETA: 0s - loss: 0.0585 - accuracy: 0.97 - ETA: 0s - loss: 0.0588 - accuracy: 0.97 - ETA: 0s - loss: 0.0585 - accuracy: 0.97 - ETA: 0s - loss: 0.0591 - accuracy: 0.97 - ETA: 0s - loss: 0.0591 - accuracy: 0.97 - ETA: 0s - loss: 0.0588 - accuracy: 0.97 - ETA: 0s - loss: 0.0586 - accuracy: 0.97 - ETA: 0s - loss: 0.0584 - accuracy: 0.97 - 7s 625us/step - loss: 0.0580 - accuracy: 0.9786 - val_loss: 0.0265 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00794\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0344 - accuracy: 0.98 - ETA: 6s - loss: 0.0330 - accuracy: 0.98 - ETA: 6s - loss: 0.0436 - accuracy: 0.98 - ETA: 5s - loss: 0.0374 - accuracy: 0.98 - ETA: 5s - loss: 0.0460 - accuracy: 0.98 - ETA: 5s - loss: 0.0528 - accuracy: 0.98 - ETA: 5s - loss: 0.0570 - accuracy: 0.97 - ETA: 5s - loss: 0.0525 - accuracy: 0.98 - ETA: 5s - loss: 0.0504 - accuracy: 0.98 - ETA: 5s - loss: 0.0506 - accuracy: 0.98 - ETA: 5s - loss: 0.0560 - accuracy: 0.98 - ETA: 5s - loss: 0.0591 - accuracy: 0.97 - ETA: 5s - loss: 0.0582 - accuracy: 0.97 - ETA: 5s - loss: 0.0556 - accuracy: 0.98 - ETA: 5s - loss: 0.0536 - accuracy: 0.98 - ETA: 5s - loss: 0.0516 - accuracy: 0.98 - ETA: 4s - loss: 0.0504 - accuracy: 0.98 - ETA: 4s - loss: 0.0505 - accuracy: 0.98 - ETA: 4s - loss: 0.0494 - accuracy: 0.98 - ETA: 4s - loss: 0.0503 - accuracy: 0.98 - ETA: 4s - loss: 0.0500 - accuracy: 0.98 - ETA: 4s - loss: 0.0508 - accuracy: 0.98 - ETA: 4s - loss: 0.0538 - accuracy: 0.98 - ETA: 4s - loss: 0.0573 - accuracy: 0.97 - ETA: 4s - loss: 0.0565 - accuracy: 0.98 - ETA: 4s - loss: 0.0553 - accuracy: 0.98 - ETA: 4s - loss: 0.0545 - accuracy: 0.98 - ETA: 4s - loss: 0.0536 - accuracy: 0.98 - ETA: 3s - loss: 0.0551 - accuracy: 0.98 - ETA: 3s - loss: 0.0541 - accuracy: 0.98 - ETA: 3s - loss: 0.0552 - accuracy: 0.98 - ETA: 3s - loss: 0.0542 - accuracy: 0.98 - ETA: 3s - loss: 0.0534 - accuracy: 0.98 - ETA: 3s - loss: 0.0531 - accuracy: 0.98 - ETA: 3s - loss: 0.0522 - accuracy: 0.98 - ETA: 3s - loss: 0.0514 - accuracy: 0.98 - ETA: 3s - loss: 0.0527 - accuracy: 0.98 - ETA: 3s - loss: 0.0549 - accuracy: 0.98 - ETA: 3s - loss: 0.0541 - accuracy: 0.98 - ETA: 2s - loss: 0.0536 - accuracy: 0.98 - ETA: 2s - loss: 0.0531 - accuracy: 0.98 - ETA: 2s - loss: 0.0528 - accuracy: 0.98 - ETA: 2s - loss: 0.0530 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 2s - loss: 0.0512 - accuracy: 0.98 - ETA: 2s - loss: 0.0506 - accuracy: 0.98 - ETA: 2s - loss: 0.0523 - accuracy: 0.98 - ETA: 2s - loss: 0.0605 - accuracy: 0.97 - ETA: 2s - loss: 0.0608 - accuracy: 0.97 - ETA: 2s - loss: 0.0604 - accuracy: 0.97 - ETA: 1s - loss: 0.0599 - accuracy: 0.98 - ETA: 1s - loss: 0.0596 - accuracy: 0.98 - ETA: 1s - loss: 0.0605 - accuracy: 0.97 - ETA: 1s - loss: 0.0602 - accuracy: 0.97 - ETA: 1s - loss: 0.0607 - accuracy: 0.97 - ETA: 1s - loss: 0.0598 - accuracy: 0.97 - ETA: 1s - loss: 0.0594 - accuracy: 0.97 - ETA: 1s - loss: 0.0589 - accuracy: 0.97 - ETA: 1s - loss: 0.0582 - accuracy: 0.98 - ETA: 1s - loss: 0.0577 - accuracy: 0.98 - ETA: 1s - loss: 0.0569 - accuracy: 0.98 - ETA: 1s - loss: 0.0568 - accuracy: 0.98 - ETA: 0s - loss: 0.0565 - accuracy: 0.98 - ETA: 0s - loss: 0.0565 - accuracy: 0.98 - ETA: 0s - loss: 0.0572 - accuracy: 0.98 - ETA: 0s - loss: 0.0571 - accuracy: 0.98 - ETA: 0s - loss: 0.0566 - accuracy: 0.98 - ETA: 0s - loss: 0.0559 - accuracy: 0.98 - ETA: 0s - loss: 0.0559 - accuracy: 0.98 - ETA: 0s - loss: 0.0559 - accuracy: 0.98 - ETA: 0s - loss: 0.0557 - accuracy: 0.98 - ETA: 0s - loss: 0.0553 - accuracy: 0.98 - ETA: 0s - loss: 0.0551 - accuracy: 0.98 - 7s 624us/step - loss: 0.0558 - accuracy: 0.9808 - val_loss: 0.1009 - val_accuracy: 0.9646\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00794\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0782 - accuracy: 0.97 - ETA: 6s - loss: 0.0668 - accuracy: 0.97 - ETA: 5s - loss: 0.0660 - accuracy: 0.97 - ETA: 5s - loss: 0.0556 - accuracy: 0.98 - ETA: 5s - loss: 0.0521 - accuracy: 0.98 - ETA: 5s - loss: 0.0558 - accuracy: 0.97 - ETA: 5s - loss: 0.0521 - accuracy: 0.98 - ETA: 5s - loss: 0.0473 - accuracy: 0.98 - ETA: 5s - loss: 0.0456 - accuracy: 0.98 - ETA: 5s - loss: 0.0473 - accuracy: 0.98 - ETA: 5s - loss: 0.0482 - accuracy: 0.98 - ETA: 5s - loss: 0.0491 - accuracy: 0.98 - ETA: 5s - loss: 0.0484 - accuracy: 0.98 - ETA: 5s - loss: 0.0477 - accuracy: 0.98 - ETA: 5s - loss: 0.0464 - accuracy: 0.98 - ETA: 4s - loss: 0.0452 - accuracy: 0.98 - ETA: 4s - loss: 0.0470 - accuracy: 0.98 - ETA: 4s - loss: 0.0486 - accuracy: 0.98 - ETA: 4s - loss: 0.0475 - accuracy: 0.98 - ETA: 4s - loss: 0.0469 - accuracy: 0.98 - ETA: 4s - loss: 0.0461 - accuracy: 0.98 - ETA: 4s - loss: 0.0461 - accuracy: 0.98 - ETA: 4s - loss: 0.0447 - accuracy: 0.98 - ETA: 4s - loss: 0.0439 - accuracy: 0.98 - ETA: 4s - loss: 0.0436 - accuracy: 0.98 - ETA: 4s - loss: 0.0424 - accuracy: 0.98 - ETA: 4s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0432 - accuracy: 0.98 - ETA: 3s - loss: 0.0454 - accuracy: 0.98 - ETA: 3s - loss: 0.0459 - accuracy: 0.98 - ETA: 3s - loss: 0.0498 - accuracy: 0.98 - ETA: 3s - loss: 0.0515 - accuracy: 0.98 - ETA: 3s - loss: 0.0510 - accuracy: 0.98 - ETA: 3s - loss: 0.0508 - accuracy: 0.98 - ETA: 3s - loss: 0.0500 - accuracy: 0.98 - ETA: 3s - loss: 0.0493 - accuracy: 0.98 - ETA: 3s - loss: 0.0495 - accuracy: 0.98 - ETA: 3s - loss: 0.0492 - accuracy: 0.98 - ETA: 2s - loss: 0.0485 - accuracy: 0.98 - ETA: 2s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0471 - accuracy: 0.98 - ETA: 2s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0480 - accuracy: 0.98 - ETA: 2s - loss: 0.0476 - accuracy: 0.98 - ETA: 2s - loss: 0.0471 - accuracy: 0.98 - ETA: 2s - loss: 0.0467 - accuracy: 0.98 - ETA: 2s - loss: 0.0463 - accuracy: 0.98 - ETA: 2s - loss: 0.0461 - accuracy: 0.98 - ETA: 2s - loss: 0.0459 - accuracy: 0.98 - ETA: 2s - loss: 0.0460 - accuracy: 0.98 - ETA: 1s - loss: 0.0464 - accuracy: 0.98 - ETA: 1s - loss: 0.0465 - accuracy: 0.98 - ETA: 1s - loss: 0.0463 - accuracy: 0.98 - ETA: 1s - loss: 0.0465 - accuracy: 0.98 - ETA: 1s - loss: 0.0463 - accuracy: 0.98 - ETA: 1s - loss: 0.0459 - accuracy: 0.98 - ETA: 1s - loss: 0.0453 - accuracy: 0.98 - ETA: 1s - loss: 0.0450 - accuracy: 0.98 - ETA: 1s - loss: 0.0445 - accuracy: 0.98 - ETA: 1s - loss: 0.0439 - accuracy: 0.98 - ETA: 1s - loss: 0.0435 - accuracy: 0.98 - ETA: 1s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0427 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0446 - accuracy: 0.98 - ETA: 0s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0506 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0503 - accuracy: 0.98 - ETA: 0s - loss: 0.0498 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - 7s 610us/step - loss: 0.0493 - accuracy: 0.9826 - val_loss: 0.0300 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00794\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0352 - accuracy: 0.99 - ETA: 5s - loss: 0.0233 - accuracy: 0.99 - ETA: 5s - loss: 0.0328 - accuracy: 0.99 - ETA: 5s - loss: 0.0312 - accuracy: 0.98 - ETA: 5s - loss: 0.0272 - accuracy: 0.99 - ETA: 5s - loss: 0.0278 - accuracy: 0.98 - ETA: 5s - loss: 0.0371 - accuracy: 0.98 - ETA: 5s - loss: 0.0336 - accuracy: 0.98 - ETA: 5s - loss: 0.0316 - accuracy: 0.98 - ETA: 5s - loss: 0.0299 - accuracy: 0.98 - ETA: 5s - loss: 0.0316 - accuracy: 0.98 - ETA: 5s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0363 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0343 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0328 - accuracy: 0.98 - ETA: 4s - loss: 0.0323 - accuracy: 0.98 - ETA: 4s - loss: 0.0322 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 4s - loss: 0.0298 - accuracy: 0.98 - ETA: 4s - loss: 0.0292 - accuracy: 0.98 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0345 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0369 - accuracy: 0.98 - ETA: 3s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0369 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0349 - accuracy: 0.98 - ETA: 3s - loss: 0.0351 - accuracy: 0.98 - ETA: 3s - loss: 0.0349 - accuracy: 0.98 - ETA: 3s - loss: 0.0352 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0334 - accuracy: 0.98 - ETA: 2s - loss: 0.0329 - accuracy: 0.98 - ETA: 2s - loss: 0.0325 - accuracy: 0.98 - ETA: 2s - loss: 0.0332 - accuracy: 0.98 - ETA: 2s - loss: 0.0337 - accuracy: 0.98 - ETA: 2s - loss: 0.0365 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 2s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0379 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0372 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0370 - accuracy: 0.98 - ETA: 1s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0364 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0375 - accuracy: 0.98 - 7s 635us/step - loss: 0.0382 - accuracy: 0.9867 - val_loss: 0.0289 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00794\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0261 - accuracy: 0.99 - ETA: 5s - loss: 0.0348 - accuracy: 0.99 - ETA: 5s - loss: 0.0321 - accuracy: 0.99 - ETA: 5s - loss: 0.0269 - accuracy: 0.99 - ETA: 5s - loss: 0.0325 - accuracy: 0.99 - ETA: 5s - loss: 0.0299 - accuracy: 0.99 - ETA: 6s - loss: 0.0283 - accuracy: 0.99 - ETA: 6s - loss: 0.0275 - accuracy: 0.99 - ETA: 6s - loss: 0.0290 - accuracy: 0.98 - ETA: 6s - loss: 0.0306 - accuracy: 0.98 - ETA: 5s - loss: 0.0288 - accuracy: 0.98 - ETA: 5s - loss: 0.0296 - accuracy: 0.98 - ETA: 5s - loss: 0.0307 - accuracy: 0.98 - ETA: 5s - loss: 0.0315 - accuracy: 0.98 - ETA: 5s - loss: 0.0300 - accuracy: 0.98 - ETA: 5s - loss: 0.0286 - accuracy: 0.98 - ETA: 5s - loss: 0.0275 - accuracy: 0.98 - ETA: 5s - loss: 0.0264 - accuracy: 0.98 - ETA: 4s - loss: 0.0259 - accuracy: 0.98 - ETA: 4s - loss: 0.0252 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.98 - ETA: 4s - loss: 0.0283 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.98 - ETA: 4s - loss: 0.0279 - accuracy: 0.98 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0304 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 3s - loss: 0.0308 - accuracy: 0.98 - ETA: 3s - loss: 0.0305 - accuracy: 0.98 - ETA: 3s - loss: 0.0327 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0360 - accuracy: 0.98 - ETA: 3s - loss: 0.0364 - accuracy: 0.98 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 3s - loss: 0.0357 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0352 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0354 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0344 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 1s - loss: 0.0373 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0363 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0400 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0395 - accuracy: 0.98 - ETA: 0s - loss: 0.0401 - accuracy: 0.98 - ETA: 0s - loss: 0.0398 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0391 - accuracy: 0.98 - ETA: 0s - loss: 0.0386 - accuracy: 0.98 - ETA: 0s - loss: 0.0386 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0384 - accuracy: 0.98 - ETA: 0s - loss: 0.0381 - accuracy: 0.98 - ETA: 0s - loss: 0.0381 - accuracy: 0.98 - ETA: 0s - loss: 0.0381 - accuracy: 0.98 - 7s 627us/step - loss: 0.0378 - accuracy: 0.9873 - val_loss: 0.0156 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00794\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0406 - accuracy: 0.97 - ETA: 5s - loss: 0.0251 - accuracy: 0.98 - ETA: 5s - loss: 0.0283 - accuracy: 0.98 - ETA: 5s - loss: 0.0281 - accuracy: 0.98 - ETA: 5s - loss: 0.0264 - accuracy: 0.98 - ETA: 5s - loss: 0.0254 - accuracy: 0.98 - ETA: 5s - loss: 0.0236 - accuracy: 0.98 - ETA: 5s - loss: 0.0217 - accuracy: 0.98 - ETA: 5s - loss: 0.0208 - accuracy: 0.99 - ETA: 5s - loss: 0.0258 - accuracy: 0.98 - ETA: 5s - loss: 0.0319 - accuracy: 0.98 - ETA: 4s - loss: 0.0376 - accuracy: 0.98 - ETA: 4s - loss: 0.0368 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0346 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0327 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0315 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 4s - loss: 0.0298 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0317 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0434 - accuracy: 0.98 - ETA: 3s - loss: 0.0425 - accuracy: 0.98 - ETA: 3s - loss: 0.0422 - accuracy: 0.98 - ETA: 3s - loss: 0.0410 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0393 - accuracy: 0.98 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0377 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0370 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0393 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0378 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 2s - loss: 0.0394 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 2s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0395 - accuracy: 0.98 - ETA: 1s - loss: 0.0389 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0379 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0387 - accuracy: 0.98 - ETA: 1s - loss: 0.0382 - accuracy: 0.98 - ETA: 0s - loss: 0.0377 - accuracy: 0.98 - ETA: 0s - loss: 0.0375 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - 6s 586us/step - loss: 0.0382 - accuracy: 0.9873 - val_loss: 0.0287 - val_accuracy: 0.9874\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00794\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 5s - loss: 0.0222 - accuracy: 0.99 - ETA: 5s - loss: 0.0171 - accuracy: 0.99 - ETA: 5s - loss: 0.0162 - accuracy: 0.99 - ETA: 5s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0216 - accuracy: 0.99 - ETA: 4s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0169 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0159 - accuracy: 0.99 - ETA: 4s - loss: 0.0163 - accuracy: 0.99 - ETA: 4s - loss: 0.0153 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0184 - accuracy: 0.99 - ETA: 4s - loss: 0.0224 - accuracy: 0.99 - ETA: 4s - loss: 0.0331 - accuracy: 0.98 - ETA: 4s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0344 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0340 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0317 - accuracy: 0.98 - ETA: 4s - loss: 0.0307 - accuracy: 0.98 - ETA: 3s - loss: 0.0302 - accuracy: 0.99 - ETA: 3s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0296 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0319 - accuracy: 0.98 - ETA: 3s - loss: 0.0327 - accuracy: 0.98 - ETA: 3s - loss: 0.0324 - accuracy: 0.98 - ETA: 3s - loss: 0.0316 - accuracy: 0.98 - ETA: 3s - loss: 0.0331 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 2s - loss: 0.0354 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0344 - accuracy: 0.98 - ETA: 2s - loss: 0.0338 - accuracy: 0.98 - ETA: 2s - loss: 0.0330 - accuracy: 0.98 - ETA: 2s - loss: 0.0323 - accuracy: 0.98 - ETA: 2s - loss: 0.0318 - accuracy: 0.98 - ETA: 2s - loss: 0.0322 - accuracy: 0.98 - ETA: 2s - loss: 0.0321 - accuracy: 0.98 - ETA: 2s - loss: 0.0316 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0312 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0335 - accuracy: 0.98 - ETA: 1s - loss: 0.0340 - accuracy: 0.98 - ETA: 1s - loss: 0.0338 - accuracy: 0.98 - ETA: 1s - loss: 0.0349 - accuracy: 0.98 - ETA: 1s - loss: 0.0370 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0386 - accuracy: 0.98 - ETA: 0s - loss: 0.0389 - accuracy: 0.98 - ETA: 0s - loss: 0.0388 - accuracy: 0.98 - ETA: 0s - loss: 0.0384 - accuracy: 0.98 - ETA: 0s - loss: 0.0381 - accuracy: 0.98 - ETA: 0s - loss: 0.0376 - accuracy: 0.98 - ETA: 0s - loss: 0.0375 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - 6s 555us/step - loss: 0.0357 - accuracy: 0.9884 - val_loss: 0.0154 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00794\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 7s - loss: 0.0911 - accuracy: 0.97 - ETA: 7s - loss: 0.0761 - accuracy: 0.98 - ETA: 7s - loss: 0.0531 - accuracy: 0.99 - ETA: 6s - loss: 0.0499 - accuracy: 0.99 - ETA: 6s - loss: 0.0444 - accuracy: 0.99 - ETA: 6s - loss: 0.0383 - accuracy: 0.99 - ETA: 6s - loss: 0.0336 - accuracy: 0.99 - ETA: 5s - loss: 0.0346 - accuracy: 0.99 - ETA: 5s - loss: 0.0322 - accuracy: 0.99 - ETA: 5s - loss: 0.0322 - accuracy: 0.99 - ETA: 5s - loss: 0.0329 - accuracy: 0.99 - ETA: 5s - loss: 0.0328 - accuracy: 0.99 - ETA: 5s - loss: 0.0315 - accuracy: 0.99 - ETA: 5s - loss: 0.0307 - accuracy: 0.99 - ETA: 5s - loss: 0.0289 - accuracy: 0.99 - ETA: 4s - loss: 0.0307 - accuracy: 0.99 - ETA: 4s - loss: 0.0292 - accuracy: 0.99 - ETA: 4s - loss: 0.0279 - accuracy: 0.99 - ETA: 4s - loss: 0.0270 - accuracy: 0.99 - ETA: 4s - loss: 0.0259 - accuracy: 0.99 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - ETA: 4s - loss: 0.0264 - accuracy: 0.99 - ETA: 4s - loss: 0.0283 - accuracy: 0.99 - ETA: 4s - loss: 0.0283 - accuracy: 0.99 - ETA: 4s - loss: 0.0310 - accuracy: 0.99 - ETA: 4s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0474 - accuracy: 0.98 - ETA: 3s - loss: 0.0460 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0442 - accuracy: 0.98 - ETA: 3s - loss: 0.0431 - accuracy: 0.98 - ETA: 3s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0435 - accuracy: 0.98 - ETA: 3s - loss: 0.0427 - accuracy: 0.98 - ETA: 3s - loss: 0.0424 - accuracy: 0.98 - ETA: 3s - loss: 0.0416 - accuracy: 0.98 - ETA: 3s - loss: 0.0407 - accuracy: 0.98 - ETA: 3s - loss: 0.0404 - accuracy: 0.98 - ETA: 2s - loss: 0.0398 - accuracy: 0.98 - ETA: 2s - loss: 0.0390 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 2s - loss: 0.0373 - accuracy: 0.98 - ETA: 2s - loss: 0.0376 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0359 - accuracy: 0.98 - ETA: 1s - loss: 0.0355 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.98 - ETA: 1s - loss: 0.0352 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.98 - ETA: 1s - loss: 0.0352 - accuracy: 0.98 - ETA: 1s - loss: 0.0349 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0336 - accuracy: 0.98 - ETA: 0s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0334 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0327 - accuracy: 0.98 - ETA: 0s - loss: 0.0327 - accuracy: 0.98 - ETA: 0s - loss: 0.0337 - accuracy: 0.98 - ETA: 0s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0350 - accuracy: 0.98 - ETA: 0s - loss: 0.0351 - accuracy: 0.98 - 6s 583us/step - loss: 0.0349 - accuracy: 0.9886 - val_loss: 0.0196 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00794\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0115 - accuracy: 0.99 - ETA: 5s - loss: 0.0130 - accuracy: 0.99 - ETA: 5s - loss: 0.0136 - accuracy: 0.99 - ETA: 5s - loss: 0.0157 - accuracy: 0.99 - ETA: 5s - loss: 0.0178 - accuracy: 0.99 - ETA: 5s - loss: 0.0152 - accuracy: 0.99 - ETA: 5s - loss: 0.0224 - accuracy: 0.99 - ETA: 5s - loss: 0.0243 - accuracy: 0.99 - ETA: 5s - loss: 0.0238 - accuracy: 0.99 - ETA: 4s - loss: 0.0223 - accuracy: 0.99 - ETA: 4s - loss: 0.0219 - accuracy: 0.99 - ETA: 4s - loss: 0.0327 - accuracy: 0.99 - ETA: 4s - loss: 0.0330 - accuracy: 0.99 - ETA: 4s - loss: 0.0345 - accuracy: 0.99 - ETA: 4s - loss: 0.0361 - accuracy: 0.98 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0346 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0324 - accuracy: 0.99 - ETA: 4s - loss: 0.0317 - accuracy: 0.99 - ETA: 4s - loss: 0.0307 - accuracy: 0.99 - ETA: 4s - loss: 0.0294 - accuracy: 0.99 - ETA: 4s - loss: 0.0284 - accuracy: 0.99 - ETA: 4s - loss: 0.0308 - accuracy: 0.99 - ETA: 4s - loss: 0.0319 - accuracy: 0.99 - ETA: 4s - loss: 0.0312 - accuracy: 0.99 - ETA: 4s - loss: 0.0310 - accuracy: 0.99 - ETA: 4s - loss: 0.0303 - accuracy: 0.99 - ETA: 4s - loss: 0.0302 - accuracy: 0.99 - ETA: 4s - loss: 0.0296 - accuracy: 0.99 - ETA: 3s - loss: 0.0293 - accuracy: 0.99 - ETA: 3s - loss: 0.0288 - accuracy: 0.99 - ETA: 3s - loss: 0.0285 - accuracy: 0.99 - ETA: 3s - loss: 0.0280 - accuracy: 0.99 - ETA: 3s - loss: 0.0273 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0267 - accuracy: 0.99 - ETA: 3s - loss: 0.0261 - accuracy: 0.99 - ETA: 3s - loss: 0.0256 - accuracy: 0.99 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 3s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0289 - accuracy: 0.99 - ETA: 2s - loss: 0.0292 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.99 - ETA: 2s - loss: 0.0281 - accuracy: 0.99 - ETA: 2s - loss: 0.0276 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - 8s 703us/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.0301 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00794\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 7s - loss: 0.0506 - accuracy: 0.98 - ETA: 7s - loss: 0.0696 - accuracy: 0.97 - ETA: 7s - loss: 0.0697 - accuracy: 0.97 - ETA: 7s - loss: 0.0555 - accuracy: 0.98 - ETA: 7s - loss: 0.0456 - accuracy: 0.98 - ETA: 6s - loss: 0.0392 - accuracy: 0.98 - ETA: 6s - loss: 0.0341 - accuracy: 0.99 - ETA: 6s - loss: 0.0311 - accuracy: 0.99 - ETA: 6s - loss: 0.0284 - accuracy: 0.99 - ETA: 6s - loss: 0.0267 - accuracy: 0.99 - ETA: 6s - loss: 0.0594 - accuracy: 0.98 - ETA: 6s - loss: 0.0712 - accuracy: 0.98 - ETA: 6s - loss: 0.0698 - accuracy: 0.98 - ETA: 6s - loss: 0.0667 - accuracy: 0.98 - ETA: 6s - loss: 0.0629 - accuracy: 0.98 - ETA: 5s - loss: 0.0591 - accuracy: 0.98 - ETA: 5s - loss: 0.0572 - accuracy: 0.98 - ETA: 5s - loss: 0.0549 - accuracy: 0.98 - ETA: 5s - loss: 0.0523 - accuracy: 0.98 - ETA: 5s - loss: 0.0525 - accuracy: 0.98 - ETA: 5s - loss: 0.0529 - accuracy: 0.98 - ETA: 5s - loss: 0.0511 - accuracy: 0.98 - ETA: 5s - loss: 0.0506 - accuracy: 0.98 - ETA: 5s - loss: 0.0486 - accuracy: 0.98 - ETA: 4s - loss: 0.0470 - accuracy: 0.98 - ETA: 4s - loss: 0.0453 - accuracy: 0.98 - ETA: 4s - loss: 0.0449 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0424 - accuracy: 0.98 - ETA: 4s - loss: 0.0418 - accuracy: 0.98 - ETA: 4s - loss: 0.0410 - accuracy: 0.98 - ETA: 4s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0389 - accuracy: 0.98 - ETA: 3s - loss: 0.0389 - accuracy: 0.98 - ETA: 3s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0373 - accuracy: 0.99 - ETA: 3s - loss: 0.0366 - accuracy: 0.99 - ETA: 3s - loss: 0.0377 - accuracy: 0.98 - ETA: 3s - loss: 0.0379 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0374 - accuracy: 0.98 - ETA: 3s - loss: 0.0366 - accuracy: 0.99 - ETA: 3s - loss: 0.0361 - accuracy: 0.99 - ETA: 3s - loss: 0.0354 - accuracy: 0.99 - ETA: 3s - loss: 0.0347 - accuracy: 0.99 - ETA: 3s - loss: 0.0347 - accuracy: 0.99 - ETA: 3s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 2s - loss: 0.0378 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0376 - accuracy: 0.98 - ETA: 2s - loss: 0.0373 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0359 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0348 - accuracy: 0.98 - ETA: 0s - loss: 0.0349 - accuracy: 0.98 - ETA: 0s - loss: 0.0345 - accuracy: 0.98 - ETA: 0s - loss: 0.0346 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0334 - accuracy: 0.98 - ETA: 0s - loss: 0.0333 - accuracy: 0.99 - 8s 796us/step - loss: 0.0331 - accuracy: 0.9901 - val_loss: 0.0113 - val_accuracy: 0.9975\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00794\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0048 - accuracy: 1.00 - ETA: 7s - loss: 0.0053 - accuracy: 1.00 - ETA: 7s - loss: 0.0045 - accuracy: 1.00 - ETA: 7s - loss: 0.0065 - accuracy: 0.99 - ETA: 7s - loss: 0.0119 - accuracy: 0.99 - ETA: 7s - loss: 0.0131 - accuracy: 0.99 - ETA: 7s - loss: 0.0197 - accuracy: 0.99 - ETA: 7s - loss: 0.0216 - accuracy: 0.99 - ETA: 6s - loss: 0.0273 - accuracy: 0.99 - ETA: 6s - loss: 0.0318 - accuracy: 0.98 - ETA: 6s - loss: 0.0301 - accuracy: 0.99 - ETA: 6s - loss: 0.0281 - accuracy: 0.99 - ETA: 6s - loss: 0.0271 - accuracy: 0.99 - ETA: 6s - loss: 0.0257 - accuracy: 0.99 - ETA: 6s - loss: 0.0296 - accuracy: 0.99 - ETA: 6s - loss: 0.0283 - accuracy: 0.99 - ETA: 6s - loss: 0.0295 - accuracy: 0.99 - ETA: 6s - loss: 0.0304 - accuracy: 0.99 - ETA: 6s - loss: 0.0291 - accuracy: 0.99 - ETA: 6s - loss: 0.0301 - accuracy: 0.99 - ETA: 6s - loss: 0.0297 - accuracy: 0.99 - ETA: 6s - loss: 0.0295 - accuracy: 0.99 - ETA: 6s - loss: 0.0299 - accuracy: 0.99 - ETA: 6s - loss: 0.0310 - accuracy: 0.99 - ETA: 6s - loss: 0.0301 - accuracy: 0.99 - ETA: 5s - loss: 0.0307 - accuracy: 0.99 - ETA: 5s - loss: 0.0315 - accuracy: 0.99 - ETA: 5s - loss: 0.0315 - accuracy: 0.99 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0322 - accuracy: 0.98 - ETA: 5s - loss: 0.0313 - accuracy: 0.99 - ETA: 4s - loss: 0.0307 - accuracy: 0.99 - ETA: 4s - loss: 0.0302 - accuracy: 0.99 - ETA: 4s - loss: 0.0298 - accuracy: 0.99 - ETA: 4s - loss: 0.0290 - accuracy: 0.99 - ETA: 4s - loss: 0.0283 - accuracy: 0.99 - ETA: 4s - loss: 0.0279 - accuracy: 0.99 - ETA: 4s - loss: 0.0274 - accuracy: 0.99 - ETA: 4s - loss: 0.0279 - accuracy: 0.99 - ETA: 4s - loss: 0.0282 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0272 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0272 - accuracy: 0.99 - ETA: 3s - loss: 0.0270 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 3s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0265 - accuracy: 0.99 - ETA: 2s - loss: 0.0261 - accuracy: 0.99 - ETA: 2s - loss: 0.0258 - accuracy: 0.99 - ETA: 2s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0285 - accuracy: 0.99 - ETA: 2s - loss: 0.0291 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0289 - accuracy: 0.99 - ETA: 1s - loss: 0.0295 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 1s - loss: 0.0290 - accuracy: 0.99 - ETA: 1s - loss: 0.0290 - accuracy: 0.99 - ETA: 1s - loss: 0.0286 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0280 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.99 - 9s 804us/step - loss: 0.0285 - accuracy: 0.9909 - val_loss: 0.1062 - val_accuracy: 0.9680\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00794\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 9s - loss: 0.0822 - accuracy: 0.97 - ETA: 7s - loss: 0.0549 - accuracy: 0.97 - ETA: 7s - loss: 0.0407 - accuracy: 0.98 - ETA: 7s - loss: 0.0368 - accuracy: 0.98 - ETA: 7s - loss: 0.0297 - accuracy: 0.98 - ETA: 6s - loss: 0.0328 - accuracy: 0.98 - ETA: 6s - loss: 0.0331 - accuracy: 0.98 - ETA: 6s - loss: 0.0344 - accuracy: 0.98 - ETA: 6s - loss: 0.0319 - accuracy: 0.99 - ETA: 6s - loss: 0.0315 - accuracy: 0.99 - ETA: 6s - loss: 0.0341 - accuracy: 0.98 - ETA: 6s - loss: 0.0359 - accuracy: 0.98 - ETA: 6s - loss: 0.0362 - accuracy: 0.98 - ETA: 6s - loss: 0.0345 - accuracy: 0.98 - ETA: 5s - loss: 0.0337 - accuracy: 0.98 - ETA: 5s - loss: 0.0320 - accuracy: 0.99 - ETA: 5s - loss: 0.0309 - accuracy: 0.98 - ETA: 5s - loss: 0.0312 - accuracy: 0.98 - ETA: 5s - loss: 0.0304 - accuracy: 0.98 - ETA: 5s - loss: 0.0318 - accuracy: 0.98 - ETA: 5s - loss: 0.0382 - accuracy: 0.98 - ETA: 5s - loss: 0.0368 - accuracy: 0.98 - ETA: 5s - loss: 0.0381 - accuracy: 0.98 - ETA: 4s - loss: 0.0385 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0380 - accuracy: 0.98 - ETA: 4s - loss: 0.0383 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 4s - loss: 0.0357 - accuracy: 0.98 - ETA: 4s - loss: 0.0363 - accuracy: 0.98 - ETA: 4s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0369 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0366 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 3s - loss: 0.0340 - accuracy: 0.98 - ETA: 3s - loss: 0.0334 - accuracy: 0.98 - ETA: 3s - loss: 0.0334 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 2s - loss: 0.0321 - accuracy: 0.98 - ETA: 2s - loss: 0.0316 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.98 - ETA: 2s - loss: 0.0302 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0315 - accuracy: 0.98 - ETA: 2s - loss: 0.0311 - accuracy: 0.98 - ETA: 1s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0310 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0302 - accuracy: 0.98 - ETA: 1s - loss: 0.0297 - accuracy: 0.98 - ETA: 1s - loss: 0.0300 - accuracy: 0.98 - ETA: 1s - loss: 0.0296 - accuracy: 0.98 - ETA: 1s - loss: 0.0293 - accuracy: 0.98 - ETA: 1s - loss: 0.0295 - accuracy: 0.98 - ETA: 1s - loss: 0.0293 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0288 - accuracy: 0.99 - ETA: 0s - loss: 0.0285 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0280 - accuracy: 0.99 - 7s 675us/step - loss: 0.0278 - accuracy: 0.9904 - val_loss: 0.0161 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00794\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0035 - accuracy: 1.00 - ETA: 5s - loss: 0.0123 - accuracy: 0.99 - ETA: 5s - loss: 0.0111 - accuracy: 0.99 - ETA: 5s - loss: 0.0107 - accuracy: 0.99 - ETA: 5s - loss: 0.0087 - accuracy: 0.99 - ETA: 5s - loss: 0.0099 - accuracy: 0.99 - ETA: 5s - loss: 0.0086 - accuracy: 0.99 - ETA: 5s - loss: 0.0077 - accuracy: 0.99 - ETA: 5s - loss: 0.0071 - accuracy: 0.99 - ETA: 5s - loss: 0.0092 - accuracy: 0.99 - ETA: 5s - loss: 0.0154 - accuracy: 0.99 - ETA: 5s - loss: 0.0157 - accuracy: 0.99 - ETA: 5s - loss: 0.0153 - accuracy: 0.99 - ETA: 5s - loss: 0.0160 - accuracy: 0.99 - ETA: 5s - loss: 0.0177 - accuracy: 0.99 - ETA: 5s - loss: 0.0169 - accuracy: 0.99 - ETA: 5s - loss: 0.0163 - accuracy: 0.99 - ETA: 5s - loss: 0.0155 - accuracy: 0.99 - ETA: 5s - loss: 0.0169 - accuracy: 0.99 - ETA: 5s - loss: 0.0173 - accuracy: 0.99 - ETA: 5s - loss: 0.0245 - accuracy: 0.99 - ETA: 5s - loss: 0.0238 - accuracy: 0.99 - ETA: 5s - loss: 0.0233 - accuracy: 0.99 - ETA: 4s - loss: 0.0228 - accuracy: 0.99 - ETA: 4s - loss: 0.0223 - accuracy: 0.99 - ETA: 4s - loss: 0.0215 - accuracy: 0.99 - ETA: 4s - loss: 0.0212 - accuracy: 0.99 - ETA: 4s - loss: 0.0232 - accuracy: 0.99 - ETA: 4s - loss: 0.0234 - accuracy: 0.99 - ETA: 4s - loss: 0.0233 - accuracy: 0.99 - ETA: 4s - loss: 0.0229 - accuracy: 0.99 - ETA: 4s - loss: 0.0231 - accuracy: 0.99 - ETA: 4s - loss: 0.0230 - accuracy: 0.99 - ETA: 3s - loss: 0.0239 - accuracy: 0.99 - ETA: 3s - loss: 0.0239 - accuracy: 0.99 - ETA: 3s - loss: 0.0238 - accuracy: 0.99 - ETA: 3s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0265 - accuracy: 0.99 - ETA: 3s - loss: 0.0284 - accuracy: 0.99 - ETA: 3s - loss: 0.0281 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0282 - accuracy: 0.99 - ETA: 2s - loss: 0.0282 - accuracy: 0.99 - ETA: 2s - loss: 0.0281 - accuracy: 0.99 - ETA: 2s - loss: 0.0285 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.99 - ETA: 2s - loss: 0.0288 - accuracy: 0.99 - ETA: 2s - loss: 0.0295 - accuracy: 0.99 - ETA: 2s - loss: 0.0297 - accuracy: 0.99 - ETA: 2s - loss: 0.0296 - accuracy: 0.99 - ETA: 2s - loss: 0.0294 - accuracy: 0.99 - ETA: 2s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0284 - accuracy: 0.99 - ETA: 1s - loss: 0.0281 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0270 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - 7s 661us/step - loss: 0.0272 - accuracy: 0.9909 - val_loss: 0.0089 - val_accuracy: 0.9983\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00794\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0018 - accuracy: 1.00 - ETA: 5s - loss: 0.0130 - accuracy: 0.99 - ETA: 5s - loss: 0.0267 - accuracy: 0.99 - ETA: 5s - loss: 0.0221 - accuracy: 0.99 - ETA: 5s - loss: 0.0196 - accuracy: 0.99 - ETA: 5s - loss: 0.0168 - accuracy: 0.99 - ETA: 5s - loss: 0.0161 - accuracy: 0.99 - ETA: 5s - loss: 0.0168 - accuracy: 0.99 - ETA: 5s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0175 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0235 - accuracy: 0.99 - ETA: 4s - loss: 0.0263 - accuracy: 0.99 - ETA: 4s - loss: 0.0317 - accuracy: 0.99 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0333 - accuracy: 0.98 - ETA: 4s - loss: 0.0320 - accuracy: 0.98 - ETA: 4s - loss: 0.0314 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 3s - loss: 0.0310 - accuracy: 0.98 - ETA: 3s - loss: 0.0326 - accuracy: 0.98 - ETA: 3s - loss: 0.0320 - accuracy: 0.98 - ETA: 3s - loss: 0.0322 - accuracy: 0.98 - ETA: 3s - loss: 0.0314 - accuracy: 0.98 - ETA: 3s - loss: 0.0304 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0286 - accuracy: 0.99 - ETA: 3s - loss: 0.0277 - accuracy: 0.99 - ETA: 3s - loss: 0.0281 - accuracy: 0.99 - ETA: 3s - loss: 0.0281 - accuracy: 0.99 - ETA: 3s - loss: 0.0277 - accuracy: 0.99 - ETA: 3s - loss: 0.0273 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0256 - accuracy: 0.99 - ETA: 2s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0249 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0230 - accuracy: 0.99 - ETA: 2s - loss: 0.0226 - accuracy: 0.99 - ETA: 2s - loss: 0.0222 - accuracy: 0.99 - ETA: 2s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0215 - accuracy: 0.99 - ETA: 1s - loss: 0.0211 - accuracy: 0.99 - ETA: 1s - loss: 0.0207 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0229 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0223 - accuracy: 0.99 - ETA: 0s - loss: 0.0225 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0225 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0227 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0228 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0230 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - 6s 563us/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0139 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00794\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0164 - accuracy: 0.99 - ETA: 5s - loss: 0.0101 - accuracy: 0.99 - ETA: 5s - loss: 0.0088 - accuracy: 0.99 - ETA: 5s - loss: 0.0076 - accuracy: 0.99 - ETA: 5s - loss: 0.0071 - accuracy: 0.99 - ETA: 5s - loss: 0.0090 - accuracy: 0.99 - ETA: 5s - loss: 0.0078 - accuracy: 0.99 - ETA: 5s - loss: 0.0071 - accuracy: 0.99 - ETA: 5s - loss: 0.0084 - accuracy: 0.99 - ETA: 4s - loss: 0.0098 - accuracy: 0.99 - ETA: 4s - loss: 0.0149 - accuracy: 0.99 - ETA: 4s - loss: 0.0180 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0203 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0217 - accuracy: 0.99 - ETA: 4s - loss: 0.0220 - accuracy: 0.99 - ETA: 4s - loss: 0.0215 - accuracy: 0.99 - ETA: 4s - loss: 0.0218 - accuracy: 0.99 - ETA: 4s - loss: 0.0209 - accuracy: 0.99 - ETA: 4s - loss: 0.0213 - accuracy: 0.99 - ETA: 4s - loss: 0.0222 - accuracy: 0.99 - ETA: 3s - loss: 0.0216 - accuracy: 0.99 - ETA: 3s - loss: 0.0208 - accuracy: 0.99 - ETA: 3s - loss: 0.0205 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0211 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0206 - accuracy: 0.99 - ETA: 3s - loss: 0.0211 - accuracy: 0.99 - ETA: 3s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0215 - accuracy: 0.99 - ETA: 3s - loss: 0.0213 - accuracy: 0.99 - ETA: 3s - loss: 0.0267 - accuracy: 0.99 - ETA: 3s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0282 - accuracy: 0.99 - ETA: 2s - loss: 0.0283 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.99 - ETA: 2s - loss: 0.0281 - accuracy: 0.99 - ETA: 2s - loss: 0.0277 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0256 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0257 - accuracy: 0.99 - ETA: 1s - loss: 0.0254 - accuracy: 0.99 - ETA: 1s - loss: 0.0254 - accuracy: 0.99 - ETA: 1s - loss: 0.0253 - accuracy: 0.99 - ETA: 1s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0338 - accuracy: 0.99 - ETA: 0s - loss: 0.0344 - accuracy: 0.99 - ETA: 0s - loss: 0.0341 - accuracy: 0.99 - 6s 582us/step - loss: 0.0339 - accuracy: 0.9904 - val_loss: 0.0221 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00794\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0203 - accuracy: 0.99 - ETA: 5s - loss: 0.0288 - accuracy: 0.98 - ETA: 5s - loss: 0.0252 - accuracy: 0.99 - ETA: 5s - loss: 0.0228 - accuracy: 0.99 - ETA: 4s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0175 - accuracy: 0.99 - ETA: 4s - loss: 0.0169 - accuracy: 0.99 - ETA: 4s - loss: 0.0161 - accuracy: 0.99 - ETA: 4s - loss: 0.0148 - accuracy: 0.99 - ETA: 4s - loss: 0.0169 - accuracy: 0.99 - ETA: 4s - loss: 0.0160 - accuracy: 0.99 - ETA: 4s - loss: 0.0152 - accuracy: 0.99 - ETA: 4s - loss: 0.0162 - accuracy: 0.99 - ETA: 4s - loss: 0.0184 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0182 - accuracy: 0.99 - ETA: 4s - loss: 0.0219 - accuracy: 0.99 - ETA: 4s - loss: 0.0210 - accuracy: 0.99 - ETA: 4s - loss: 0.0213 - accuracy: 0.99 - ETA: 4s - loss: 0.0217 - accuracy: 0.99 - ETA: 3s - loss: 0.0208 - accuracy: 0.99 - ETA: 3s - loss: 0.0201 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0186 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 3s - loss: 0.0184 - accuracy: 0.99 - ETA: 3s - loss: 0.0178 - accuracy: 0.99 - ETA: 3s - loss: 0.0183 - accuracy: 0.99 - ETA: 3s - loss: 0.0186 - accuracy: 0.99 - ETA: 3s - loss: 0.0180 - accuracy: 0.99 - ETA: 3s - loss: 0.0180 - accuracy: 0.99 - ETA: 3s - loss: 0.0181 - accuracy: 0.99 - ETA: 3s - loss: 0.0178 - accuracy: 0.99 - ETA: 3s - loss: 0.0173 - accuracy: 0.99 - ETA: 2s - loss: 0.0179 - accuracy: 0.99 - ETA: 2s - loss: 0.0181 - accuracy: 0.99 - ETA: 2s - loss: 0.0180 - accuracy: 0.99 - ETA: 2s - loss: 0.0176 - accuracy: 0.99 - ETA: 2s - loss: 0.0174 - accuracy: 0.99 - ETA: 2s - loss: 0.0177 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0258 - accuracy: 0.99 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0258 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0248 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0241 - accuracy: 0.99 - ETA: 0s - loss: 0.0243 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - 6s 555us/step - loss: 0.0241 - accuracy: 0.9934 - val_loss: 0.0200 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00794\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0274 - accuracy: 0.98 - ETA: 5s - loss: 0.0206 - accuracy: 0.98 - ETA: 5s - loss: 0.0138 - accuracy: 0.99 - ETA: 5s - loss: 0.0191 - accuracy: 0.99 - ETA: 4s - loss: 0.0296 - accuracy: 0.99 - ETA: 4s - loss: 0.0287 - accuracy: 0.99 - ETA: 4s - loss: 0.0250 - accuracy: 0.99 - ETA: 4s - loss: 0.0229 - accuracy: 0.99 - ETA: 4s - loss: 0.0209 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0200 - accuracy: 0.99 - ETA: 4s - loss: 0.0188 - accuracy: 0.99 - ETA: 4s - loss: 0.0178 - accuracy: 0.99 - ETA: 4s - loss: 0.0168 - accuracy: 0.99 - ETA: 4s - loss: 0.0161 - accuracy: 0.99 - ETA: 4s - loss: 0.0152 - accuracy: 0.99 - ETA: 4s - loss: 0.0145 - accuracy: 0.99 - ETA: 4s - loss: 0.0137 - accuracy: 0.99 - ETA: 4s - loss: 0.0148 - accuracy: 0.99 - ETA: 4s - loss: 0.0165 - accuracy: 0.99 - ETA: 3s - loss: 0.0184 - accuracy: 0.99 - ETA: 3s - loss: 0.0178 - accuracy: 0.99 - ETA: 3s - loss: 0.0174 - accuracy: 0.99 - ETA: 3s - loss: 0.0170 - accuracy: 0.99 - ETA: 3s - loss: 0.0183 - accuracy: 0.99 - ETA: 3s - loss: 0.0185 - accuracy: 0.99 - ETA: 3s - loss: 0.0188 - accuracy: 0.99 - ETA: 3s - loss: 0.0186 - accuracy: 0.99 - ETA: 3s - loss: 0.0184 - accuracy: 0.99 - ETA: 3s - loss: 0.0180 - accuracy: 0.99 - ETA: 3s - loss: 0.0179 - accuracy: 0.99 - ETA: 3s - loss: 0.0177 - accuracy: 0.99 - ETA: 3s - loss: 0.0172 - accuracy: 0.99 - ETA: 2s - loss: 0.0169 - accuracy: 0.99 - ETA: 2s - loss: 0.0169 - accuracy: 0.99 - ETA: 2s - loss: 0.0165 - accuracy: 0.99 - ETA: 2s - loss: 0.0161 - accuracy: 0.99 - ETA: 2s - loss: 0.0160 - accuracy: 0.99 - ETA: 2s - loss: 0.0165 - accuracy: 0.99 - ETA: 2s - loss: 0.0165 - accuracy: 0.99 - ETA: 2s - loss: 0.0182 - accuracy: 0.99 - ETA: 2s - loss: 0.0187 - accuracy: 0.99 - ETA: 2s - loss: 0.0189 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0196 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0208 - accuracy: 0.99 - ETA: 1s - loss: 0.0286 - accuracy: 0.99 - ETA: 1s - loss: 0.0289 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0290 - accuracy: 0.99 - ETA: 1s - loss: 0.0286 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 1s - loss: 0.0279 - accuracy: 0.99 - ETA: 1s - loss: 0.0277 - accuracy: 0.99 - ETA: 1s - loss: 0.0274 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - ETA: 0s - loss: 0.0274 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - 6s 538us/step - loss: 0.0271 - accuracy: 0.9914 - val_loss: 0.0287 - val_accuracy: 0.9907\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00794\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0254 - accuracy: 0.99 - ETA: 5s - loss: 0.0241 - accuracy: 0.99 - ETA: 5s - loss: 0.0266 - accuracy: 0.99 - ETA: 5s - loss: 0.0263 - accuracy: 0.99 - ETA: 5s - loss: 0.0225 - accuracy: 0.99 - ETA: 5s - loss: 0.0216 - accuracy: 0.99 - ETA: 5s - loss: 0.0212 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0169 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0155 - accuracy: 0.99 - ETA: 4s - loss: 0.0148 - accuracy: 0.99 - ETA: 4s - loss: 0.0144 - accuracy: 0.99 - ETA: 4s - loss: 0.0136 - accuracy: 0.99 - ETA: 4s - loss: 0.0129 - accuracy: 0.99 - ETA: 4s - loss: 0.0124 - accuracy: 0.99 - ETA: 4s - loss: 0.0125 - accuracy: 0.99 - ETA: 4s - loss: 0.0146 - accuracy: 0.99 - ETA: 4s - loss: 0.0147 - accuracy: 0.99 - ETA: 4s - loss: 0.0147 - accuracy: 0.99 - ETA: 3s - loss: 0.0147 - accuracy: 0.99 - ETA: 3s - loss: 0.0176 - accuracy: 0.99 - ETA: 3s - loss: 0.0180 - accuracy: 0.99 - ETA: 3s - loss: 0.0177 - accuracy: 0.99 - ETA: 3s - loss: 0.0173 - accuracy: 0.99 - ETA: 3s - loss: 0.0177 - accuracy: 0.99 - ETA: 3s - loss: 0.0215 - accuracy: 0.99 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 3s - loss: 0.0264 - accuracy: 0.99 - ETA: 3s - loss: 0.0259 - accuracy: 0.99 - ETA: 3s - loss: 0.0255 - accuracy: 0.99 - ETA: 3s - loss: 0.0257 - accuracy: 0.99 - ETA: 3s - loss: 0.0252 - accuracy: 0.99 - ETA: 2s - loss: 0.0252 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0249 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0237 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 1s - loss: 0.0242 - accuracy: 0.99 - ETA: 1s - loss: 0.0240 - accuracy: 0.99 - ETA: 1s - loss: 0.0237 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0238 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0232 - accuracy: 0.99 - ETA: 1s - loss: 0.0228 - accuracy: 0.99 - ETA: 1s - loss: 0.0225 - accuracy: 0.99 - ETA: 1s - loss: 0.0224 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0225 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0217 - accuracy: 0.99 - ETA: 0s - loss: 0.0214 - accuracy: 0.99 - ETA: 0s - loss: 0.0212 - accuracy: 0.99 - ETA: 0s - loss: 0.0227 - accuracy: 0.99 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0223 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0217 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - 6s 544us/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.0401 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00794\n",
      "Epoch 00024: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 209us/step\n",
      "[2020-05-18 16:36:10 RAM75.6% 1.6GB] Val Score : [0.03561328319176732, 0.985595166683197]\n",
      "[2020-05-18 16:36:10 RAM75.6% 1.6GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:36:10 RAM75.6% 1.6GB] Training on Fold : 9\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 26s - loss: 3.2691 - accuracy: 0.434 - ETA: 15s - loss: 4.0609 - accuracy: 0.527 - ETA: 12s - loss: 3.0370 - accuracy: 0.542 - ETA: 10s - loss: 2.4888 - accuracy: 0.531 - ETA: 8s - loss: 2.1558 - accuracy: 0.533 - ETA: 8s - loss: 1.9247 - accuracy: 0.53 - ETA: 7s - loss: 1.7620 - accuracy: 0.53 - ETA: 7s - loss: 1.6311 - accuracy: 0.53 - ETA: 6s - loss: 1.5315 - accuracy: 0.53 - ETA: 6s - loss: 1.4506 - accuracy: 0.53 - ETA: 6s - loss: 1.3799 - accuracy: 0.54 - ETA: 6s - loss: 1.3221 - accuracy: 0.55 - ETA: 5s - loss: 1.2715 - accuracy: 0.55 - ETA: 5s - loss: 1.2323 - accuracy: 0.55 - ETA: 5s - loss: 1.1956 - accuracy: 0.55 - ETA: 5s - loss: 1.1623 - accuracy: 0.56 - ETA: 5s - loss: 1.1336 - accuracy: 0.56 - ETA: 5s - loss: 1.1061 - accuracy: 0.56 - ETA: 4s - loss: 1.0827 - accuracy: 0.57 - ETA: 4s - loss: 1.0643 - accuracy: 0.56 - ETA: 4s - loss: 1.0423 - accuracy: 0.57 - ETA: 4s - loss: 1.0235 - accuracy: 0.57 - ETA: 4s - loss: 1.0063 - accuracy: 0.58 - ETA: 4s - loss: 0.9901 - accuracy: 0.58 - ETA: 4s - loss: 0.9769 - accuracy: 0.58 - ETA: 4s - loss: 0.9638 - accuracy: 0.58 - ETA: 3s - loss: 0.9565 - accuracy: 0.58 - ETA: 3s - loss: 0.9470 - accuracy: 0.58 - ETA: 3s - loss: 0.9399 - accuracy: 0.58 - ETA: 3s - loss: 0.9294 - accuracy: 0.58 - ETA: 3s - loss: 0.9214 - accuracy: 0.58 - ETA: 3s - loss: 0.9124 - accuracy: 0.59 - ETA: 3s - loss: 0.9038 - accuracy: 0.59 - ETA: 3s - loss: 0.8949 - accuracy: 0.59 - ETA: 3s - loss: 0.8860 - accuracy: 0.59 - ETA: 3s - loss: 0.8776 - accuracy: 0.60 - ETA: 3s - loss: 0.8703 - accuracy: 0.60 - ETA: 2s - loss: 0.8629 - accuracy: 0.60 - ETA: 2s - loss: 0.8568 - accuracy: 0.60 - ETA: 2s - loss: 0.8505 - accuracy: 0.60 - ETA: 2s - loss: 0.8436 - accuracy: 0.60 - ETA: 2s - loss: 0.8368 - accuracy: 0.61 - ETA: 2s - loss: 0.8300 - accuracy: 0.61 - ETA: 2s - loss: 0.8221 - accuracy: 0.61 - ETA: 2s - loss: 0.8172 - accuracy: 0.61 - ETA: 2s - loss: 0.8122 - accuracy: 0.61 - ETA: 2s - loss: 0.8053 - accuracy: 0.62 - ETA: 2s - loss: 0.7993 - accuracy: 0.62 - ETA: 1s - loss: 0.7933 - accuracy: 0.62 - ETA: 1s - loss: 0.7873 - accuracy: 0.62 - ETA: 1s - loss: 0.7817 - accuracy: 0.62 - ETA: 1s - loss: 0.7765 - accuracy: 0.63 - ETA: 1s - loss: 0.7708 - accuracy: 0.63 - ETA: 1s - loss: 0.7642 - accuracy: 0.63 - ETA: 1s - loss: 0.7589 - accuracy: 0.63 - ETA: 1s - loss: 0.7548 - accuracy: 0.64 - ETA: 1s - loss: 0.7501 - accuracy: 0.64 - ETA: 1s - loss: 0.7444 - accuracy: 0.64 - ETA: 1s - loss: 0.7384 - accuracy: 0.64 - ETA: 1s - loss: 0.7336 - accuracy: 0.64 - ETA: 1s - loss: 0.7294 - accuracy: 0.65 - ETA: 0s - loss: 0.7235 - accuracy: 0.65 - ETA: 0s - loss: 0.7178 - accuracy: 0.65 - ETA: 0s - loss: 0.7137 - accuracy: 0.65 - ETA: 0s - loss: 0.7086 - accuracy: 0.65 - ETA: 0s - loss: 0.7037 - accuracy: 0.66 - ETA: 0s - loss: 0.6996 - accuracy: 0.66 - ETA: 0s - loss: 0.6938 - accuracy: 0.66 - ETA: 0s - loss: 0.6878 - accuracy: 0.66 - ETA: 0s - loss: 0.6830 - accuracy: 0.67 - ETA: 0s - loss: 0.6782 - accuracy: 0.67 - ETA: 0s - loss: 0.6746 - accuracy: 0.67 - ETA: 0s - loss: 0.6699 - accuracy: 0.67 - 6s 566us/step - loss: 0.6669 - accuracy: 0.6798 - val_loss: 0.3336 - val_accuracy: 0.7826\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00794\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.3455 - accuracy: 0.84 - ETA: 5s - loss: 0.3300 - accuracy: 0.83 - ETA: 5s - loss: 0.3149 - accuracy: 0.84 - ETA: 5s - loss: 0.3206 - accuracy: 0.83 - ETA: 4s - loss: 0.3767 - accuracy: 0.80 - ETA: 4s - loss: 0.3916 - accuracy: 0.79 - ETA: 4s - loss: 0.3869 - accuracy: 0.80 - ETA: 4s - loss: 0.3753 - accuracy: 0.81 - ETA: 4s - loss: 0.3723 - accuracy: 0.82 - ETA: 4s - loss: 0.3665 - accuracy: 0.82 - ETA: 4s - loss: 0.3574 - accuracy: 0.83 - ETA: 4s - loss: 0.3481 - accuracy: 0.83 - ETA: 4s - loss: 0.3449 - accuracy: 0.83 - ETA: 4s - loss: 0.3449 - accuracy: 0.83 - ETA: 4s - loss: 0.3417 - accuracy: 0.83 - ETA: 4s - loss: 0.3383 - accuracy: 0.83 - ETA: 4s - loss: 0.3343 - accuracy: 0.84 - ETA: 4s - loss: 0.3367 - accuracy: 0.84 - ETA: 4s - loss: 0.3406 - accuracy: 0.83 - ETA: 4s - loss: 0.3369 - accuracy: 0.84 - ETA: 3s - loss: 0.3321 - accuracy: 0.84 - ETA: 3s - loss: 0.3286 - accuracy: 0.84 - ETA: 3s - loss: 0.3268 - accuracy: 0.84 - ETA: 3s - loss: 0.3211 - accuracy: 0.85 - ETA: 3s - loss: 0.3166 - accuracy: 0.85 - ETA: 3s - loss: 0.3134 - accuracy: 0.85 - ETA: 3s - loss: 0.3094 - accuracy: 0.85 - ETA: 3s - loss: 0.3146 - accuracy: 0.85 - ETA: 3s - loss: 0.3230 - accuracy: 0.85 - ETA: 3s - loss: 0.3254 - accuracy: 0.84 - ETA: 3s - loss: 0.3228 - accuracy: 0.85 - ETA: 3s - loss: 0.3209 - accuracy: 0.85 - ETA: 3s - loss: 0.3193 - accuracy: 0.85 - ETA: 2s - loss: 0.3170 - accuracy: 0.85 - ETA: 2s - loss: 0.3142 - accuracy: 0.85 - ETA: 2s - loss: 0.3110 - accuracy: 0.85 - ETA: 2s - loss: 0.3096 - accuracy: 0.85 - ETA: 2s - loss: 0.3085 - accuracy: 0.85 - ETA: 2s - loss: 0.3053 - accuracy: 0.86 - ETA: 2s - loss: 0.3031 - accuracy: 0.86 - ETA: 2s - loss: 0.3010 - accuracy: 0.86 - ETA: 2s - loss: 0.3012 - accuracy: 0.86 - ETA: 2s - loss: 0.3010 - accuracy: 0.86 - ETA: 2s - loss: 0.3020 - accuracy: 0.86 - ETA: 2s - loss: 0.2997 - accuracy: 0.86 - ETA: 2s - loss: 0.2989 - accuracy: 0.86 - ETA: 1s - loss: 0.2966 - accuracy: 0.86 - ETA: 1s - loss: 0.2974 - accuracy: 0.86 - ETA: 1s - loss: 0.2974 - accuracy: 0.86 - ETA: 1s - loss: 0.2955 - accuracy: 0.86 - ETA: 1s - loss: 0.2939 - accuracy: 0.86 - ETA: 1s - loss: 0.2929 - accuracy: 0.86 - ETA: 1s - loss: 0.2897 - accuracy: 0.87 - ETA: 1s - loss: 0.2876 - accuracy: 0.87 - ETA: 1s - loss: 0.2878 - accuracy: 0.87 - ETA: 1s - loss: 0.2889 - accuracy: 0.87 - ETA: 1s - loss: 0.2896 - accuracy: 0.87 - ETA: 1s - loss: 0.2874 - accuracy: 0.87 - ETA: 1s - loss: 0.2859 - accuracy: 0.87 - ETA: 1s - loss: 0.2844 - accuracy: 0.87 - ETA: 0s - loss: 0.2826 - accuracy: 0.87 - ETA: 0s - loss: 0.2813 - accuracy: 0.87 - ETA: 0s - loss: 0.2799 - accuracy: 0.87 - ETA: 0s - loss: 0.2810 - accuracy: 0.87 - ETA: 0s - loss: 0.2814 - accuracy: 0.87 - ETA: 0s - loss: 0.2804 - accuracy: 0.87 - ETA: 0s - loss: 0.2787 - accuracy: 0.87 - ETA: 0s - loss: 0.2771 - accuracy: 0.87 - ETA: 0s - loss: 0.2778 - accuracy: 0.87 - ETA: 0s - loss: 0.2817 - accuracy: 0.87 - ETA: 0s - loss: 0.2803 - accuracy: 0.87 - ETA: 0s - loss: 0.2792 - accuracy: 0.87 - ETA: 0s - loss: 0.2781 - accuracy: 0.87 - 6s 534us/step - loss: 0.2776 - accuracy: 0.8781 - val_loss: 0.1503 - val_accuracy: 0.9545\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00794\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1461 - accuracy: 0.97 - ETA: 5s - loss: 0.1594 - accuracy: 0.95 - ETA: 5s - loss: 0.1796 - accuracy: 0.93 - ETA: 4s - loss: 0.1814 - accuracy: 0.92 - ETA: 4s - loss: 0.1750 - accuracy: 0.92 - ETA: 4s - loss: 0.1722 - accuracy: 0.92 - ETA: 4s - loss: 0.1779 - accuracy: 0.92 - ETA: 4s - loss: 0.1749 - accuracy: 0.92 - ETA: 4s - loss: 0.1729 - accuracy: 0.92 - ETA: 4s - loss: 0.1728 - accuracy: 0.92 - ETA: 4s - loss: 0.1729 - accuracy: 0.92 - ETA: 4s - loss: 0.1766 - accuracy: 0.92 - ETA: 4s - loss: 0.1789 - accuracy: 0.92 - ETA: 4s - loss: 0.1793 - accuracy: 0.92 - ETA: 4s - loss: 0.1806 - accuracy: 0.92 - ETA: 4s - loss: 0.1949 - accuracy: 0.92 - ETA: 4s - loss: 0.2103 - accuracy: 0.91 - ETA: 4s - loss: 0.2122 - accuracy: 0.91 - ETA: 4s - loss: 0.2113 - accuracy: 0.91 - ETA: 3s - loss: 0.2117 - accuracy: 0.91 - ETA: 3s - loss: 0.2100 - accuracy: 0.91 - ETA: 3s - loss: 0.2093 - accuracy: 0.91 - ETA: 3s - loss: 0.2054 - accuracy: 0.91 - ETA: 3s - loss: 0.2023 - accuracy: 0.91 - ETA: 3s - loss: 0.1998 - accuracy: 0.91 - ETA: 3s - loss: 0.1974 - accuracy: 0.92 - ETA: 3s - loss: 0.1959 - accuracy: 0.92 - ETA: 3s - loss: 0.1948 - accuracy: 0.92 - ETA: 3s - loss: 0.1954 - accuracy: 0.92 - ETA: 3s - loss: 0.1995 - accuracy: 0.91 - ETA: 3s - loss: 0.2063 - accuracy: 0.91 - ETA: 3s - loss: 0.2051 - accuracy: 0.91 - ETA: 3s - loss: 0.2023 - accuracy: 0.91 - ETA: 2s - loss: 0.2006 - accuracy: 0.91 - ETA: 2s - loss: 0.1995 - accuracy: 0.91 - ETA: 2s - loss: 0.1977 - accuracy: 0.91 - ETA: 2s - loss: 0.1966 - accuracy: 0.91 - ETA: 2s - loss: 0.1959 - accuracy: 0.91 - ETA: 2s - loss: 0.1963 - accuracy: 0.91 - ETA: 2s - loss: 0.1947 - accuracy: 0.91 - ETA: 2s - loss: 0.1935 - accuracy: 0.91 - ETA: 2s - loss: 0.1932 - accuracy: 0.91 - ETA: 2s - loss: 0.1958 - accuracy: 0.91 - ETA: 2s - loss: 0.2037 - accuracy: 0.91 - ETA: 2s - loss: 0.2045 - accuracy: 0.91 - ETA: 2s - loss: 0.2049 - accuracy: 0.91 - ETA: 1s - loss: 0.2040 - accuracy: 0.91 - ETA: 1s - loss: 0.2039 - accuracy: 0.91 - ETA: 1s - loss: 0.2025 - accuracy: 0.91 - ETA: 1s - loss: 0.2025 - accuracy: 0.91 - ETA: 1s - loss: 0.2014 - accuracy: 0.91 - ETA: 1s - loss: 0.2001 - accuracy: 0.91 - ETA: 1s - loss: 0.2013 - accuracy: 0.91 - ETA: 1s - loss: 0.2006 - accuracy: 0.91 - ETA: 1s - loss: 0.2001 - accuracy: 0.91 - ETA: 1s - loss: 0.1999 - accuracy: 0.91 - ETA: 1s - loss: 0.2015 - accuracy: 0.91 - ETA: 1s - loss: 0.2012 - accuracy: 0.91 - ETA: 1s - loss: 0.2014 - accuracy: 0.91 - ETA: 1s - loss: 0.2016 - accuracy: 0.91 - ETA: 0s - loss: 0.2010 - accuracy: 0.91 - ETA: 0s - loss: 0.2004 - accuracy: 0.91 - ETA: 0s - loss: 0.2007 - accuracy: 0.91 - ETA: 0s - loss: 0.1998 - accuracy: 0.91 - ETA: 0s - loss: 0.1997 - accuracy: 0.91 - ETA: 0s - loss: 0.1997 - accuracy: 0.91 - ETA: 0s - loss: 0.2008 - accuracy: 0.91 - ETA: 0s - loss: 0.2042 - accuracy: 0.91 - ETA: 0s - loss: 0.2050 - accuracy: 0.91 - ETA: 0s - loss: 0.2045 - accuracy: 0.91 - ETA: 0s - loss: 0.2046 - accuracy: 0.91 - ETA: 0s - loss: 0.2041 - accuracy: 0.91 - ETA: 0s - loss: 0.2027 - accuracy: 0.91 - 6s 532us/step - loss: 0.2021 - accuracy: 0.9177 - val_loss: 0.1252 - val_accuracy: 0.9562\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00794\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.1411 - accuracy: 0.94 - ETA: 5s - loss: 0.1459 - accuracy: 0.93 - ETA: 5s - loss: 0.1375 - accuracy: 0.94 - ETA: 5s - loss: 0.1382 - accuracy: 0.94 - ETA: 5s - loss: 0.1342 - accuracy: 0.94 - ETA: 5s - loss: 0.1271 - accuracy: 0.94 - ETA: 5s - loss: 0.1373 - accuracy: 0.94 - ETA: 4s - loss: 0.1382 - accuracy: 0.94 - ETA: 4s - loss: 0.1387 - accuracy: 0.94 - ETA: 4s - loss: 0.1397 - accuracy: 0.94 - ETA: 4s - loss: 0.1404 - accuracy: 0.94 - ETA: 4s - loss: 0.1468 - accuracy: 0.94 - ETA: 4s - loss: 0.1684 - accuracy: 0.93 - ETA: 4s - loss: 0.1689 - accuracy: 0.93 - ETA: 4s - loss: 0.1672 - accuracy: 0.93 - ETA: 4s - loss: 0.1670 - accuracy: 0.93 - ETA: 4s - loss: 0.1657 - accuracy: 0.93 - ETA: 4s - loss: 0.1625 - accuracy: 0.93 - ETA: 4s - loss: 0.1676 - accuracy: 0.93 - ETA: 4s - loss: 0.1734 - accuracy: 0.93 - ETA: 3s - loss: 0.1762 - accuracy: 0.93 - ETA: 3s - loss: 0.1763 - accuracy: 0.92 - ETA: 3s - loss: 0.1774 - accuracy: 0.92 - ETA: 3s - loss: 0.1766 - accuracy: 0.92 - ETA: 3s - loss: 0.1751 - accuracy: 0.92 - ETA: 3s - loss: 0.1745 - accuracy: 0.92 - ETA: 3s - loss: 0.1759 - accuracy: 0.92 - ETA: 3s - loss: 0.1748 - accuracy: 0.92 - ETA: 3s - loss: 0.1746 - accuracy: 0.92 - ETA: 3s - loss: 0.1726 - accuracy: 0.92 - ETA: 3s - loss: 0.1716 - accuracy: 0.93 - ETA: 3s - loss: 0.1696 - accuracy: 0.93 - ETA: 3s - loss: 0.1680 - accuracy: 0.93 - ETA: 2s - loss: 0.1748 - accuracy: 0.92 - ETA: 2s - loss: 0.1813 - accuracy: 0.92 - ETA: 2s - loss: 0.1829 - accuracy: 0.92 - ETA: 2s - loss: 0.1823 - accuracy: 0.92 - ETA: 2s - loss: 0.1825 - accuracy: 0.92 - ETA: 2s - loss: 0.1804 - accuracy: 0.92 - ETA: 2s - loss: 0.1789 - accuracy: 0.92 - ETA: 2s - loss: 0.1787 - accuracy: 0.92 - ETA: 2s - loss: 0.1787 - accuracy: 0.92 - ETA: 2s - loss: 0.1773 - accuracy: 0.92 - ETA: 2s - loss: 0.1758 - accuracy: 0.92 - ETA: 2s - loss: 0.1756 - accuracy: 0.92 - ETA: 2s - loss: 0.1755 - accuracy: 0.92 - ETA: 1s - loss: 0.1748 - accuracy: 0.92 - ETA: 1s - loss: 0.1751 - accuracy: 0.92 - ETA: 1s - loss: 0.1746 - accuracy: 0.92 - ETA: 1s - loss: 0.1735 - accuracy: 0.92 - ETA: 1s - loss: 0.1737 - accuracy: 0.92 - ETA: 1s - loss: 0.1734 - accuracy: 0.92 - ETA: 1s - loss: 0.1727 - accuracy: 0.92 - ETA: 1s - loss: 0.1728 - accuracy: 0.92 - ETA: 1s - loss: 0.1743 - accuracy: 0.92 - ETA: 1s - loss: 0.1759 - accuracy: 0.92 - ETA: 1s - loss: 0.1758 - accuracy: 0.92 - ETA: 1s - loss: 0.1747 - accuracy: 0.92 - ETA: 1s - loss: 0.1742 - accuracy: 0.92 - ETA: 1s - loss: 0.1737 - accuracy: 0.92 - ETA: 0s - loss: 0.1742 - accuracy: 0.92 - ETA: 0s - loss: 0.1736 - accuracy: 0.92 - ETA: 0s - loss: 0.1724 - accuracy: 0.92 - ETA: 0s - loss: 0.1716 - accuracy: 0.92 - ETA: 0s - loss: 0.1707 - accuracy: 0.92 - ETA: 0s - loss: 0.1698 - accuracy: 0.92 - ETA: 0s - loss: 0.1690 - accuracy: 0.92 - ETA: 0s - loss: 0.1682 - accuracy: 0.92 - ETA: 0s - loss: 0.1681 - accuracy: 0.92 - ETA: 0s - loss: 0.1715 - accuracy: 0.92 - ETA: 0s - loss: 0.1722 - accuracy: 0.92 - ETA: 0s - loss: 0.1718 - accuracy: 0.92 - ETA: 0s - loss: 0.1703 - accuracy: 0.92 - 6s 539us/step - loss: 0.1706 - accuracy: 0.9280 - val_loss: 0.1114 - val_accuracy: 0.9562\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00794\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1430 - accuracy: 0.93 - ETA: 5s - loss: 0.1253 - accuracy: 0.94 - ETA: 5s - loss: 0.1336 - accuracy: 0.93 - ETA: 5s - loss: 0.1290 - accuracy: 0.93 - ETA: 5s - loss: 0.1400 - accuracy: 0.93 - ETA: 5s - loss: 0.1358 - accuracy: 0.94 - ETA: 5s - loss: 0.1252 - accuracy: 0.94 - ETA: 5s - loss: 0.1207 - accuracy: 0.94 - ETA: 4s - loss: 0.1258 - accuracy: 0.94 - ETA: 4s - loss: 0.1290 - accuracy: 0.94 - ETA: 4s - loss: 0.1306 - accuracy: 0.94 - ETA: 4s - loss: 0.1288 - accuracy: 0.94 - ETA: 4s - loss: 0.1345 - accuracy: 0.94 - ETA: 4s - loss: 0.1435 - accuracy: 0.94 - ETA: 4s - loss: 0.1423 - accuracy: 0.94 - ETA: 4s - loss: 0.1402 - accuracy: 0.94 - ETA: 4s - loss: 0.1372 - accuracy: 0.94 - ETA: 4s - loss: 0.1375 - accuracy: 0.94 - ETA: 4s - loss: 0.1372 - accuracy: 0.94 - ETA: 4s - loss: 0.1354 - accuracy: 0.94 - ETA: 4s - loss: 0.1348 - accuracy: 0.94 - ETA: 4s - loss: 0.1354 - accuracy: 0.94 - ETA: 3s - loss: 0.1345 - accuracy: 0.94 - ETA: 3s - loss: 0.1325 - accuracy: 0.94 - ETA: 3s - loss: 0.1331 - accuracy: 0.94 - ETA: 3s - loss: 0.1360 - accuracy: 0.94 - ETA: 3s - loss: 0.1346 - accuracy: 0.94 - ETA: 3s - loss: 0.1338 - accuracy: 0.94 - ETA: 3s - loss: 0.1320 - accuracy: 0.94 - ETA: 3s - loss: 0.1313 - accuracy: 0.94 - ETA: 3s - loss: 0.1308 - accuracy: 0.94 - ETA: 3s - loss: 0.1322 - accuracy: 0.94 - ETA: 3s - loss: 0.1326 - accuracy: 0.94 - ETA: 3s - loss: 0.1310 - accuracy: 0.94 - ETA: 3s - loss: 0.1329 - accuracy: 0.94 - ETA: 2s - loss: 0.1445 - accuracy: 0.94 - ETA: 2s - loss: 0.1467 - accuracy: 0.93 - ETA: 2s - loss: 0.1471 - accuracy: 0.93 - ETA: 2s - loss: 0.1461 - accuracy: 0.93 - ETA: 2s - loss: 0.1449 - accuracy: 0.93 - ETA: 2s - loss: 0.1453 - accuracy: 0.93 - ETA: 2s - loss: 0.1432 - accuracy: 0.93 - ETA: 2s - loss: 0.1425 - accuracy: 0.93 - ETA: 2s - loss: 0.1426 - accuracy: 0.93 - ETA: 2s - loss: 0.1412 - accuracy: 0.93 - ETA: 2s - loss: 0.1406 - accuracy: 0.93 - ETA: 2s - loss: 0.1399 - accuracy: 0.93 - ETA: 2s - loss: 0.1380 - accuracy: 0.94 - ETA: 1s - loss: 0.1370 - accuracy: 0.94 - ETA: 1s - loss: 0.1360 - accuracy: 0.94 - ETA: 1s - loss: 0.1355 - accuracy: 0.94 - ETA: 1s - loss: 0.1351 - accuracy: 0.94 - ETA: 1s - loss: 0.1346 - accuracy: 0.94 - ETA: 1s - loss: 0.1340 - accuracy: 0.94 - ETA: 1s - loss: 0.1353 - accuracy: 0.94 - ETA: 1s - loss: 0.1382 - accuracy: 0.93 - ETA: 1s - loss: 0.1393 - accuracy: 0.93 - ETA: 1s - loss: 0.1408 - accuracy: 0.93 - ETA: 1s - loss: 0.1418 - accuracy: 0.93 - ETA: 1s - loss: 0.1410 - accuracy: 0.93 - ETA: 1s - loss: 0.1402 - accuracy: 0.93 - ETA: 0s - loss: 0.1404 - accuracy: 0.93 - ETA: 0s - loss: 0.1408 - accuracy: 0.93 - ETA: 0s - loss: 0.1399 - accuracy: 0.93 - ETA: 0s - loss: 0.1383 - accuracy: 0.94 - ETA: 0s - loss: 0.1389 - accuracy: 0.94 - ETA: 0s - loss: 0.1387 - accuracy: 0.94 - ETA: 0s - loss: 0.1377 - accuracy: 0.94 - ETA: 0s - loss: 0.1372 - accuracy: 0.94 - ETA: 0s - loss: 0.1363 - accuracy: 0.94 - ETA: 0s - loss: 0.1359 - accuracy: 0.94 - ETA: 0s - loss: 0.1367 - accuracy: 0.94 - ETA: 0s - loss: 0.1371 - accuracy: 0.94 - 6s 570us/step - loss: 0.1369 - accuracy: 0.9414 - val_loss: 0.0792 - val_accuracy: 0.9739\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00794\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1367 - accuracy: 0.94 - ETA: 5s - loss: 0.1236 - accuracy: 0.95 - ETA: 5s - loss: 0.1178 - accuracy: 0.95 - ETA: 5s - loss: 0.1150 - accuracy: 0.95 - ETA: 5s - loss: 0.1118 - accuracy: 0.95 - ETA: 5s - loss: 0.1120 - accuracy: 0.95 - ETA: 5s - loss: 0.1136 - accuracy: 0.95 - ETA: 4s - loss: 0.1182 - accuracy: 0.94 - ETA: 4s - loss: 0.1155 - accuracy: 0.94 - ETA: 4s - loss: 0.1146 - accuracy: 0.94 - ETA: 4s - loss: 0.1139 - accuracy: 0.94 - ETA: 4s - loss: 0.1122 - accuracy: 0.95 - ETA: 4s - loss: 0.1087 - accuracy: 0.95 - ETA: 4s - loss: 0.1047 - accuracy: 0.95 - ETA: 4s - loss: 0.1027 - accuracy: 0.95 - ETA: 4s - loss: 0.1111 - accuracy: 0.95 - ETA: 4s - loss: 0.1250 - accuracy: 0.94 - ETA: 4s - loss: 0.1285 - accuracy: 0.94 - ETA: 4s - loss: 0.1269 - accuracy: 0.94 - ETA: 4s - loss: 0.1261 - accuracy: 0.94 - ETA: 4s - loss: 0.1263 - accuracy: 0.94 - ETA: 3s - loss: 0.1243 - accuracy: 0.94 - ETA: 3s - loss: 0.1223 - accuracy: 0.94 - ETA: 3s - loss: 0.1204 - accuracy: 0.95 - ETA: 3s - loss: 0.1178 - accuracy: 0.95 - ETA: 3s - loss: 0.1175 - accuracy: 0.95 - ETA: 3s - loss: 0.1161 - accuracy: 0.95 - ETA: 3s - loss: 0.1158 - accuracy: 0.95 - ETA: 3s - loss: 0.1149 - accuracy: 0.95 - ETA: 3s - loss: 0.1134 - accuracy: 0.95 - ETA: 3s - loss: 0.1129 - accuracy: 0.95 - ETA: 3s - loss: 0.1126 - accuracy: 0.95 - ETA: 3s - loss: 0.1139 - accuracy: 0.95 - ETA: 3s - loss: 0.1131 - accuracy: 0.95 - ETA: 2s - loss: 0.1124 - accuracy: 0.95 - ETA: 2s - loss: 0.1114 - accuracy: 0.95 - ETA: 2s - loss: 0.1096 - accuracy: 0.95 - ETA: 2s - loss: 0.1090 - accuracy: 0.95 - ETA: 2s - loss: 0.1097 - accuracy: 0.95 - ETA: 2s - loss: 0.1150 - accuracy: 0.95 - ETA: 2s - loss: 0.1162 - accuracy: 0.95 - ETA: 2s - loss: 0.1158 - accuracy: 0.95 - ETA: 2s - loss: 0.1155 - accuracy: 0.95 - ETA: 2s - loss: 0.1150 - accuracy: 0.95 - ETA: 2s - loss: 0.1148 - accuracy: 0.95 - ETA: 2s - loss: 0.1138 - accuracy: 0.95 - ETA: 2s - loss: 0.1127 - accuracy: 0.95 - ETA: 2s - loss: 0.1109 - accuracy: 0.95 - ETA: 1s - loss: 0.1098 - accuracy: 0.95 - ETA: 1s - loss: 0.1093 - accuracy: 0.95 - ETA: 1s - loss: 0.1081 - accuracy: 0.95 - ETA: 1s - loss: 0.1071 - accuracy: 0.95 - ETA: 1s - loss: 0.1062 - accuracy: 0.95 - ETA: 1s - loss: 0.1060 - accuracy: 0.95 - ETA: 1s - loss: 0.1067 - accuracy: 0.95 - ETA: 1s - loss: 0.1080 - accuracy: 0.95 - ETA: 1s - loss: 0.1089 - accuracy: 0.95 - ETA: 1s - loss: 0.1082 - accuracy: 0.95 - ETA: 1s - loss: 0.1079 - accuracy: 0.95 - ETA: 1s - loss: 0.1086 - accuracy: 0.95 - ETA: 1s - loss: 0.1103 - accuracy: 0.95 - ETA: 0s - loss: 0.1103 - accuracy: 0.95 - ETA: 0s - loss: 0.1097 - accuracy: 0.95 - ETA: 0s - loss: 0.1090 - accuracy: 0.95 - ETA: 0s - loss: 0.1097 - accuracy: 0.95 - ETA: 0s - loss: 0.1099 - accuracy: 0.95 - ETA: 0s - loss: 0.1106 - accuracy: 0.95 - ETA: 0s - loss: 0.1103 - accuracy: 0.95 - ETA: 0s - loss: 0.1101 - accuracy: 0.95 - ETA: 0s - loss: 0.1102 - accuracy: 0.95 - ETA: 0s - loss: 0.1098 - accuracy: 0.95 - ETA: 0s - loss: 0.1089 - accuracy: 0.95 - ETA: 0s - loss: 0.1082 - accuracy: 0.95 - 6s 574us/step - loss: 0.1077 - accuracy: 0.9552 - val_loss: 0.0516 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00794\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0481 - accuracy: 0.96 - ETA: 5s - loss: 0.0781 - accuracy: 0.96 - ETA: 5s - loss: 0.1105 - accuracy: 0.94 - ETA: 5s - loss: 0.1127 - accuracy: 0.94 - ETA: 5s - loss: 0.0979 - accuracy: 0.95 - ETA: 4s - loss: 0.0894 - accuracy: 0.95 - ETA: 4s - loss: 0.0835 - accuracy: 0.96 - ETA: 4s - loss: 0.0788 - accuracy: 0.96 - ETA: 4s - loss: 0.0795 - accuracy: 0.96 - ETA: 4s - loss: 0.0784 - accuracy: 0.96 - ETA: 4s - loss: 0.0845 - accuracy: 0.96 - ETA: 4s - loss: 0.0890 - accuracy: 0.96 - ETA: 4s - loss: 0.0876 - accuracy: 0.95 - ETA: 4s - loss: 0.0863 - accuracy: 0.96 - ETA: 4s - loss: 0.0843 - accuracy: 0.96 - ETA: 4s - loss: 0.0856 - accuracy: 0.95 - ETA: 4s - loss: 0.0838 - accuracy: 0.96 - ETA: 4s - loss: 0.0830 - accuracy: 0.96 - ETA: 4s - loss: 0.0847 - accuracy: 0.96 - ETA: 4s - loss: 0.0870 - accuracy: 0.96 - ETA: 3s - loss: 0.0872 - accuracy: 0.96 - ETA: 3s - loss: 0.0860 - accuracy: 0.96 - ETA: 3s - loss: 0.0866 - accuracy: 0.96 - ETA: 3s - loss: 0.0875 - accuracy: 0.96 - ETA: 3s - loss: 0.0848 - accuracy: 0.96 - ETA: 3s - loss: 0.0841 - accuracy: 0.96 - ETA: 3s - loss: 0.0868 - accuracy: 0.96 - ETA: 3s - loss: 0.0878 - accuracy: 0.96 - ETA: 3s - loss: 0.0905 - accuracy: 0.95 - ETA: 3s - loss: 0.0901 - accuracy: 0.95 - ETA: 3s - loss: 0.0918 - accuracy: 0.95 - ETA: 3s - loss: 0.0911 - accuracy: 0.96 - ETA: 3s - loss: 0.0903 - accuracy: 0.96 - ETA: 2s - loss: 0.0891 - accuracy: 0.96 - ETA: 2s - loss: 0.0882 - accuracy: 0.96 - ETA: 2s - loss: 0.0876 - accuracy: 0.96 - ETA: 2s - loss: 0.0866 - accuracy: 0.96 - ETA: 2s - loss: 0.0856 - accuracy: 0.96 - ETA: 2s - loss: 0.0852 - accuracy: 0.96 - ETA: 2s - loss: 0.0840 - accuracy: 0.96 - ETA: 2s - loss: 0.0831 - accuracy: 0.96 - ETA: 2s - loss: 0.0824 - accuracy: 0.96 - ETA: 2s - loss: 0.0835 - accuracy: 0.96 - ETA: 2s - loss: 0.0855 - accuracy: 0.96 - ETA: 2s - loss: 0.0905 - accuracy: 0.96 - ETA: 2s - loss: 0.0898 - accuracy: 0.96 - ETA: 2s - loss: 0.0889 - accuracy: 0.96 - ETA: 1s - loss: 0.0885 - accuracy: 0.96 - ETA: 1s - loss: 0.0870 - accuracy: 0.96 - ETA: 1s - loss: 0.0872 - accuracy: 0.96 - ETA: 1s - loss: 0.0870 - accuracy: 0.96 - ETA: 1s - loss: 0.0888 - accuracy: 0.96 - ETA: 1s - loss: 0.0892 - accuracy: 0.96 - ETA: 1s - loss: 0.0888 - accuracy: 0.96 - ETA: 1s - loss: 0.0882 - accuracy: 0.96 - ETA: 1s - loss: 0.0872 - accuracy: 0.96 - ETA: 1s - loss: 0.0866 - accuracy: 0.96 - ETA: 1s - loss: 0.0866 - accuracy: 0.96 - ETA: 1s - loss: 0.0869 - accuracy: 0.96 - ETA: 1s - loss: 0.0875 - accuracy: 0.96 - ETA: 0s - loss: 0.0897 - accuracy: 0.96 - ETA: 0s - loss: 0.0905 - accuracy: 0.96 - ETA: 0s - loss: 0.0902 - accuracy: 0.96 - ETA: 0s - loss: 0.0900 - accuracy: 0.96 - ETA: 0s - loss: 0.0893 - accuracy: 0.96 - ETA: 0s - loss: 0.0886 - accuracy: 0.96 - ETA: 0s - loss: 0.0883 - accuracy: 0.96 - ETA: 0s - loss: 0.0882 - accuracy: 0.96 - ETA: 0s - loss: 0.0878 - accuracy: 0.96 - ETA: 0s - loss: 0.0880 - accuracy: 0.96 - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - ETA: 0s - loss: 0.0875 - accuracy: 0.96 - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - 6s 541us/step - loss: 0.0870 - accuracy: 0.9642 - val_loss: 0.0454 - val_accuracy: 0.9857\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00794\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0463 - accuracy: 0.97 - ETA: 5s - loss: 0.0469 - accuracy: 0.97 - ETA: 5s - loss: 0.0691 - accuracy: 0.97 - ETA: 5s - loss: 0.0669 - accuracy: 0.97 - ETA: 5s - loss: 0.0671 - accuracy: 0.97 - ETA: 4s - loss: 0.0744 - accuracy: 0.97 - ETA: 4s - loss: 0.0745 - accuracy: 0.97 - ETA: 4s - loss: 0.0810 - accuracy: 0.96 - ETA: 4s - loss: 0.0878 - accuracy: 0.96 - ETA: 4s - loss: 0.0863 - accuracy: 0.96 - ETA: 4s - loss: 0.0836 - accuracy: 0.96 - ETA: 4s - loss: 0.0795 - accuracy: 0.96 - ETA: 4s - loss: 0.0771 - accuracy: 0.97 - ETA: 4s - loss: 0.0747 - accuracy: 0.97 - ETA: 4s - loss: 0.0764 - accuracy: 0.97 - ETA: 4s - loss: 0.0852 - accuracy: 0.96 - ETA: 4s - loss: 0.0870 - accuracy: 0.96 - ETA: 4s - loss: 0.0847 - accuracy: 0.96 - ETA: 4s - loss: 0.0816 - accuracy: 0.97 - ETA: 4s - loss: 0.0814 - accuracy: 0.97 - ETA: 3s - loss: 0.0798 - accuracy: 0.97 - ETA: 3s - loss: 0.0782 - accuracy: 0.97 - ETA: 3s - loss: 0.0778 - accuracy: 0.97 - ETA: 3s - loss: 0.0779 - accuracy: 0.97 - ETA: 3s - loss: 0.0758 - accuracy: 0.97 - ETA: 3s - loss: 0.0750 - accuracy: 0.97 - ETA: 3s - loss: 0.0742 - accuracy: 0.97 - ETA: 3s - loss: 0.0737 - accuracy: 0.97 - ETA: 3s - loss: 0.0722 - accuracy: 0.97 - ETA: 3s - loss: 0.0712 - accuracy: 0.97 - ETA: 3s - loss: 0.0703 - accuracy: 0.97 - ETA: 3s - loss: 0.0690 - accuracy: 0.97 - ETA: 3s - loss: 0.0678 - accuracy: 0.97 - ETA: 2s - loss: 0.0686 - accuracy: 0.97 - ETA: 2s - loss: 0.0705 - accuracy: 0.97 - ETA: 2s - loss: 0.0788 - accuracy: 0.97 - ETA: 2s - loss: 0.0827 - accuracy: 0.96 - ETA: 2s - loss: 0.0813 - accuracy: 0.97 - ETA: 2s - loss: 0.0815 - accuracy: 0.96 - ETA: 2s - loss: 0.0804 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 2s - loss: 0.0774 - accuracy: 0.97 - ETA: 2s - loss: 0.0765 - accuracy: 0.97 - ETA: 2s - loss: 0.0756 - accuracy: 0.97 - ETA: 2s - loss: 0.0749 - accuracy: 0.97 - ETA: 2s - loss: 0.0748 - accuracy: 0.97 - ETA: 2s - loss: 0.0743 - accuracy: 0.97 - ETA: 1s - loss: 0.0733 - accuracy: 0.97 - ETA: 1s - loss: 0.0736 - accuracy: 0.97 - ETA: 1s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0733 - accuracy: 0.97 - ETA: 1s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0733 - accuracy: 0.97 - ETA: 1s - loss: 0.0733 - accuracy: 0.97 - ETA: 1s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0741 - accuracy: 0.97 - ETA: 1s - loss: 0.0743 - accuracy: 0.97 - ETA: 1s - loss: 0.0742 - accuracy: 0.97 - ETA: 1s - loss: 0.0741 - accuracy: 0.97 - ETA: 1s - loss: 0.0737 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0742 - accuracy: 0.97 - ETA: 0s - loss: 0.0741 - accuracy: 0.97 - ETA: 0s - loss: 0.0735 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - ETA: 0s - loss: 0.0724 - accuracy: 0.97 - ETA: 0s - loss: 0.0730 - accuracy: 0.97 - ETA: 0s - loss: 0.0737 - accuracy: 0.97 - ETA: 0s - loss: 0.0735 - accuracy: 0.97 - ETA: 0s - loss: 0.0731 - accuracy: 0.97 - ETA: 0s - loss: 0.0732 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0749 - accuracy: 0.97 - 6s 554us/step - loss: 0.0757 - accuracy: 0.9727 - val_loss: 0.0517 - val_accuracy: 0.9781\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00794\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0784 - accuracy: 0.95 - ETA: 5s - loss: 0.0624 - accuracy: 0.97 - ETA: 5s - loss: 0.0586 - accuracy: 0.97 - ETA: 4s - loss: 0.0627 - accuracy: 0.97 - ETA: 4s - loss: 0.0669 - accuracy: 0.97 - ETA: 4s - loss: 0.0610 - accuracy: 0.97 - ETA: 4s - loss: 0.0675 - accuracy: 0.97 - ETA: 4s - loss: 0.0615 - accuracy: 0.97 - ETA: 4s - loss: 0.0607 - accuracy: 0.97 - ETA: 4s - loss: 0.0601 - accuracy: 0.97 - ETA: 4s - loss: 0.0632 - accuracy: 0.97 - ETA: 4s - loss: 0.0666 - accuracy: 0.97 - ETA: 4s - loss: 0.0666 - accuracy: 0.97 - ETA: 4s - loss: 0.0699 - accuracy: 0.97 - ETA: 4s - loss: 0.0732 - accuracy: 0.97 - ETA: 4s - loss: 0.0710 - accuracy: 0.97 - ETA: 4s - loss: 0.0701 - accuracy: 0.97 - ETA: 4s - loss: 0.0672 - accuracy: 0.97 - ETA: 4s - loss: 0.0656 - accuracy: 0.97 - ETA: 4s - loss: 0.0641 - accuracy: 0.97 - ETA: 3s - loss: 0.0641 - accuracy: 0.97 - ETA: 3s - loss: 0.0643 - accuracy: 0.97 - ETA: 3s - loss: 0.0639 - accuracy: 0.97 - ETA: 3s - loss: 0.0617 - accuracy: 0.97 - ETA: 3s - loss: 0.0618 - accuracy: 0.97 - ETA: 3s - loss: 0.0629 - accuracy: 0.97 - ETA: 3s - loss: 0.0626 - accuracy: 0.97 - ETA: 3s - loss: 0.0639 - accuracy: 0.97 - ETA: 3s - loss: 0.0645 - accuracy: 0.97 - ETA: 3s - loss: 0.0655 - accuracy: 0.97 - ETA: 3s - loss: 0.0651 - accuracy: 0.97 - ETA: 3s - loss: 0.0666 - accuracy: 0.97 - ETA: 3s - loss: 0.0689 - accuracy: 0.97 - ETA: 2s - loss: 0.0695 - accuracy: 0.97 - ETA: 2s - loss: 0.0698 - accuracy: 0.97 - ETA: 2s - loss: 0.0704 - accuracy: 0.97 - ETA: 2s - loss: 0.0694 - accuracy: 0.97 - ETA: 2s - loss: 0.0689 - accuracy: 0.97 - ETA: 2s - loss: 0.0681 - accuracy: 0.97 - ETA: 2s - loss: 0.0671 - accuracy: 0.97 - ETA: 2s - loss: 0.0662 - accuracy: 0.97 - ETA: 2s - loss: 0.0655 - accuracy: 0.97 - ETA: 2s - loss: 0.0674 - accuracy: 0.97 - ETA: 2s - loss: 0.0694 - accuracy: 0.97 - ETA: 2s - loss: 0.0689 - accuracy: 0.97 - ETA: 2s - loss: 0.0691 - accuracy: 0.97 - ETA: 2s - loss: 0.0689 - accuracy: 0.97 - ETA: 1s - loss: 0.0680 - accuracy: 0.97 - ETA: 1s - loss: 0.0680 - accuracy: 0.97 - ETA: 1s - loss: 0.0683 - accuracy: 0.97 - ETA: 1s - loss: 0.0677 - accuracy: 0.97 - ETA: 1s - loss: 0.0665 - accuracy: 0.97 - ETA: 1s - loss: 0.0656 - accuracy: 0.97 - ETA: 1s - loss: 0.0654 - accuracy: 0.97 - ETA: 1s - loss: 0.0663 - accuracy: 0.97 - ETA: 1s - loss: 0.0657 - accuracy: 0.97 - ETA: 1s - loss: 0.0650 - accuracy: 0.97 - ETA: 1s - loss: 0.0644 - accuracy: 0.97 - ETA: 1s - loss: 0.0637 - accuracy: 0.97 - ETA: 1s - loss: 0.0636 - accuracy: 0.97 - ETA: 0s - loss: 0.0630 - accuracy: 0.97 - ETA: 0s - loss: 0.0628 - accuracy: 0.97 - ETA: 0s - loss: 0.0620 - accuracy: 0.97 - ETA: 0s - loss: 0.0618 - accuracy: 0.97 - ETA: 0s - loss: 0.0620 - accuracy: 0.97 - ETA: 0s - loss: 0.0624 - accuracy: 0.97 - ETA: 0s - loss: 0.0617 - accuracy: 0.97 - ETA: 0s - loss: 0.0615 - accuracy: 0.97 - ETA: 0s - loss: 0.0608 - accuracy: 0.97 - ETA: 0s - loss: 0.0601 - accuracy: 0.97 - ETA: 0s - loss: 0.0598 - accuracy: 0.97 - ETA: 0s - loss: 0.0596 - accuracy: 0.97 - ETA: 0s - loss: 0.0597 - accuracy: 0.97 - 6s 537us/step - loss: 0.0610 - accuracy: 0.9765 - val_loss: 0.1095 - val_accuracy: 0.9528\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00794\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1201 - accuracy: 0.95 - ETA: 5s - loss: 0.0746 - accuracy: 0.97 - ETA: 5s - loss: 0.0559 - accuracy: 0.98 - ETA: 5s - loss: 0.0498 - accuracy: 0.98 - ETA: 5s - loss: 0.0461 - accuracy: 0.98 - ETA: 5s - loss: 0.0453 - accuracy: 0.98 - ETA: 5s - loss: 0.0464 - accuracy: 0.98 - ETA: 4s - loss: 0.0450 - accuracy: 0.98 - ETA: 4s - loss: 0.0473 - accuracy: 0.98 - ETA: 4s - loss: 0.0494 - accuracy: 0.97 - ETA: 4s - loss: 0.0487 - accuracy: 0.97 - ETA: 4s - loss: 0.0458 - accuracy: 0.98 - ETA: 4s - loss: 0.0431 - accuracy: 0.98 - ETA: 4s - loss: 0.0427 - accuracy: 0.98 - ETA: 4s - loss: 0.0429 - accuracy: 0.98 - ETA: 4s - loss: 0.0443 - accuracy: 0.98 - ETA: 4s - loss: 0.0461 - accuracy: 0.98 - ETA: 4s - loss: 0.0523 - accuracy: 0.98 - ETA: 4s - loss: 0.0569 - accuracy: 0.97 - ETA: 4s - loss: 0.0602 - accuracy: 0.97 - ETA: 3s - loss: 0.0593 - accuracy: 0.97 - ETA: 3s - loss: 0.0598 - accuracy: 0.97 - ETA: 3s - loss: 0.0582 - accuracy: 0.97 - ETA: 3s - loss: 0.0568 - accuracy: 0.97 - ETA: 3s - loss: 0.0565 - accuracy: 0.97 - ETA: 3s - loss: 0.0557 - accuracy: 0.97 - ETA: 3s - loss: 0.0540 - accuracy: 0.97 - ETA: 3s - loss: 0.0536 - accuracy: 0.97 - ETA: 3s - loss: 0.0555 - accuracy: 0.97 - ETA: 3s - loss: 0.0567 - accuracy: 0.97 - ETA: 3s - loss: 0.0564 - accuracy: 0.97 - ETA: 3s - loss: 0.0557 - accuracy: 0.97 - ETA: 3s - loss: 0.0546 - accuracy: 0.97 - ETA: 2s - loss: 0.0533 - accuracy: 0.97 - ETA: 2s - loss: 0.0538 - accuracy: 0.97 - ETA: 2s - loss: 0.0537 - accuracy: 0.97 - ETA: 2s - loss: 0.0529 - accuracy: 0.97 - ETA: 2s - loss: 0.0516 - accuracy: 0.98 - ETA: 2s - loss: 0.0504 - accuracy: 0.98 - ETA: 2s - loss: 0.0496 - accuracy: 0.98 - ETA: 2s - loss: 0.0484 - accuracy: 0.98 - ETA: 2s - loss: 0.0485 - accuracy: 0.98 - ETA: 2s - loss: 0.0505 - accuracy: 0.98 - ETA: 2s - loss: 0.0503 - accuracy: 0.98 - ETA: 2s - loss: 0.0497 - accuracy: 0.98 - ETA: 2s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0507 - accuracy: 0.98 - ETA: 1s - loss: 0.0533 - accuracy: 0.97 - ETA: 1s - loss: 0.0592 - accuracy: 0.97 - ETA: 1s - loss: 0.0588 - accuracy: 0.97 - ETA: 1s - loss: 0.0589 - accuracy: 0.97 - ETA: 1s - loss: 0.0583 - accuracy: 0.97 - ETA: 1s - loss: 0.0576 - accuracy: 0.97 - ETA: 1s - loss: 0.0569 - accuracy: 0.97 - ETA: 1s - loss: 0.0568 - accuracy: 0.97 - ETA: 1s - loss: 0.0560 - accuracy: 0.97 - ETA: 1s - loss: 0.0567 - accuracy: 0.97 - ETA: 1s - loss: 0.0561 - accuracy: 0.97 - ETA: 1s - loss: 0.0554 - accuracy: 0.97 - ETA: 1s - loss: 0.0547 - accuracy: 0.98 - ETA: 0s - loss: 0.0543 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0534 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0525 - accuracy: 0.98 - ETA: 0s - loss: 0.0519 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0541 - accuracy: 0.98 - ETA: 0s - loss: 0.0544 - accuracy: 0.98 - ETA: 0s - loss: 0.0565 - accuracy: 0.98 - 6s 535us/step - loss: 0.0566 - accuracy: 0.9798 - val_loss: 0.0334 - val_accuracy: 0.9907\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00794\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0890 - accuracy: 0.97 - ETA: 5s - loss: 0.1196 - accuracy: 0.95 - ETA: 5s - loss: 0.1178 - accuracy: 0.96 - ETA: 5s - loss: 0.1025 - accuracy: 0.96 - ETA: 5s - loss: 0.0882 - accuracy: 0.97 - ETA: 5s - loss: 0.0759 - accuracy: 0.97 - ETA: 4s - loss: 0.0679 - accuracy: 0.97 - ETA: 4s - loss: 0.0658 - accuracy: 0.97 - ETA: 4s - loss: 0.0679 - accuracy: 0.97 - ETA: 4s - loss: 0.0704 - accuracy: 0.97 - ETA: 4s - loss: 0.0679 - accuracy: 0.97 - ETA: 4s - loss: 0.0674 - accuracy: 0.97 - ETA: 4s - loss: 0.0652 - accuracy: 0.97 - ETA: 4s - loss: 0.0620 - accuracy: 0.97 - ETA: 4s - loss: 0.0611 - accuracy: 0.97 - ETA: 4s - loss: 0.0594 - accuracy: 0.97 - ETA: 4s - loss: 0.0581 - accuracy: 0.97 - ETA: 4s - loss: 0.0573 - accuracy: 0.98 - ETA: 4s - loss: 0.0556 - accuracy: 0.98 - ETA: 4s - loss: 0.0536 - accuracy: 0.98 - ETA: 3s - loss: 0.0533 - accuracy: 0.98 - ETA: 3s - loss: 0.0529 - accuracy: 0.98 - ETA: 3s - loss: 0.0525 - accuracy: 0.98 - ETA: 3s - loss: 0.0525 - accuracy: 0.98 - ETA: 3s - loss: 0.0529 - accuracy: 0.98 - ETA: 3s - loss: 0.0530 - accuracy: 0.98 - ETA: 3s - loss: 0.0536 - accuracy: 0.98 - ETA: 3s - loss: 0.0536 - accuracy: 0.98 - ETA: 3s - loss: 0.0528 - accuracy: 0.98 - ETA: 3s - loss: 0.0518 - accuracy: 0.98 - ETA: 3s - loss: 0.0506 - accuracy: 0.98 - ETA: 3s - loss: 0.0498 - accuracy: 0.98 - ETA: 3s - loss: 0.0487 - accuracy: 0.98 - ETA: 2s - loss: 0.0477 - accuracy: 0.98 - ETA: 2s - loss: 0.0480 - accuracy: 0.98 - ETA: 2s - loss: 0.0483 - accuracy: 0.98 - ETA: 2s - loss: 0.0473 - accuracy: 0.98 - ETA: 2s - loss: 0.0467 - accuracy: 0.98 - ETA: 2s - loss: 0.0464 - accuracy: 0.98 - ETA: 2s - loss: 0.0465 - accuracy: 0.98 - ETA: 2s - loss: 0.0471 - accuracy: 0.98 - ETA: 2s - loss: 0.0492 - accuracy: 0.98 - ETA: 2s - loss: 0.0484 - accuracy: 0.98 - ETA: 2s - loss: 0.0483 - accuracy: 0.98 - ETA: 2s - loss: 0.0485 - accuracy: 0.98 - ETA: 2s - loss: 0.0477 - accuracy: 0.98 - ETA: 2s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0466 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 1s - loss: 0.0475 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0467 - accuracy: 0.98 - ETA: 1s - loss: 0.0465 - accuracy: 0.98 - ETA: 1s - loss: 0.0461 - accuracy: 0.98 - ETA: 1s - loss: 0.0459 - accuracy: 0.98 - ETA: 1s - loss: 0.0458 - accuracy: 0.98 - ETA: 1s - loss: 0.0464 - accuracy: 0.98 - ETA: 1s - loss: 0.0457 - accuracy: 0.98 - ETA: 1s - loss: 0.0461 - accuracy: 0.98 - ETA: 1s - loss: 0.0454 - accuracy: 0.98 - ETA: 0s - loss: 0.0458 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0468 - accuracy: 0.98 - ETA: 0s - loss: 0.0463 - accuracy: 0.98 - ETA: 0s - loss: 0.0456 - accuracy: 0.98 - ETA: 0s - loss: 0.0451 - accuracy: 0.98 - ETA: 0s - loss: 0.0456 - accuracy: 0.98 - ETA: 0s - loss: 0.0457 - accuracy: 0.98 - ETA: 0s - loss: 0.0456 - accuracy: 0.98 - ETA: 0s - loss: 0.0457 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - 6s 536us/step - loss: 0.0498 - accuracy: 0.9822 - val_loss: 0.0282 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00794\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0360 - accuracy: 0.98 - ETA: 5s - loss: 0.0260 - accuracy: 0.99 - ETA: 5s - loss: 0.0223 - accuracy: 0.99 - ETA: 4s - loss: 0.0189 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0192 - accuracy: 0.99 - ETA: 4s - loss: 0.0185 - accuracy: 0.99 - ETA: 4s - loss: 0.0174 - accuracy: 0.99 - ETA: 4s - loss: 0.0206 - accuracy: 0.99 - ETA: 4s - loss: 0.0251 - accuracy: 0.99 - ETA: 4s - loss: 0.0269 - accuracy: 0.99 - ETA: 4s - loss: 0.0269 - accuracy: 0.99 - ETA: 4s - loss: 0.0285 - accuracy: 0.99 - ETA: 4s - loss: 0.0281 - accuracy: 0.99 - ETA: 4s - loss: 0.0292 - accuracy: 0.99 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0301 - accuracy: 0.99 - ETA: 4s - loss: 0.0297 - accuracy: 0.99 - ETA: 3s - loss: 0.0288 - accuracy: 0.99 - ETA: 3s - loss: 0.0277 - accuracy: 0.99 - ETA: 3s - loss: 0.0265 - accuracy: 0.99 - ETA: 3s - loss: 0.0263 - accuracy: 0.99 - ETA: 3s - loss: 0.0258 - accuracy: 0.99 - ETA: 3s - loss: 0.0253 - accuracy: 0.99 - ETA: 3s - loss: 0.0253 - accuracy: 0.99 - ETA: 3s - loss: 0.0244 - accuracy: 0.99 - ETA: 3s - loss: 0.0250 - accuracy: 0.99 - ETA: 3s - loss: 0.0270 - accuracy: 0.99 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0449 - accuracy: 0.98 - ETA: 3s - loss: 0.0456 - accuracy: 0.98 - ETA: 3s - loss: 0.0448 - accuracy: 0.98 - ETA: 2s - loss: 0.0440 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0431 - accuracy: 0.98 - ETA: 2s - loss: 0.0433 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0424 - accuracy: 0.98 - ETA: 2s - loss: 0.0419 - accuracy: 0.98 - ETA: 2s - loss: 0.0421 - accuracy: 0.98 - ETA: 2s - loss: 0.0412 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0399 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0395 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 1s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0372 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0408 - accuracy: 0.98 - ETA: 1s - loss: 0.0414 - accuracy: 0.98 - ETA: 1s - loss: 0.0409 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 0s - loss: 0.0398 - accuracy: 0.98 - ETA: 0s - loss: 0.0399 - accuracy: 0.98 - ETA: 0s - loss: 0.0396 - accuracy: 0.98 - ETA: 0s - loss: 0.0393 - accuracy: 0.98 - ETA: 0s - loss: 0.0388 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0390 - accuracy: 0.98 - ETA: 0s - loss: 0.0392 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0415 - accuracy: 0.98 - ETA: 0s - loss: 0.0414 - accuracy: 0.98 - ETA: 0s - loss: 0.0411 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - 6s 551us/step - loss: 0.0427 - accuracy: 0.9846 - val_loss: 0.0357 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00794\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0253 - accuracy: 0.98 - ETA: 6s - loss: 0.0232 - accuracy: 0.98 - ETA: 6s - loss: 0.0299 - accuracy: 0.99 - ETA: 5s - loss: 0.0283 - accuracy: 0.99 - ETA: 5s - loss: 0.0283 - accuracy: 0.99 - ETA: 5s - loss: 0.0246 - accuracy: 0.99 - ETA: 5s - loss: 0.0285 - accuracy: 0.99 - ETA: 5s - loss: 0.0292 - accuracy: 0.99 - ETA: 5s - loss: 0.0319 - accuracy: 0.98 - ETA: 5s - loss: 0.0515 - accuracy: 0.98 - ETA: 5s - loss: 0.0585 - accuracy: 0.98 - ETA: 5s - loss: 0.0557 - accuracy: 0.98 - ETA: 5s - loss: 0.0525 - accuracy: 0.98 - ETA: 5s - loss: 0.0499 - accuracy: 0.98 - ETA: 5s - loss: 0.0516 - accuracy: 0.98 - ETA: 4s - loss: 0.0515 - accuracy: 0.98 - ETA: 4s - loss: 0.0530 - accuracy: 0.98 - ETA: 4s - loss: 0.0532 - accuracy: 0.98 - ETA: 4s - loss: 0.0536 - accuracy: 0.98 - ETA: 4s - loss: 0.0530 - accuracy: 0.98 - ETA: 4s - loss: 0.0511 - accuracy: 0.98 - ETA: 4s - loss: 0.0495 - accuracy: 0.98 - ETA: 4s - loss: 0.0484 - accuracy: 0.98 - ETA: 4s - loss: 0.0472 - accuracy: 0.98 - ETA: 4s - loss: 0.0465 - accuracy: 0.98 - ETA: 4s - loss: 0.0457 - accuracy: 0.98 - ETA: 4s - loss: 0.0442 - accuracy: 0.98 - ETA: 3s - loss: 0.0431 - accuracy: 0.98 - ETA: 3s - loss: 0.0417 - accuracy: 0.98 - ETA: 3s - loss: 0.0416 - accuracy: 0.98 - ETA: 3s - loss: 0.0435 - accuracy: 0.98 - ETA: 3s - loss: 0.0434 - accuracy: 0.98 - ETA: 3s - loss: 0.0423 - accuracy: 0.98 - ETA: 3s - loss: 0.0418 - accuracy: 0.98 - ETA: 3s - loss: 0.0412 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0404 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0428 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0430 - accuracy: 0.98 - ETA: 2s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0421 - accuracy: 0.98 - ETA: 2s - loss: 0.0415 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0412 - accuracy: 0.98 - ETA: 1s - loss: 0.0412 - accuracy: 0.98 - ETA: 1s - loss: 0.0412 - accuracy: 0.98 - ETA: 1s - loss: 0.0418 - accuracy: 0.98 - ETA: 1s - loss: 0.0430 - accuracy: 0.98 - ETA: 1s - loss: 0.0440 - accuracy: 0.98 - ETA: 1s - loss: 0.0439 - accuracy: 0.98 - ETA: 1s - loss: 0.0433 - accuracy: 0.98 - ETA: 1s - loss: 0.0431 - accuracy: 0.98 - ETA: 1s - loss: 0.0431 - accuracy: 0.98 - ETA: 1s - loss: 0.0424 - accuracy: 0.98 - ETA: 1s - loss: 0.0420 - accuracy: 0.98 - ETA: 1s - loss: 0.0414 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0401 - accuracy: 0.98 - ETA: 0s - loss: 0.0403 - accuracy: 0.98 - ETA: 0s - loss: 0.0398 - accuracy: 0.98 - ETA: 0s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0398 - accuracy: 0.98 - ETA: 0s - loss: 0.0405 - accuracy: 0.98 - ETA: 0s - loss: 0.0421 - accuracy: 0.98 - ETA: 0s - loss: 0.0418 - accuracy: 0.98 - ETA: 0s - loss: 0.0413 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - 6s 587us/step - loss: 0.0413 - accuracy: 0.9863 - val_loss: 0.0298 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00794\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0089 - accuracy: 1.00 - ETA: 5s - loss: 0.0204 - accuracy: 0.98 - ETA: 5s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0216 - accuracy: 0.98 - ETA: 5s - loss: 0.0217 - accuracy: 0.98 - ETA: 5s - loss: 0.0203 - accuracy: 0.98 - ETA: 5s - loss: 0.0212 - accuracy: 0.99 - ETA: 4s - loss: 0.0220 - accuracy: 0.99 - ETA: 4s - loss: 0.0234 - accuracy: 0.99 - ETA: 4s - loss: 0.0229 - accuracy: 0.99 - ETA: 4s - loss: 0.0254 - accuracy: 0.98 - ETA: 4s - loss: 0.0237 - accuracy: 0.99 - ETA: 4s - loss: 0.0220 - accuracy: 0.99 - ETA: 4s - loss: 0.0253 - accuracy: 0.99 - ETA: 4s - loss: 0.0281 - accuracy: 0.98 - ETA: 4s - loss: 0.0292 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0322 - accuracy: 0.98 - ETA: 4s - loss: 0.0326 - accuracy: 0.98 - ETA: 4s - loss: 0.0349 - accuracy: 0.98 - ETA: 3s - loss: 0.0345 - accuracy: 0.98 - ETA: 3s - loss: 0.0346 - accuracy: 0.98 - ETA: 3s - loss: 0.0338 - accuracy: 0.98 - ETA: 3s - loss: 0.0337 - accuracy: 0.98 - ETA: 3s - loss: 0.0325 - accuracy: 0.98 - ETA: 3s - loss: 0.0315 - accuracy: 0.98 - ETA: 3s - loss: 0.0308 - accuracy: 0.98 - ETA: 3s - loss: 0.0301 - accuracy: 0.98 - ETA: 3s - loss: 0.0303 - accuracy: 0.98 - ETA: 3s - loss: 0.0305 - accuracy: 0.98 - ETA: 3s - loss: 0.0324 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0404 - accuracy: 0.98 - ETA: 2s - loss: 0.0393 - accuracy: 0.98 - ETA: 2s - loss: 0.0398 - accuracy: 0.98 - ETA: 2s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0382 - accuracy: 0.98 - ETA: 2s - loss: 0.0404 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0409 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0389 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0383 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0373 - accuracy: 0.98 - ETA: 1s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0371 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0392 - accuracy: 0.98 - ETA: 0s - loss: 0.0402 - accuracy: 0.98 - ETA: 0s - loss: 0.0401 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - 6s 534us/step - loss: 0.0411 - accuracy: 0.9850 - val_loss: 0.0258 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00794\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0084 - accuracy: 1.00 - ETA: 5s - loss: 0.0324 - accuracy: 0.98 - ETA: 5s - loss: 0.0419 - accuracy: 0.98 - ETA: 5s - loss: 0.0335 - accuracy: 0.99 - ETA: 5s - loss: 0.0272 - accuracy: 0.99 - ETA: 5s - loss: 0.0292 - accuracy: 0.99 - ETA: 5s - loss: 0.0288 - accuracy: 0.99 - ETA: 5s - loss: 0.0277 - accuracy: 0.99 - ETA: 4s - loss: 0.0286 - accuracy: 0.99 - ETA: 4s - loss: 0.0285 - accuracy: 0.99 - ETA: 4s - loss: 0.0271 - accuracy: 0.99 - ETA: 4s - loss: 0.0263 - accuracy: 0.99 - ETA: 4s - loss: 0.0265 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0245 - accuracy: 0.99 - ETA: 4s - loss: 0.0243 - accuracy: 0.99 - ETA: 4s - loss: 0.0232 - accuracy: 0.99 - ETA: 4s - loss: 0.0257 - accuracy: 0.99 - ETA: 4s - loss: 0.0294 - accuracy: 0.99 - ETA: 4s - loss: 0.0287 - accuracy: 0.99 - ETA: 3s - loss: 0.0289 - accuracy: 0.99 - ETA: 3s - loss: 0.0287 - accuracy: 0.99 - ETA: 3s - loss: 0.0312 - accuracy: 0.99 - ETA: 3s - loss: 0.0326 - accuracy: 0.98 - ETA: 3s - loss: 0.0336 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 3s - loss: 0.0373 - accuracy: 0.98 - ETA: 3s - loss: 0.0365 - accuracy: 0.98 - ETA: 3s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0358 - accuracy: 0.98 - ETA: 3s - loss: 0.0351 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0366 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 2s - loss: 0.0392 - accuracy: 0.98 - ETA: 2s - loss: 0.0384 - accuracy: 0.98 - ETA: 2s - loss: 0.0382 - accuracy: 0.98 - ETA: 2s - loss: 0.0379 - accuracy: 0.98 - ETA: 2s - loss: 0.0374 - accuracy: 0.98 - ETA: 2s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0362 - accuracy: 0.98 - ETA: 2s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0366 - accuracy: 0.98 - ETA: 2s - loss: 0.0377 - accuracy: 0.98 - ETA: 2s - loss: 0.0385 - accuracy: 0.98 - ETA: 2s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0391 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0371 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0364 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0357 - accuracy: 0.98 - ETA: 0s - loss: 0.0356 - accuracy: 0.98 - ETA: 0s - loss: 0.0358 - accuracy: 0.98 - ETA: 0s - loss: 0.0356 - accuracy: 0.98 - ETA: 0s - loss: 0.0353 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0349 - accuracy: 0.98 - 6s 538us/step - loss: 0.0350 - accuracy: 0.9882 - val_loss: 0.1019 - val_accuracy: 0.9671\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00794\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0777 - accuracy: 0.97 - ETA: 5s - loss: 0.0497 - accuracy: 0.98 - ETA: 5s - loss: 0.0513 - accuracy: 0.98 - ETA: 4s - loss: 0.0635 - accuracy: 0.97 - ETA: 4s - loss: 0.0555 - accuracy: 0.98 - ETA: 4s - loss: 0.0632 - accuracy: 0.97 - ETA: 4s - loss: 0.0613 - accuracy: 0.97 - ETA: 4s - loss: 0.0549 - accuracy: 0.98 - ETA: 4s - loss: 0.0534 - accuracy: 0.98 - ETA: 4s - loss: 0.0509 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0460 - accuracy: 0.98 - ETA: 4s - loss: 0.0439 - accuracy: 0.98 - ETA: 4s - loss: 0.0421 - accuracy: 0.98 - ETA: 4s - loss: 0.0400 - accuracy: 0.98 - ETA: 4s - loss: 0.0397 - accuracy: 0.98 - ETA: 4s - loss: 0.0379 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0356 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 3s - loss: 0.0335 - accuracy: 0.98 - ETA: 3s - loss: 0.0362 - accuracy: 0.98 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0379 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 3s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0376 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 3s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 2s - loss: 0.0357 - accuracy: 0.98 - ETA: 2s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0341 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0355 - accuracy: 0.98 - ETA: 2s - loss: 0.0351 - accuracy: 0.98 - ETA: 2s - loss: 0.0388 - accuracy: 0.98 - ETA: 2s - loss: 0.0423 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 1s - loss: 0.0405 - accuracy: 0.98 - ETA: 1s - loss: 0.0402 - accuracy: 0.98 - ETA: 1s - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0399 - accuracy: 0.98 - ETA: 1s - loss: 0.0392 - accuracy: 0.98 - ETA: 1s - loss: 0.0387 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0374 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0363 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - ETA: 0s - loss: 0.0358 - accuracy: 0.98 - ETA: 0s - loss: 0.0353 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0356 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0356 - accuracy: 0.98 - ETA: 0s - loss: 0.0357 - accuracy: 0.98 - ETA: 0s - loss: 0.0362 - accuracy: 0.98 - 6s 543us/step - loss: 0.0361 - accuracy: 0.9881 - val_loss: 0.0193 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00794\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0088 - accuracy: 1.00 - ETA: 5s - loss: 0.0337 - accuracy: 0.98 - ETA: 5s - loss: 0.0275 - accuracy: 0.99 - ETA: 5s - loss: 0.0407 - accuracy: 0.98 - ETA: 5s - loss: 0.0376 - accuracy: 0.98 - ETA: 5s - loss: 0.0374 - accuracy: 0.98 - ETA: 4s - loss: 0.0469 - accuracy: 0.98 - ETA: 4s - loss: 0.0723 - accuracy: 0.97 - ETA: 4s - loss: 0.0763 - accuracy: 0.97 - ETA: 4s - loss: 0.0712 - accuracy: 0.97 - ETA: 4s - loss: 0.0661 - accuracy: 0.98 - ETA: 4s - loss: 0.0618 - accuracy: 0.98 - ETA: 4s - loss: 0.0573 - accuracy: 0.98 - ETA: 4s - loss: 0.0540 - accuracy: 0.98 - ETA: 4s - loss: 0.0528 - accuracy: 0.98 - ETA: 4s - loss: 0.0527 - accuracy: 0.98 - ETA: 4s - loss: 0.0504 - accuracy: 0.98 - ETA: 4s - loss: 0.0479 - accuracy: 0.98 - ETA: 4s - loss: 0.0464 - accuracy: 0.98 - ETA: 3s - loss: 0.0448 - accuracy: 0.98 - ETA: 3s - loss: 0.0430 - accuracy: 0.98 - ETA: 3s - loss: 0.0417 - accuracy: 0.98 - ETA: 3s - loss: 0.0402 - accuracy: 0.98 - ETA: 3s - loss: 0.0399 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 3s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0377 - accuracy: 0.98 - ETA: 3s - loss: 0.0390 - accuracy: 0.98 - ETA: 3s - loss: 0.0415 - accuracy: 0.98 - ETA: 3s - loss: 0.0415 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0422 - accuracy: 0.98 - ETA: 3s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0454 - accuracy: 0.98 - ETA: 2s - loss: 0.0452 - accuracy: 0.98 - ETA: 2s - loss: 0.0446 - accuracy: 0.98 - ETA: 2s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0431 - accuracy: 0.98 - ETA: 2s - loss: 0.0435 - accuracy: 0.98 - ETA: 2s - loss: 0.0433 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0421 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0414 - accuracy: 0.98 - ETA: 2s - loss: 0.0407 - accuracy: 0.98 - ETA: 2s - loss: 0.0399 - accuracy: 0.98 - ETA: 2s - loss: 0.0399 - accuracy: 0.98 - ETA: 1s - loss: 0.0396 - accuracy: 0.98 - ETA: 1s - loss: 0.0398 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0399 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 1s - loss: 0.0390 - accuracy: 0.98 - ETA: 1s - loss: 0.0389 - accuracy: 0.98 - ETA: 1s - loss: 0.0384 - accuracy: 0.98 - ETA: 1s - loss: 0.0378 - accuracy: 0.98 - ETA: 1s - loss: 0.0373 - accuracy: 0.98 - ETA: 1s - loss: 0.0375 - accuracy: 0.98 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0376 - accuracy: 0.98 - ETA: 0s - loss: 0.0373 - accuracy: 0.98 - ETA: 0s - loss: 0.0372 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0356 - accuracy: 0.98 - ETA: 0s - loss: 0.0352 - accuracy: 0.98 - ETA: 0s - loss: 0.0348 - accuracy: 0.98 - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - ETA: 0s - loss: 0.0355 - accuracy: 0.98 - ETA: 0s - loss: 0.0358 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - 6s 536us/step - loss: 0.0375 - accuracy: 0.9873 - val_loss: 0.0697 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00794\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0374 - accuracy: 0.98 - ETA: 5s - loss: 0.0658 - accuracy: 0.97 - ETA: 5s - loss: 0.0680 - accuracy: 0.98 - ETA: 5s - loss: 0.0592 - accuracy: 0.98 - ETA: 5s - loss: 0.0531 - accuracy: 0.98 - ETA: 5s - loss: 0.0559 - accuracy: 0.98 - ETA: 5s - loss: 0.0540 - accuracy: 0.98 - ETA: 4s - loss: 0.0480 - accuracy: 0.98 - ETA: 4s - loss: 0.0457 - accuracy: 0.98 - ETA: 4s - loss: 0.0447 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0314 - accuracy: 0.99 - ETA: 4s - loss: 0.0302 - accuracy: 0.99 - ETA: 4s - loss: 0.0288 - accuracy: 0.99 - ETA: 4s - loss: 0.0283 - accuracy: 0.99 - ETA: 4s - loss: 0.0281 - accuracy: 0.99 - ETA: 4s - loss: 0.0273 - accuracy: 0.99 - ETA: 3s - loss: 0.0263 - accuracy: 0.99 - ETA: 3s - loss: 0.0262 - accuracy: 0.99 - ETA: 3s - loss: 0.0252 - accuracy: 0.99 - ETA: 3s - loss: 0.0244 - accuracy: 0.99 - ETA: 3s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0239 - accuracy: 0.99 - ETA: 3s - loss: 0.0258 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0325 - accuracy: 0.99 - ETA: 3s - loss: 0.0323 - accuracy: 0.99 - ETA: 3s - loss: 0.0317 - accuracy: 0.99 - ETA: 3s - loss: 0.0313 - accuracy: 0.99 - ETA: 3s - loss: 0.0307 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 2s - loss: 0.0296 - accuracy: 0.99 - ETA: 2s - loss: 0.0302 - accuracy: 0.99 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 2s - loss: 0.0326 - accuracy: 0.98 - ETA: 2s - loss: 0.0321 - accuracy: 0.98 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 2s - loss: 0.0312 - accuracy: 0.98 - ETA: 2s - loss: 0.0308 - accuracy: 0.98 - ETA: 2s - loss: 0.0309 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.99 - ETA: 2s - loss: 0.0302 - accuracy: 0.99 - ETA: 2s - loss: 0.0296 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0284 - accuracy: 0.99 - ETA: 1s - loss: 0.0281 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 1s - loss: 0.0281 - accuracy: 0.99 - ETA: 1s - loss: 0.0277 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0291 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0292 - accuracy: 0.99 - ETA: 0s - loss: 0.0292 - accuracy: 0.99 - ETA: 0s - loss: 0.0291 - accuracy: 0.99 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0290 - accuracy: 0.99 - ETA: 0s - loss: 0.0287 - accuracy: 0.99 - 6s 535us/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.0709 - val_accuracy: 0.9857\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00794\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.1147 - accuracy: 0.97 - ETA: 4s - loss: 0.0694 - accuracy: 0.98 - ETA: 5s - loss: 0.0589 - accuracy: 0.98 - ETA: 5s - loss: 0.0528 - accuracy: 0.98 - ETA: 5s - loss: 0.0475 - accuracy: 0.98 - ETA: 4s - loss: 0.0471 - accuracy: 0.98 - ETA: 4s - loss: 0.0443 - accuracy: 0.98 - ETA: 4s - loss: 0.0392 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0350 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0348 - accuracy: 0.98 - ETA: 4s - loss: 0.0330 - accuracy: 0.98 - ETA: 4s - loss: 0.0320 - accuracy: 0.99 - ETA: 4s - loss: 0.0306 - accuracy: 0.99 - ETA: 4s - loss: 0.0298 - accuracy: 0.99 - ETA: 4s - loss: 0.0465 - accuracy: 0.98 - ETA: 4s - loss: 0.0455 - accuracy: 0.98 - ETA: 4s - loss: 0.0441 - accuracy: 0.98 - ETA: 4s - loss: 0.0426 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 3s - loss: 0.0376 - accuracy: 0.99 - ETA: 3s - loss: 0.0368 - accuracy: 0.99 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0409 - accuracy: 0.98 - ETA: 3s - loss: 0.0397 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0401 - accuracy: 0.98 - ETA: 3s - loss: 0.0389 - accuracy: 0.98 - ETA: 3s - loss: 0.0381 - accuracy: 0.98 - ETA: 3s - loss: 0.0370 - accuracy: 0.98 - ETA: 3s - loss: 0.0360 - accuracy: 0.98 - ETA: 2s - loss: 0.0362 - accuracy: 0.98 - ETA: 2s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0340 - accuracy: 0.98 - ETA: 2s - loss: 0.0341 - accuracy: 0.99 - ETA: 2s - loss: 0.0346 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.99 - ETA: 2s - loss: 0.0336 - accuracy: 0.99 - ETA: 2s - loss: 0.0335 - accuracy: 0.99 - ETA: 2s - loss: 0.0330 - accuracy: 0.99 - ETA: 2s - loss: 0.0329 - accuracy: 0.99 - ETA: 2s - loss: 0.0338 - accuracy: 0.99 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 1s - loss: 0.0353 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0350 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0344 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0346 - accuracy: 0.98 - ETA: 1s - loss: 0.0340 - accuracy: 0.98 - ETA: 1s - loss: 0.0334 - accuracy: 0.98 - ETA: 1s - loss: 0.0329 - accuracy: 0.99 - ETA: 1s - loss: 0.0326 - accuracy: 0.99 - ETA: 1s - loss: 0.0339 - accuracy: 0.98 - ETA: 0s - loss: 0.0367 - accuracy: 0.98 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0388 - accuracy: 0.98 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0381 - accuracy: 0.98 - ETA: 0s - loss: 0.0379 - accuracy: 0.98 - ETA: 0s - loss: 0.0374 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0366 - accuracy: 0.98 - ETA: 0s - loss: 0.0362 - accuracy: 0.98 - ETA: 0s - loss: 0.0361 - accuracy: 0.98 - ETA: 0s - loss: 0.0357 - accuracy: 0.98 - ETA: 0s - loss: 0.0353 - accuracy: 0.98 - 6s 538us/step - loss: 0.0351 - accuracy: 0.9891 - val_loss: 0.0552 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00794\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.1164 - accuracy: 0.97 - ETA: 5s - loss: 0.0712 - accuracy: 0.97 - ETA: 4s - loss: 0.0597 - accuracy: 0.98 - ETA: 4s - loss: 0.0470 - accuracy: 0.98 - ETA: 4s - loss: 0.0386 - accuracy: 0.99 - ETA: 4s - loss: 0.0326 - accuracy: 0.99 - ETA: 4s - loss: 0.0330 - accuracy: 0.99 - ETA: 4s - loss: 0.0291 - accuracy: 0.99 - ETA: 4s - loss: 0.0271 - accuracy: 0.99 - ETA: 4s - loss: 0.0246 - accuracy: 0.99 - ETA: 4s - loss: 0.0225 - accuracy: 0.99 - ETA: 4s - loss: 0.0227 - accuracy: 0.99 - ETA: 4s - loss: 0.0237 - accuracy: 0.99 - ETA: 4s - loss: 0.0225 - accuracy: 0.99 - ETA: 4s - loss: 0.0217 - accuracy: 0.99 - ETA: 4s - loss: 0.0226 - accuracy: 0.99 - ETA: 4s - loss: 0.0234 - accuracy: 0.99 - ETA: 4s - loss: 0.0222 - accuracy: 0.99 - ETA: 4s - loss: 0.0215 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 3s - loss: 0.0287 - accuracy: 0.99 - ETA: 3s - loss: 0.0283 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0287 - accuracy: 0.99 - ETA: 3s - loss: 0.0280 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0273 - accuracy: 0.99 - ETA: 2s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0269 - accuracy: 0.99 - ETA: 2s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0261 - accuracy: 0.99 - ETA: 2s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0255 - accuracy: 0.99 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 2s - loss: 0.0252 - accuracy: 0.99 - ETA: 2s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 1s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0291 - accuracy: 0.99 - ETA: 1s - loss: 0.0287 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 1s - loss: 0.0290 - accuracy: 0.99 - ETA: 1s - loss: 0.0338 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0343 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.99 - ETA: 0s - loss: 0.0336 - accuracy: 0.99 - ETA: 0s - loss: 0.0334 - accuracy: 0.99 - ETA: 0s - loss: 0.0330 - accuracy: 0.99 - ETA: 0s - loss: 0.0328 - accuracy: 0.99 - ETA: 0s - loss: 0.0327 - accuracy: 0.99 - ETA: 0s - loss: 0.0325 - accuracy: 0.99 - ETA: 0s - loss: 0.0321 - accuracy: 0.99 - ETA: 0s - loss: 0.0317 - accuracy: 0.99 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0312 - accuracy: 0.99 - ETA: 0s - loss: 0.0313 - accuracy: 0.99 - 6s 533us/step - loss: 0.0311 - accuracy: 0.9906 - val_loss: 0.0133 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00794\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0074 - accuracy: 1.00 - ETA: 5s - loss: 0.0071 - accuracy: 1.00 - ETA: 5s - loss: 0.0069 - accuracy: 1.00 - ETA: 5s - loss: 0.0138 - accuracy: 0.99 - ETA: 5s - loss: 0.0180 - accuracy: 0.99 - ETA: 5s - loss: 0.0156 - accuracy: 0.99 - ETA: 4s - loss: 0.0142 - accuracy: 0.99 - ETA: 4s - loss: 0.0137 - accuracy: 0.99 - ETA: 4s - loss: 0.0131 - accuracy: 0.99 - ETA: 4s - loss: 0.0131 - accuracy: 0.99 - ETA: 4s - loss: 0.0126 - accuracy: 0.99 - ETA: 4s - loss: 0.0124 - accuracy: 0.99 - ETA: 4s - loss: 0.0121 - accuracy: 0.99 - ETA: 4s - loss: 0.0117 - accuracy: 0.99 - ETA: 4s - loss: 0.0113 - accuracy: 0.99 - ETA: 4s - loss: 0.0110 - accuracy: 0.99 - ETA: 4s - loss: 0.0126 - accuracy: 0.99 - ETA: 4s - loss: 0.0137 - accuracy: 0.99 - ETA: 4s - loss: 0.0150 - accuracy: 0.99 - ETA: 4s - loss: 0.0147 - accuracy: 0.99 - ETA: 3s - loss: 0.0172 - accuracy: 0.99 - ETA: 3s - loss: 0.0168 - accuracy: 0.99 - ETA: 3s - loss: 0.0166 - accuracy: 0.99 - ETA: 3s - loss: 0.0159 - accuracy: 0.99 - ETA: 3s - loss: 0.0156 - accuracy: 0.99 - ETA: 3s - loss: 0.0155 - accuracy: 0.99 - ETA: 3s - loss: 0.0159 - accuracy: 0.99 - ETA: 3s - loss: 0.0173 - accuracy: 0.99 - ETA: 3s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0382 - accuracy: 0.98 - ETA: 3s - loss: 0.0408 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0405 - accuracy: 0.98 - ETA: 2s - loss: 0.0397 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0383 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0378 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0376 - accuracy: 0.98 - ETA: 2s - loss: 0.0376 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0365 - accuracy: 0.98 - ETA: 2s - loss: 0.0358 - accuracy: 0.98 - ETA: 2s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0350 - accuracy: 0.98 - ETA: 1s - loss: 0.0343 - accuracy: 0.98 - ETA: 1s - loss: 0.0341 - accuracy: 0.98 - ETA: 1s - loss: 0.0337 - accuracy: 0.98 - ETA: 1s - loss: 0.0332 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0327 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0327 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0312 - accuracy: 0.99 - ETA: 0s - loss: 0.0319 - accuracy: 0.98 - ETA: 0s - loss: 0.0317 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0313 - accuracy: 0.98 - 6s 537us/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.0671 - val_accuracy: 0.9857\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00794\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0128 - accuracy: 0.99 - ETA: 5s - loss: 0.0423 - accuracy: 0.98 - ETA: 4s - loss: 0.0407 - accuracy: 0.98 - ETA: 4s - loss: 0.0424 - accuracy: 0.98 - ETA: 4s - loss: 0.0445 - accuracy: 0.98 - ETA: 4s - loss: 0.0432 - accuracy: 0.98 - ETA: 4s - loss: 0.0418 - accuracy: 0.98 - ETA: 4s - loss: 0.0388 - accuracy: 0.98 - ETA: 4s - loss: 0.0385 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0323 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0315 - accuracy: 0.98 - ETA: 4s - loss: 0.0302 - accuracy: 0.98 - ETA: 4s - loss: 0.0303 - accuracy: 0.98 - ETA: 4s - loss: 0.0294 - accuracy: 0.98 - ETA: 4s - loss: 0.0293 - accuracy: 0.98 - ETA: 4s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0318 - accuracy: 0.98 - ETA: 3s - loss: 0.0322 - accuracy: 0.98 - ETA: 3s - loss: 0.0322 - accuracy: 0.98 - ETA: 3s - loss: 0.0317 - accuracy: 0.98 - ETA: 3s - loss: 0.0309 - accuracy: 0.98 - ETA: 3s - loss: 0.0300 - accuracy: 0.98 - ETA: 3s - loss: 0.0290 - accuracy: 0.99 - ETA: 3s - loss: 0.0280 - accuracy: 0.99 - ETA: 3s - loss: 0.0272 - accuracy: 0.99 - ETA: 3s - loss: 0.0266 - accuracy: 0.99 - ETA: 3s - loss: 0.0259 - accuracy: 0.99 - ETA: 3s - loss: 0.0258 - accuracy: 0.99 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 3s - loss: 0.0273 - accuracy: 0.99 - ETA: 3s - loss: 0.0268 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0258 - accuracy: 0.99 - ETA: 2s - loss: 0.0277 - accuracy: 0.99 - ETA: 2s - loss: 0.0335 - accuracy: 0.98 - ETA: 2s - loss: 0.0332 - accuracy: 0.98 - ETA: 2s - loss: 0.0328 - accuracy: 0.98 - ETA: 2s - loss: 0.0331 - accuracy: 0.98 - ETA: 2s - loss: 0.0331 - accuracy: 0.98 - ETA: 2s - loss: 0.0325 - accuracy: 0.98 - ETA: 2s - loss: 0.0321 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0313 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.99 - ETA: 1s - loss: 0.0301 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0299 - accuracy: 0.99 - ETA: 1s - loss: 0.0298 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0294 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 1s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0285 - accuracy: 0.99 - ETA: 1s - loss: 0.0283 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0270 - accuracy: 0.99 - ETA: 0s - loss: 0.0270 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0276 - accuracy: 0.99 - 6s 556us/step - loss: 0.0273 - accuracy: 0.9904 - val_loss: 0.0164 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00794\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0054 - accuracy: 1.00 - ETA: 5s - loss: 0.0057 - accuracy: 1.00 - ETA: 5s - loss: 0.0057 - accuracy: 1.00 - ETA: 5s - loss: 0.0084 - accuracy: 0.99 - ETA: 4s - loss: 0.0092 - accuracy: 0.99 - ETA: 4s - loss: 0.0131 - accuracy: 0.99 - ETA: 4s - loss: 0.0143 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0168 - accuracy: 0.99 - ETA: 4s - loss: 0.0189 - accuracy: 0.99 - ETA: 4s - loss: 0.0179 - accuracy: 0.99 - ETA: 4s - loss: 0.0170 - accuracy: 0.99 - ETA: 4s - loss: 0.0159 - accuracy: 0.99 - ETA: 4s - loss: 0.0156 - accuracy: 0.99 - ETA: 4s - loss: 0.0159 - accuracy: 0.99 - ETA: 4s - loss: 0.0209 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0211 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 3s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0191 - accuracy: 0.99 - ETA: 3s - loss: 0.0185 - accuracy: 0.99 - ETA: 3s - loss: 0.0182 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0218 - accuracy: 0.99 - ETA: 3s - loss: 0.0255 - accuracy: 0.99 - ETA: 3s - loss: 0.0324 - accuracy: 0.98 - ETA: 3s - loss: 0.0314 - accuracy: 0.98 - ETA: 3s - loss: 0.0330 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0327 - accuracy: 0.98 - ETA: 3s - loss: 0.0326 - accuracy: 0.98 - ETA: 2s - loss: 0.0320 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0305 - accuracy: 0.98 - ETA: 2s - loss: 0.0298 - accuracy: 0.99 - ETA: 2s - loss: 0.0293 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.99 - ETA: 2s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0296 - accuracy: 0.99 - ETA: 2s - loss: 0.0295 - accuracy: 0.99 - ETA: 2s - loss: 0.0291 - accuracy: 0.99 - ETA: 2s - loss: 0.0285 - accuracy: 0.99 - ETA: 2s - loss: 0.0280 - accuracy: 0.99 - ETA: 2s - loss: 0.0276 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0268 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0265 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0260 - accuracy: 0.99 - ETA: 0s - loss: 0.0260 - accuracy: 0.99 - ETA: 0s - loss: 0.0268 - accuracy: 0.99 - ETA: 0s - loss: 0.0268 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0253 - accuracy: 0.99 - 6s 534us/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 0.0093 - val_accuracy: 0.9958\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00794\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0103 - accuracy: 0.99 - ETA: 4s - loss: 0.0077 - accuracy: 0.99 - ETA: 5s - loss: 0.0065 - accuracy: 0.99 - ETA: 5s - loss: 0.0066 - accuracy: 0.99 - ETA: 5s - loss: 0.0169 - accuracy: 0.99 - ETA: 5s - loss: 0.0263 - accuracy: 0.98 - ETA: 4s - loss: 0.0368 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 4s - loss: 0.0376 - accuracy: 0.98 - ETA: 4s - loss: 0.0360 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0316 - accuracy: 0.98 - ETA: 4s - loss: 0.0297 - accuracy: 0.98 - ETA: 4s - loss: 0.0286 - accuracy: 0.98 - ETA: 4s - loss: 0.0281 - accuracy: 0.98 - ETA: 4s - loss: 0.0270 - accuracy: 0.98 - ETA: 4s - loss: 0.0272 - accuracy: 0.98 - ETA: 3s - loss: 0.0282 - accuracy: 0.98 - ETA: 3s - loss: 0.0274 - accuracy: 0.98 - ETA: 3s - loss: 0.0270 - accuracy: 0.98 - ETA: 3s - loss: 0.0260 - accuracy: 0.98 - ETA: 3s - loss: 0.0259 - accuracy: 0.99 - ETA: 3s - loss: 0.0252 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 3s - loss: 0.0237 - accuracy: 0.99 - ETA: 3s - loss: 0.0230 - accuracy: 0.99 - ETA: 3s - loss: 0.0223 - accuracy: 0.99 - ETA: 3s - loss: 0.0222 - accuracy: 0.99 - ETA: 3s - loss: 0.0216 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 2s - loss: 0.0213 - accuracy: 0.99 - ETA: 2s - loss: 0.0219 - accuracy: 0.99 - ETA: 2s - loss: 0.0219 - accuracy: 0.99 - ETA: 2s - loss: 0.0217 - accuracy: 0.99 - ETA: 2s - loss: 0.0213 - accuracy: 0.99 - ETA: 2s - loss: 0.0208 - accuracy: 0.99 - ETA: 2s - loss: 0.0211 - accuracy: 0.99 - ETA: 2s - loss: 0.0221 - accuracy: 0.99 - ETA: 2s - loss: 0.0252 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0280 - accuracy: 0.99 - ETA: 2s - loss: 0.0287 - accuracy: 0.99 - ETA: 2s - loss: 0.0283 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0273 - accuracy: 0.99 - ETA: 1s - loss: 0.0274 - accuracy: 0.99 - ETA: 1s - loss: 0.0277 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0268 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0263 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0247 - accuracy: 0.99 - 6s 541us/step - loss: 0.0249 - accuracy: 0.9914 - val_loss: 0.0197 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00794\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0181 - accuracy: 0.99 - ETA: 5s - loss: 0.0219 - accuracy: 0.98 - ETA: 5s - loss: 0.0163 - accuracy: 0.99 - ETA: 5s - loss: 0.0128 - accuracy: 0.99 - ETA: 5s - loss: 0.0133 - accuracy: 0.99 - ETA: 5s - loss: 0.0179 - accuracy: 0.99 - ETA: 4s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0211 - accuracy: 0.99 - ETA: 4s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0184 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0222 - accuracy: 0.99 - ETA: 4s - loss: 0.0226 - accuracy: 0.99 - ETA: 4s - loss: 0.0232 - accuracy: 0.99 - ETA: 4s - loss: 0.0225 - accuracy: 0.99 - ETA: 4s - loss: 0.0234 - accuracy: 0.99 - ETA: 4s - loss: 0.0265 - accuracy: 0.99 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - ETA: 4s - loss: 0.0247 - accuracy: 0.99 - ETA: 4s - loss: 0.0236 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 3s - loss: 0.0243 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0242 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 3s - loss: 0.0238 - accuracy: 0.99 - ETA: 3s - loss: 0.0232 - accuracy: 0.99 - ETA: 3s - loss: 0.0226 - accuracy: 0.99 - ETA: 3s - loss: 0.0222 - accuracy: 0.99 - ETA: 3s - loss: 0.0218 - accuracy: 0.99 - ETA: 2s - loss: 0.0226 - accuracy: 0.99 - ETA: 2s - loss: 0.0249 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0284 - accuracy: 0.99 - ETA: 2s - loss: 0.0279 - accuracy: 0.99 - ETA: 2s - loss: 0.0276 - accuracy: 0.99 - ETA: 2s - loss: 0.0277 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0266 - accuracy: 0.99 - ETA: 2s - loss: 0.0263 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 1s - loss: 0.0275 - accuracy: 0.99 - ETA: 1s - loss: 0.0280 - accuracy: 0.99 - ETA: 1s - loss: 0.0278 - accuracy: 0.99 - ETA: 1s - loss: 0.0276 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0268 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 1s - loss: 0.0262 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0260 - accuracy: 0.99 - ETA: 0s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0269 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0274 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0267 - accuracy: 0.99 - ETA: 0s - loss: 0.0264 - accuracy: 0.99 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - 6s 533us/step - loss: 0.0268 - accuracy: 0.9912 - val_loss: 0.0159 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00794\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0207 - accuracy: 0.98 - ETA: 5s - loss: 0.0134 - accuracy: 0.99 - ETA: 4s - loss: 0.0139 - accuracy: 0.99 - ETA: 4s - loss: 0.0116 - accuracy: 0.99 - ETA: 4s - loss: 0.0103 - accuracy: 0.99 - ETA: 4s - loss: 0.0093 - accuracy: 0.99 - ETA: 4s - loss: 0.0097 - accuracy: 0.99 - ETA: 4s - loss: 0.0517 - accuracy: 0.98 - ETA: 4s - loss: 0.0566 - accuracy: 0.98 - ETA: 4s - loss: 0.0538 - accuracy: 0.98 - ETA: 4s - loss: 0.0490 - accuracy: 0.98 - ETA: 4s - loss: 0.0466 - accuracy: 0.98 - ETA: 4s - loss: 0.0443 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 4s - loss: 0.0446 - accuracy: 0.98 - ETA: 4s - loss: 0.0435 - accuracy: 0.98 - ETA: 4s - loss: 0.0415 - accuracy: 0.98 - ETA: 4s - loss: 0.0393 - accuracy: 0.98 - ETA: 4s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0358 - accuracy: 0.99 - ETA: 3s - loss: 0.0343 - accuracy: 0.99 - ETA: 3s - loss: 0.0329 - accuracy: 0.99 - ETA: 3s - loss: 0.0315 - accuracy: 0.99 - ETA: 3s - loss: 0.0303 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0285 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 3s - loss: 0.0266 - accuracy: 0.99 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 3s - loss: 0.0254 - accuracy: 0.99 - ETA: 3s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0236 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0223 - accuracy: 0.99 - ETA: 2s - loss: 0.0222 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0259 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0250 - accuracy: 0.99 - ETA: 2s - loss: 0.0249 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0249 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 1s - loss: 0.0240 - accuracy: 0.99 - ETA: 1s - loss: 0.0243 - accuracy: 0.99 - ETA: 1s - loss: 0.0246 - accuracy: 0.99 - ETA: 1s - loss: 0.0243 - accuracy: 0.99 - ETA: 1s - loss: 0.0243 - accuracy: 0.99 - ETA: 1s - loss: 0.0241 - accuracy: 0.99 - ETA: 1s - loss: 0.0237 - accuracy: 0.99 - ETA: 1s - loss: 0.0234 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0225 - accuracy: 0.99 - ETA: 1s - loss: 0.0228 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 0s - loss: 0.0230 - accuracy: 0.99 - ETA: 0s - loss: 0.0227 - accuracy: 0.99 - ETA: 0s - loss: 0.0224 - accuracy: 0.99 - ETA: 0s - loss: 0.0222 - accuracy: 0.99 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0231 - accuracy: 0.99 - ETA: 0s - loss: 0.0229 - accuracy: 0.99 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0228 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - 6s 534us/step - loss: 0.0245 - accuracy: 0.9928 - val_loss: 0.0381 - val_accuracy: 0.9832\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00794\n",
      "Epoch 27/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0801 - accuracy: 0.96 - ETA: 5s - loss: 0.0473 - accuracy: 0.97 - ETA: 5s - loss: 0.0400 - accuracy: 0.98 - ETA: 4s - loss: 0.0355 - accuracy: 0.98 - ETA: 5s - loss: 0.0352 - accuracy: 0.98 - ETA: 5s - loss: 0.0371 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0309 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 4s - loss: 0.0297 - accuracy: 0.98 - ETA: 4s - loss: 0.0281 - accuracy: 0.98 - ETA: 4s - loss: 0.0278 - accuracy: 0.98 - ETA: 4s - loss: 0.0268 - accuracy: 0.98 - ETA: 4s - loss: 0.0256 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0230 - accuracy: 0.99 - ETA: 4s - loss: 0.0217 - accuracy: 0.99 - ETA: 4s - loss: 0.0209 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0205 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0191 - accuracy: 0.99 - ETA: 3s - loss: 0.0188 - accuracy: 0.99 - ETA: 3s - loss: 0.0183 - accuracy: 0.99 - ETA: 3s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0203 - accuracy: 0.99 - ETA: 3s - loss: 0.0233 - accuracy: 0.99 - ETA: 3s - loss: 0.0256 - accuracy: 0.99 - ETA: 3s - loss: 0.0262 - accuracy: 0.99 - ETA: 3s - loss: 0.0262 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0267 - accuracy: 0.99 - ETA: 2s - loss: 0.0262 - accuracy: 0.99 - ETA: 2s - loss: 0.0257 - accuracy: 0.99 - ETA: 2s - loss: 0.0260 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0249 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0243 - accuracy: 0.99 - ETA: 2s - loss: 0.0257 - accuracy: 0.99 - ETA: 2s - loss: 0.0254 - accuracy: 0.99 - ETA: 2s - loss: 0.0252 - accuracy: 0.99 - ETA: 1s - loss: 0.0247 - accuracy: 0.99 - ETA: 1s - loss: 0.0247 - accuracy: 0.99 - ETA: 1s - loss: 0.0250 - accuracy: 0.99 - ETA: 1s - loss: 0.0252 - accuracy: 0.99 - ETA: 1s - loss: 0.0256 - accuracy: 0.99 - ETA: 1s - loss: 0.0251 - accuracy: 0.99 - ETA: 1s - loss: 0.0250 - accuracy: 0.99 - ETA: 1s - loss: 0.0252 - accuracy: 0.99 - ETA: 1s - loss: 0.0250 - accuracy: 0.99 - ETA: 1s - loss: 0.0247 - accuracy: 0.99 - ETA: 1s - loss: 0.0243 - accuracy: 0.99 - ETA: 1s - loss: 0.0239 - accuracy: 0.99 - ETA: 1s - loss: 0.0244 - accuracy: 0.99 - ETA: 1s - loss: 0.0242 - accuracy: 0.99 - ETA: 0s - loss: 0.0240 - accuracy: 0.99 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0248 - accuracy: 0.99 - 6s 537us/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.0145 - val_accuracy: 0.9966\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00794\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0019 - accuracy: 1.00 - ETA: 5s - loss: 0.0185 - accuracy: 0.99 - ETA: 5s - loss: 0.0136 - accuracy: 0.99 - ETA: 5s - loss: 0.0134 - accuracy: 0.99 - ETA: 5s - loss: 0.0152 - accuracy: 0.99 - ETA: 5s - loss: 0.0132 - accuracy: 0.99 - ETA: 5s - loss: 0.0163 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0296 - accuracy: 0.98 - ETA: 4s - loss: 0.0326 - accuracy: 0.98 - ETA: 4s - loss: 0.0305 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.99 - ETA: 4s - loss: 0.0272 - accuracy: 0.99 - ETA: 4s - loss: 0.0261 - accuracy: 0.99 - ETA: 4s - loss: 0.0254 - accuracy: 0.99 - ETA: 4s - loss: 0.0241 - accuracy: 0.99 - ETA: 4s - loss: 0.0233 - accuracy: 0.99 - ETA: 4s - loss: 0.0224 - accuracy: 0.99 - ETA: 4s - loss: 0.0218 - accuracy: 0.99 - ETA: 4s - loss: 0.0213 - accuracy: 0.99 - ETA: 3s - loss: 0.0209 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0200 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0190 - accuracy: 0.99 - ETA: 3s - loss: 0.0184 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 2s - loss: 0.0189 - accuracy: 0.99 - ETA: 2s - loss: 0.0205 - accuracy: 0.99 - ETA: 2s - loss: 0.0251 - accuracy: 0.99 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 2s - loss: 0.0253 - accuracy: 0.99 - ETA: 2s - loss: 0.0247 - accuracy: 0.99 - ETA: 2s - loss: 0.0243 - accuracy: 0.99 - ETA: 2s - loss: 0.0244 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 2s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0230 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0214 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0219 - accuracy: 0.99 - ETA: 1s - loss: 0.0217 - accuracy: 0.99 - ETA: 1s - loss: 0.0215 - accuracy: 0.99 - ETA: 1s - loss: 0.0212 - accuracy: 0.99 - ETA: 1s - loss: 0.0209 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0207 - accuracy: 0.99 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 0s - loss: 0.0209 - accuracy: 0.99 - ETA: 0s - loss: 0.0208 - accuracy: 0.99 - ETA: 0s - loss: 0.0205 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0202 - accuracy: 0.99 - ETA: 0s - loss: 0.0199 - accuracy: 0.99 - ETA: 0s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0197 - accuracy: 0.99 - ETA: 0s - loss: 0.0200 - accuracy: 0.99 - ETA: 0s - loss: 0.0197 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - ETA: 0s - loss: 0.0206 - accuracy: 0.99 - ETA: 0s - loss: 0.0203 - accuracy: 0.99 - 6s 533us/step - loss: 0.0202 - accuracy: 0.9929 - val_loss: 0.0157 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00794\n",
      "Epoch 00028: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 217us/step\n",
      "[2020-05-18 16:38:54 RAM76.7% 1.7GB] Val Score : [0.017609915755924976, 0.9939348101615906]\n",
      "[2020-05-18 16:38:54 RAM76.7% 1.7GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:38:54 RAM76.7% 1.7GB] Training on Fold : 10\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 25s - loss: 9.9371 - accuracy: 0.406 - ETA: 15s - loss: 9.4551 - accuracy: 0.517 - ETA: 11s - loss: 7.0563 - accuracy: 0.528 - ETA: 9s - loss: 5.8744 - accuracy: 0.519 - ETA: 8s - loss: 5.0929 - accuracy: 0.51 - ETA: 8s - loss: 4.5310 - accuracy: 0.51 - ETA: 7s - loss: 4.0639 - accuracy: 0.52 - ETA: 7s - loss: 3.7204 - accuracy: 0.52 - ETA: 6s - loss: 3.4249 - accuracy: 0.53 - ETA: 6s - loss: 3.1997 - accuracy: 0.53 - ETA: 6s - loss: 2.9911 - accuracy: 0.54 - ETA: 5s - loss: 2.8275 - accuracy: 0.54 - ETA: 5s - loss: 2.6902 - accuracy: 0.54 - ETA: 5s - loss: 2.5631 - accuracy: 0.54 - ETA: 5s - loss: 2.4563 - accuracy: 0.54 - ETA: 5s - loss: 2.3516 - accuracy: 0.55 - ETA: 5s - loss: 2.2614 - accuracy: 0.55 - ETA: 5s - loss: 2.1833 - accuracy: 0.55 - ETA: 4s - loss: 2.1059 - accuracy: 0.55 - ETA: 4s - loss: 2.0349 - accuracy: 0.56 - ETA: 4s - loss: 1.9683 - accuracy: 0.56 - ETA: 4s - loss: 1.9107 - accuracy: 0.56 - ETA: 4s - loss: 1.8533 - accuracy: 0.57 - ETA: 4s - loss: 1.8020 - accuracy: 0.58 - ETA: 4s - loss: 1.7505 - accuracy: 0.58 - ETA: 4s - loss: 1.7034 - accuracy: 0.59 - ETA: 3s - loss: 1.6632 - accuracy: 0.59 - ETA: 3s - loss: 1.6274 - accuracy: 0.60 - ETA: 3s - loss: 1.6023 - accuracy: 0.60 - ETA: 3s - loss: 1.5686 - accuracy: 0.60 - ETA: 3s - loss: 1.5353 - accuracy: 0.61 - ETA: 3s - loss: 1.5046 - accuracy: 0.61 - ETA: 3s - loss: 1.4756 - accuracy: 0.61 - ETA: 3s - loss: 1.4498 - accuracy: 0.61 - ETA: 3s - loss: 1.4268 - accuracy: 0.61 - ETA: 3s - loss: 1.4041 - accuracy: 0.61 - ETA: 3s - loss: 1.3815 - accuracy: 0.62 - ETA: 2s - loss: 1.3593 - accuracy: 0.62 - ETA: 2s - loss: 1.3366 - accuracy: 0.62 - ETA: 2s - loss: 1.3142 - accuracy: 0.63 - ETA: 2s - loss: 1.2925 - accuracy: 0.63 - ETA: 2s - loss: 1.2717 - accuracy: 0.63 - ETA: 2s - loss: 1.2549 - accuracy: 0.64 - ETA: 2s - loss: 1.2410 - accuracy: 0.64 - ETA: 2s - loss: 1.2235 - accuracy: 0.64 - ETA: 2s - loss: 1.2064 - accuracy: 0.64 - ETA: 2s - loss: 1.1894 - accuracy: 0.65 - ETA: 2s - loss: 1.1733 - accuracy: 0.65 - ETA: 1s - loss: 1.1584 - accuracy: 0.65 - ETA: 1s - loss: 1.1454 - accuracy: 0.66 - ETA: 1s - loss: 1.1301 - accuracy: 0.66 - ETA: 1s - loss: 1.1153 - accuracy: 0.66 - ETA: 1s - loss: 1.1028 - accuracy: 0.67 - ETA: 1s - loss: 1.0920 - accuracy: 0.67 - ETA: 1s - loss: 1.0785 - accuracy: 0.67 - ETA: 1s - loss: 1.0661 - accuracy: 0.67 - ETA: 1s - loss: 1.0554 - accuracy: 0.67 - ETA: 1s - loss: 1.0441 - accuracy: 0.68 - ETA: 1s - loss: 1.0320 - accuracy: 0.68 - ETA: 1s - loss: 1.0211 - accuracy: 0.68 - ETA: 1s - loss: 1.0116 - accuracy: 0.68 - ETA: 0s - loss: 1.0041 - accuracy: 0.68 - ETA: 0s - loss: 0.9970 - accuracy: 0.68 - ETA: 0s - loss: 0.9870 - accuracy: 0.69 - ETA: 0s - loss: 0.9769 - accuracy: 0.69 - ETA: 0s - loss: 0.9675 - accuracy: 0.69 - ETA: 0s - loss: 0.9590 - accuracy: 0.69 - ETA: 0s - loss: 0.9497 - accuracy: 0.70 - ETA: 0s - loss: 0.9416 - accuracy: 0.70 - ETA: 0s - loss: 0.9321 - accuracy: 0.70 - ETA: 0s - loss: 0.9228 - accuracy: 0.70 - ETA: 0s - loss: 0.9138 - accuracy: 0.71 - ETA: 0s - loss: 0.9056 - accuracy: 0.71 - 6s 569us/step - loss: 0.9001 - accuracy: 0.7158 - val_loss: 0.2213 - val_accuracy: 0.9377\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00794\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.2952 - accuracy: 0.90 - ETA: 5s - loss: 0.2798 - accuracy: 0.90 - ETA: 4s - loss: 0.2823 - accuracy: 0.89 - ETA: 5s - loss: 0.2791 - accuracy: 0.88 - ETA: 5s - loss: 0.2942 - accuracy: 0.88 - ETA: 4s - loss: 0.2881 - accuracy: 0.88 - ETA: 4s - loss: 0.2901 - accuracy: 0.88 - ETA: 4s - loss: 0.2909 - accuracy: 0.88 - ETA: 4s - loss: 0.2994 - accuracy: 0.88 - ETA: 4s - loss: 0.3138 - accuracy: 0.87 - ETA: 4s - loss: 0.3092 - accuracy: 0.87 - ETA: 4s - loss: 0.3012 - accuracy: 0.88 - ETA: 4s - loss: 0.2984 - accuracy: 0.88 - ETA: 4s - loss: 0.2955 - accuracy: 0.88 - ETA: 4s - loss: 0.2939 - accuracy: 0.88 - ETA: 4s - loss: 0.3176 - accuracy: 0.87 - ETA: 4s - loss: 0.3307 - accuracy: 0.86 - ETA: 4s - loss: 0.3266 - accuracy: 0.87 - ETA: 4s - loss: 0.3209 - accuracy: 0.87 - ETA: 3s - loss: 0.3147 - accuracy: 0.87 - ETA: 3s - loss: 0.3116 - accuracy: 0.87 - ETA: 3s - loss: 0.3106 - accuracy: 0.87 - ETA: 3s - loss: 0.3069 - accuracy: 0.88 - ETA: 3s - loss: 0.3064 - accuracy: 0.87 - ETA: 3s - loss: 0.3047 - accuracy: 0.88 - ETA: 3s - loss: 0.3063 - accuracy: 0.88 - ETA: 3s - loss: 0.3205 - accuracy: 0.87 - ETA: 3s - loss: 0.3212 - accuracy: 0.87 - ETA: 3s - loss: 0.3197 - accuracy: 0.87 - ETA: 3s - loss: 0.3174 - accuracy: 0.87 - ETA: 3s - loss: 0.3150 - accuracy: 0.87 - ETA: 3s - loss: 0.3111 - accuracy: 0.87 - ETA: 3s - loss: 0.3080 - accuracy: 0.87 - ETA: 2s - loss: 0.3049 - accuracy: 0.87 - ETA: 2s - loss: 0.3021 - accuracy: 0.88 - ETA: 2s - loss: 0.3003 - accuracy: 0.88 - ETA: 2s - loss: 0.2988 - accuracy: 0.88 - ETA: 2s - loss: 0.2964 - accuracy: 0.88 - ETA: 2s - loss: 0.2947 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2950 - accuracy: 0.88 - ETA: 2s - loss: 0.2940 - accuracy: 0.88 - ETA: 2s - loss: 0.2930 - accuracy: 0.88 - ETA: 2s - loss: 0.2903 - accuracy: 0.88 - ETA: 2s - loss: 0.2879 - accuracy: 0.88 - ETA: 2s - loss: 0.2863 - accuracy: 0.88 - ETA: 1s - loss: 0.2839 - accuracy: 0.88 - ETA: 1s - loss: 0.2822 - accuracy: 0.88 - ETA: 1s - loss: 0.2814 - accuracy: 0.88 - ETA: 1s - loss: 0.2788 - accuracy: 0.88 - ETA: 1s - loss: 0.2780 - accuracy: 0.89 - ETA: 1s - loss: 0.2757 - accuracy: 0.89 - ETA: 1s - loss: 0.2742 - accuracy: 0.89 - ETA: 1s - loss: 0.2738 - accuracy: 0.89 - ETA: 1s - loss: 0.2725 - accuracy: 0.89 - ETA: 1s - loss: 0.2754 - accuracy: 0.89 - ETA: 1s - loss: 0.2743 - accuracy: 0.89 - ETA: 1s - loss: 0.2747 - accuracy: 0.89 - ETA: 1s - loss: 0.2731 - accuracy: 0.89 - ETA: 1s - loss: 0.2712 - accuracy: 0.89 - ETA: 0s - loss: 0.2698 - accuracy: 0.89 - ETA: 0s - loss: 0.2682 - accuracy: 0.89 - ETA: 0s - loss: 0.2664 - accuracy: 0.89 - ETA: 0s - loss: 0.2646 - accuracy: 0.89 - ETA: 0s - loss: 0.2632 - accuracy: 0.89 - ETA: 0s - loss: 0.2639 - accuracy: 0.89 - ETA: 0s - loss: 0.2633 - accuracy: 0.89 - ETA: 0s - loss: 0.2635 - accuracy: 0.89 - ETA: 0s - loss: 0.2635 - accuracy: 0.89 - ETA: 0s - loss: 0.2625 - accuracy: 0.89 - ETA: 0s - loss: 0.2623 - accuracy: 0.89 - ETA: 0s - loss: 0.2615 - accuracy: 0.89 - ETA: 0s - loss: 0.2601 - accuracy: 0.89 - 6s 536us/step - loss: 0.2590 - accuracy: 0.8978 - val_loss: 0.1679 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00794\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.1525 - accuracy: 0.95 - ETA: 5s - loss: 0.1860 - accuracy: 0.92 - ETA: 4s - loss: 0.1710 - accuracy: 0.92 - ETA: 4s - loss: 0.1845 - accuracy: 0.92 - ETA: 5s - loss: 0.1864 - accuracy: 0.92 - ETA: 4s - loss: 0.1951 - accuracy: 0.91 - ETA: 4s - loss: 0.1945 - accuracy: 0.91 - ETA: 4s - loss: 0.1949 - accuracy: 0.91 - ETA: 4s - loss: 0.1981 - accuracy: 0.91 - ETA: 4s - loss: 0.1987 - accuracy: 0.91 - ETA: 4s - loss: 0.2259 - accuracy: 0.90 - ETA: 4s - loss: 0.2501 - accuracy: 0.89 - ETA: 4s - loss: 0.2484 - accuracy: 0.90 - ETA: 4s - loss: 0.2431 - accuracy: 0.90 - ETA: 4s - loss: 0.2349 - accuracy: 0.90 - ETA: 4s - loss: 0.2297 - accuracy: 0.90 - ETA: 4s - loss: 0.2259 - accuracy: 0.91 - ETA: 4s - loss: 0.2214 - accuracy: 0.91 - ETA: 4s - loss: 0.2168 - accuracy: 0.91 - ETA: 3s - loss: 0.2134 - accuracy: 0.91 - ETA: 3s - loss: 0.2093 - accuracy: 0.91 - ETA: 3s - loss: 0.2089 - accuracy: 0.91 - ETA: 3s - loss: 0.2166 - accuracy: 0.91 - ETA: 3s - loss: 0.2179 - accuracy: 0.91 - ETA: 3s - loss: 0.2204 - accuracy: 0.91 - ETA: 3s - loss: 0.2170 - accuracy: 0.91 - ETA: 3s - loss: 0.2155 - accuracy: 0.91 - ETA: 3s - loss: 0.2154 - accuracy: 0.91 - ETA: 3s - loss: 0.2142 - accuracy: 0.91 - ETA: 3s - loss: 0.2143 - accuracy: 0.91 - ETA: 3s - loss: 0.2182 - accuracy: 0.91 - ETA: 3s - loss: 0.2181 - accuracy: 0.91 - ETA: 3s - loss: 0.2165 - accuracy: 0.91 - ETA: 2s - loss: 0.2134 - accuracy: 0.91 - ETA: 2s - loss: 0.2119 - accuracy: 0.91 - ETA: 2s - loss: 0.2119 - accuracy: 0.91 - ETA: 2s - loss: 0.2118 - accuracy: 0.91 - ETA: 2s - loss: 0.2096 - accuracy: 0.91 - ETA: 2s - loss: 0.2093 - accuracy: 0.91 - ETA: 2s - loss: 0.2082 - accuracy: 0.91 - ETA: 2s - loss: 0.2066 - accuracy: 0.91 - ETA: 2s - loss: 0.2047 - accuracy: 0.92 - ETA: 2s - loss: 0.2032 - accuracy: 0.92 - ETA: 2s - loss: 0.2010 - accuracy: 0.92 - ETA: 2s - loss: 0.2004 - accuracy: 0.92 - ETA: 2s - loss: 0.1986 - accuracy: 0.92 - ETA: 2s - loss: 0.1966 - accuracy: 0.92 - ETA: 1s - loss: 0.1947 - accuracy: 0.92 - ETA: 1s - loss: 0.1948 - accuracy: 0.92 - ETA: 1s - loss: 0.1939 - accuracy: 0.92 - ETA: 1s - loss: 0.1949 - accuracy: 0.92 - ETA: 1s - loss: 0.2002 - accuracy: 0.92 - ETA: 1s - loss: 0.2035 - accuracy: 0.91 - ETA: 1s - loss: 0.2035 - accuracy: 0.91 - ETA: 1s - loss: 0.2032 - accuracy: 0.91 - ETA: 1s - loss: 0.2024 - accuracy: 0.92 - ETA: 1s - loss: 0.2013 - accuracy: 0.92 - ETA: 1s - loss: 0.2019 - accuracy: 0.91 - ETA: 1s - loss: 0.2008 - accuracy: 0.92 - ETA: 1s - loss: 0.1993 - accuracy: 0.92 - ETA: 0s - loss: 0.1975 - accuracy: 0.92 - ETA: 0s - loss: 0.1966 - accuracy: 0.92 - ETA: 0s - loss: 0.1958 - accuracy: 0.92 - ETA: 0s - loss: 0.1953 - accuracy: 0.92 - ETA: 0s - loss: 0.1942 - accuracy: 0.92 - ETA: 0s - loss: 0.1932 - accuracy: 0.92 - ETA: 0s - loss: 0.1927 - accuracy: 0.92 - ETA: 0s - loss: 0.1933 - accuracy: 0.92 - ETA: 0s - loss: 0.1931 - accuracy: 0.92 - ETA: 0s - loss: 0.1947 - accuracy: 0.92 - ETA: 0s - loss: 0.1966 - accuracy: 0.92 - ETA: 0s - loss: 0.1997 - accuracy: 0.92 - ETA: 0s - loss: 0.1988 - accuracy: 0.92 - 6s 535us/step - loss: 0.1980 - accuracy: 0.9225 - val_loss: 0.1012 - val_accuracy: 0.9612\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00794\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.1870 - accuracy: 0.89 - ETA: 5s - loss: 0.1643 - accuracy: 0.91 - ETA: 5s - loss: 0.1553 - accuracy: 0.92 - ETA: 5s - loss: 0.1475 - accuracy: 0.92 - ETA: 4s - loss: 0.1481 - accuracy: 0.93 - ETA: 5s - loss: 0.1564 - accuracy: 0.92 - ETA: 4s - loss: 0.1536 - accuracy: 0.93 - ETA: 4s - loss: 0.1458 - accuracy: 0.93 - ETA: 4s - loss: 0.1428 - accuracy: 0.93 - ETA: 4s - loss: 0.1415 - accuracy: 0.93 - ETA: 4s - loss: 0.1394 - accuracy: 0.93 - ETA: 4s - loss: 0.1448 - accuracy: 0.93 - ETA: 4s - loss: 0.1476 - accuracy: 0.93 - ETA: 4s - loss: 0.1527 - accuracy: 0.93 - ETA: 4s - loss: 0.1569 - accuracy: 0.93 - ETA: 4s - loss: 0.1549 - accuracy: 0.93 - ETA: 4s - loss: 0.1539 - accuracy: 0.93 - ETA: 4s - loss: 0.1503 - accuracy: 0.94 - ETA: 4s - loss: 0.1492 - accuracy: 0.94 - ETA: 4s - loss: 0.1535 - accuracy: 0.93 - ETA: 3s - loss: 0.1519 - accuracy: 0.94 - ETA: 3s - loss: 0.1489 - accuracy: 0.94 - ETA: 3s - loss: 0.1457 - accuracy: 0.94 - ETA: 3s - loss: 0.1463 - accuracy: 0.94 - ETA: 3s - loss: 0.1444 - accuracy: 0.94 - ETA: 3s - loss: 0.1429 - accuracy: 0.94 - ETA: 3s - loss: 0.1420 - accuracy: 0.94 - ETA: 3s - loss: 0.1407 - accuracy: 0.94 - ETA: 3s - loss: 0.1422 - accuracy: 0.94 - ETA: 3s - loss: 0.1456 - accuracy: 0.94 - ETA: 3s - loss: 0.1483 - accuracy: 0.94 - ETA: 3s - loss: 0.1466 - accuracy: 0.94 - ETA: 3s - loss: 0.1458 - accuracy: 0.94 - ETA: 2s - loss: 0.1441 - accuracy: 0.94 - ETA: 2s - loss: 0.1444 - accuracy: 0.94 - ETA: 2s - loss: 0.1453 - accuracy: 0.94 - ETA: 2s - loss: 0.1463 - accuracy: 0.94 - ETA: 2s - loss: 0.1451 - accuracy: 0.94 - ETA: 2s - loss: 0.1443 - accuracy: 0.94 - ETA: 2s - loss: 0.1425 - accuracy: 0.94 - ETA: 2s - loss: 0.1430 - accuracy: 0.94 - ETA: 2s - loss: 0.1415 - accuracy: 0.94 - ETA: 2s - loss: 0.1399 - accuracy: 0.94 - ETA: 2s - loss: 0.1407 - accuracy: 0.94 - ETA: 2s - loss: 0.1404 - accuracy: 0.94 - ETA: 2s - loss: 0.1409 - accuracy: 0.94 - ETA: 1s - loss: 0.1409 - accuracy: 0.94 - ETA: 1s - loss: 0.1397 - accuracy: 0.94 - ETA: 1s - loss: 0.1408 - accuracy: 0.94 - ETA: 1s - loss: 0.1432 - accuracy: 0.94 - ETA: 1s - loss: 0.1450 - accuracy: 0.94 - ETA: 1s - loss: 0.1436 - accuracy: 0.94 - ETA: 1s - loss: 0.1424 - accuracy: 0.94 - ETA: 1s - loss: 0.1414 - accuracy: 0.94 - ETA: 1s - loss: 0.1406 - accuracy: 0.94 - ETA: 1s - loss: 0.1397 - accuracy: 0.94 - ETA: 1s - loss: 0.1383 - accuracy: 0.94 - ETA: 1s - loss: 0.1383 - accuracy: 0.94 - ETA: 1s - loss: 0.1401 - accuracy: 0.94 - ETA: 1s - loss: 0.1421 - accuracy: 0.94 - ETA: 0s - loss: 0.1419 - accuracy: 0.94 - ETA: 0s - loss: 0.1414 - accuracy: 0.94 - ETA: 0s - loss: 0.1426 - accuracy: 0.94 - ETA: 0s - loss: 0.1431 - accuracy: 0.94 - ETA: 0s - loss: 0.1429 - accuracy: 0.94 - ETA: 0s - loss: 0.1419 - accuracy: 0.94 - ETA: 0s - loss: 0.1415 - accuracy: 0.94 - ETA: 0s - loss: 0.1414 - accuracy: 0.94 - ETA: 0s - loss: 0.1408 - accuracy: 0.94 - ETA: 0s - loss: 0.1404 - accuracy: 0.94 - ETA: 0s - loss: 0.1395 - accuracy: 0.94 - ETA: 0s - loss: 0.1402 - accuracy: 0.94 - ETA: 0s - loss: 0.1400 - accuracy: 0.94 - 6s 534us/step - loss: 0.1393 - accuracy: 0.9444 - val_loss: 0.0939 - val_accuracy: 0.9612\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00794\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1384 - accuracy: 0.93 - ETA: 5s - loss: 0.1148 - accuracy: 0.94 - ETA: 5s - loss: 0.1168 - accuracy: 0.94 - ETA: 5s - loss: 0.1110 - accuracy: 0.95 - ETA: 5s - loss: 0.1190 - accuracy: 0.94 - ETA: 5s - loss: 0.1129 - accuracy: 0.94 - ETA: 5s - loss: 0.1108 - accuracy: 0.95 - ETA: 5s - loss: 0.1182 - accuracy: 0.94 - ETA: 4s - loss: 0.1161 - accuracy: 0.95 - ETA: 4s - loss: 0.1108 - accuracy: 0.95 - ETA: 4s - loss: 0.1114 - accuracy: 0.95 - ETA: 4s - loss: 0.1050 - accuracy: 0.95 - ETA: 4s - loss: 0.1007 - accuracy: 0.95 - ETA: 4s - loss: 0.1046 - accuracy: 0.95 - ETA: 4s - loss: 0.1165 - accuracy: 0.95 - ETA: 4s - loss: 0.1335 - accuracy: 0.94 - ETA: 4s - loss: 0.1342 - accuracy: 0.94 - ETA: 4s - loss: 0.1345 - accuracy: 0.94 - ETA: 4s - loss: 0.1344 - accuracy: 0.94 - ETA: 4s - loss: 0.1309 - accuracy: 0.94 - ETA: 4s - loss: 0.1311 - accuracy: 0.94 - ETA: 3s - loss: 0.1287 - accuracy: 0.94 - ETA: 3s - loss: 0.1277 - accuracy: 0.94 - ETA: 3s - loss: 0.1277 - accuracy: 0.94 - ETA: 3s - loss: 0.1306 - accuracy: 0.94 - ETA: 3s - loss: 0.1291 - accuracy: 0.94 - ETA: 3s - loss: 0.1270 - accuracy: 0.94 - ETA: 3s - loss: 0.1247 - accuracy: 0.95 - ETA: 3s - loss: 0.1255 - accuracy: 0.94 - ETA: 3s - loss: 0.1246 - accuracy: 0.95 - ETA: 3s - loss: 0.1234 - accuracy: 0.95 - ETA: 3s - loss: 0.1224 - accuracy: 0.95 - ETA: 3s - loss: 0.1215 - accuracy: 0.95 - ETA: 3s - loss: 0.1214 - accuracy: 0.95 - ETA: 2s - loss: 0.1207 - accuracy: 0.95 - ETA: 2s - loss: 0.1215 - accuracy: 0.95 - ETA: 2s - loss: 0.1212 - accuracy: 0.95 - ETA: 2s - loss: 0.1192 - accuracy: 0.95 - ETA: 2s - loss: 0.1190 - accuracy: 0.95 - ETA: 2s - loss: 0.1181 - accuracy: 0.95 - ETA: 2s - loss: 0.1162 - accuracy: 0.95 - ETA: 2s - loss: 0.1163 - accuracy: 0.95 - ETA: 2s - loss: 0.1157 - accuracy: 0.95 - ETA: 2s - loss: 0.1154 - accuracy: 0.95 - ETA: 2s - loss: 0.1150 - accuracy: 0.95 - ETA: 2s - loss: 0.1142 - accuracy: 0.95 - ETA: 2s - loss: 0.1146 - accuracy: 0.95 - ETA: 1s - loss: 0.1148 - accuracy: 0.95 - ETA: 1s - loss: 0.1132 - accuracy: 0.95 - ETA: 1s - loss: 0.1124 - accuracy: 0.95 - ETA: 1s - loss: 0.1117 - accuracy: 0.95 - ETA: 1s - loss: 0.1114 - accuracy: 0.95 - ETA: 1s - loss: 0.1105 - accuracy: 0.95 - ETA: 1s - loss: 0.1105 - accuracy: 0.95 - ETA: 1s - loss: 0.1119 - accuracy: 0.95 - ETA: 1s - loss: 0.1139 - accuracy: 0.95 - ETA: 1s - loss: 0.1131 - accuracy: 0.95 - ETA: 1s - loss: 0.1122 - accuracy: 0.95 - ETA: 1s - loss: 0.1120 - accuracy: 0.95 - ETA: 1s - loss: 0.1122 - accuracy: 0.95 - ETA: 0s - loss: 0.1132 - accuracy: 0.95 - ETA: 0s - loss: 0.1138 - accuracy: 0.95 - ETA: 0s - loss: 0.1129 - accuracy: 0.95 - ETA: 0s - loss: 0.1124 - accuracy: 0.95 - ETA: 0s - loss: 0.1117 - accuracy: 0.95 - ETA: 0s - loss: 0.1114 - accuracy: 0.95 - ETA: 0s - loss: 0.1104 - accuracy: 0.95 - ETA: 0s - loss: 0.1097 - accuracy: 0.95 - ETA: 0s - loss: 0.1091 - accuracy: 0.95 - ETA: 0s - loss: 0.1082 - accuracy: 0.95 - ETA: 0s - loss: 0.1079 - accuracy: 0.95 - ETA: 0s - loss: 0.1088 - accuracy: 0.95 - ETA: 0s - loss: 0.1085 - accuracy: 0.95 - 6s 539us/step - loss: 0.1083 - accuracy: 0.9578 - val_loss: 0.1044 - val_accuracy: 0.9587\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00794\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.1129 - accuracy: 0.96 - ETA: 6s - loss: 0.0925 - accuracy: 0.97 - ETA: 6s - loss: 0.0977 - accuracy: 0.96 - ETA: 6s - loss: 0.1000 - accuracy: 0.96 - ETA: 6s - loss: 0.1051 - accuracy: 0.96 - ETA: 6s - loss: 0.1070 - accuracy: 0.96 - ETA: 6s - loss: 0.1001 - accuracy: 0.96 - ETA: 5s - loss: 0.0911 - accuracy: 0.96 - ETA: 5s - loss: 0.0858 - accuracy: 0.96 - ETA: 5s - loss: 0.0870 - accuracy: 0.96 - ETA: 5s - loss: 0.0883 - accuracy: 0.96 - ETA: 5s - loss: 0.0930 - accuracy: 0.96 - ETA: 5s - loss: 0.0900 - accuracy: 0.96 - ETA: 5s - loss: 0.0918 - accuracy: 0.96 - ETA: 4s - loss: 0.0912 - accuracy: 0.96 - ETA: 4s - loss: 0.0885 - accuracy: 0.96 - ETA: 4s - loss: 0.0861 - accuracy: 0.96 - ETA: 4s - loss: 0.0857 - accuracy: 0.96 - ETA: 4s - loss: 0.0866 - accuracy: 0.96 - ETA: 4s - loss: 0.0879 - accuracy: 0.96 - ETA: 4s - loss: 0.0904 - accuracy: 0.96 - ETA: 4s - loss: 0.0917 - accuracy: 0.96 - ETA: 4s - loss: 0.0924 - accuracy: 0.96 - ETA: 3s - loss: 0.0939 - accuracy: 0.96 - ETA: 3s - loss: 0.0953 - accuracy: 0.96 - ETA: 3s - loss: 0.0940 - accuracy: 0.96 - ETA: 3s - loss: 0.0916 - accuracy: 0.96 - ETA: 3s - loss: 0.0929 - accuracy: 0.96 - ETA: 3s - loss: 0.0978 - accuracy: 0.96 - ETA: 3s - loss: 0.0990 - accuracy: 0.96 - ETA: 3s - loss: 0.0979 - accuracy: 0.96 - ETA: 3s - loss: 0.0984 - accuracy: 0.96 - ETA: 3s - loss: 0.0977 - accuracy: 0.96 - ETA: 3s - loss: 0.0960 - accuracy: 0.96 - ETA: 3s - loss: 0.0947 - accuracy: 0.96 - ETA: 2s - loss: 0.0947 - accuracy: 0.96 - ETA: 2s - loss: 0.0934 - accuracy: 0.96 - ETA: 2s - loss: 0.0924 - accuracy: 0.96 - ETA: 2s - loss: 0.0915 - accuracy: 0.96 - ETA: 2s - loss: 0.0906 - accuracy: 0.96 - ETA: 2s - loss: 0.0900 - accuracy: 0.96 - ETA: 2s - loss: 0.0906 - accuracy: 0.96 - ETA: 2s - loss: 0.0945 - accuracy: 0.96 - ETA: 2s - loss: 0.0966 - accuracy: 0.96 - ETA: 2s - loss: 0.0983 - accuracy: 0.96 - ETA: 2s - loss: 0.0989 - accuracy: 0.96 - ETA: 2s - loss: 0.0987 - accuracy: 0.96 - ETA: 1s - loss: 0.0990 - accuracy: 0.96 - ETA: 1s - loss: 0.1000 - accuracy: 0.96 - ETA: 1s - loss: 0.1005 - accuracy: 0.96 - ETA: 1s - loss: 0.0995 - accuracy: 0.96 - ETA: 1s - loss: 0.0997 - accuracy: 0.96 - ETA: 1s - loss: 0.0991 - accuracy: 0.96 - ETA: 1s - loss: 0.0984 - accuracy: 0.96 - ETA: 1s - loss: 0.0977 - accuracy: 0.96 - ETA: 1s - loss: 0.0979 - accuracy: 0.96 - ETA: 1s - loss: 0.0972 - accuracy: 0.96 - ETA: 1s - loss: 0.0968 - accuracy: 0.96 - ETA: 1s - loss: 0.0956 - accuracy: 0.96 - ETA: 1s - loss: 0.0948 - accuracy: 0.96 - ETA: 0s - loss: 0.0953 - accuracy: 0.96 - ETA: 0s - loss: 0.0948 - accuracy: 0.96 - ETA: 0s - loss: 0.0946 - accuracy: 0.96 - ETA: 0s - loss: 0.0944 - accuracy: 0.96 - ETA: 0s - loss: 0.0978 - accuracy: 0.96 - ETA: 0s - loss: 0.0981 - accuracy: 0.96 - ETA: 0s - loss: 0.0976 - accuracy: 0.96 - ETA: 0s - loss: 0.0972 - accuracy: 0.96 - ETA: 0s - loss: 0.0961 - accuracy: 0.96 - ETA: 0s - loss: 0.0958 - accuracy: 0.96 - ETA: 0s - loss: 0.0952 - accuracy: 0.96 - ETA: 0s - loss: 0.0952 - accuracy: 0.96 - ETA: 0s - loss: 0.0945 - accuracy: 0.96 - 6s 546us/step - loss: 0.0943 - accuracy: 0.9653 - val_loss: 0.0631 - val_accuracy: 0.9773\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00794\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0658 - accuracy: 0.97 - ETA: 5s - loss: 0.0571 - accuracy: 0.97 - ETA: 4s - loss: 0.0526 - accuracy: 0.98 - ETA: 4s - loss: 0.0526 - accuracy: 0.98 - ETA: 4s - loss: 0.0479 - accuracy: 0.98 - ETA: 4s - loss: 0.0421 - accuracy: 0.98 - ETA: 4s - loss: 0.0547 - accuracy: 0.98 - ETA: 4s - loss: 0.0590 - accuracy: 0.97 - ETA: 4s - loss: 0.0539 - accuracy: 0.98 - ETA: 4s - loss: 0.0666 - accuracy: 0.97 - ETA: 4s - loss: 0.0672 - accuracy: 0.97 - ETA: 4s - loss: 0.0643 - accuracy: 0.97 - ETA: 4s - loss: 0.0643 - accuracy: 0.97 - ETA: 4s - loss: 0.0637 - accuracy: 0.97 - ETA: 4s - loss: 0.0631 - accuracy: 0.97 - ETA: 4s - loss: 0.0657 - accuracy: 0.97 - ETA: 4s - loss: 0.0714 - accuracy: 0.97 - ETA: 4s - loss: 0.0797 - accuracy: 0.97 - ETA: 4s - loss: 0.0785 - accuracy: 0.97 - ETA: 3s - loss: 0.0770 - accuracy: 0.97 - ETA: 3s - loss: 0.0749 - accuracy: 0.97 - ETA: 3s - loss: 0.0738 - accuracy: 0.97 - ETA: 3s - loss: 0.0761 - accuracy: 0.97 - ETA: 3s - loss: 0.0797 - accuracy: 0.97 - ETA: 3s - loss: 0.0814 - accuracy: 0.97 - ETA: 3s - loss: 0.0816 - accuracy: 0.97 - ETA: 3s - loss: 0.0844 - accuracy: 0.97 - ETA: 3s - loss: 0.0838 - accuracy: 0.97 - ETA: 3s - loss: 0.0829 - accuracy: 0.97 - ETA: 3s - loss: 0.0831 - accuracy: 0.97 - ETA: 3s - loss: 0.0831 - accuracy: 0.97 - ETA: 3s - loss: 0.0828 - accuracy: 0.97 - ETA: 3s - loss: 0.0816 - accuracy: 0.97 - ETA: 2s - loss: 0.0811 - accuracy: 0.97 - ETA: 2s - loss: 0.0801 - accuracy: 0.97 - ETA: 2s - loss: 0.0808 - accuracy: 0.97 - ETA: 2s - loss: 0.0806 - accuracy: 0.97 - ETA: 2s - loss: 0.0796 - accuracy: 0.97 - ETA: 2s - loss: 0.0795 - accuracy: 0.97 - ETA: 2s - loss: 0.0787 - accuracy: 0.97 - ETA: 2s - loss: 0.0787 - accuracy: 0.97 - ETA: 2s - loss: 0.0778 - accuracy: 0.97 - ETA: 2s - loss: 0.0765 - accuracy: 0.97 - ETA: 2s - loss: 0.0762 - accuracy: 0.97 - ETA: 2s - loss: 0.0760 - accuracy: 0.97 - ETA: 2s - loss: 0.0754 - accuracy: 0.97 - ETA: 1s - loss: 0.0754 - accuracy: 0.97 - ETA: 1s - loss: 0.0746 - accuracy: 0.97 - ETA: 1s - loss: 0.0748 - accuracy: 0.97 - ETA: 1s - loss: 0.0755 - accuracy: 0.97 - ETA: 1s - loss: 0.0783 - accuracy: 0.97 - ETA: 1s - loss: 0.0855 - accuracy: 0.96 - ETA: 1s - loss: 0.0850 - accuracy: 0.96 - ETA: 1s - loss: 0.0846 - accuracy: 0.96 - ETA: 1s - loss: 0.0839 - accuracy: 0.96 - ETA: 1s - loss: 0.0838 - accuracy: 0.96 - ETA: 1s - loss: 0.0838 - accuracy: 0.96 - ETA: 1s - loss: 0.0836 - accuracy: 0.96 - ETA: 1s - loss: 0.0830 - accuracy: 0.96 - ETA: 1s - loss: 0.0834 - accuracy: 0.96 - ETA: 0s - loss: 0.0824 - accuracy: 0.96 - ETA: 0s - loss: 0.0821 - accuracy: 0.97 - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - ETA: 0s - loss: 0.0813 - accuracy: 0.97 - ETA: 0s - loss: 0.0807 - accuracy: 0.97 - ETA: 0s - loss: 0.0799 - accuracy: 0.97 - ETA: 0s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0783 - accuracy: 0.97 - ETA: 0s - loss: 0.0780 - accuracy: 0.97 - ETA: 0s - loss: 0.0772 - accuracy: 0.97 - ETA: 0s - loss: 0.0762 - accuracy: 0.97 - ETA: 0s - loss: 0.0770 - accuracy: 0.97 - 6s 534us/step - loss: 0.0776 - accuracy: 0.9720 - val_loss: 0.2190 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00794\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.1896 - accuracy: 0.92 - ETA: 4s - loss: 0.1322 - accuracy: 0.94 - ETA: 5s - loss: 0.1284 - accuracy: 0.95 - ETA: 5s - loss: 0.1162 - accuracy: 0.96 - ETA: 5s - loss: 0.1109 - accuracy: 0.96 - ETA: 5s - loss: 0.1004 - accuracy: 0.96 - ETA: 4s - loss: 0.0912 - accuracy: 0.96 - ETA: 4s - loss: 0.0828 - accuracy: 0.97 - ETA: 4s - loss: 0.0764 - accuracy: 0.97 - ETA: 4s - loss: 0.0720 - accuracy: 0.97 - ETA: 4s - loss: 0.0752 - accuracy: 0.97 - ETA: 4s - loss: 0.0722 - accuracy: 0.97 - ETA: 4s - loss: 0.0698 - accuracy: 0.97 - ETA: 4s - loss: 0.0740 - accuracy: 0.97 - ETA: 4s - loss: 0.0747 - accuracy: 0.97 - ETA: 4s - loss: 0.0727 - accuracy: 0.97 - ETA: 4s - loss: 0.0721 - accuracy: 0.97 - ETA: 4s - loss: 0.0745 - accuracy: 0.97 - ETA: 4s - loss: 0.0728 - accuracy: 0.97 - ETA: 4s - loss: 0.0719 - accuracy: 0.97 - ETA: 3s - loss: 0.0788 - accuracy: 0.97 - ETA: 3s - loss: 0.0812 - accuracy: 0.97 - ETA: 3s - loss: 0.0813 - accuracy: 0.97 - ETA: 3s - loss: 0.0811 - accuracy: 0.97 - ETA: 3s - loss: 0.0793 - accuracy: 0.97 - ETA: 3s - loss: 0.0818 - accuracy: 0.97 - ETA: 3s - loss: 0.0811 - accuracy: 0.97 - ETA: 3s - loss: 0.0826 - accuracy: 0.97 - ETA: 3s - loss: 0.0815 - accuracy: 0.97 - ETA: 3s - loss: 0.0798 - accuracy: 0.97 - ETA: 3s - loss: 0.0775 - accuracy: 0.97 - ETA: 3s - loss: 0.0758 - accuracy: 0.97 - ETA: 3s - loss: 0.0762 - accuracy: 0.97 - ETA: 2s - loss: 0.0754 - accuracy: 0.97 - ETA: 2s - loss: 0.0749 - accuracy: 0.97 - ETA: 2s - loss: 0.0737 - accuracy: 0.97 - ETA: 2s - loss: 0.0728 - accuracy: 0.97 - ETA: 2s - loss: 0.0728 - accuracy: 0.97 - ETA: 2s - loss: 0.0718 - accuracy: 0.97 - ETA: 2s - loss: 0.0719 - accuracy: 0.97 - ETA: 2s - loss: 0.0706 - accuracy: 0.97 - ETA: 2s - loss: 0.0699 - accuracy: 0.97 - ETA: 2s - loss: 0.0697 - accuracy: 0.97 - ETA: 2s - loss: 0.0697 - accuracy: 0.97 - ETA: 2s - loss: 0.0686 - accuracy: 0.97 - ETA: 2s - loss: 0.0679 - accuracy: 0.97 - ETA: 2s - loss: 0.0674 - accuracy: 0.97 - ETA: 1s - loss: 0.0669 - accuracy: 0.97 - ETA: 1s - loss: 0.0667 - accuracy: 0.97 - ETA: 1s - loss: 0.0663 - accuracy: 0.97 - ETA: 1s - loss: 0.0658 - accuracy: 0.97 - ETA: 1s - loss: 0.0650 - accuracy: 0.97 - ETA: 1s - loss: 0.0658 - accuracy: 0.97 - ETA: 1s - loss: 0.0657 - accuracy: 0.97 - ETA: 1s - loss: 0.0656 - accuracy: 0.97 - ETA: 1s - loss: 0.0661 - accuracy: 0.97 - ETA: 1s - loss: 0.0690 - accuracy: 0.97 - ETA: 1s - loss: 0.0694 - accuracy: 0.97 - ETA: 1s - loss: 0.0688 - accuracy: 0.97 - ETA: 1s - loss: 0.0688 - accuracy: 0.97 - ETA: 0s - loss: 0.0687 - accuracy: 0.97 - ETA: 0s - loss: 0.0690 - accuracy: 0.97 - ETA: 0s - loss: 0.0689 - accuracy: 0.97 - ETA: 0s - loss: 0.0684 - accuracy: 0.97 - ETA: 0s - loss: 0.0680 - accuracy: 0.97 - ETA: 0s - loss: 0.0679 - accuracy: 0.97 - ETA: 0s - loss: 0.0680 - accuracy: 0.97 - ETA: 0s - loss: 0.0692 - accuracy: 0.97 - ETA: 0s - loss: 0.0684 - accuracy: 0.97 - ETA: 0s - loss: 0.0679 - accuracy: 0.97 - ETA: 0s - loss: 0.0692 - accuracy: 0.97 - ETA: 0s - loss: 0.0697 - accuracy: 0.97 - ETA: 0s - loss: 0.0692 - accuracy: 0.97 - 6s 536us/step - loss: 0.0693 - accuracy: 0.9759 - val_loss: 0.0419 - val_accuracy: 0.9874\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00794\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 5s - loss: 0.0259 - accuracy: 0.98 - ETA: 4s - loss: 0.0234 - accuracy: 0.98 - ETA: 5s - loss: 0.0281 - accuracy: 0.98 - ETA: 4s - loss: 0.0470 - accuracy: 0.98 - ETA: 4s - loss: 0.0544 - accuracy: 0.98 - ETA: 4s - loss: 0.0622 - accuracy: 0.97 - ETA: 4s - loss: 0.0634 - accuracy: 0.97 - ETA: 4s - loss: 0.0598 - accuracy: 0.97 - ETA: 4s - loss: 0.0582 - accuracy: 0.97 - ETA: 4s - loss: 0.0643 - accuracy: 0.97 - ETA: 4s - loss: 0.0684 - accuracy: 0.97 - ETA: 4s - loss: 0.0707 - accuracy: 0.97 - ETA: 4s - loss: 0.0698 - accuracy: 0.97 - ETA: 4s - loss: 0.0677 - accuracy: 0.97 - ETA: 4s - loss: 0.0652 - accuracy: 0.97 - ETA: 4s - loss: 0.0645 - accuracy: 0.97 - ETA: 4s - loss: 0.0651 - accuracy: 0.97 - ETA: 4s - loss: 0.0639 - accuracy: 0.97 - ETA: 4s - loss: 0.0657 - accuracy: 0.97 - ETA: 3s - loss: 0.0695 - accuracy: 0.97 - ETA: 3s - loss: 0.0715 - accuracy: 0.97 - ETA: 3s - loss: 0.0703 - accuracy: 0.97 - ETA: 3s - loss: 0.0700 - accuracy: 0.97 - ETA: 3s - loss: 0.0681 - accuracy: 0.97 - ETA: 3s - loss: 0.0662 - accuracy: 0.97 - ETA: 3s - loss: 0.0646 - accuracy: 0.97 - ETA: 3s - loss: 0.0658 - accuracy: 0.97 - ETA: 3s - loss: 0.0649 - accuracy: 0.97 - ETA: 3s - loss: 0.0639 - accuracy: 0.97 - ETA: 3s - loss: 0.0623 - accuracy: 0.97 - ETA: 3s - loss: 0.0621 - accuracy: 0.97 - ETA: 3s - loss: 0.0615 - accuracy: 0.97 - ETA: 2s - loss: 0.0606 - accuracy: 0.97 - ETA: 2s - loss: 0.0601 - accuracy: 0.97 - ETA: 2s - loss: 0.0594 - accuracy: 0.97 - ETA: 2s - loss: 0.0615 - accuracy: 0.97 - ETA: 2s - loss: 0.0660 - accuracy: 0.97 - ETA: 2s - loss: 0.0682 - accuracy: 0.97 - ETA: 2s - loss: 0.0693 - accuracy: 0.97 - ETA: 2s - loss: 0.0689 - accuracy: 0.97 - ETA: 2s - loss: 0.0683 - accuracy: 0.97 - ETA: 2s - loss: 0.0675 - accuracy: 0.97 - ETA: 2s - loss: 0.0665 - accuracy: 0.97 - ETA: 2s - loss: 0.0655 - accuracy: 0.97 - ETA: 2s - loss: 0.0647 - accuracy: 0.97 - ETA: 1s - loss: 0.0636 - accuracy: 0.97 - ETA: 1s - loss: 0.0632 - accuracy: 0.97 - ETA: 1s - loss: 0.0640 - accuracy: 0.97 - ETA: 1s - loss: 0.0637 - accuracy: 0.97 - ETA: 1s - loss: 0.0633 - accuracy: 0.97 - ETA: 1s - loss: 0.0629 - accuracy: 0.97 - ETA: 1s - loss: 0.0620 - accuracy: 0.97 - ETA: 1s - loss: 0.0617 - accuracy: 0.97 - ETA: 1s - loss: 0.0611 - accuracy: 0.97 - ETA: 1s - loss: 0.0612 - accuracy: 0.97 - ETA: 1s - loss: 0.0607 - accuracy: 0.97 - ETA: 1s - loss: 0.0609 - accuracy: 0.97 - ETA: 1s - loss: 0.0611 - accuracy: 0.97 - ETA: 1s - loss: 0.0609 - accuracy: 0.97 - ETA: 0s - loss: 0.0607 - accuracy: 0.97 - ETA: 0s - loss: 0.0601 - accuracy: 0.97 - ETA: 0s - loss: 0.0600 - accuracy: 0.97 - ETA: 0s - loss: 0.0599 - accuracy: 0.97 - ETA: 0s - loss: 0.0600 - accuracy: 0.97 - ETA: 0s - loss: 0.0612 - accuracy: 0.97 - ETA: 0s - loss: 0.0621 - accuracy: 0.97 - ETA: 0s - loss: 0.0621 - accuracy: 0.97 - ETA: 0s - loss: 0.0619 - accuracy: 0.97 - ETA: 0s - loss: 0.0612 - accuracy: 0.97 - ETA: 0s - loss: 0.0605 - accuracy: 0.97 - ETA: 0s - loss: 0.0604 - accuracy: 0.97 - ETA: 0s - loss: 0.0598 - accuracy: 0.97 - 6s 534us/step - loss: 0.0594 - accuracy: 0.9785 - val_loss: 0.0241 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00794\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0296 - accuracy: 0.98 - ETA: 5s - loss: 0.1104 - accuracy: 0.96 - ETA: 5s - loss: 0.2159 - accuracy: 0.93 - ETA: 4s - loss: 0.1682 - accuracy: 0.94 - ETA: 4s - loss: 0.1383 - accuracy: 0.95 - ETA: 4s - loss: 0.1199 - accuracy: 0.96 - ETA: 4s - loss: 0.1050 - accuracy: 0.96 - ETA: 4s - loss: 0.1016 - accuracy: 0.96 - ETA: 4s - loss: 0.0949 - accuracy: 0.96 - ETA: 4s - loss: 0.0891 - accuracy: 0.97 - ETA: 4s - loss: 0.0856 - accuracy: 0.97 - ETA: 4s - loss: 0.0802 - accuracy: 0.97 - ETA: 4s - loss: 0.0761 - accuracy: 0.97 - ETA: 4s - loss: 0.0729 - accuracy: 0.97 - ETA: 4s - loss: 0.0693 - accuracy: 0.97 - ETA: 4s - loss: 0.0680 - accuracy: 0.97 - ETA: 4s - loss: 0.0681 - accuracy: 0.97 - ETA: 4s - loss: 0.0670 - accuracy: 0.97 - ETA: 4s - loss: 0.0662 - accuracy: 0.97 - ETA: 4s - loss: 0.0660 - accuracy: 0.97 - ETA: 3s - loss: 0.0640 - accuracy: 0.97 - ETA: 3s - loss: 0.0633 - accuracy: 0.97 - ETA: 3s - loss: 0.0608 - accuracy: 0.97 - ETA: 3s - loss: 0.0603 - accuracy: 0.97 - ETA: 3s - loss: 0.0606 - accuracy: 0.97 - ETA: 3s - loss: 0.0588 - accuracy: 0.97 - ETA: 3s - loss: 0.0571 - accuracy: 0.97 - ETA: 3s - loss: 0.0563 - accuracy: 0.97 - ETA: 3s - loss: 0.0548 - accuracy: 0.98 - ETA: 3s - loss: 0.0553 - accuracy: 0.98 - ETA: 3s - loss: 0.0544 - accuracy: 0.98 - ETA: 3s - loss: 0.0531 - accuracy: 0.98 - ETA: 3s - loss: 0.0524 - accuracy: 0.98 - ETA: 2s - loss: 0.0528 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 2s - loss: 0.0520 - accuracy: 0.98 - ETA: 2s - loss: 0.0512 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 2s - loss: 0.0535 - accuracy: 0.98 - ETA: 2s - loss: 0.0534 - accuracy: 0.98 - ETA: 2s - loss: 0.0532 - accuracy: 0.98 - ETA: 2s - loss: 0.0524 - accuracy: 0.98 - ETA: 2s - loss: 0.0528 - accuracy: 0.98 - ETA: 2s - loss: 0.0529 - accuracy: 0.98 - ETA: 2s - loss: 0.0532 - accuracy: 0.98 - ETA: 2s - loss: 0.0535 - accuracy: 0.98 - ETA: 2s - loss: 0.0540 - accuracy: 0.98 - ETA: 1s - loss: 0.0552 - accuracy: 0.98 - ETA: 1s - loss: 0.0553 - accuracy: 0.98 - ETA: 1s - loss: 0.0564 - accuracy: 0.98 - ETA: 1s - loss: 0.0565 - accuracy: 0.98 - ETA: 1s - loss: 0.0567 - accuracy: 0.98 - ETA: 1s - loss: 0.0575 - accuracy: 0.98 - ETA: 1s - loss: 0.0577 - accuracy: 0.97 - ETA: 1s - loss: 0.0569 - accuracy: 0.98 - ETA: 1s - loss: 0.0566 - accuracy: 0.98 - ETA: 1s - loss: 0.0573 - accuracy: 0.97 - ETA: 1s - loss: 0.0579 - accuracy: 0.97 - ETA: 1s - loss: 0.0576 - accuracy: 0.97 - ETA: 1s - loss: 0.0577 - accuracy: 0.97 - ETA: 0s - loss: 0.0572 - accuracy: 0.98 - ETA: 0s - loss: 0.0569 - accuracy: 0.98 - ETA: 0s - loss: 0.0576 - accuracy: 0.97 - ETA: 0s - loss: 0.0604 - accuracy: 0.97 - ETA: 0s - loss: 0.0621 - accuracy: 0.97 - ETA: 0s - loss: 0.0618 - accuracy: 0.97 - ETA: 0s - loss: 0.0619 - accuracy: 0.97 - ETA: 0s - loss: 0.0614 - accuracy: 0.97 - ETA: 0s - loss: 0.0608 - accuracy: 0.97 - ETA: 0s - loss: 0.0605 - accuracy: 0.97 - ETA: 0s - loss: 0.0608 - accuracy: 0.97 - ETA: 0s - loss: 0.0605 - accuracy: 0.97 - ETA: 0s - loss: 0.0600 - accuracy: 0.97 - 6s 563us/step - loss: 0.0597 - accuracy: 0.9790 - val_loss: 0.0266 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00794\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0210 - accuracy: 0.99 - ETA: 6s - loss: 0.0229 - accuracy: 0.98 - ETA: 5s - loss: 0.0533 - accuracy: 0.98 - ETA: 5s - loss: 0.0516 - accuracy: 0.98 - ETA: 5s - loss: 0.0709 - accuracy: 0.98 - ETA: 5s - loss: 0.0844 - accuracy: 0.97 - ETA: 5s - loss: 0.0805 - accuracy: 0.97 - ETA: 5s - loss: 0.0800 - accuracy: 0.97 - ETA: 5s - loss: 0.0743 - accuracy: 0.97 - ETA: 5s - loss: 0.0683 - accuracy: 0.97 - ETA: 5s - loss: 0.0655 - accuracy: 0.97 - ETA: 5s - loss: 0.0640 - accuracy: 0.97 - ETA: 5s - loss: 0.0622 - accuracy: 0.97 - ETA: 5s - loss: 0.0615 - accuracy: 0.97 - ETA: 4s - loss: 0.0589 - accuracy: 0.97 - ETA: 4s - loss: 0.0565 - accuracy: 0.98 - ETA: 4s - loss: 0.0541 - accuracy: 0.98 - ETA: 4s - loss: 0.0523 - accuracy: 0.98 - ETA: 4s - loss: 0.0508 - accuracy: 0.98 - ETA: 4s - loss: 0.0489 - accuracy: 0.98 - ETA: 4s - loss: 0.0468 - accuracy: 0.98 - ETA: 4s - loss: 0.0461 - accuracy: 0.98 - ETA: 4s - loss: 0.0460 - accuracy: 0.98 - ETA: 4s - loss: 0.0509 - accuracy: 0.98 - ETA: 4s - loss: 0.0508 - accuracy: 0.98 - ETA: 3s - loss: 0.0501 - accuracy: 0.98 - ETA: 3s - loss: 0.0502 - accuracy: 0.98 - ETA: 3s - loss: 0.0501 - accuracy: 0.98 - ETA: 3s - loss: 0.0530 - accuracy: 0.98 - ETA: 3s - loss: 0.0517 - accuracy: 0.98 - ETA: 3s - loss: 0.0520 - accuracy: 0.98 - ETA: 3s - loss: 0.0508 - accuracy: 0.98 - ETA: 3s - loss: 0.0526 - accuracy: 0.98 - ETA: 3s - loss: 0.0526 - accuracy: 0.98 - ETA: 3s - loss: 0.0533 - accuracy: 0.98 - ETA: 3s - loss: 0.0525 - accuracy: 0.98 - ETA: 3s - loss: 0.0518 - accuracy: 0.98 - ETA: 3s - loss: 0.0509 - accuracy: 0.98 - ETA: 2s - loss: 0.0503 - accuracy: 0.98 - ETA: 2s - loss: 0.0491 - accuracy: 0.98 - ETA: 2s - loss: 0.0481 - accuracy: 0.98 - ETA: 2s - loss: 0.0475 - accuracy: 0.98 - ETA: 2s - loss: 0.0477 - accuracy: 0.98 - ETA: 2s - loss: 0.0478 - accuracy: 0.98 - ETA: 2s - loss: 0.0487 - accuracy: 0.98 - ETA: 2s - loss: 0.0487 - accuracy: 0.98 - ETA: 2s - loss: 0.0493 - accuracy: 0.98 - ETA: 2s - loss: 0.0485 - accuracy: 0.98 - ETA: 2s - loss: 0.0481 - accuracy: 0.98 - ETA: 1s - loss: 0.0480 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 1s - loss: 0.0485 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0471 - accuracy: 0.98 - ETA: 1s - loss: 0.0470 - accuracy: 0.98 - ETA: 1s - loss: 0.0485 - accuracy: 0.98 - ETA: 1s - loss: 0.0523 - accuracy: 0.98 - ETA: 1s - loss: 0.0543 - accuracy: 0.98 - ETA: 1s - loss: 0.0540 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0536 - accuracy: 0.98 - ETA: 0s - loss: 0.0532 - accuracy: 0.98 - ETA: 0s - loss: 0.0525 - accuracy: 0.98 - ETA: 0s - loss: 0.0523 - accuracy: 0.98 - ETA: 0s - loss: 0.0520 - accuracy: 0.98 - ETA: 0s - loss: 0.0520 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0528 - accuracy: 0.98 - ETA: 0s - loss: 0.0523 - accuracy: 0.98 - ETA: 0s - loss: 0.0523 - accuracy: 0.98 - 6s 602us/step - loss: 0.0522 - accuracy: 0.9816 - val_loss: 0.0348 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00794\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0360 - accuracy: 0.99 - ETA: 5s - loss: 0.0219 - accuracy: 0.99 - ETA: 5s - loss: 0.0225 - accuracy: 0.99 - ETA: 5s - loss: 0.0220 - accuracy: 0.99 - ETA: 5s - loss: 0.0200 - accuracy: 0.99 - ETA: 5s - loss: 0.0192 - accuracy: 0.99 - ETA: 5s - loss: 0.0212 - accuracy: 0.99 - ETA: 5s - loss: 0.0231 - accuracy: 0.99 - ETA: 5s - loss: 0.0229 - accuracy: 0.99 - ETA: 5s - loss: 0.0223 - accuracy: 0.99 - ETA: 5s - loss: 0.0271 - accuracy: 0.99 - ETA: 5s - loss: 0.0328 - accuracy: 0.98 - ETA: 4s - loss: 0.0312 - accuracy: 0.98 - ETA: 4s - loss: 0.0306 - accuracy: 0.98 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0425 - accuracy: 0.98 - ETA: 4s - loss: 0.0463 - accuracy: 0.98 - ETA: 4s - loss: 0.0470 - accuracy: 0.98 - ETA: 4s - loss: 0.0456 - accuracy: 0.98 - ETA: 4s - loss: 0.0476 - accuracy: 0.98 - ETA: 4s - loss: 0.0469 - accuracy: 0.98 - ETA: 4s - loss: 0.0472 - accuracy: 0.98 - ETA: 4s - loss: 0.0466 - accuracy: 0.98 - ETA: 3s - loss: 0.0457 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0451 - accuracy: 0.98 - ETA: 3s - loss: 0.0465 - accuracy: 0.98 - ETA: 3s - loss: 0.0468 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 3s - loss: 0.0546 - accuracy: 0.98 - ETA: 3s - loss: 0.0576 - accuracy: 0.98 - ETA: 3s - loss: 0.0570 - accuracy: 0.98 - ETA: 3s - loss: 0.0567 - accuracy: 0.98 - ETA: 3s - loss: 0.0562 - accuracy: 0.98 - ETA: 3s - loss: 0.0556 - accuracy: 0.98 - ETA: 2s - loss: 0.0544 - accuracy: 0.98 - ETA: 2s - loss: 0.0550 - accuracy: 0.98 - ETA: 2s - loss: 0.0563 - accuracy: 0.98 - ETA: 2s - loss: 0.0567 - accuracy: 0.98 - ETA: 2s - loss: 0.0564 - accuracy: 0.98 - ETA: 2s - loss: 0.0557 - accuracy: 0.98 - ETA: 2s - loss: 0.0554 - accuracy: 0.98 - ETA: 2s - loss: 0.0546 - accuracy: 0.98 - ETA: 2s - loss: 0.0538 - accuracy: 0.98 - ETA: 2s - loss: 0.0534 - accuracy: 0.98 - ETA: 2s - loss: 0.0553 - accuracy: 0.98 - ETA: 2s - loss: 0.0555 - accuracy: 0.98 - ETA: 1s - loss: 0.0553 - accuracy: 0.98 - ETA: 1s - loss: 0.0548 - accuracy: 0.98 - ETA: 1s - loss: 0.0549 - accuracy: 0.98 - ETA: 1s - loss: 0.0544 - accuracy: 0.98 - ETA: 1s - loss: 0.0537 - accuracy: 0.98 - ETA: 1s - loss: 0.0536 - accuracy: 0.98 - ETA: 1s - loss: 0.0533 - accuracy: 0.98 - ETA: 1s - loss: 0.0525 - accuracy: 0.98 - ETA: 1s - loss: 0.0526 - accuracy: 0.98 - ETA: 1s - loss: 0.0525 - accuracy: 0.98 - ETA: 1s - loss: 0.0519 - accuracy: 0.98 - ETA: 1s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0501 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0502 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0565 - accuracy: 0.98 - ETA: 0s - loss: 0.0563 - accuracy: 0.98 - ETA: 0s - loss: 0.0561 - accuracy: 0.98 - ETA: 0s - loss: 0.0556 - accuracy: 0.98 - ETA: 0s - loss: 0.0554 - accuracy: 0.98 - ETA: 0s - loss: 0.0556 - accuracy: 0.98 - 6s 581us/step - loss: 0.0554 - accuracy: 0.9812 - val_loss: 0.0283 - val_accuracy: 0.9874\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00794\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0336 - accuracy: 0.97 - ETA: 5s - loss: 0.0300 - accuracy: 0.98 - ETA: 5s - loss: 0.0325 - accuracy: 0.98 - ETA: 5s - loss: 0.0423 - accuracy: 0.98 - ETA: 5s - loss: 0.0373 - accuracy: 0.98 - ETA: 5s - loss: 0.0338 - accuracy: 0.98 - ETA: 5s - loss: 0.0382 - accuracy: 0.98 - ETA: 5s - loss: 0.0389 - accuracy: 0.98 - ETA: 5s - loss: 0.0365 - accuracy: 0.98 - ETA: 5s - loss: 0.0432 - accuracy: 0.98 - ETA: 5s - loss: 0.0436 - accuracy: 0.98 - ETA: 5s - loss: 0.0434 - accuracy: 0.98 - ETA: 5s - loss: 0.0420 - accuracy: 0.98 - ETA: 4s - loss: 0.0403 - accuracy: 0.98 - ETA: 4s - loss: 0.0394 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0364 - accuracy: 0.98 - ETA: 4s - loss: 0.0391 - accuracy: 0.98 - ETA: 4s - loss: 0.0395 - accuracy: 0.98 - ETA: 4s - loss: 0.0383 - accuracy: 0.98 - ETA: 4s - loss: 0.0382 - accuracy: 0.98 - ETA: 4s - loss: 0.0378 - accuracy: 0.98 - ETA: 4s - loss: 0.0372 - accuracy: 0.98 - ETA: 4s - loss: 0.0392 - accuracy: 0.98 - ETA: 4s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0414 - accuracy: 0.98 - ETA: 3s - loss: 0.0407 - accuracy: 0.98 - ETA: 3s - loss: 0.0398 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0380 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 3s - loss: 0.0405 - accuracy: 0.98 - ETA: 3s - loss: 0.0427 - accuracy: 0.98 - ETA: 3s - loss: 0.0436 - accuracy: 0.98 - ETA: 3s - loss: 0.0439 - accuracy: 0.98 - ETA: 3s - loss: 0.0435 - accuracy: 0.98 - ETA: 2s - loss: 0.0431 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0432 - accuracy: 0.98 - ETA: 2s - loss: 0.0428 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0425 - accuracy: 0.98 - ETA: 2s - loss: 0.0428 - accuracy: 0.98 - ETA: 2s - loss: 0.0429 - accuracy: 0.98 - ETA: 2s - loss: 0.0427 - accuracy: 0.98 - ETA: 2s - loss: 0.0436 - accuracy: 0.98 - ETA: 2s - loss: 0.0435 - accuracy: 0.98 - ETA: 2s - loss: 0.0428 - accuracy: 0.98 - ETA: 1s - loss: 0.0434 - accuracy: 0.98 - ETA: 1s - loss: 0.0433 - accuracy: 0.98 - ETA: 1s - loss: 0.0430 - accuracy: 0.98 - ETA: 1s - loss: 0.0435 - accuracy: 0.98 - ETA: 1s - loss: 0.0432 - accuracy: 0.98 - ETA: 1s - loss: 0.0428 - accuracy: 0.98 - ETA: 1s - loss: 0.0431 - accuracy: 0.98 - ETA: 1s - loss: 0.0432 - accuracy: 0.98 - ETA: 1s - loss: 0.0428 - accuracy: 0.98 - ETA: 1s - loss: 0.0427 - accuracy: 0.98 - ETA: 1s - loss: 0.0421 - accuracy: 0.98 - ETA: 1s - loss: 0.0416 - accuracy: 0.98 - ETA: 0s - loss: 0.0411 - accuracy: 0.98 - ETA: 0s - loss: 0.0411 - accuracy: 0.98 - ETA: 0s - loss: 0.0417 - accuracy: 0.98 - ETA: 0s - loss: 0.0412 - accuracy: 0.98 - ETA: 0s - loss: 0.0409 - accuracy: 0.98 - ETA: 0s - loss: 0.0409 - accuracy: 0.98 - ETA: 0s - loss: 0.0414 - accuracy: 0.98 - ETA: 0s - loss: 0.0411 - accuracy: 0.98 - ETA: 0s - loss: 0.0409 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - 6s 594us/step - loss: 0.0431 - accuracy: 0.9851 - val_loss: 0.0218 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00794\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0282 - accuracy: 0.98 - ETA: 5s - loss: 0.0239 - accuracy: 0.98 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 5s - loss: 0.0380 - accuracy: 0.98 - ETA: 5s - loss: 0.0408 - accuracy: 0.98 - ETA: 5s - loss: 0.0353 - accuracy: 0.98 - ETA: 5s - loss: 0.0315 - accuracy: 0.98 - ETA: 5s - loss: 0.0310 - accuracy: 0.98 - ETA: 5s - loss: 0.0292 - accuracy: 0.98 - ETA: 5s - loss: 0.0285 - accuracy: 0.98 - ETA: 5s - loss: 0.0272 - accuracy: 0.98 - ETA: 5s - loss: 0.0263 - accuracy: 0.99 - ETA: 4s - loss: 0.0289 - accuracy: 0.98 - ETA: 4s - loss: 0.0295 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 4s - loss: 0.0338 - accuracy: 0.98 - ETA: 4s - loss: 0.0366 - accuracy: 0.98 - ETA: 4s - loss: 0.0428 - accuracy: 0.98 - ETA: 4s - loss: 0.0431 - accuracy: 0.98 - ETA: 4s - loss: 0.0419 - accuracy: 0.98 - ETA: 4s - loss: 0.0402 - accuracy: 0.98 - ETA: 4s - loss: 0.0394 - accuracy: 0.98 - ETA: 4s - loss: 0.0396 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 3s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0380 - accuracy: 0.98 - ETA: 3s - loss: 0.0379 - accuracy: 0.98 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0375 - accuracy: 0.98 - ETA: 3s - loss: 0.0368 - accuracy: 0.98 - ETA: 3s - loss: 0.0361 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0351 - accuracy: 0.98 - ETA: 3s - loss: 0.0350 - accuracy: 0.98 - ETA: 3s - loss: 0.0346 - accuracy: 0.98 - ETA: 2s - loss: 0.0345 - accuracy: 0.98 - ETA: 2s - loss: 0.0357 - accuracy: 0.98 - ETA: 2s - loss: 0.0360 - accuracy: 0.98 - ETA: 2s - loss: 0.0356 - accuracy: 0.98 - ETA: 2s - loss: 0.0351 - accuracy: 0.98 - ETA: 2s - loss: 0.0348 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0351 - accuracy: 0.98 - ETA: 2s - loss: 0.0350 - accuracy: 0.98 - ETA: 2s - loss: 0.0343 - accuracy: 0.98 - ETA: 2s - loss: 0.0336 - accuracy: 0.98 - ETA: 2s - loss: 0.0336 - accuracy: 0.98 - ETA: 2s - loss: 0.0337 - accuracy: 0.98 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 1s - loss: 0.0351 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0362 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0361 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0407 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0409 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0404 - accuracy: 0.98 - ETA: 0s - loss: 0.0399 - accuracy: 0.98 - ETA: 0s - loss: 0.0396 - accuracy: 0.98 - ETA: 0s - loss: 0.0397 - accuracy: 0.98 - ETA: 0s - loss: 0.0399 - accuracy: 0.98 - ETA: 0s - loss: 0.0396 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0400 - accuracy: 0.98 - 6s 581us/step - loss: 0.0398 - accuracy: 0.9849 - val_loss: 0.0263 - val_accuracy: 0.9941\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00794\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0243 - accuracy: 0.99 - ETA: 5s - loss: 0.0172 - accuracy: 0.99 - ETA: 5s - loss: 0.0253 - accuracy: 0.99 - ETA: 5s - loss: 0.0234 - accuracy: 0.99 - ETA: 5s - loss: 0.0311 - accuracy: 0.98 - ETA: 5s - loss: 0.0395 - accuracy: 0.98 - ETA: 5s - loss: 0.0358 - accuracy: 0.98 - ETA: 5s - loss: 0.0353 - accuracy: 0.98 - ETA: 5s - loss: 0.0320 - accuracy: 0.99 - ETA: 5s - loss: 0.0333 - accuracy: 0.98 - ETA: 5s - loss: 0.0326 - accuracy: 0.98 - ETA: 5s - loss: 0.0366 - accuracy: 0.98 - ETA: 4s - loss: 0.0363 - accuracy: 0.98 - ETA: 4s - loss: 0.0367 - accuracy: 0.98 - ETA: 4s - loss: 0.0352 - accuracy: 0.98 - ETA: 4s - loss: 0.0345 - accuracy: 0.98 - ETA: 4s - loss: 0.0341 - accuracy: 0.98 - ETA: 4s - loss: 0.0336 - accuracy: 0.98 - ETA: 4s - loss: 0.0332 - accuracy: 0.98 - ETA: 4s - loss: 0.0325 - accuracy: 0.98 - ETA: 4s - loss: 0.0321 - accuracy: 0.98 - ETA: 4s - loss: 0.0327 - accuracy: 0.98 - ETA: 4s - loss: 0.0338 - accuracy: 0.98 - ETA: 4s - loss: 0.0327 - accuracy: 0.98 - ETA: 3s - loss: 0.0384 - accuracy: 0.98 - ETA: 3s - loss: 0.0409 - accuracy: 0.98 - ETA: 3s - loss: 0.0443 - accuracy: 0.98 - ETA: 3s - loss: 0.0446 - accuracy: 0.98 - ETA: 3s - loss: 0.0442 - accuracy: 0.98 - ETA: 3s - loss: 0.0436 - accuracy: 0.98 - ETA: 3s - loss: 0.0437 - accuracy: 0.98 - ETA: 3s - loss: 0.0440 - accuracy: 0.98 - ETA: 3s - loss: 0.0433 - accuracy: 0.98 - ETA: 3s - loss: 0.0440 - accuracy: 0.98 - ETA: 3s - loss: 0.0431 - accuracy: 0.98 - ETA: 3s - loss: 0.0420 - accuracy: 0.98 - ETA: 2s - loss: 0.0418 - accuracy: 0.98 - ETA: 2s - loss: 0.0426 - accuracy: 0.98 - ETA: 2s - loss: 0.0448 - accuracy: 0.98 - ETA: 2s - loss: 0.0440 - accuracy: 0.98 - ETA: 2s - loss: 0.0437 - accuracy: 0.98 - ETA: 2s - loss: 0.0432 - accuracy: 0.98 - ETA: 2s - loss: 0.0424 - accuracy: 0.98 - ETA: 2s - loss: 0.0416 - accuracy: 0.98 - ETA: 2s - loss: 0.0411 - accuracy: 0.98 - ETA: 2s - loss: 0.0414 - accuracy: 0.98 - ETA: 2s - loss: 0.0413 - accuracy: 0.98 - ETA: 2s - loss: 0.0417 - accuracy: 0.98 - ETA: 2s - loss: 0.0416 - accuracy: 0.98 - ETA: 1s - loss: 0.0413 - accuracy: 0.98 - ETA: 1s - loss: 0.0408 - accuracy: 0.98 - ETA: 1s - loss: 0.0405 - accuracy: 0.98 - ETA: 1s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0399 - accuracy: 0.98 - ETA: 1s - loss: 0.0405 - accuracy: 0.98 - ETA: 1s - loss: 0.0406 - accuracy: 0.98 - ETA: 1s - loss: 0.0403 - accuracy: 0.98 - ETA: 1s - loss: 0.0397 - accuracy: 0.98 - ETA: 1s - loss: 0.0393 - accuracy: 0.98 - ETA: 0s - loss: 0.0390 - accuracy: 0.98 - ETA: 0s - loss: 0.0386 - accuracy: 0.98 - ETA: 0s - loss: 0.0392 - accuracy: 0.98 - ETA: 0s - loss: 0.0389 - accuracy: 0.98 - ETA: 0s - loss: 0.0384 - accuracy: 0.98 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0388 - accuracy: 0.98 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0381 - accuracy: 0.98 - ETA: 0s - loss: 0.0377 - accuracy: 0.98 - ETA: 0s - loss: 0.0379 - accuracy: 0.98 - ETA: 0s - loss: 0.0375 - accuracy: 0.98 - 6s 584us/step - loss: 0.0372 - accuracy: 0.9864 - val_loss: 0.0250 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00794\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 6s - loss: 0.0037 - accuracy: 1.00 - ETA: 6s - loss: 0.0240 - accuracy: 0.98 - ETA: 6s - loss: 0.0186 - accuracy: 0.99 - ETA: 5s - loss: 0.0182 - accuracy: 0.99 - ETA: 5s - loss: 0.0222 - accuracy: 0.99 - ETA: 5s - loss: 0.0252 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0305 - accuracy: 0.98 - ETA: 5s - loss: 0.0301 - accuracy: 0.98 - ETA: 5s - loss: 0.0324 - accuracy: 0.98 - ETA: 5s - loss: 0.0302 - accuracy: 0.98 - ETA: 5s - loss: 0.0280 - accuracy: 0.98 - ETA: 5s - loss: 0.0287 - accuracy: 0.98 - ETA: 4s - loss: 0.0333 - accuracy: 0.98 - ETA: 4s - loss: 0.0337 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0343 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0329 - accuracy: 0.98 - ETA: 4s - loss: 0.0322 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 4s - loss: 0.0391 - accuracy: 0.98 - ETA: 4s - loss: 0.0376 - accuracy: 0.98 - ETA: 4s - loss: 0.0391 - accuracy: 0.98 - ETA: 3s - loss: 0.0381 - accuracy: 0.98 - ETA: 3s - loss: 0.0378 - accuracy: 0.98 - ETA: 3s - loss: 0.0372 - accuracy: 0.98 - ETA: 3s - loss: 0.0367 - accuracy: 0.98 - ETA: 3s - loss: 0.0406 - accuracy: 0.98 - ETA: 3s - loss: 0.0427 - accuracy: 0.98 - ETA: 3s - loss: 0.0427 - accuracy: 0.98 - ETA: 3s - loss: 0.0419 - accuracy: 0.98 - ETA: 3s - loss: 0.0413 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0403 - accuracy: 0.98 - ETA: 3s - loss: 0.0394 - accuracy: 0.98 - ETA: 3s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0381 - accuracy: 0.98 - ETA: 2s - loss: 0.0375 - accuracy: 0.98 - ETA: 2s - loss: 0.0371 - accuracy: 0.98 - ETA: 2s - loss: 0.0372 - accuracy: 0.98 - ETA: 2s - loss: 0.0387 - accuracy: 0.98 - ETA: 2s - loss: 0.0401 - accuracy: 0.98 - ETA: 2s - loss: 0.0395 - accuracy: 0.98 - ETA: 2s - loss: 0.0388 - accuracy: 0.98 - ETA: 2s - loss: 0.0391 - accuracy: 0.98 - ETA: 2s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0386 - accuracy: 0.98 - ETA: 1s - loss: 0.0381 - accuracy: 0.98 - ETA: 1s - loss: 0.0382 - accuracy: 0.98 - ETA: 1s - loss: 0.0377 - accuracy: 0.98 - ETA: 1s - loss: 0.0373 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0367 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0385 - accuracy: 0.98 - ETA: 1s - loss: 0.0423 - accuracy: 0.98 - ETA: 1s - loss: 0.0433 - accuracy: 0.98 - ETA: 1s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0434 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.98 - ETA: 0s - loss: 0.0434 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.98 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.98 - ETA: 0s - loss: 0.0416 - accuracy: 0.98 - ETA: 0s - loss: 0.0415 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - 6s 586us/step - loss: 0.0409 - accuracy: 0.9874 - val_loss: 0.0310 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00794\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.0606 - accuracy: 0.97 - ETA: 5s - loss: 0.0780 - accuracy: 0.96 - ETA: 5s - loss: 0.0586 - accuracy: 0.97 - ETA: 5s - loss: 0.0460 - accuracy: 0.98 - ETA: 5s - loss: 0.0480 - accuracy: 0.98 - ETA: 5s - loss: 0.0433 - accuracy: 0.98 - ETA: 5s - loss: 0.0382 - accuracy: 0.98 - ETA: 5s - loss: 0.0347 - accuracy: 0.98 - ETA: 5s - loss: 0.0334 - accuracy: 0.98 - ETA: 5s - loss: 0.0313 - accuracy: 0.98 - ETA: 5s - loss: 0.0327 - accuracy: 0.98 - ETA: 5s - loss: 0.0325 - accuracy: 0.98 - ETA: 5s - loss: 0.0306 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.99 - ETA: 4s - loss: 0.0297 - accuracy: 0.99 - ETA: 4s - loss: 0.0324 - accuracy: 0.99 - ETA: 4s - loss: 0.0317 - accuracy: 0.99 - ETA: 4s - loss: 0.0315 - accuracy: 0.99 - ETA: 4s - loss: 0.0311 - accuracy: 0.99 - ETA: 4s - loss: 0.0314 - accuracy: 0.99 - ETA: 4s - loss: 0.0320 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.99 - ETA: 4s - loss: 0.0303 - accuracy: 0.99 - ETA: 4s - loss: 0.0295 - accuracy: 0.99 - ETA: 4s - loss: 0.0287 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0300 - accuracy: 0.99 - ETA: 3s - loss: 0.0313 - accuracy: 0.99 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 3s - loss: 0.0392 - accuracy: 0.98 - ETA: 3s - loss: 0.0383 - accuracy: 0.98 - ETA: 3s - loss: 0.0376 - accuracy: 0.98 - ETA: 3s - loss: 0.0369 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0355 - accuracy: 0.98 - ETA: 3s - loss: 0.0352 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0341 - accuracy: 0.98 - ETA: 2s - loss: 0.0336 - accuracy: 0.98 - ETA: 2s - loss: 0.0339 - accuracy: 0.98 - ETA: 2s - loss: 0.0347 - accuracy: 0.98 - ETA: 2s - loss: 0.0349 - accuracy: 0.98 - ETA: 2s - loss: 0.0353 - accuracy: 0.98 - ETA: 2s - loss: 0.0368 - accuracy: 0.98 - ETA: 2s - loss: 0.0373 - accuracy: 0.98 - ETA: 2s - loss: 0.0366 - accuracy: 0.98 - ETA: 2s - loss: 0.0364 - accuracy: 0.98 - ETA: 2s - loss: 0.0359 - accuracy: 0.98 - ETA: 1s - loss: 0.0361 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0362 - accuracy: 0.98 - ETA: 1s - loss: 0.0364 - accuracy: 0.98 - ETA: 1s - loss: 0.0368 - accuracy: 0.98 - ETA: 1s - loss: 0.0366 - accuracy: 0.98 - ETA: 1s - loss: 0.0369 - accuracy: 0.98 - ETA: 1s - loss: 0.0365 - accuracy: 0.98 - ETA: 1s - loss: 0.0360 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.98 - ETA: 1s - loss: 0.0356 - accuracy: 0.98 - ETA: 0s - loss: 0.0359 - accuracy: 0.98 - ETA: 0s - loss: 0.0353 - accuracy: 0.98 - ETA: 0s - loss: 0.0351 - accuracy: 0.98 - ETA: 0s - loss: 0.0346 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0335 - accuracy: 0.98 - ETA: 0s - loss: 0.0331 - accuracy: 0.98 - ETA: 0s - loss: 0.0332 - accuracy: 0.98 - ETA: 0s - loss: 0.0329 - accuracy: 0.98 - ETA: 0s - loss: 0.0330 - accuracy: 0.98 - ETA: 0s - loss: 0.0335 - accuracy: 0.98 - 6s 604us/step - loss: 0.0338 - accuracy: 0.9882 - val_loss: 0.0772 - val_accuracy: 0.9756\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00794\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.2300 - accuracy: 0.93 - ETA: 5s - loss: 0.1510 - accuracy: 0.95 - ETA: 5s - loss: 0.1296 - accuracy: 0.96 - ETA: 5s - loss: 0.1033 - accuracy: 0.96 - ETA: 5s - loss: 0.0831 - accuracy: 0.97 - ETA: 5s - loss: 0.0698 - accuracy: 0.97 - ETA: 5s - loss: 0.0612 - accuracy: 0.98 - ETA: 5s - loss: 0.0610 - accuracy: 0.98 - ETA: 5s - loss: 0.0566 - accuracy: 0.98 - ETA: 5s - loss: 0.0575 - accuracy: 0.98 - ETA: 5s - loss: 0.0553 - accuracy: 0.98 - ETA: 5s - loss: 0.0514 - accuracy: 0.98 - ETA: 5s - loss: 0.0493 - accuracy: 0.98 - ETA: 5s - loss: 0.0474 - accuracy: 0.98 - ETA: 4s - loss: 0.0471 - accuracy: 0.98 - ETA: 4s - loss: 0.0455 - accuracy: 0.98 - ETA: 4s - loss: 0.0433 - accuracy: 0.98 - ETA: 4s - loss: 0.0412 - accuracy: 0.98 - ETA: 4s - loss: 0.0396 - accuracy: 0.98 - ETA: 4s - loss: 0.0384 - accuracy: 0.98 - ETA: 4s - loss: 0.0390 - accuracy: 0.98 - ETA: 4s - loss: 0.0391 - accuracy: 0.98 - ETA: 4s - loss: 0.0397 - accuracy: 0.98 - ETA: 4s - loss: 0.0385 - accuracy: 0.98 - ETA: 4s - loss: 0.0376 - accuracy: 0.98 - ETA: 4s - loss: 0.0373 - accuracy: 0.98 - ETA: 3s - loss: 0.0363 - accuracy: 0.98 - ETA: 3s - loss: 0.0359 - accuracy: 0.98 - ETA: 3s - loss: 0.0348 - accuracy: 0.98 - ETA: 3s - loss: 0.0343 - accuracy: 0.98 - ETA: 3s - loss: 0.0337 - accuracy: 0.98 - ETA: 3s - loss: 0.0328 - accuracy: 0.98 - ETA: 3s - loss: 0.0318 - accuracy: 0.98 - ETA: 3s - loss: 0.0311 - accuracy: 0.98 - ETA: 3s - loss: 0.0303 - accuracy: 0.98 - ETA: 3s - loss: 0.0297 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.98 - ETA: 3s - loss: 0.0301 - accuracy: 0.98 - ETA: 2s - loss: 0.0314 - accuracy: 0.98 - ETA: 2s - loss: 0.0315 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0330 - accuracy: 0.98 - ETA: 2s - loss: 0.0330 - accuracy: 0.98 - ETA: 2s - loss: 0.0327 - accuracy: 0.98 - ETA: 2s - loss: 0.0323 - accuracy: 0.98 - ETA: 2s - loss: 0.0324 - accuracy: 0.98 - ETA: 2s - loss: 0.0327 - accuracy: 0.98 - ETA: 2s - loss: 0.0322 - accuracy: 0.98 - ETA: 2s - loss: 0.0317 - accuracy: 0.98 - ETA: 2s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.99 - ETA: 1s - loss: 0.0303 - accuracy: 0.99 - ETA: 1s - loss: 0.0303 - accuracy: 0.99 - ETA: 1s - loss: 0.0299 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 1s - loss: 0.0295 - accuracy: 0.99 - ETA: 1s - loss: 0.0293 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0292 - accuracy: 0.99 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0295 - accuracy: 0.99 - ETA: 0s - loss: 0.0294 - accuracy: 0.99 - ETA: 0s - loss: 0.0298 - accuracy: 0.99 - ETA: 0s - loss: 0.0312 - accuracy: 0.99 - ETA: 0s - loss: 0.0316 - accuracy: 0.99 - ETA: 0s - loss: 0.0336 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0338 - accuracy: 0.98 - ETA: 0s - loss: 0.0342 - accuracy: 0.98 - ETA: 0s - loss: 0.0345 - accuracy: 0.98 - ETA: 0s - loss: 0.0341 - accuracy: 0.98 - 7s 616us/step - loss: 0.0340 - accuracy: 0.9892 - val_loss: 0.0226 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00794\n",
      "Epoch 00018: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 241us/step\n",
      "[2020-05-18 16:40:44 RAM77.9% 1.79GB] Val Score : [0.010637786796661072, 0.9962092638015747]\n",
      "[2020-05-18 16:40:44 RAM77.9% 1.79GB] ============================================================================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save the model history in a list after fitting so we can plot later \n",
    "model_history=[]\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold :\", i+1)\n",
    "    x_t, x_v, y_t, y_v = train_test_split(x_train, y_train, test_size=0.10, random_state = 2020)\n",
    "    model_history.append(fit_and_evaluate(x_t, x_v, y_t, y_v, epochs, batch_size))\n",
    "    print(\"=============\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot to see how models are performing \n",
    "\n",
    "plt.title('Accuracies vs Epochs ')\n",
    "plt.plot(model_history[0].history['accuracy'], label='Training Fold 1')\n",
    "plt.plot(model_history[1].history['accuracy'], label='Training Fold 2')\n",
    "plt.plot(model_history[2].history['accuracy'], label='Training Fold 3')\n",
    "plt.plot(model_history[3].history['accuracy'], label='Training Fold 4')\n",
    "plt.plot(model_history[4].history['accuracy'], label='Training Fold 5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Train Accuracy vs Val Accuracy')\n",
    "plt.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\n",
    "plt.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='black', linestyle = \"dashdot\")\n",
    "plt.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red', )\n",
    "plt.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='red', linestyle = \"dashdot\")\n",
    "plt.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green', )\n",
    "plt.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='green', linestyle = \"dashdot\")\n",
    "#plt.plot(model_history[3].history['accuracy'], label='Train Accuracy Fold 4', color='steelblue', )\n",
    "#plt.plot(model_history[3].history['val_accuracy'], label='Val Accuracy Fold 4', color='steelblue', linestyle = \"dashdot\")\n",
    "#plt.plot(model_history[4].history['accuracy'], label='Train Accuracy Fold 5', color='purple', )\n",
    "#plt.plot(model_history[4].history['val_accuracy'], label='Val Accuracy Fold 5', color='purple', linestyle = \"dashdot\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "#Load the model that was saved by ModelCheckpoint\n",
    "model = load_model('cnn_bee_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4518/4518 [==============================] - ETA: 13 - ETA: 1 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 229us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5031788966033395, 0.8820274472236633]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on data that  his never seen \n",
    "#model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels: \n",
    "predicted_classes= model.predict(x_test)\n",
    "\n",
    "predicted_classes=np.argmax(np.round(predicted_classes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_predictions = model.predict_classes(x_test, batch_size=128, verbose=0)\n",
    "rounded_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rounded_labels=np.argmax(y_test, axis=1)\n",
    "rounded_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:42:09 RAM77.0% 1.78GB] Confusion matrix, without normalization\n",
      "[2020-05-18 16:42:09 RAM77.0% 1.78GB] [[ 287  531]\n",
      " [   2 3698]]\n",
      "[2020-05-18 16:42:09 RAM77.0% 1.78GB] Normalized confusion matrix\n",
      "[2020-05-18 16:42:09 RAM77.0% 1.78GB] [[3.51e-01 6.49e-01]\n",
      " [5.41e-04 9.99e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:42:09 RAM77.0% 1.78GB] \n",
      "Clasification report MFCCs+ CNN:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "missing_queen       0.99      0.35      0.52       818\n",
      "       active       0.87      1.00      0.93      3700\n",
      "\n",
      "     accuracy                           0.88      4518\n",
      "    macro avg       0.93      0.68      0.73      4518\n",
      " weighted avg       0.90      0.88      0.86      4518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(rounded_labels, rounded_predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='MFCCs+ CNN Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "target_names=['missing_queen', 'active']\n",
    "print ('\\nClasification report MFCCs+ CNN:\\n', classification_report(rounded_labels, rounded_predictions, target_names=target_names ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: TTBOX+ SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_workingFolder_=path_workingFolder='C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_1_second'+str(block_size)+'sec'+os.sep  # path where to save a\n",
    "#X_ttbox =get_features_from_samples(path_workingFolder, sample_ids, 'TTBOX', 'NO', 0)\n",
    "\n",
    "def get_list_samples_name_TTBOX(path_audioSegments, extension='.mat'):\n",
    "    states=['active','missing queen','swarm' ]\n",
    "    X_ttbox=[]\n",
    "    labels=[]\n",
    "    Y=[]\n",
    "    sample_ids=[]\n",
    "    # Recupèrer tout les audios d'extention .wav\"\"\"\"\"\" glob.glob(path_audioSegments_folder+'*'+extension)\"\"\"\"\"\"\n",
    "    #list_mfcc=[os.path.basename(x) for x in glob.glob(path_audioSegments+'*'+extension)]\n",
    "    \n",
    "    for x in glob.glob(path_audioSegments+'*'+extension): \n",
    "        sample=x[63:]\n",
    "        sample_ids.append(sample)\n",
    "        l= read_HiveState_fromSampleName( sample, states)\n",
    "        labels.append(l)\n",
    "        m=scipy.io.loadmat(x)\n",
    "        X_ttbox.append(m['ttb_vec'])\n",
    "    \n",
    "    Y= labels2binary('active', labels)\n",
    "        \n",
    "    return X_ttbox,  labels , Y, sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_HiveState_fromSampleName' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0e40ab74f30b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath_save_audio\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'sec'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\ttb_mat'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_ttbox\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mlabels_ttbox\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY_ttbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_ids_ttbox\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_list_samples_name_TTBOX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_save_audio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-51401f4cf82d>\u001b[0m in \u001b[0;36mget_list_samples_name_TTBOX\u001b[1;34m(path_audioSegments, extension)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m63\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msample_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mread_HiveState_fromSampleName\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_HiveState_fromSampleName' is not defined"
     ]
    }
   ],
   "source": [
    "path_save_audio= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\ttb_mat'+os.sep \n",
    "X_ttbox,  labels_ttbox , Y_ttbox, sample_ids_ttbox= get_list_samples_name_TTBOX(path_save_audio, extension='.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'block_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-90b16c3d4d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpath_save_audio\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'sec'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\ttb_mat'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m  \u001b[1;31m# path where to save audio segments and labels files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_list_samples_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_audioSegments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX_ttbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msample_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Recupèrer tout les audios d'extention .mat\"\"\"\"\"\" glob.glob(path_audioSegments_folder+'*'+extension)\"\"\"\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'block_size' is not defined"
     ]
    }
   ],
   "source": [
    "path_save_audio= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\ttb_mat'+os.sep  # path where to save audio segments and labels files.\n",
    "def get_list_samples_name(path_audioSegments, extension='.mat'):\n",
    "    X_ttbox=[]\n",
    "    sample_ids=[]\n",
    "    # Recupèrer tout les audios d'extention .mat\"\"\"\"\"\" glob.glob(path_audioSegments_folder+'*'+extension)\"\"\"\"\"\"\n",
    "    list_ttbox=[os.path.basename(x) for x in glob.glob(path_audioSegments+'*'+extension)]\n",
    "    for x in glob.glob(path_audioSegments+'*'+extension):\n",
    "        \n",
    "        sample_ids.append(x[63:])\n",
    "        m=scipy.io.loadmat(x)\n",
    "        X_ttbox.append(m['ttb_vec'])\n",
    "    return X_ttbox, sample_ids, list_ttbox\n",
    "  \n",
    "X_ttbox, sample_ids_mat, list_ttbox=get_list_samples_name(path_save_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_save_audio= 'C:\\\\Users\\\\PC\\\\python\\\\Stage\\\\dataset_BeeNoBee_2_second'+str(block_size)+'sec'+'\\\\ttb_mat'+os.sep  # path\n",
    "len(path_save_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_GT_labels_fromFiles(path_workingFolder, sample_ids_mat, labels2read)\n",
    "Y_ttbox= labels2binary('active', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-19 00:03:13 RAM69.2% 0.32GB] Balancing training data:\n",
      "[2020-05-19 00:03:13 RAM69.2% 0.32GB] will randomly replicate samples from least represented class\n"
     ]
    }
   ],
   "source": [
    "y_ttbox, x_ttbox, sample_ids_concat= BalanceData_online(Y_ttbox, X_ttbox, sample_ids_ttbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18524, 164), (18524,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z= x_ttbox.shape\n",
    "x_ttbox=x_ttbox.reshape(x, y*z)\n",
    "x_ttbox.shape , y_ttbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14005, 164), (4518, 164), (14005,), (4518,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset \n",
    "x_train= x_ttbox[4519:]\n",
    "x_test=x_ttbox[: 4518]\n",
    "y_test=y_ttbox[:4518]\n",
    "y_train= y_ttbox[4519:]\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x_ttbox, y_ttbox, test_size=0.3, random_state = 2020, shuffle=False)\n",
    "#x_train.shape, x_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-19 00:09:39 RAM72.8% 0.34GB] \n",
      "\n",
      "[2020-05-19 00:09:39 RAM72.8% 0.34GB] Starting classification with SVM:\n",
      "[2020-05-19 00:09:39 RAM72.8% 0.34GB] \n",
      "\n",
      "[2020-05-19 00:09:39 RAM72.8% 0.34GB] classification Beehive State into : Active or Missing Queen\n"
     ]
    }
   ],
   "source": [
    "CLF, Test_GroundT, Train_GroundT, Test_Preds, Train_Preds, Test_Preds_Proba, Train_Preds_Proba = SVM_Classification_BeehiveSTATE(x_train, y_train , x_test, y_test, kerneloption='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-19 00:28:33 RAM74.6% 0.22GB] Accuracy:  0.013944223107569721\n",
      "[2020-05-19 00:28:33 RAM74.6% 0.22GB] Precision: 1.0\n",
      "[2020-05-19 00:28:33 RAM74.6% 0.22GB] Recall: 0.013944223107569721\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score( Test_GroundT, Test_Preds))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(Test_GroundT, Test_Preds))\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(Test_GroundT, Test_Preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:50:13 RAM77.6% 1.67GB] Confusion matrix, without normalization\n",
      "[2020-05-18 16:50:13 RAM77.6% 1.67GB] [[ 577  241]\n",
      " [1080 2620]]\n",
      "[2020-05-18 16:50:13 RAM77.6% 1.67GB] Normalized confusion matrix\n",
      "[2020-05-18 16:50:13 RAM77.6% 1.67GB] [[0.71 0.29]\n",
      " [0.29 0.71]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:50:13 RAM77.6% 1.67GB] \n",
      "Clasification report TTBOX + SVM:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "missing_queen       0.35      0.71      0.47       818\n",
      "       active       0.92      0.71      0.80      3700\n",
      "\n",
      "     accuracy                           0.71      4518\n",
      "    macro avg       0.63      0.71      0.63      4518\n",
      " weighted avg       0.81      0.71      0.74      4518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Test_GroundT, Test_Preds )\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='TTBOX + SVM Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "target_names=['missing_queen', 'active']\n",
    "print ('\\nClasification report TTBOX + SVM:\\n', classification_report(Test_GroundT, Test_Preds , target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: TTBOX+ CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17702, 1, 164, 1), (17702, 1))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ttbox= x_ttbox.reshape(-1, 1, 164, 1)\n",
    "Y_ttbox=y_ttbox.reshape(-1, 1)\n",
    "X_ttbox.shape, Y_ttbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "\n",
    "X_ttbox = np.array(X_ttbox.tolist())\n",
    "Y_ttbox = np.array(Y_ttbox.tolist())\n",
    "# Encode the classification labels\n",
    "te = LabelEncoder()\n",
    "y_ttbox = to_categorical(te.fit_transform(Y_ttbox)) \n",
    "\n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "#train_x, test_x, train_y, test_y = train_test_split(X_ttbox, y_ttbox, test_size=0.3, random_state = 2020, shuffle=False)\n",
    "#train_x.shape, test_x.shape, train_y.shape, test_y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13183, 1, 164, 1), (4518, 1, 164, 1), (13183, 2), (4518, 2))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset \n",
    "train_x= X_ttbox[4519:]\n",
    "test_x=X_ttbox[: 4518]\n",
    "test_y=y_ttbox[:4518]\n",
    "train_y= y_ttbox[4519:]\n",
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 1, 164, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 1, 164, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 82, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 82, 16)         2320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 1, 82, 16)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 41, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 41, 16)         784       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 1, 41, 16)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 21, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 1, 21, 16)         784       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1, 21, 16)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 11, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 11, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               45312     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 57,650\n",
      "Trainable params: 57,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#size=(1, 164,1 )\n",
    "size=(1, 164, 1)\n",
    "model1= deep_model(size)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping critiria\n",
    "pat=5 # this is the number of epocks with no improvment after which the training will stop \n",
    "\n",
    "early_stopping= EarlyStopping(monitor='val_loss', patience=pat, verbose=1 )\n",
    "\n",
    "# define the model check point callback -> this will keep saving the model as a physical file \n",
    "model_checkpoint= ModelCheckpoint('ttbox_deep_model.h5', verbose=1, save_best_only=True )\n",
    "\n",
    "# define a function to fit the model\n",
    "\n",
    "def fit_and_evaluate(train_x, val_x, train_y, val_y, EPOCHS=50, BATCH_SIZE=145 ):\n",
    "    model=None\n",
    "    model=deep_model((1, 164,1))\n",
    "    results= model.fit(train_x, train_y, epochs=EPOCHS, batch_size= BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], verbose=1, validation_split=0.1)\n",
    "    print(\"Val Score :\", model.evaluate(val_x, val_y))\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 15:55:11 RAM62.7% 0.51GB] Training on Fold : 1\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 36s - loss: 2.2729 - accuracy: 0.627 - ETA: 13s - loss: 3.5517 - accuracy: 0.554 - ETA: 9s - loss: 2.5540 - accuracy: 0.582 - ETA: 8s - loss: 2.2962 - accuracy: 0.57 - ETA: 6s - loss: 1.9104 - accuracy: 0.58 - ETA: 5s - loss: 1.6824 - accuracy: 0.59 - ETA: 5s - loss: 1.5147 - accuracy: 0.60 - ETA: 4s - loss: 1.3907 - accuracy: 0.61 - ETA: 4s - loss: 1.3433 - accuracy: 0.61 - ETA: 4s - loss: 1.3008 - accuracy: 0.62 - ETA: 4s - loss: 1.2622 - accuracy: 0.62 - ETA: 3s - loss: 1.1941 - accuracy: 0.63 - ETA: 3s - loss: 1.1371 - accuracy: 0.63 - ETA: 3s - loss: 1.0902 - accuracy: 0.64 - ETA: 3s - loss: 1.0506 - accuracy: 0.64 - ETA: 2s - loss: 1.0152 - accuracy: 0.65 - ETA: 2s - loss: 0.9876 - accuracy: 0.65 - ETA: 2s - loss: 0.9717 - accuracy: 0.65 - ETA: 2s - loss: 0.9459 - accuracy: 0.66 - ETA: 2s - loss: 0.9338 - accuracy: 0.66 - ETA: 2s - loss: 0.9096 - accuracy: 0.66 - ETA: 2s - loss: 0.8882 - accuracy: 0.67 - ETA: 1s - loss: 0.8714 - accuracy: 0.67 - ETA: 1s - loss: 0.8650 - accuracy: 0.67 - ETA: 1s - loss: 0.8567 - accuracy: 0.68 - ETA: 1s - loss: 0.8493 - accuracy: 0.68 - ETA: 1s - loss: 0.8377 - accuracy: 0.68 - ETA: 1s - loss: 0.8268 - accuracy: 0.68 - ETA: 1s - loss: 0.8161 - accuracy: 0.68 - ETA: 1s - loss: 0.8073 - accuracy: 0.68 - ETA: 1s - loss: 0.7970 - accuracy: 0.68 - ETA: 1s - loss: 0.7866 - accuracy: 0.69 - ETA: 0s - loss: 0.7781 - accuracy: 0.69 - ETA: 0s - loss: 0.7697 - accuracy: 0.69 - ETA: 0s - loss: 0.7633 - accuracy: 0.69 - ETA: 0s - loss: 0.7589 - accuracy: 0.69 - ETA: 0s - loss: 0.7519 - accuracy: 0.69 - ETA: 0s - loss: 0.7445 - accuracy: 0.69 - ETA: 0s - loss: 0.7433 - accuracy: 0.69 - ETA: 0s - loss: 0.7372 - accuracy: 0.69 - ETA: 0s - loss: 0.7347 - accuracy: 0.69 - ETA: 0s - loss: 0.7284 - accuracy: 0.70 - ETA: 0s - loss: 0.7248 - accuracy: 0.70 - 4s 373us/step - loss: 0.7222 - accuracy: 0.7038 - val_loss: 0.4798 - val_accuracy: 0.8071\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47985, saving model to ttbox_deep_model.h5\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4743 - accuracy: 0.80 - ETA: 2s - loss: 0.5122 - accuracy: 0.76 - ETA: 2s - loss: 0.5391 - accuracy: 0.75 - ETA: 2s - loss: 0.5418 - accuracy: 0.75 - ETA: 2s - loss: 0.5442 - accuracy: 0.75 - ETA: 2s - loss: 0.5428 - accuracy: 0.75 - ETA: 2s - loss: 0.5388 - accuracy: 0.75 - ETA: 2s - loss: 0.5397 - accuracy: 0.75 - ETA: 2s - loss: 0.5444 - accuracy: 0.75 - ETA: 2s - loss: 0.5373 - accuracy: 0.75 - ETA: 2s - loss: 0.5344 - accuracy: 0.75 - ETA: 2s - loss: 0.5319 - accuracy: 0.75 - ETA: 2s - loss: 0.5362 - accuracy: 0.75 - ETA: 2s - loss: 0.5376 - accuracy: 0.75 - ETA: 2s - loss: 0.5394 - accuracy: 0.75 - ETA: 2s - loss: 0.5358 - accuracy: 0.75 - ETA: 2s - loss: 0.5340 - accuracy: 0.75 - ETA: 1s - loss: 0.5376 - accuracy: 0.75 - ETA: 1s - loss: 0.5346 - accuracy: 0.75 - ETA: 1s - loss: 0.5306 - accuracy: 0.75 - ETA: 1s - loss: 0.5303 - accuracy: 0.75 - ETA: 1s - loss: 0.5304 - accuracy: 0.75 - ETA: 1s - loss: 0.5304 - accuracy: 0.75 - ETA: 1s - loss: 0.5279 - accuracy: 0.75 - ETA: 1s - loss: 0.5298 - accuracy: 0.75 - ETA: 1s - loss: 0.5297 - accuracy: 0.75 - ETA: 1s - loss: 0.5313 - accuracy: 0.75 - ETA: 1s - loss: 0.5315 - accuracy: 0.75 - ETA: 1s - loss: 0.5331 - accuracy: 0.75 - ETA: 1s - loss: 0.5327 - accuracy: 0.75 - ETA: 0s - loss: 0.5311 - accuracy: 0.75 - ETA: 0s - loss: 0.5293 - accuracy: 0.75 - ETA: 0s - loss: 0.5285 - accuracy: 0.75 - ETA: 0s - loss: 0.5275 - accuracy: 0.76 - ETA: 0s - loss: 0.5274 - accuracy: 0.76 - ETA: 0s - loss: 0.5269 - accuracy: 0.75 - ETA: 0s - loss: 0.5268 - accuracy: 0.76 - ETA: 0s - loss: 0.5273 - accuracy: 0.75 - ETA: 0s - loss: 0.5271 - accuracy: 0.75 - ETA: 0s - loss: 0.5264 - accuracy: 0.75 - ETA: 0s - loss: 0.5249 - accuracy: 0.76 - ETA: 0s - loss: 0.5249 - accuracy: 0.76 - ETA: 0s - loss: 0.5248 - accuracy: 0.76 - 3s 316us/step - loss: 0.5245 - accuracy: 0.7610 - val_loss: 0.4763 - val_accuracy: 0.8180\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47985 to 0.47625, saving model to ttbox_deep_model.h5\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5321 - accuracy: 0.75 - ETA: 2s - loss: 0.5330 - accuracy: 0.74 - ETA: 2s - loss: 0.5488 - accuracy: 0.72 - ETA: 2s - loss: 0.5442 - accuracy: 0.73 - ETA: 2s - loss: 0.5515 - accuracy: 0.73 - ETA: 2s - loss: 0.5532 - accuracy: 0.73 - ETA: 2s - loss: 0.5435 - accuracy: 0.74 - ETA: 2s - loss: 0.5341 - accuracy: 0.75 - ETA: 2s - loss: 0.5316 - accuracy: 0.75 - ETA: 2s - loss: 0.5258 - accuracy: 0.75 - ETA: 2s - loss: 0.5216 - accuracy: 0.76 - ETA: 2s - loss: 0.5198 - accuracy: 0.76 - ETA: 2s - loss: 0.5187 - accuracy: 0.76 - ETA: 2s - loss: 0.5128 - accuracy: 0.76 - ETA: 2s - loss: 0.5091 - accuracy: 0.76 - ETA: 1s - loss: 0.5120 - accuracy: 0.76 - ETA: 1s - loss: 0.5132 - accuracy: 0.76 - ETA: 1s - loss: 0.5112 - accuracy: 0.76 - ETA: 1s - loss: 0.5138 - accuracy: 0.76 - ETA: 1s - loss: 0.5121 - accuracy: 0.76 - ETA: 1s - loss: 0.5113 - accuracy: 0.76 - ETA: 1s - loss: 0.5132 - accuracy: 0.76 - ETA: 1s - loss: 0.5137 - accuracy: 0.76 - ETA: 1s - loss: 0.5131 - accuracy: 0.76 - ETA: 1s - loss: 0.5129 - accuracy: 0.76 - ETA: 1s - loss: 0.5120 - accuracy: 0.76 - ETA: 1s - loss: 0.5104 - accuracy: 0.76 - ETA: 1s - loss: 0.5106 - accuracy: 0.76 - ETA: 1s - loss: 0.5098 - accuracy: 0.77 - ETA: 0s - loss: 0.5097 - accuracy: 0.76 - ETA: 0s - loss: 0.5088 - accuracy: 0.76 - ETA: 0s - loss: 0.5083 - accuracy: 0.77 - ETA: 0s - loss: 0.5072 - accuracy: 0.77 - ETA: 0s - loss: 0.5070 - accuracy: 0.77 - ETA: 0s - loss: 0.5072 - accuracy: 0.77 - ETA: 0s - loss: 0.5060 - accuracy: 0.77 - ETA: 0s - loss: 0.5042 - accuracy: 0.77 - ETA: 0s - loss: 0.5051 - accuracy: 0.77 - ETA: 0s - loss: 0.5043 - accuracy: 0.77 - ETA: 0s - loss: 0.5056 - accuracy: 0.77 - ETA: 0s - loss: 0.5049 - accuracy: 0.77 - ETA: 0s - loss: 0.5046 - accuracy: 0.77 - 3s 318us/step - loss: 0.5044 - accuracy: 0.7735 - val_loss: 0.4682 - val_accuracy: 0.8062\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47625 to 0.46820, saving model to ttbox_deep_model.h5\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5596 - accuracy: 0.72 - ETA: 2s - loss: 0.5420 - accuracy: 0.74 - ETA: 2s - loss: 0.5236 - accuracy: 0.75 - ETA: 2s - loss: 0.5161 - accuracy: 0.75 - ETA: 3s - loss: 0.5011 - accuracy: 0.76 - ETA: 3s - loss: 0.4975 - accuracy: 0.76 - ETA: 2s - loss: 0.4961 - accuracy: 0.76 - ETA: 2s - loss: 0.5015 - accuracy: 0.75 - ETA: 2s - loss: 0.5043 - accuracy: 0.75 - ETA: 2s - loss: 0.4974 - accuracy: 0.75 - ETA: 2s - loss: 0.4989 - accuracy: 0.75 - ETA: 2s - loss: 0.4958 - accuracy: 0.76 - ETA: 2s - loss: 0.4927 - accuracy: 0.76 - ETA: 2s - loss: 0.4932 - accuracy: 0.76 - ETA: 2s - loss: 0.4907 - accuracy: 0.76 - ETA: 1s - loss: 0.4898 - accuracy: 0.77 - ETA: 1s - loss: 0.4891 - accuracy: 0.77 - ETA: 1s - loss: 0.4878 - accuracy: 0.77 - ETA: 1s - loss: 0.4884 - accuracy: 0.77 - ETA: 1s - loss: 0.4880 - accuracy: 0.77 - ETA: 1s - loss: 0.4840 - accuracy: 0.77 - ETA: 1s - loss: 0.4838 - accuracy: 0.77 - ETA: 1s - loss: 0.4844 - accuracy: 0.77 - ETA: 1s - loss: 0.4859 - accuracy: 0.77 - ETA: 1s - loss: 0.4839 - accuracy: 0.77 - ETA: 1s - loss: 0.4824 - accuracy: 0.77 - ETA: 1s - loss: 0.4802 - accuracy: 0.77 - ETA: 1s - loss: 0.4789 - accuracy: 0.77 - ETA: 0s - loss: 0.4797 - accuracy: 0.77 - ETA: 0s - loss: 0.4807 - accuracy: 0.77 - ETA: 0s - loss: 0.4797 - accuracy: 0.77 - ETA: 0s - loss: 0.4797 - accuracy: 0.77 - ETA: 0s - loss: 0.4780 - accuracy: 0.78 - ETA: 0s - loss: 0.4773 - accuracy: 0.78 - ETA: 0s - loss: 0.4768 - accuracy: 0.78 - ETA: 0s - loss: 0.4765 - accuracy: 0.78 - ETA: 0s - loss: 0.4764 - accuracy: 0.78 - ETA: 0s - loss: 0.4775 - accuracy: 0.78 - ETA: 0s - loss: 0.4760 - accuracy: 0.78 - ETA: 0s - loss: 0.4763 - accuracy: 0.78 - ETA: 0s - loss: 0.4759 - accuracy: 0.78 - 3s 316us/step - loss: 0.4766 - accuracy: 0.7818 - val_loss: 0.4420 - val_accuracy: 0.8054\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46820 to 0.44197, saving model to ttbox_deep_model.h5\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4546 - accuracy: 0.77 - ETA: 2s - loss: 0.4802 - accuracy: 0.76 - ETA: 3s - loss: 0.4939 - accuracy: 0.75 - ETA: 3s - loss: 0.4891 - accuracy: 0.76 - ETA: 3s - loss: 0.4727 - accuracy: 0.77 - ETA: 3s - loss: 0.4704 - accuracy: 0.77 - ETA: 3s - loss: 0.4716 - accuracy: 0.77 - ETA: 2s - loss: 0.4719 - accuracy: 0.78 - ETA: 2s - loss: 0.4687 - accuracy: 0.78 - ETA: 2s - loss: 0.4672 - accuracy: 0.78 - ETA: 2s - loss: 0.4673 - accuracy: 0.78 - ETA: 2s - loss: 0.4696 - accuracy: 0.78 - ETA: 2s - loss: 0.4722 - accuracy: 0.78 - ETA: 2s - loss: 0.4674 - accuracy: 0.78 - ETA: 2s - loss: 0.4668 - accuracy: 0.78 - ETA: 2s - loss: 0.4651 - accuracy: 0.78 - ETA: 1s - loss: 0.4624 - accuracy: 0.79 - ETA: 1s - loss: 0.4624 - accuracy: 0.78 - ETA: 1s - loss: 0.4612 - accuracy: 0.79 - ETA: 1s - loss: 0.4628 - accuracy: 0.78 - ETA: 1s - loss: 0.4604 - accuracy: 0.78 - ETA: 1s - loss: 0.4626 - accuracy: 0.78 - ETA: 1s - loss: 0.4624 - accuracy: 0.78 - ETA: 1s - loss: 0.4596 - accuracy: 0.78 - ETA: 1s - loss: 0.4602 - accuracy: 0.78 - ETA: 1s - loss: 0.4605 - accuracy: 0.78 - ETA: 1s - loss: 0.4601 - accuracy: 0.78 - ETA: 1s - loss: 0.4601 - accuracy: 0.78 - ETA: 1s - loss: 0.4608 - accuracy: 0.78 - ETA: 1s - loss: 0.4615 - accuracy: 0.78 - ETA: 0s - loss: 0.4606 - accuracy: 0.78 - ETA: 0s - loss: 0.4600 - accuracy: 0.78 - ETA: 0s - loss: 0.4626 - accuracy: 0.78 - ETA: 0s - loss: 0.4635 - accuracy: 0.78 - ETA: 0s - loss: 0.4651 - accuracy: 0.78 - ETA: 0s - loss: 0.4647 - accuracy: 0.78 - ETA: 0s - loss: 0.4654 - accuracy: 0.78 - ETA: 0s - loss: 0.4655 - accuracy: 0.78 - ETA: 0s - loss: 0.4642 - accuracy: 0.78 - ETA: 0s - loss: 0.4629 - accuracy: 0.78 - ETA: 0s - loss: 0.4644 - accuracy: 0.78 - ETA: 0s - loss: 0.4653 - accuracy: 0.78 - 3s 320us/step - loss: 0.4645 - accuracy: 0.7866 - val_loss: 0.4050 - val_accuracy: 0.8357\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44197 to 0.40495, saving model to ttbox_deep_model.h5\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4069 - accuracy: 0.83 - ETA: 3s - loss: 0.4385 - accuracy: 0.80 - ETA: 3s - loss: 0.4385 - accuracy: 0.79 - ETA: 3s - loss: 0.4274 - accuracy: 0.80 - ETA: 3s - loss: 0.4252 - accuracy: 0.81 - ETA: 3s - loss: 0.4221 - accuracy: 0.81 - ETA: 2s - loss: 0.4314 - accuracy: 0.80 - ETA: 2s - loss: 0.4330 - accuracy: 0.80 - ETA: 2s - loss: 0.4334 - accuracy: 0.80 - ETA: 2s - loss: 0.4283 - accuracy: 0.80 - ETA: 2s - loss: 0.4242 - accuracy: 0.81 - ETA: 2s - loss: 0.4247 - accuracy: 0.81 - ETA: 2s - loss: 0.4282 - accuracy: 0.80 - ETA: 2s - loss: 0.4299 - accuracy: 0.80 - ETA: 2s - loss: 0.4329 - accuracy: 0.80 - ETA: 2s - loss: 0.4329 - accuracy: 0.80 - ETA: 2s - loss: 0.4320 - accuracy: 0.80 - ETA: 2s - loss: 0.4327 - accuracy: 0.80 - ETA: 1s - loss: 0.4359 - accuracy: 0.80 - ETA: 1s - loss: 0.4369 - accuracy: 0.80 - ETA: 1s - loss: 0.4391 - accuracy: 0.79 - ETA: 1s - loss: 0.4378 - accuracy: 0.80 - ETA: 1s - loss: 0.4389 - accuracy: 0.80 - ETA: 1s - loss: 0.4396 - accuracy: 0.79 - ETA: 1s - loss: 0.4398 - accuracy: 0.79 - ETA: 1s - loss: 0.4399 - accuracy: 0.79 - ETA: 1s - loss: 0.4379 - accuracy: 0.80 - ETA: 1s - loss: 0.4387 - accuracy: 0.80 - ETA: 1s - loss: 0.4401 - accuracy: 0.80 - ETA: 1s - loss: 0.4410 - accuracy: 0.80 - ETA: 1s - loss: 0.4414 - accuracy: 0.80 - ETA: 0s - loss: 0.4408 - accuracy: 0.80 - ETA: 0s - loss: 0.4429 - accuracy: 0.80 - ETA: 0s - loss: 0.4426 - accuracy: 0.80 - ETA: 0s - loss: 0.4448 - accuracy: 0.79 - ETA: 0s - loss: 0.4459 - accuracy: 0.79 - ETA: 0s - loss: 0.4465 - accuracy: 0.79 - ETA: 0s - loss: 0.4479 - accuracy: 0.79 - ETA: 0s - loss: 0.4463 - accuracy: 0.79 - ETA: 0s - loss: 0.4466 - accuracy: 0.79 - ETA: 0s - loss: 0.4452 - accuracy: 0.79 - ETA: 0s - loss: 0.4460 - accuracy: 0.79 - ETA: 0s - loss: 0.4463 - accuracy: 0.79 - ETA: 0s - loss: 0.4448 - accuracy: 0.79 - 3s 325us/step - loss: 0.4446 - accuracy: 0.7984 - val_loss: 0.3844 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.40495 to 0.38442, saving model to ttbox_deep_model.h5\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5548 - accuracy: 0.74 - ETA: 2s - loss: 0.4963 - accuracy: 0.77 - ETA: 2s - loss: 0.4947 - accuracy: 0.77 - ETA: 2s - loss: 0.4748 - accuracy: 0.77 - ETA: 2s - loss: 0.4733 - accuracy: 0.77 - ETA: 2s - loss: 0.4624 - accuracy: 0.78 - ETA: 2s - loss: 0.4625 - accuracy: 0.78 - ETA: 2s - loss: 0.4638 - accuracy: 0.77 - ETA: 2s - loss: 0.4602 - accuracy: 0.78 - ETA: 2s - loss: 0.4560 - accuracy: 0.78 - ETA: 2s - loss: 0.4531 - accuracy: 0.78 - ETA: 2s - loss: 0.4479 - accuracy: 0.78 - ETA: 2s - loss: 0.4460 - accuracy: 0.78 - ETA: 2s - loss: 0.4450 - accuracy: 0.78 - ETA: 2s - loss: 0.4484 - accuracy: 0.78 - ETA: 1s - loss: 0.4444 - accuracy: 0.79 - ETA: 1s - loss: 0.4462 - accuracy: 0.79 - ETA: 1s - loss: 0.4451 - accuracy: 0.79 - ETA: 1s - loss: 0.4441 - accuracy: 0.79 - ETA: 1s - loss: 0.4435 - accuracy: 0.79 - ETA: 1s - loss: 0.4436 - accuracy: 0.79 - ETA: 1s - loss: 0.4428 - accuracy: 0.79 - ETA: 1s - loss: 0.4403 - accuracy: 0.79 - ETA: 1s - loss: 0.4388 - accuracy: 0.79 - ETA: 1s - loss: 0.4384 - accuracy: 0.79 - ETA: 1s - loss: 0.4368 - accuracy: 0.79 - ETA: 1s - loss: 0.4363 - accuracy: 0.79 - ETA: 0s - loss: 0.4360 - accuracy: 0.79 - ETA: 0s - loss: 0.4332 - accuracy: 0.79 - ETA: 0s - loss: 0.4342 - accuracy: 0.79 - ETA: 0s - loss: 0.4335 - accuracy: 0.79 - ETA: 0s - loss: 0.4343 - accuracy: 0.79 - ETA: 0s - loss: 0.4356 - accuracy: 0.79 - ETA: 0s - loss: 0.4360 - accuracy: 0.79 - ETA: 0s - loss: 0.4352 - accuracy: 0.79 - ETA: 0s - loss: 0.4348 - accuracy: 0.79 - ETA: 0s - loss: 0.4327 - accuracy: 0.79 - ETA: 0s - loss: 0.4314 - accuracy: 0.79 - ETA: 0s - loss: 0.4305 - accuracy: 0.79 - 3s 308us/step - loss: 0.4299 - accuracy: 0.7996 - val_loss: 0.3937 - val_accuracy: 0.8189\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38442\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4047 - accuracy: 0.80 - ETA: 2s - loss: 0.4294 - accuracy: 0.80 - ETA: 2s - loss: 0.4318 - accuracy: 0.80 - ETA: 2s - loss: 0.4285 - accuracy: 0.81 - ETA: 2s - loss: 0.4285 - accuracy: 0.81 - ETA: 2s - loss: 0.4187 - accuracy: 0.81 - ETA: 2s - loss: 0.4225 - accuracy: 0.81 - ETA: 2s - loss: 0.4259 - accuracy: 0.80 - ETA: 2s - loss: 0.4299 - accuracy: 0.80 - ETA: 2s - loss: 0.4249 - accuracy: 0.80 - ETA: 2s - loss: 0.4271 - accuracy: 0.80 - ETA: 2s - loss: 0.4263 - accuracy: 0.80 - ETA: 2s - loss: 0.4296 - accuracy: 0.80 - ETA: 2s - loss: 0.4287 - accuracy: 0.80 - ETA: 2s - loss: 0.4243 - accuracy: 0.81 - ETA: 1s - loss: 0.4272 - accuracy: 0.81 - ETA: 1s - loss: 0.4271 - accuracy: 0.81 - ETA: 1s - loss: 0.4249 - accuracy: 0.81 - ETA: 1s - loss: 0.4293 - accuracy: 0.80 - ETA: 1s - loss: 0.4289 - accuracy: 0.80 - ETA: 1s - loss: 0.4303 - accuracy: 0.80 - ETA: 1s - loss: 0.4289 - accuracy: 0.80 - ETA: 1s - loss: 0.4272 - accuracy: 0.81 - ETA: 1s - loss: 0.4291 - accuracy: 0.81 - ETA: 1s - loss: 0.4289 - accuracy: 0.80 - ETA: 1s - loss: 0.4286 - accuracy: 0.80 - ETA: 1s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4262 - accuracy: 0.81 - ETA: 0s - loss: 0.4259 - accuracy: 0.81 - ETA: 0s - loss: 0.4241 - accuracy: 0.81 - ETA: 0s - loss: 0.4248 - accuracy: 0.81 - ETA: 0s - loss: 0.4237 - accuracy: 0.81 - ETA: 0s - loss: 0.4218 - accuracy: 0.81 - ETA: 0s - loss: 0.4251 - accuracy: 0.81 - ETA: 0s - loss: 0.4253 - accuracy: 0.81 - ETA: 0s - loss: 0.4248 - accuracy: 0.80 - ETA: 0s - loss: 0.4244 - accuracy: 0.80 - ETA: 0s - loss: 0.4249 - accuracy: 0.80 - 3s 310us/step - loss: 0.4242 - accuracy: 0.8091 - val_loss: 0.3675 - val_accuracy: 0.8357\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.38442 to 0.36746, saving model to ttbox_deep_model.h5\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4596 - accuracy: 0.76 - ETA: 2s - loss: 0.4445 - accuracy: 0.79 - ETA: 2s - loss: 0.4250 - accuracy: 0.79 - ETA: 2s - loss: 0.4195 - accuracy: 0.79 - ETA: 2s - loss: 0.4215 - accuracy: 0.80 - ETA: 2s - loss: 0.4104 - accuracy: 0.81 - ETA: 2s - loss: 0.4233 - accuracy: 0.80 - ETA: 2s - loss: 0.4169 - accuracy: 0.81 - ETA: 2s - loss: 0.4191 - accuracy: 0.80 - ETA: 2s - loss: 0.4281 - accuracy: 0.80 - ETA: 2s - loss: 0.4288 - accuracy: 0.80 - ETA: 2s - loss: 0.4278 - accuracy: 0.80 - ETA: 2s - loss: 0.4270 - accuracy: 0.80 - ETA: 2s - loss: 0.4260 - accuracy: 0.80 - ETA: 2s - loss: 0.4267 - accuracy: 0.80 - ETA: 1s - loss: 0.4282 - accuracy: 0.80 - ETA: 1s - loss: 0.4261 - accuracy: 0.80 - ETA: 1s - loss: 0.4262 - accuracy: 0.80 - ETA: 1s - loss: 0.4241 - accuracy: 0.80 - ETA: 1s - loss: 0.4206 - accuracy: 0.80 - ETA: 1s - loss: 0.4189 - accuracy: 0.80 - ETA: 1s - loss: 0.4197 - accuracy: 0.80 - ETA: 1s - loss: 0.4172 - accuracy: 0.80 - ETA: 1s - loss: 0.4168 - accuracy: 0.80 - ETA: 1s - loss: 0.4164 - accuracy: 0.80 - ETA: 1s - loss: 0.4155 - accuracy: 0.81 - ETA: 1s - loss: 0.4129 - accuracy: 0.81 - ETA: 1s - loss: 0.4131 - accuracy: 0.81 - ETA: 1s - loss: 0.4109 - accuracy: 0.81 - ETA: 0s - loss: 0.4115 - accuracy: 0.81 - ETA: 0s - loss: 0.4125 - accuracy: 0.81 - ETA: 0s - loss: 0.4130 - accuracy: 0.81 - ETA: 0s - loss: 0.4148 - accuracy: 0.80 - ETA: 0s - loss: 0.4148 - accuracy: 0.80 - ETA: 0s - loss: 0.4144 - accuracy: 0.80 - ETA: 0s - loss: 0.4138 - accuracy: 0.81 - ETA: 0s - loss: 0.4153 - accuracy: 0.80 - ETA: 0s - loss: 0.4134 - accuracy: 0.80 - ETA: 0s - loss: 0.4131 - accuracy: 0.80 - ETA: 0s - loss: 0.4119 - accuracy: 0.80 - ETA: 0s - loss: 0.4129 - accuracy: 0.80 - ETA: 0s - loss: 0.4124 - accuracy: 0.80 - ETA: 0s - loss: 0.4129 - accuracy: 0.80 - 3s 325us/step - loss: 0.4128 - accuracy: 0.8092 - val_loss: 0.3660 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.36746 to 0.36597, saving model to ttbox_deep_model.h5\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3966 - accuracy: 0.83 - ETA: 2s - loss: 0.4100 - accuracy: 0.82 - ETA: 2s - loss: 0.4013 - accuracy: 0.83 - ETA: 2s - loss: 0.4165 - accuracy: 0.82 - ETA: 2s - loss: 0.4087 - accuracy: 0.81 - ETA: 2s - loss: 0.4027 - accuracy: 0.81 - ETA: 2s - loss: 0.4016 - accuracy: 0.82 - ETA: 2s - loss: 0.4028 - accuracy: 0.82 - ETA: 2s - loss: 0.4058 - accuracy: 0.81 - ETA: 2s - loss: 0.4040 - accuracy: 0.81 - ETA: 2s - loss: 0.4015 - accuracy: 0.81 - ETA: 2s - loss: 0.4041 - accuracy: 0.81 - ETA: 2s - loss: 0.4033 - accuracy: 0.81 - ETA: 2s - loss: 0.4019 - accuracy: 0.81 - ETA: 2s - loss: 0.4071 - accuracy: 0.81 - ETA: 2s - loss: 0.4069 - accuracy: 0.81 - ETA: 2s - loss: 0.4082 - accuracy: 0.81 - ETA: 1s - loss: 0.4079 - accuracy: 0.81 - ETA: 1s - loss: 0.4071 - accuracy: 0.81 - ETA: 1s - loss: 0.4036 - accuracy: 0.81 - ETA: 1s - loss: 0.4046 - accuracy: 0.81 - ETA: 1s - loss: 0.4040 - accuracy: 0.81 - ETA: 1s - loss: 0.4034 - accuracy: 0.81 - ETA: 1s - loss: 0.4026 - accuracy: 0.81 - ETA: 1s - loss: 0.4026 - accuracy: 0.81 - ETA: 1s - loss: 0.4025 - accuracy: 0.81 - ETA: 1s - loss: 0.4016 - accuracy: 0.81 - ETA: 1s - loss: 0.4014 - accuracy: 0.81 - ETA: 1s - loss: 0.4008 - accuracy: 0.81 - ETA: 1s - loss: 0.4013 - accuracy: 0.81 - ETA: 0s - loss: 0.4026 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.4012 - accuracy: 0.81 - ETA: 0s - loss: 0.4008 - accuracy: 0.81 - ETA: 0s - loss: 0.4002 - accuracy: 0.81 - ETA: 0s - loss: 0.4020 - accuracy: 0.81 - ETA: 0s - loss: 0.4018 - accuracy: 0.81 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - ETA: 0s - loss: 0.4044 - accuracy: 0.81 - 3s 326us/step - loss: 0.4050 - accuracy: 0.8160 - val_loss: 0.3776 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.36597\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3583 - accuracy: 0.84 - ETA: 2s - loss: 0.3881 - accuracy: 0.81 - ETA: 2s - loss: 0.4126 - accuracy: 0.81 - ETA: 2s - loss: 0.4056 - accuracy: 0.81 - ETA: 2s - loss: 0.4005 - accuracy: 0.82 - ETA: 2s - loss: 0.3983 - accuracy: 0.82 - ETA: 2s - loss: 0.4008 - accuracy: 0.82 - ETA: 2s - loss: 0.4060 - accuracy: 0.81 - ETA: 2s - loss: 0.4086 - accuracy: 0.81 - ETA: 2s - loss: 0.4050 - accuracy: 0.81 - ETA: 2s - loss: 0.4098 - accuracy: 0.81 - ETA: 2s - loss: 0.4071 - accuracy: 0.81 - ETA: 2s - loss: 0.4052 - accuracy: 0.81 - ETA: 2s - loss: 0.4039 - accuracy: 0.81 - ETA: 2s - loss: 0.4053 - accuracy: 0.81 - ETA: 2s - loss: 0.4046 - accuracy: 0.81 - ETA: 2s - loss: 0.4011 - accuracy: 0.81 - ETA: 1s - loss: 0.3972 - accuracy: 0.81 - ETA: 1s - loss: 0.3978 - accuracy: 0.81 - ETA: 1s - loss: 0.3981 - accuracy: 0.81 - ETA: 1s - loss: 0.3995 - accuracy: 0.81 - ETA: 1s - loss: 0.4025 - accuracy: 0.81 - ETA: 1s - loss: 0.4034 - accuracy: 0.81 - ETA: 1s - loss: 0.4017 - accuracy: 0.81 - ETA: 1s - loss: 0.4019 - accuracy: 0.81 - ETA: 1s - loss: 0.4004 - accuracy: 0.81 - ETA: 1s - loss: 0.4005 - accuracy: 0.81 - ETA: 1s - loss: 0.4004 - accuracy: 0.81 - ETA: 1s - loss: 0.3998 - accuracy: 0.81 - ETA: 1s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.3991 - accuracy: 0.81 - ETA: 0s - loss: 0.4024 - accuracy: 0.81 - ETA: 0s - loss: 0.4014 - accuracy: 0.81 - ETA: 0s - loss: 0.4026 - accuracy: 0.81 - ETA: 0s - loss: 0.4029 - accuracy: 0.81 - ETA: 0s - loss: 0.4004 - accuracy: 0.81 - ETA: 0s - loss: 0.4011 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4011 - accuracy: 0.81 - ETA: 0s - loss: 0.4010 - accuracy: 0.81 - ETA: 0s - loss: 0.4003 - accuracy: 0.81 - ETA: 0s - loss: 0.4010 - accuracy: 0.81 - ETA: 0s - loss: 0.4027 - accuracy: 0.81 - ETA: 0s - loss: 0.4034 - accuracy: 0.81 - 4s 328us/step - loss: 0.4032 - accuracy: 0.8157 - val_loss: 0.3635 - val_accuracy: 0.8357\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36597 to 0.36351, saving model to ttbox_deep_model.h5\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3964 - accuracy: 0.78 - ETA: 2s - loss: 0.3990 - accuracy: 0.80 - ETA: 2s - loss: 0.3936 - accuracy: 0.81 - ETA: 2s - loss: 0.3904 - accuracy: 0.81 - ETA: 2s - loss: 0.3849 - accuracy: 0.81 - ETA: 2s - loss: 0.3916 - accuracy: 0.81 - ETA: 2s - loss: 0.3897 - accuracy: 0.81 - ETA: 2s - loss: 0.3906 - accuracy: 0.81 - ETA: 2s - loss: 0.3902 - accuracy: 0.81 - ETA: 2s - loss: 0.3859 - accuracy: 0.82 - ETA: 2s - loss: 0.3887 - accuracy: 0.82 - ETA: 2s - loss: 0.3860 - accuracy: 0.82 - ETA: 2s - loss: 0.3800 - accuracy: 0.82 - ETA: 2s - loss: 0.3843 - accuracy: 0.82 - ETA: 2s - loss: 0.3791 - accuracy: 0.82 - ETA: 2s - loss: 0.3771 - accuracy: 0.82 - ETA: 1s - loss: 0.3807 - accuracy: 0.82 - ETA: 1s - loss: 0.3824 - accuracy: 0.82 - ETA: 1s - loss: 0.3848 - accuracy: 0.82 - ETA: 1s - loss: 0.3851 - accuracy: 0.82 - ETA: 1s - loss: 0.3868 - accuracy: 0.82 - ETA: 1s - loss: 0.3881 - accuracy: 0.82 - ETA: 1s - loss: 0.3896 - accuracy: 0.82 - ETA: 1s - loss: 0.3921 - accuracy: 0.82 - ETA: 1s - loss: 0.3912 - accuracy: 0.82 - ETA: 1s - loss: 0.3924 - accuracy: 0.82 - ETA: 1s - loss: 0.3903 - accuracy: 0.82 - ETA: 1s - loss: 0.3915 - accuracy: 0.82 - ETA: 1s - loss: 0.3929 - accuracy: 0.82 - ETA: 1s - loss: 0.3932 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - ETA: 0s - loss: 0.3906 - accuracy: 0.82 - ETA: 0s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - 4s 331us/step - loss: 0.3926 - accuracy: 0.8210 - val_loss: 0.3918 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.36351\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4828 - accuracy: 0.78 - ETA: 3s - loss: 0.4474 - accuracy: 0.78 - ETA: 2s - loss: 0.4255 - accuracy: 0.79 - ETA: 2s - loss: 0.3983 - accuracy: 0.80 - ETA: 2s - loss: 0.3905 - accuracy: 0.81 - ETA: 2s - loss: 0.3915 - accuracy: 0.81 - ETA: 2s - loss: 0.3918 - accuracy: 0.81 - ETA: 2s - loss: 0.4051 - accuracy: 0.80 - ETA: 2s - loss: 0.4025 - accuracy: 0.81 - ETA: 2s - loss: 0.4007 - accuracy: 0.81 - ETA: 2s - loss: 0.3938 - accuracy: 0.81 - ETA: 2s - loss: 0.3911 - accuracy: 0.81 - ETA: 2s - loss: 0.3975 - accuracy: 0.81 - ETA: 2s - loss: 0.3969 - accuracy: 0.81 - ETA: 2s - loss: 0.3942 - accuracy: 0.81 - ETA: 2s - loss: 0.3924 - accuracy: 0.81 - ETA: 1s - loss: 0.3955 - accuracy: 0.81 - ETA: 1s - loss: 0.3945 - accuracy: 0.81 - ETA: 1s - loss: 0.3929 - accuracy: 0.81 - ETA: 1s - loss: 0.3914 - accuracy: 0.81 - ETA: 1s - loss: 0.3907 - accuracy: 0.82 - ETA: 1s - loss: 0.3887 - accuracy: 0.82 - ETA: 1s - loss: 0.3878 - accuracy: 0.82 - ETA: 1s - loss: 0.3904 - accuracy: 0.81 - ETA: 1s - loss: 0.3899 - accuracy: 0.82 - ETA: 1s - loss: 0.3899 - accuracy: 0.82 - ETA: 1s - loss: 0.3888 - accuracy: 0.82 - ETA: 1s - loss: 0.3882 - accuracy: 0.82 - ETA: 1s - loss: 0.3887 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.82 - ETA: 0s - loss: 0.3868 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.81 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3881 - accuracy: 0.82 - ETA: 0s - loss: 0.3881 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3858 - accuracy: 0.82 - ETA: 0s - loss: 0.3859 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.82 - 3s 320us/step - loss: 0.3859 - accuracy: 0.8220 - val_loss: 0.3624 - val_accuracy: 0.8290\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.36351 to 0.36238, saving model to ttbox_deep_model.h5\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3976 - accuracy: 0.79 - ETA: 2s - loss: 0.3829 - accuracy: 0.81 - ETA: 2s - loss: 0.4076 - accuracy: 0.80 - ETA: 3s - loss: 0.4155 - accuracy: 0.80 - ETA: 3s - loss: 0.4125 - accuracy: 0.80 - ETA: 3s - loss: 0.4116 - accuracy: 0.80 - ETA: 2s - loss: 0.4119 - accuracy: 0.80 - ETA: 2s - loss: 0.4062 - accuracy: 0.80 - ETA: 2s - loss: 0.3988 - accuracy: 0.81 - ETA: 2s - loss: 0.3965 - accuracy: 0.81 - ETA: 2s - loss: 0.3969 - accuracy: 0.81 - ETA: 2s - loss: 0.3923 - accuracy: 0.81 - ETA: 2s - loss: 0.3973 - accuracy: 0.81 - ETA: 2s - loss: 0.3943 - accuracy: 0.81 - ETA: 2s - loss: 0.3948 - accuracy: 0.81 - ETA: 2s - loss: 0.3933 - accuracy: 0.81 - ETA: 2s - loss: 0.3905 - accuracy: 0.81 - ETA: 2s - loss: 0.3904 - accuracy: 0.81 - ETA: 1s - loss: 0.3863 - accuracy: 0.82 - ETA: 1s - loss: 0.3854 - accuracy: 0.82 - ETA: 1s - loss: 0.3868 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3862 - accuracy: 0.82 - ETA: 1s - loss: 0.3848 - accuracy: 0.82 - ETA: 1s - loss: 0.3853 - accuracy: 0.82 - ETA: 1s - loss: 0.3855 - accuracy: 0.82 - ETA: 1s - loss: 0.3865 - accuracy: 0.82 - ETA: 1s - loss: 0.3861 - accuracy: 0.82 - ETA: 1s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3886 - accuracy: 0.82 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3876 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3862 - accuracy: 0.82 - ETA: 0s - loss: 0.3865 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - 3s 320us/step - loss: 0.3880 - accuracy: 0.8262 - val_loss: 0.3701 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36238\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4030 - accuracy: 0.84 - ETA: 3s - loss: 0.3911 - accuracy: 0.82 - ETA: 3s - loss: 0.3916 - accuracy: 0.82 - ETA: 3s - loss: 0.4008 - accuracy: 0.82 - ETA: 3s - loss: 0.4098 - accuracy: 0.81 - ETA: 2s - loss: 0.3996 - accuracy: 0.82 - ETA: 2s - loss: 0.4041 - accuracy: 0.81 - ETA: 2s - loss: 0.3959 - accuracy: 0.82 - ETA: 2s - loss: 0.3933 - accuracy: 0.82 - ETA: 2s - loss: 0.3932 - accuracy: 0.82 - ETA: 2s - loss: 0.3867 - accuracy: 0.82 - ETA: 2s - loss: 0.3849 - accuracy: 0.82 - ETA: 2s - loss: 0.3792 - accuracy: 0.82 - ETA: 2s - loss: 0.3766 - accuracy: 0.82 - ETA: 2s - loss: 0.3737 - accuracy: 0.83 - ETA: 2s - loss: 0.3758 - accuracy: 0.83 - ETA: 2s - loss: 0.3754 - accuracy: 0.83 - ETA: 1s - loss: 0.3753 - accuracy: 0.83 - ETA: 1s - loss: 0.3751 - accuracy: 0.83 - ETA: 1s - loss: 0.3774 - accuracy: 0.82 - ETA: 1s - loss: 0.3772 - accuracy: 0.82 - ETA: 1s - loss: 0.3760 - accuracy: 0.82 - ETA: 1s - loss: 0.3736 - accuracy: 0.83 - ETA: 1s - loss: 0.3744 - accuracy: 0.83 - ETA: 1s - loss: 0.3739 - accuracy: 0.82 - ETA: 1s - loss: 0.3736 - accuracy: 0.83 - ETA: 1s - loss: 0.3721 - accuracy: 0.83 - ETA: 1s - loss: 0.3723 - accuracy: 0.83 - ETA: 0s - loss: 0.3728 - accuracy: 0.83 - ETA: 0s - loss: 0.3729 - accuracy: 0.83 - ETA: 0s - loss: 0.3719 - accuracy: 0.83 - ETA: 0s - loss: 0.3710 - accuracy: 0.83 - ETA: 0s - loss: 0.3703 - accuracy: 0.83 - ETA: 0s - loss: 0.3728 - accuracy: 0.83 - ETA: 0s - loss: 0.3751 - accuracy: 0.82 - ETA: 0s - loss: 0.3763 - accuracy: 0.82 - ETA: 0s - loss: 0.3777 - accuracy: 0.82 - ETA: 0s - loss: 0.3764 - accuracy: 0.82 - ETA: 0s - loss: 0.3784 - accuracy: 0.82 - ETA: 0s - loss: 0.3781 - accuracy: 0.82 - ETA: 0s - loss: 0.3786 - accuracy: 0.82 - 3s 326us/step - loss: 0.3789 - accuracy: 0.8269 - val_loss: 0.3404 - val_accuracy: 0.8576\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.36238 to 0.34044, saving model to ttbox_deep_model.h5\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3600 - accuracy: 0.83 - ETA: 3s - loss: 0.3722 - accuracy: 0.82 - ETA: 3s - loss: 0.3671 - accuracy: 0.82 - ETA: 3s - loss: 0.3727 - accuracy: 0.81 - ETA: 3s - loss: 0.3818 - accuracy: 0.81 - ETA: 3s - loss: 0.3838 - accuracy: 0.81 - ETA: 2s - loss: 0.3799 - accuracy: 0.81 - ETA: 2s - loss: 0.3902 - accuracy: 0.80 - ETA: 2s - loss: 0.3912 - accuracy: 0.81 - ETA: 2s - loss: 0.3891 - accuracy: 0.81 - ETA: 2s - loss: 0.3911 - accuracy: 0.81 - ETA: 2s - loss: 0.3892 - accuracy: 0.81 - ETA: 2s - loss: 0.3870 - accuracy: 0.81 - ETA: 2s - loss: 0.3844 - accuracy: 0.82 - ETA: 2s - loss: 0.3826 - accuracy: 0.82 - ETA: 2s - loss: 0.3795 - accuracy: 0.82 - ETA: 1s - loss: 0.3828 - accuracy: 0.82 - ETA: 1s - loss: 0.3820 - accuracy: 0.82 - ETA: 1s - loss: 0.3828 - accuracy: 0.82 - ETA: 1s - loss: 0.3832 - accuracy: 0.82 - ETA: 1s - loss: 0.3823 - accuracy: 0.82 - ETA: 1s - loss: 0.3816 - accuracy: 0.82 - ETA: 1s - loss: 0.3802 - accuracy: 0.82 - ETA: 1s - loss: 0.3783 - accuracy: 0.82 - ETA: 1s - loss: 0.3759 - accuracy: 0.82 - ETA: 1s - loss: 0.3772 - accuracy: 0.82 - ETA: 1s - loss: 0.3759 - accuracy: 0.82 - ETA: 1s - loss: 0.3758 - accuracy: 0.82 - ETA: 1s - loss: 0.3750 - accuracy: 0.82 - ETA: 1s - loss: 0.3750 - accuracy: 0.82 - ETA: 0s - loss: 0.3747 - accuracy: 0.82 - ETA: 0s - loss: 0.3753 - accuracy: 0.82 - ETA: 0s - loss: 0.3743 - accuracy: 0.82 - ETA: 0s - loss: 0.3756 - accuracy: 0.82 - ETA: 0s - loss: 0.3742 - accuracy: 0.82 - ETA: 0s - loss: 0.3759 - accuracy: 0.82 - ETA: 0s - loss: 0.3787 - accuracy: 0.82 - ETA: 0s - loss: 0.3790 - accuracy: 0.82 - ETA: 0s - loss: 0.3790 - accuracy: 0.82 - ETA: 0s - loss: 0.3774 - accuracy: 0.82 - ETA: 0s - loss: 0.3762 - accuracy: 0.82 - ETA: 0s - loss: 0.3766 - accuracy: 0.82 - ETA: 0s - loss: 0.3779 - accuracy: 0.82 - ETA: 0s - loss: 0.3789 - accuracy: 0.82 - 4s 330us/step - loss: 0.3790 - accuracy: 0.8273 - val_loss: 0.3489 - val_accuracy: 0.8399\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34044\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3572 - accuracy: 0.83 - ETA: 3s - loss: 0.3561 - accuracy: 0.83 - ETA: 3s - loss: 0.3536 - accuracy: 0.84 - ETA: 2s - loss: 0.3605 - accuracy: 0.84 - ETA: 2s - loss: 0.3595 - accuracy: 0.84 - ETA: 2s - loss: 0.3678 - accuracy: 0.84 - ETA: 2s - loss: 0.3626 - accuracy: 0.84 - ETA: 2s - loss: 0.3742 - accuracy: 0.83 - ETA: 2s - loss: 0.3745 - accuracy: 0.83 - ETA: 2s - loss: 0.3691 - accuracy: 0.83 - ETA: 2s - loss: 0.3670 - accuracy: 0.83 - ETA: 2s - loss: 0.3755 - accuracy: 0.82 - ETA: 2s - loss: 0.3733 - accuracy: 0.83 - ETA: 2s - loss: 0.3711 - accuracy: 0.83 - ETA: 2s - loss: 0.3770 - accuracy: 0.83 - ETA: 2s - loss: 0.3769 - accuracy: 0.83 - ETA: 1s - loss: 0.3770 - accuracy: 0.82 - ETA: 1s - loss: 0.3774 - accuracy: 0.82 - ETA: 1s - loss: 0.3786 - accuracy: 0.82 - ETA: 1s - loss: 0.3749 - accuracy: 0.83 - ETA: 1s - loss: 0.3771 - accuracy: 0.82 - ETA: 1s - loss: 0.3766 - accuracy: 0.82 - ETA: 1s - loss: 0.3756 - accuracy: 0.83 - ETA: 1s - loss: 0.3734 - accuracy: 0.83 - ETA: 1s - loss: 0.3740 - accuracy: 0.83 - ETA: 1s - loss: 0.3748 - accuracy: 0.83 - ETA: 1s - loss: 0.3739 - accuracy: 0.83 - ETA: 1s - loss: 0.3766 - accuracy: 0.83 - ETA: 1s - loss: 0.3782 - accuracy: 0.82 - ETA: 0s - loss: 0.3779 - accuracy: 0.82 - ETA: 0s - loss: 0.3774 - accuracy: 0.82 - ETA: 0s - loss: 0.3759 - accuracy: 0.82 - ETA: 0s - loss: 0.3768 - accuracy: 0.82 - ETA: 0s - loss: 0.3758 - accuracy: 0.82 - ETA: 0s - loss: 0.3771 - accuracy: 0.82 - ETA: 0s - loss: 0.3762 - accuracy: 0.82 - ETA: 0s - loss: 0.3764 - accuracy: 0.82 - ETA: 0s - loss: 0.3765 - accuracy: 0.82 - ETA: 0s - loss: 0.3762 - accuracy: 0.82 - ETA: 0s - loss: 0.3750 - accuracy: 0.82 - ETA: 0s - loss: 0.3748 - accuracy: 0.82 - ETA: 0s - loss: 0.3745 - accuracy: 0.82 - 3s 323us/step - loss: 0.3737 - accuracy: 0.8294 - val_loss: 0.3351 - val_accuracy: 0.8526\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.34044 to 0.33505, saving model to ttbox_deep_model.h5\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4045 - accuracy: 0.82 - ETA: 2s - loss: 0.3927 - accuracy: 0.83 - ETA: 2s - loss: 0.3851 - accuracy: 0.83 - ETA: 2s - loss: 0.3907 - accuracy: 0.82 - ETA: 2s - loss: 0.3791 - accuracy: 0.82 - ETA: 2s - loss: 0.3752 - accuracy: 0.83 - ETA: 2s - loss: 0.3738 - accuracy: 0.83 - ETA: 2s - loss: 0.3702 - accuracy: 0.83 - ETA: 2s - loss: 0.3651 - accuracy: 0.84 - ETA: 2s - loss: 0.3700 - accuracy: 0.83 - ETA: 2s - loss: 0.3750 - accuracy: 0.83 - ETA: 2s - loss: 0.3774 - accuracy: 0.83 - ETA: 2s - loss: 0.3727 - accuracy: 0.83 - ETA: 2s - loss: 0.3720 - accuracy: 0.83 - ETA: 2s - loss: 0.3721 - accuracy: 0.83 - ETA: 2s - loss: 0.3731 - accuracy: 0.83 - ETA: 2s - loss: 0.3736 - accuracy: 0.83 - ETA: 1s - loss: 0.3718 - accuracy: 0.83 - ETA: 1s - loss: 0.3745 - accuracy: 0.83 - ETA: 1s - loss: 0.3724 - accuracy: 0.83 - ETA: 1s - loss: 0.3743 - accuracy: 0.83 - ETA: 1s - loss: 0.3759 - accuracy: 0.83 - ETA: 1s - loss: 0.3790 - accuracy: 0.82 - ETA: 1s - loss: 0.3776 - accuracy: 0.83 - ETA: 1s - loss: 0.3776 - accuracy: 0.82 - ETA: 1s - loss: 0.3768 - accuracy: 0.82 - ETA: 1s - loss: 0.3771 - accuracy: 0.82 - ETA: 1s - loss: 0.3757 - accuracy: 0.82 - ETA: 1s - loss: 0.3775 - accuracy: 0.82 - ETA: 1s - loss: 0.3779 - accuracy: 0.82 - ETA: 0s - loss: 0.3766 - accuracy: 0.82 - ETA: 0s - loss: 0.3750 - accuracy: 0.83 - ETA: 0s - loss: 0.3760 - accuracy: 0.82 - ETA: 0s - loss: 0.3760 - accuracy: 0.82 - ETA: 0s - loss: 0.3759 - accuracy: 0.82 - ETA: 0s - loss: 0.3762 - accuracy: 0.82 - ETA: 0s - loss: 0.3756 - accuracy: 0.82 - ETA: 0s - loss: 0.3777 - accuracy: 0.82 - ETA: 0s - loss: 0.3780 - accuracy: 0.82 - ETA: 0s - loss: 0.3776 - accuracy: 0.82 - ETA: 0s - loss: 0.3781 - accuracy: 0.82 - ETA: 0s - loss: 0.3769 - accuracy: 0.82 - 3s 324us/step - loss: 0.3756 - accuracy: 0.8284 - val_loss: 0.3357 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.33505\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3390 - accuracy: 0.86 - ETA: 2s - loss: 0.3833 - accuracy: 0.81 - ETA: 3s - loss: 0.3808 - accuracy: 0.80 - ETA: 3s - loss: 0.3760 - accuracy: 0.81 - ETA: 3s - loss: 0.3712 - accuracy: 0.81 - ETA: 2s - loss: 0.3721 - accuracy: 0.82 - ETA: 2s - loss: 0.3762 - accuracy: 0.82 - ETA: 2s - loss: 0.3777 - accuracy: 0.82 - ETA: 2s - loss: 0.3824 - accuracy: 0.82 - ETA: 2s - loss: 0.3778 - accuracy: 0.82 - ETA: 2s - loss: 0.3728 - accuracy: 0.82 - ETA: 2s - loss: 0.3728 - accuracy: 0.82 - ETA: 2s - loss: 0.3711 - accuracy: 0.82 - ETA: 2s - loss: 0.3746 - accuracy: 0.82 - ETA: 2s - loss: 0.3749 - accuracy: 0.82 - ETA: 2s - loss: 0.3725 - accuracy: 0.83 - ETA: 2s - loss: 0.3742 - accuracy: 0.82 - ETA: 2s - loss: 0.3741 - accuracy: 0.82 - ETA: 2s - loss: 0.3731 - accuracy: 0.83 - ETA: 1s - loss: 0.3685 - accuracy: 0.83 - ETA: 1s - loss: 0.3715 - accuracy: 0.83 - ETA: 1s - loss: 0.3743 - accuracy: 0.83 - ETA: 1s - loss: 0.3751 - accuracy: 0.83 - ETA: 1s - loss: 0.3739 - accuracy: 0.83 - ETA: 1s - loss: 0.3731 - accuracy: 0.83 - ETA: 1s - loss: 0.3728 - accuracy: 0.83 - ETA: 1s - loss: 0.3728 - accuracy: 0.83 - ETA: 1s - loss: 0.3737 - accuracy: 0.82 - ETA: 1s - loss: 0.3748 - accuracy: 0.82 - ETA: 1s - loss: 0.3734 - accuracy: 0.83 - ETA: 1s - loss: 0.3761 - accuracy: 0.83 - ETA: 0s - loss: 0.3758 - accuracy: 0.83 - ETA: 0s - loss: 0.3752 - accuracy: 0.83 - ETA: 0s - loss: 0.3734 - accuracy: 0.83 - ETA: 0s - loss: 0.3726 - accuracy: 0.83 - ETA: 0s - loss: 0.3715 - accuracy: 0.83 - ETA: 0s - loss: 0.3711 - accuracy: 0.83 - ETA: 0s - loss: 0.3713 - accuracy: 0.83 - ETA: 0s - loss: 0.3708 - accuracy: 0.83 - ETA: 0s - loss: 0.3713 - accuracy: 0.83 - ETA: 0s - loss: 0.3703 - accuracy: 0.83 - ETA: 0s - loss: 0.3698 - accuracy: 0.83 - ETA: 0s - loss: 0.3693 - accuracy: 0.83 - ETA: 0s - loss: 0.3689 - accuracy: 0.83 - ETA: 0s - loss: 0.3683 - accuracy: 0.83 - ETA: 0s - loss: 0.3690 - accuracy: 0.83 - 4s 333us/step - loss: 0.3680 - accuracy: 0.8345 - val_loss: 0.3243 - val_accuracy: 0.8585\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.33505 to 0.32433, saving model to ttbox_deep_model.h5\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3147 - accuracy: 0.86 - ETA: 3s - loss: 0.3723 - accuracy: 0.82 - ETA: 3s - loss: 0.3899 - accuracy: 0.80 - ETA: 3s - loss: 0.3878 - accuracy: 0.80 - ETA: 3s - loss: 0.3836 - accuracy: 0.80 - ETA: 3s - loss: 0.3793 - accuracy: 0.81 - ETA: 3s - loss: 0.3766 - accuracy: 0.81 - ETA: 3s - loss: 0.3714 - accuracy: 0.82 - ETA: 3s - loss: 0.3584 - accuracy: 0.83 - ETA: 2s - loss: 0.3642 - accuracy: 0.82 - ETA: 2s - loss: 0.3679 - accuracy: 0.83 - ETA: 2s - loss: 0.3680 - accuracy: 0.82 - ETA: 2s - loss: 0.3617 - accuracy: 0.83 - ETA: 2s - loss: 0.3638 - accuracy: 0.83 - ETA: 2s - loss: 0.3623 - accuracy: 0.83 - ETA: 2s - loss: 0.3578 - accuracy: 0.83 - ETA: 2s - loss: 0.3559 - accuracy: 0.83 - ETA: 2s - loss: 0.3524 - accuracy: 0.83 - ETA: 2s - loss: 0.3549 - accuracy: 0.83 - ETA: 2s - loss: 0.3562 - accuracy: 0.83 - ETA: 2s - loss: 0.3584 - accuracy: 0.83 - ETA: 1s - loss: 0.3581 - accuracy: 0.83 - ETA: 1s - loss: 0.3586 - accuracy: 0.83 - ETA: 1s - loss: 0.3583 - accuracy: 0.83 - ETA: 1s - loss: 0.3579 - accuracy: 0.83 - ETA: 1s - loss: 0.3580 - accuracy: 0.83 - ETA: 1s - loss: 0.3589 - accuracy: 0.83 - ETA: 1s - loss: 0.3590 - accuracy: 0.83 - ETA: 1s - loss: 0.3578 - accuracy: 0.83 - ETA: 1s - loss: 0.3621 - accuracy: 0.83 - ETA: 1s - loss: 0.3630 - accuracy: 0.83 - ETA: 1s - loss: 0.3603 - accuracy: 0.83 - ETA: 1s - loss: 0.3600 - accuracy: 0.83 - ETA: 1s - loss: 0.3596 - accuracy: 0.83 - ETA: 1s - loss: 0.3609 - accuracy: 0.83 - ETA: 1s - loss: 0.3601 - accuracy: 0.83 - ETA: 1s - loss: 0.3615 - accuracy: 0.83 - ETA: 1s - loss: 0.3629 - accuracy: 0.83 - ETA: 1s - loss: 0.3634 - accuracy: 0.83 - ETA: 0s - loss: 0.3627 - accuracy: 0.83 - ETA: 0s - loss: 0.3629 - accuracy: 0.83 - ETA: 0s - loss: 0.3614 - accuracy: 0.83 - ETA: 0s - loss: 0.3613 - accuracy: 0.83 - ETA: 0s - loss: 0.3630 - accuracy: 0.83 - ETA: 0s - loss: 0.3645 - accuracy: 0.83 - ETA: 0s - loss: 0.3635 - accuracy: 0.83 - ETA: 0s - loss: 0.3628 - accuracy: 0.83 - ETA: 0s - loss: 0.3619 - accuracy: 0.83 - ETA: 0s - loss: 0.3623 - accuracy: 0.83 - ETA: 0s - loss: 0.3610 - accuracy: 0.83 - ETA: 0s - loss: 0.3600 - accuracy: 0.83 - ETA: 0s - loss: 0.3607 - accuracy: 0.83 - ETA: 0s - loss: 0.3613 - accuracy: 0.83 - ETA: 0s - loss: 0.3609 - accuracy: 0.83 - ETA: 0s - loss: 0.3606 - accuracy: 0.83 - ETA: 0s - loss: 0.3607 - accuracy: 0.83 - 4s 356us/step - loss: 0.3606 - accuracy: 0.8347 - val_loss: 0.3889 - val_accuracy: 0.8290\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.32433\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3836 - accuracy: 0.83 - ETA: 4s - loss: 0.4150 - accuracy: 0.82 - ETA: 3s - loss: 0.4091 - accuracy: 0.81 - ETA: 3s - loss: 0.3916 - accuracy: 0.82 - ETA: 3s - loss: 0.3708 - accuracy: 0.84 - ETA: 3s - loss: 0.3599 - accuracy: 0.85 - ETA: 3s - loss: 0.3662 - accuracy: 0.84 - ETA: 3s - loss: 0.3618 - accuracy: 0.84 - ETA: 3s - loss: 0.3544 - accuracy: 0.84 - ETA: 3s - loss: 0.3540 - accuracy: 0.84 - ETA: 2s - loss: 0.3466 - accuracy: 0.84 - ETA: 2s - loss: 0.3563 - accuracy: 0.84 - ETA: 2s - loss: 0.3596 - accuracy: 0.84 - ETA: 2s - loss: 0.3567 - accuracy: 0.84 - ETA: 2s - loss: 0.3569 - accuracy: 0.84 - ETA: 2s - loss: 0.3572 - accuracy: 0.84 - ETA: 2s - loss: 0.3550 - accuracy: 0.84 - ETA: 2s - loss: 0.3510 - accuracy: 0.84 - ETA: 2s - loss: 0.3551 - accuracy: 0.84 - ETA: 2s - loss: 0.3542 - accuracy: 0.84 - ETA: 2s - loss: 0.3554 - accuracy: 0.84 - ETA: 2s - loss: 0.3556 - accuracy: 0.84 - ETA: 1s - loss: 0.3537 - accuracy: 0.84 - ETA: 1s - loss: 0.3528 - accuracy: 0.84 - ETA: 1s - loss: 0.3539 - accuracy: 0.84 - ETA: 1s - loss: 0.3544 - accuracy: 0.84 - ETA: 1s - loss: 0.3591 - accuracy: 0.84 - ETA: 1s - loss: 0.3581 - accuracy: 0.84 - ETA: 1s - loss: 0.3575 - accuracy: 0.84 - ETA: 1s - loss: 0.3579 - accuracy: 0.84 - ETA: 1s - loss: 0.3578 - accuracy: 0.84 - ETA: 1s - loss: 0.3568 - accuracy: 0.84 - ETA: 1s - loss: 0.3578 - accuracy: 0.83 - ETA: 1s - loss: 0.3610 - accuracy: 0.83 - ETA: 0s - loss: 0.3619 - accuracy: 0.83 - ETA: 0s - loss: 0.3611 - accuracy: 0.83 - ETA: 0s - loss: 0.3599 - accuracy: 0.83 - ETA: 0s - loss: 0.3595 - accuracy: 0.83 - ETA: 0s - loss: 0.3584 - accuracy: 0.83 - ETA: 0s - loss: 0.3583 - accuracy: 0.83 - ETA: 0s - loss: 0.3597 - accuracy: 0.83 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - ETA: 0s - loss: 0.3583 - accuracy: 0.83 - ETA: 0s - loss: 0.3566 - accuracy: 0.83 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - 4s 330us/step - loss: 0.3590 - accuracy: 0.8370 - val_loss: 0.3575 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.32433\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3216 - accuracy: 0.85 - ETA: 2s - loss: 0.3479 - accuracy: 0.83 - ETA: 2s - loss: 0.3471 - accuracy: 0.84 - ETA: 2s - loss: 0.3681 - accuracy: 0.83 - ETA: 2s - loss: 0.3662 - accuracy: 0.83 - ETA: 2s - loss: 0.3752 - accuracy: 0.83 - ETA: 2s - loss: 0.3681 - accuracy: 0.83 - ETA: 2s - loss: 0.3671 - accuracy: 0.83 - ETA: 2s - loss: 0.3727 - accuracy: 0.83 - ETA: 2s - loss: 0.3629 - accuracy: 0.83 - ETA: 2s - loss: 0.3660 - accuracy: 0.83 - ETA: 2s - loss: 0.3628 - accuracy: 0.83 - ETA: 2s - loss: 0.3663 - accuracy: 0.83 - ETA: 2s - loss: 0.3661 - accuracy: 0.83 - ETA: 1s - loss: 0.3610 - accuracy: 0.83 - ETA: 1s - loss: 0.3615 - accuracy: 0.83 - ETA: 1s - loss: 0.3602 - accuracy: 0.83 - ETA: 1s - loss: 0.3581 - accuracy: 0.83 - ETA: 1s - loss: 0.3637 - accuracy: 0.83 - ETA: 1s - loss: 0.3637 - accuracy: 0.83 - ETA: 1s - loss: 0.3647 - accuracy: 0.83 - ETA: 1s - loss: 0.3661 - accuracy: 0.83 - ETA: 1s - loss: 0.3660 - accuracy: 0.83 - ETA: 1s - loss: 0.3659 - accuracy: 0.83 - ETA: 1s - loss: 0.3644 - accuracy: 0.83 - ETA: 1s - loss: 0.3626 - accuracy: 0.83 - ETA: 1s - loss: 0.3611 - accuracy: 0.83 - ETA: 1s - loss: 0.3596 - accuracy: 0.83 - ETA: 0s - loss: 0.3604 - accuracy: 0.83 - ETA: 0s - loss: 0.3617 - accuracy: 0.83 - ETA: 0s - loss: 0.3596 - accuracy: 0.83 - ETA: 0s - loss: 0.3599 - accuracy: 0.83 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - ETA: 0s - loss: 0.3567 - accuracy: 0.83 - ETA: 0s - loss: 0.3556 - accuracy: 0.83 - ETA: 0s - loss: 0.3561 - accuracy: 0.83 - ETA: 0s - loss: 0.3570 - accuracy: 0.83 - ETA: 0s - loss: 0.3576 - accuracy: 0.83 - ETA: 0s - loss: 0.3558 - accuracy: 0.83 - 3s 312us/step - loss: 0.3552 - accuracy: 0.8375 - val_loss: 0.3484 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.32433\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3202 - accuracy: 0.87 - ETA: 2s - loss: 0.3464 - accuracy: 0.85 - ETA: 2s - loss: 0.3707 - accuracy: 0.83 - ETA: 2s - loss: 0.3662 - accuracy: 0.84 - ETA: 2s - loss: 0.3611 - accuracy: 0.84 - ETA: 2s - loss: 0.3758 - accuracy: 0.83 - ETA: 2s - loss: 0.3714 - accuracy: 0.83 - ETA: 2s - loss: 0.3654 - accuracy: 0.83 - ETA: 2s - loss: 0.3617 - accuracy: 0.84 - ETA: 2s - loss: 0.3607 - accuracy: 0.83 - ETA: 2s - loss: 0.3624 - accuracy: 0.83 - ETA: 2s - loss: 0.3637 - accuracy: 0.83 - ETA: 2s - loss: 0.3639 - accuracy: 0.83 - ETA: 2s - loss: 0.3612 - accuracy: 0.83 - ETA: 2s - loss: 0.3599 - accuracy: 0.83 - ETA: 1s - loss: 0.3626 - accuracy: 0.83 - ETA: 1s - loss: 0.3642 - accuracy: 0.83 - ETA: 1s - loss: 0.3666 - accuracy: 0.83 - ETA: 1s - loss: 0.3661 - accuracy: 0.83 - ETA: 1s - loss: 0.3635 - accuracy: 0.83 - ETA: 1s - loss: 0.3667 - accuracy: 0.83 - ETA: 1s - loss: 0.3682 - accuracy: 0.83 - ETA: 1s - loss: 0.3682 - accuracy: 0.83 - ETA: 1s - loss: 0.3671 - accuracy: 0.83 - ETA: 1s - loss: 0.3649 - accuracy: 0.83 - ETA: 1s - loss: 0.3641 - accuracy: 0.83 - ETA: 1s - loss: 0.3664 - accuracy: 0.83 - ETA: 0s - loss: 0.3659 - accuracy: 0.83 - ETA: 0s - loss: 0.3650 - accuracy: 0.83 - ETA: 0s - loss: 0.3648 - accuracy: 0.83 - ETA: 0s - loss: 0.3633 - accuracy: 0.83 - ETA: 0s - loss: 0.3637 - accuracy: 0.83 - ETA: 0s - loss: 0.3619 - accuracy: 0.83 - ETA: 0s - loss: 0.3633 - accuracy: 0.83 - ETA: 0s - loss: 0.3631 - accuracy: 0.83 - ETA: 0s - loss: 0.3627 - accuracy: 0.83 - ETA: 0s - loss: 0.3622 - accuracy: 0.83 - ETA: 0s - loss: 0.3605 - accuracy: 0.83 - ETA: 0s - loss: 0.3596 - accuracy: 0.83 - 3s 309us/step - loss: 0.3621 - accuracy: 0.8378 - val_loss: 0.3320 - val_accuracy: 0.8585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: val_loss did not improve from 0.32433\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.4170 - accuracy: 0.81 - ETA: 3s - loss: 0.3451 - accuracy: 0.84 - ETA: 3s - loss: 0.3399 - accuracy: 0.86 - ETA: 3s - loss: 0.3466 - accuracy: 0.85 - ETA: 3s - loss: 0.3506 - accuracy: 0.85 - ETA: 3s - loss: 0.3535 - accuracy: 0.85 - ETA: 3s - loss: 0.3571 - accuracy: 0.84 - ETA: 3s - loss: 0.3696 - accuracy: 0.83 - ETA: 2s - loss: 0.3733 - accuracy: 0.83 - ETA: 2s - loss: 0.3685 - accuracy: 0.83 - ETA: 2s - loss: 0.3593 - accuracy: 0.83 - ETA: 2s - loss: 0.3591 - accuracy: 0.83 - ETA: 2s - loss: 0.3555 - accuracy: 0.84 - ETA: 2s - loss: 0.3535 - accuracy: 0.84 - ETA: 2s - loss: 0.3555 - accuracy: 0.84 - ETA: 2s - loss: 0.3548 - accuracy: 0.84 - ETA: 2s - loss: 0.3581 - accuracy: 0.83 - ETA: 2s - loss: 0.3572 - accuracy: 0.83 - ETA: 2s - loss: 0.3559 - accuracy: 0.83 - ETA: 1s - loss: 0.3568 - accuracy: 0.83 - ETA: 1s - loss: 0.3558 - accuracy: 0.83 - ETA: 1s - loss: 0.3561 - accuracy: 0.83 - ETA: 1s - loss: 0.3562 - accuracy: 0.83 - ETA: 1s - loss: 0.3583 - accuracy: 0.83 - ETA: 1s - loss: 0.3575 - accuracy: 0.83 - ETA: 1s - loss: 0.3560 - accuracy: 0.83 - ETA: 1s - loss: 0.3544 - accuracy: 0.84 - ETA: 1s - loss: 0.3544 - accuracy: 0.84 - ETA: 1s - loss: 0.3556 - accuracy: 0.83 - ETA: 1s - loss: 0.3558 - accuracy: 0.83 - ETA: 1s - loss: 0.3533 - accuracy: 0.84 - ETA: 0s - loss: 0.3534 - accuracy: 0.84 - ETA: 0s - loss: 0.3507 - accuracy: 0.84 - ETA: 0s - loss: 0.3502 - accuracy: 0.84 - ETA: 0s - loss: 0.3504 - accuracy: 0.84 - ETA: 0s - loss: 0.3521 - accuracy: 0.84 - ETA: 0s - loss: 0.3516 - accuracy: 0.84 - ETA: 0s - loss: 0.3506 - accuracy: 0.84 - ETA: 0s - loss: 0.3505 - accuracy: 0.84 - ETA: 0s - loss: 0.3506 - accuracy: 0.84 - ETA: 0s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3516 - accuracy: 0.84 - ETA: 0s - loss: 0.3507 - accuracy: 0.84 - 3s 323us/step - loss: 0.3508 - accuracy: 0.8420 - val_loss: 0.3380 - val_accuracy: 0.8543\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.32433\n",
      "Epoch 00024: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 183us/step\n",
      "[2020-05-18 15:56:37 RAM63.4% 0.54GB] Val Score : [0.3393996849689997, 0.8415466547012329]\n",
      "[2020-05-18 15:56:37 RAM63.5% 0.54GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 15:56:37 RAM63.5% 0.54GB] Training on Fold : 2\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 33s - loss: 2.5418 - accuracy: 0.469 - ETA: 18s - loss: 2.8797 - accuracy: 0.527 - ETA: 13s - loss: 2.1855 - accuracy: 0.567 - ETA: 10s - loss: 1.8722 - accuracy: 0.570 - ETA: 7s - loss: 1.4999 - accuracy: 0.595 - ETA: 6s - loss: 1.3107 - accuracy: 0.58 - ETA: 5s - loss: 1.1923 - accuracy: 0.59 - ETA: 5s - loss: 1.1400 - accuracy: 0.60 - ETA: 4s - loss: 1.0626 - accuracy: 0.61 - ETA: 4s - loss: 1.0346 - accuracy: 0.61 - ETA: 4s - loss: 1.0131 - accuracy: 0.61 - ETA: 4s - loss: 0.9877 - accuracy: 0.61 - ETA: 3s - loss: 0.9580 - accuracy: 0.61 - ETA: 3s - loss: 0.9272 - accuracy: 0.62 - ETA: 3s - loss: 0.8994 - accuracy: 0.63 - ETA: 3s - loss: 0.8700 - accuracy: 0.64 - ETA: 2s - loss: 0.8498 - accuracy: 0.64 - ETA: 2s - loss: 0.8314 - accuracy: 0.65 - ETA: 2s - loss: 0.8172 - accuracy: 0.65 - ETA: 2s - loss: 0.8007 - accuracy: 0.66 - ETA: 2s - loss: 0.7945 - accuracy: 0.66 - ETA: 2s - loss: 0.7805 - accuracy: 0.66 - ETA: 2s - loss: 0.7685 - accuracy: 0.67 - ETA: 1s - loss: 0.7597 - accuracy: 0.67 - ETA: 1s - loss: 0.7559 - accuracy: 0.67 - ETA: 1s - loss: 0.7490 - accuracy: 0.67 - ETA: 1s - loss: 0.7409 - accuracy: 0.68 - ETA: 1s - loss: 0.7360 - accuracy: 0.68 - ETA: 1s - loss: 0.7294 - accuracy: 0.68 - ETA: 1s - loss: 0.7230 - accuracy: 0.68 - ETA: 1s - loss: 0.7161 - accuracy: 0.68 - ETA: 1s - loss: 0.7090 - accuracy: 0.68 - ETA: 0s - loss: 0.7031 - accuracy: 0.69 - ETA: 0s - loss: 0.6972 - accuracy: 0.69 - ETA: 0s - loss: 0.6908 - accuracy: 0.69 - ETA: 0s - loss: 0.6879 - accuracy: 0.69 - ETA: 0s - loss: 0.6833 - accuracy: 0.69 - ETA: 0s - loss: 0.6797 - accuracy: 0.69 - ETA: 0s - loss: 0.6773 - accuracy: 0.70 - ETA: 0s - loss: 0.6730 - accuracy: 0.70 - ETA: 0s - loss: 0.6681 - accuracy: 0.70 - ETA: 0s - loss: 0.6643 - accuracy: 0.70 - 4s 367us/step - loss: 0.6616 - accuracy: 0.7086 - val_loss: 0.4815 - val_accuracy: 0.7970\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.32433\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4799 - accuracy: 0.76 - ETA: 3s - loss: 0.5106 - accuracy: 0.76 - ETA: 3s - loss: 0.4996 - accuracy: 0.78 - ETA: 3s - loss: 0.5103 - accuracy: 0.77 - ETA: 3s - loss: 0.5173 - accuracy: 0.76 - ETA: 3s - loss: 0.5277 - accuracy: 0.75 - ETA: 3s - loss: 0.5368 - accuracy: 0.75 - ETA: 3s - loss: 0.5332 - accuracy: 0.75 - ETA: 3s - loss: 0.5335 - accuracy: 0.75 - ETA: 3s - loss: 0.5272 - accuracy: 0.76 - ETA: 3s - loss: 0.5216 - accuracy: 0.76 - ETA: 3s - loss: 0.5222 - accuracy: 0.76 - ETA: 3s - loss: 0.5227 - accuracy: 0.76 - ETA: 2s - loss: 0.5160 - accuracy: 0.76 - ETA: 2s - loss: 0.5177 - accuracy: 0.76 - ETA: 2s - loss: 0.5132 - accuracy: 0.77 - ETA: 2s - loss: 0.5164 - accuracy: 0.76 - ETA: 2s - loss: 0.5126 - accuracy: 0.76 - ETA: 2s - loss: 0.5138 - accuracy: 0.76 - ETA: 2s - loss: 0.5114 - accuracy: 0.76 - ETA: 2s - loss: 0.5136 - accuracy: 0.76 - ETA: 2s - loss: 0.5151 - accuracy: 0.76 - ETA: 2s - loss: 0.5145 - accuracy: 0.76 - ETA: 2s - loss: 0.5151 - accuracy: 0.76 - ETA: 1s - loss: 0.5144 - accuracy: 0.76 - ETA: 1s - loss: 0.5150 - accuracy: 0.76 - ETA: 1s - loss: 0.5143 - accuracy: 0.76 - ETA: 1s - loss: 0.5155 - accuracy: 0.76 - ETA: 1s - loss: 0.5129 - accuracy: 0.76 - ETA: 1s - loss: 0.5125 - accuracy: 0.76 - ETA: 1s - loss: 0.5098 - accuracy: 0.76 - ETA: 1s - loss: 0.5076 - accuracy: 0.77 - ETA: 1s - loss: 0.5093 - accuracy: 0.77 - ETA: 1s - loss: 0.5092 - accuracy: 0.76 - ETA: 1s - loss: 0.5091 - accuracy: 0.77 - ETA: 0s - loss: 0.5114 - accuracy: 0.76 - ETA: 0s - loss: 0.5114 - accuracy: 0.76 - ETA: 0s - loss: 0.5113 - accuracy: 0.77 - ETA: 0s - loss: 0.5105 - accuracy: 0.77 - ETA: 0s - loss: 0.5099 - accuracy: 0.77 - ETA: 0s - loss: 0.5088 - accuracy: 0.77 - ETA: 0s - loss: 0.5096 - accuracy: 0.77 - ETA: 0s - loss: 0.5103 - accuracy: 0.77 - ETA: 0s - loss: 0.5098 - accuracy: 0.77 - ETA: 0s - loss: 0.5096 - accuracy: 0.77 - ETA: 0s - loss: 0.5085 - accuracy: 0.77 - ETA: 0s - loss: 0.5082 - accuracy: 0.77 - ETA: 0s - loss: 0.5077 - accuracy: 0.77 - ETA: 0s - loss: 0.5088 - accuracy: 0.77 - 4s 341us/step - loss: 0.5083 - accuracy: 0.7719 - val_loss: 0.4577 - val_accuracy: 0.8113\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.32433\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5180 - accuracy: 0.75 - ETA: 3s - loss: 0.5146 - accuracy: 0.78 - ETA: 3s - loss: 0.5446 - accuracy: 0.77 - ETA: 3s - loss: 0.5234 - accuracy: 0.76 - ETA: 3s - loss: 0.5178 - accuracy: 0.76 - ETA: 3s - loss: 0.5246 - accuracy: 0.75 - ETA: 3s - loss: 0.5185 - accuracy: 0.75 - ETA: 3s - loss: 0.5124 - accuracy: 0.75 - ETA: 3s - loss: 0.5111 - accuracy: 0.75 - ETA: 3s - loss: 0.5061 - accuracy: 0.76 - ETA: 3s - loss: 0.4929 - accuracy: 0.77 - ETA: 2s - loss: 0.5001 - accuracy: 0.76 - ETA: 2s - loss: 0.5045 - accuracy: 0.75 - ETA: 2s - loss: 0.4969 - accuracy: 0.76 - ETA: 2s - loss: 0.4969 - accuracy: 0.76 - ETA: 2s - loss: 0.4955 - accuracy: 0.76 - ETA: 2s - loss: 0.4946 - accuracy: 0.77 - ETA: 2s - loss: 0.4945 - accuracy: 0.77 - ETA: 2s - loss: 0.4927 - accuracy: 0.77 - ETA: 2s - loss: 0.4881 - accuracy: 0.77 - ETA: 2s - loss: 0.4850 - accuracy: 0.77 - ETA: 2s - loss: 0.4811 - accuracy: 0.77 - ETA: 2s - loss: 0.4839 - accuracy: 0.77 - ETA: 2s - loss: 0.4869 - accuracy: 0.77 - ETA: 2s - loss: 0.4859 - accuracy: 0.77 - ETA: 1s - loss: 0.4857 - accuracy: 0.77 - ETA: 1s - loss: 0.4863 - accuracy: 0.77 - ETA: 1s - loss: 0.4875 - accuracy: 0.77 - ETA: 1s - loss: 0.4876 - accuracy: 0.77 - ETA: 1s - loss: 0.4882 - accuracy: 0.77 - ETA: 1s - loss: 0.4888 - accuracy: 0.77 - ETA: 1s - loss: 0.4889 - accuracy: 0.77 - ETA: 1s - loss: 0.4866 - accuracy: 0.77 - ETA: 1s - loss: 0.4861 - accuracy: 0.77 - ETA: 1s - loss: 0.4854 - accuracy: 0.77 - ETA: 1s - loss: 0.4844 - accuracy: 0.77 - ETA: 1s - loss: 0.4835 - accuracy: 0.77 - ETA: 1s - loss: 0.4844 - accuracy: 0.77 - ETA: 0s - loss: 0.4834 - accuracy: 0.77 - ETA: 0s - loss: 0.4845 - accuracy: 0.77 - ETA: 0s - loss: 0.4848 - accuracy: 0.77 - ETA: 0s - loss: 0.4853 - accuracy: 0.77 - ETA: 0s - loss: 0.4843 - accuracy: 0.77 - ETA: 0s - loss: 0.4843 - accuracy: 0.77 - ETA: 0s - loss: 0.4829 - accuracy: 0.78 - ETA: 0s - loss: 0.4831 - accuracy: 0.78 - ETA: 0s - loss: 0.4837 - accuracy: 0.77 - ETA: 0s - loss: 0.4833 - accuracy: 0.77 - ETA: 0s - loss: 0.4835 - accuracy: 0.77 - ETA: 0s - loss: 0.4837 - accuracy: 0.77 - ETA: 0s - loss: 0.4834 - accuracy: 0.78 - 4s 347us/step - loss: 0.4826 - accuracy: 0.7807 - val_loss: 0.4334 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.32433\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4959 - accuracy: 0.75 - ETA: 2s - loss: 0.4733 - accuracy: 0.78 - ETA: 2s - loss: 0.4560 - accuracy: 0.80 - ETA: 2s - loss: 0.4512 - accuracy: 0.79 - ETA: 2s - loss: 0.4395 - accuracy: 0.81 - ETA: 2s - loss: 0.4492 - accuracy: 0.80 - ETA: 2s - loss: 0.4535 - accuracy: 0.80 - ETA: 2s - loss: 0.4479 - accuracy: 0.80 - ETA: 2s - loss: 0.4406 - accuracy: 0.80 - ETA: 2s - loss: 0.4491 - accuracy: 0.80 - ETA: 2s - loss: 0.4482 - accuracy: 0.80 - ETA: 2s - loss: 0.4457 - accuracy: 0.80 - ETA: 2s - loss: 0.4438 - accuracy: 0.80 - ETA: 2s - loss: 0.4443 - accuracy: 0.80 - ETA: 2s - loss: 0.4441 - accuracy: 0.80 - ETA: 1s - loss: 0.4451 - accuracy: 0.80 - ETA: 1s - loss: 0.4437 - accuracy: 0.80 - ETA: 1s - loss: 0.4437 - accuracy: 0.80 - ETA: 1s - loss: 0.4453 - accuracy: 0.80 - ETA: 1s - loss: 0.4462 - accuracy: 0.80 - ETA: 1s - loss: 0.4462 - accuracy: 0.80 - ETA: 1s - loss: 0.4471 - accuracy: 0.80 - ETA: 1s - loss: 0.4471 - accuracy: 0.80 - ETA: 1s - loss: 0.4501 - accuracy: 0.79 - ETA: 1s - loss: 0.4512 - accuracy: 0.79 - ETA: 1s - loss: 0.4518 - accuracy: 0.79 - ETA: 1s - loss: 0.4516 - accuracy: 0.79 - ETA: 1s - loss: 0.4516 - accuracy: 0.79 - ETA: 1s - loss: 0.4525 - accuracy: 0.79 - ETA: 1s - loss: 0.4524 - accuracy: 0.79 - ETA: 0s - loss: 0.4533 - accuracy: 0.79 - ETA: 0s - loss: 0.4532 - accuracy: 0.79 - ETA: 0s - loss: 0.4537 - accuracy: 0.79 - ETA: 0s - loss: 0.4547 - accuracy: 0.79 - ETA: 0s - loss: 0.4556 - accuracy: 0.79 - ETA: 0s - loss: 0.4568 - accuracy: 0.79 - ETA: 0s - loss: 0.4558 - accuracy: 0.79 - ETA: 0s - loss: 0.4549 - accuracy: 0.79 - ETA: 0s - loss: 0.4548 - accuracy: 0.79 - ETA: 0s - loss: 0.4555 - accuracy: 0.79 - ETA: 0s - loss: 0.4550 - accuracy: 0.79 - ETA: 0s - loss: 0.4557 - accuracy: 0.79 - ETA: 0s - loss: 0.4559 - accuracy: 0.79 - ETA: 0s - loss: 0.4554 - accuracy: 0.79 - ETA: 0s - loss: 0.4560 - accuracy: 0.79 - ETA: 0s - loss: 0.4568 - accuracy: 0.79 - ETA: 0s - loss: 0.4581 - accuracy: 0.79 - 4s 330us/step - loss: 0.4585 - accuracy: 0.7915 - val_loss: 0.4208 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.32433\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4138 - accuracy: 0.77 - ETA: 3s - loss: 0.4812 - accuracy: 0.75 - ETA: 2s - loss: 0.4756 - accuracy: 0.76 - ETA: 2s - loss: 0.4714 - accuracy: 0.77 - ETA: 2s - loss: 0.4683 - accuracy: 0.77 - ETA: 2s - loss: 0.4578 - accuracy: 0.78 - ETA: 2s - loss: 0.4611 - accuracy: 0.78 - ETA: 2s - loss: 0.4581 - accuracy: 0.78 - ETA: 2s - loss: 0.4568 - accuracy: 0.78 - ETA: 2s - loss: 0.4549 - accuracy: 0.78 - ETA: 2s - loss: 0.4549 - accuracy: 0.78 - ETA: 2s - loss: 0.4612 - accuracy: 0.78 - ETA: 2s - loss: 0.4616 - accuracy: 0.78 - ETA: 2s - loss: 0.4655 - accuracy: 0.78 - ETA: 2s - loss: 0.4641 - accuracy: 0.78 - ETA: 2s - loss: 0.4596 - accuracy: 0.78 - ETA: 1s - loss: 0.4634 - accuracy: 0.78 - ETA: 1s - loss: 0.4620 - accuracy: 0.78 - ETA: 1s - loss: 0.4595 - accuracy: 0.78 - ETA: 1s - loss: 0.4606 - accuracy: 0.78 - ETA: 1s - loss: 0.4593 - accuracy: 0.78 - ETA: 1s - loss: 0.4578 - accuracy: 0.78 - ETA: 1s - loss: 0.4589 - accuracy: 0.78 - ETA: 1s - loss: 0.4577 - accuracy: 0.78 - ETA: 1s - loss: 0.4574 - accuracy: 0.78 - ETA: 1s - loss: 0.4554 - accuracy: 0.79 - ETA: 1s - loss: 0.4543 - accuracy: 0.79 - ETA: 1s - loss: 0.4557 - accuracy: 0.78 - ETA: 1s - loss: 0.4558 - accuracy: 0.78 - ETA: 1s - loss: 0.4572 - accuracy: 0.78 - ETA: 0s - loss: 0.4556 - accuracy: 0.78 - ETA: 0s - loss: 0.4557 - accuracy: 0.78 - ETA: 0s - loss: 0.4531 - accuracy: 0.79 - ETA: 0s - loss: 0.4510 - accuracy: 0.79 - ETA: 0s - loss: 0.4529 - accuracy: 0.79 - ETA: 0s - loss: 0.4513 - accuracy: 0.79 - ETA: 0s - loss: 0.4491 - accuracy: 0.79 - ETA: 0s - loss: 0.4475 - accuracy: 0.79 - ETA: 0s - loss: 0.4477 - accuracy: 0.79 - ETA: 0s - loss: 0.4473 - accuracy: 0.79 - ETA: 0s - loss: 0.4471 - accuracy: 0.79 - ETA: 0s - loss: 0.4469 - accuracy: 0.79 - 3s 319us/step - loss: 0.4474 - accuracy: 0.7949 - val_loss: 0.4290 - val_accuracy: 0.8020\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.32433\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4877 - accuracy: 0.73 - ETA: 2s - loss: 0.4177 - accuracy: 0.80 - ETA: 2s - loss: 0.4481 - accuracy: 0.79 - ETA: 2s - loss: 0.4450 - accuracy: 0.80 - ETA: 2s - loss: 0.4498 - accuracy: 0.79 - ETA: 2s - loss: 0.4495 - accuracy: 0.79 - ETA: 2s - loss: 0.4472 - accuracy: 0.80 - ETA: 2s - loss: 0.4491 - accuracy: 0.79 - ETA: 2s - loss: 0.4428 - accuracy: 0.79 - ETA: 2s - loss: 0.4472 - accuracy: 0.79 - ETA: 2s - loss: 0.4442 - accuracy: 0.79 - ETA: 2s - loss: 0.4444 - accuracy: 0.79 - ETA: 2s - loss: 0.4400 - accuracy: 0.80 - ETA: 2s - loss: 0.4433 - accuracy: 0.79 - ETA: 2s - loss: 0.4421 - accuracy: 0.79 - ETA: 1s - loss: 0.4427 - accuracy: 0.79 - ETA: 1s - loss: 0.4387 - accuracy: 0.80 - ETA: 1s - loss: 0.4417 - accuracy: 0.80 - ETA: 1s - loss: 0.4404 - accuracy: 0.80 - ETA: 1s - loss: 0.4390 - accuracy: 0.80 - ETA: 1s - loss: 0.4378 - accuracy: 0.80 - ETA: 1s - loss: 0.4376 - accuracy: 0.80 - ETA: 1s - loss: 0.4382 - accuracy: 0.80 - ETA: 1s - loss: 0.4372 - accuracy: 0.80 - ETA: 1s - loss: 0.4370 - accuracy: 0.80 - ETA: 1s - loss: 0.4394 - accuracy: 0.80 - ETA: 1s - loss: 0.4409 - accuracy: 0.80 - ETA: 1s - loss: 0.4389 - accuracy: 0.80 - ETA: 0s - loss: 0.4364 - accuracy: 0.80 - ETA: 0s - loss: 0.4391 - accuracy: 0.80 - ETA: 0s - loss: 0.4391 - accuracy: 0.80 - ETA: 0s - loss: 0.4397 - accuracy: 0.80 - ETA: 0s - loss: 0.4388 - accuracy: 0.80 - ETA: 0s - loss: 0.4408 - accuracy: 0.80 - ETA: 0s - loss: 0.4395 - accuracy: 0.80 - ETA: 0s - loss: 0.4392 - accuracy: 0.80 - ETA: 0s - loss: 0.4381 - accuracy: 0.80 - ETA: 0s - loss: 0.4385 - accuracy: 0.80 - ETA: 0s - loss: 0.4397 - accuracy: 0.80 - ETA: 0s - loss: 0.4402 - accuracy: 0.80 - ETA: 0s - loss: 0.4393 - accuracy: 0.80 - 3s 312us/step - loss: 0.4393 - accuracy: 0.8011 - val_loss: 0.4269 - val_accuracy: 0.8096\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32433\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4786 - accuracy: 0.75 - ETA: 2s - loss: 0.4252 - accuracy: 0.80 - ETA: 2s - loss: 0.4092 - accuracy: 0.82 - ETA: 2s - loss: 0.4056 - accuracy: 0.82 - ETA: 2s - loss: 0.4085 - accuracy: 0.82 - ETA: 2s - loss: 0.4095 - accuracy: 0.82 - ETA: 2s - loss: 0.4189 - accuracy: 0.81 - ETA: 2s - loss: 0.4195 - accuracy: 0.81 - ETA: 2s - loss: 0.4243 - accuracy: 0.80 - ETA: 2s - loss: 0.4213 - accuracy: 0.81 - ETA: 2s - loss: 0.4223 - accuracy: 0.80 - ETA: 2s - loss: 0.4220 - accuracy: 0.80 - ETA: 2s - loss: 0.4227 - accuracy: 0.80 - ETA: 2s - loss: 0.4237 - accuracy: 0.80 - ETA: 2s - loss: 0.4225 - accuracy: 0.80 - ETA: 1s - loss: 0.4280 - accuracy: 0.80 - ETA: 1s - loss: 0.4302 - accuracy: 0.80 - ETA: 1s - loss: 0.4264 - accuracy: 0.80 - ETA: 1s - loss: 0.4269 - accuracy: 0.80 - ETA: 1s - loss: 0.4267 - accuracy: 0.80 - ETA: 1s - loss: 0.4255 - accuracy: 0.80 - ETA: 1s - loss: 0.4251 - accuracy: 0.80 - ETA: 1s - loss: 0.4251 - accuracy: 0.80 - ETA: 1s - loss: 0.4248 - accuracy: 0.80 - ETA: 1s - loss: 0.4245 - accuracy: 0.80 - ETA: 1s - loss: 0.4237 - accuracy: 0.80 - ETA: 1s - loss: 0.4224 - accuracy: 0.80 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - ETA: 0s - loss: 0.4231 - accuracy: 0.80 - ETA: 0s - loss: 0.4239 - accuracy: 0.80 - ETA: 0s - loss: 0.4227 - accuracy: 0.80 - ETA: 0s - loss: 0.4210 - accuracy: 0.80 - ETA: 0s - loss: 0.4217 - accuracy: 0.80 - ETA: 0s - loss: 0.4211 - accuracy: 0.80 - ETA: 0s - loss: 0.4212 - accuracy: 0.80 - ETA: 0s - loss: 0.4203 - accuracy: 0.80 - ETA: 0s - loss: 0.4202 - accuracy: 0.80 - ETA: 0s - loss: 0.4228 - accuracy: 0.80 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - 3s 311us/step - loss: 0.4235 - accuracy: 0.8072 - val_loss: 0.3808 - val_accuracy: 0.8366\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.32433\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4470 - accuracy: 0.77 - ETA: 2s - loss: 0.4369 - accuracy: 0.78 - ETA: 2s - loss: 0.4196 - accuracy: 0.79 - ETA: 2s - loss: 0.4218 - accuracy: 0.80 - ETA: 2s - loss: 0.4181 - accuracy: 0.80 - ETA: 2s - loss: 0.4126 - accuracy: 0.80 - ETA: 2s - loss: 0.4086 - accuracy: 0.80 - ETA: 2s - loss: 0.4112 - accuracy: 0.80 - ETA: 2s - loss: 0.4072 - accuracy: 0.81 - ETA: 2s - loss: 0.4160 - accuracy: 0.80 - ETA: 2s - loss: 0.4128 - accuracy: 0.81 - ETA: 2s - loss: 0.4142 - accuracy: 0.81 - ETA: 2s - loss: 0.4141 - accuracy: 0.81 - ETA: 2s - loss: 0.4095 - accuracy: 0.81 - ETA: 2s - loss: 0.4099 - accuracy: 0.81 - ETA: 1s - loss: 0.4098 - accuracy: 0.81 - ETA: 1s - loss: 0.4116 - accuracy: 0.81 - ETA: 1s - loss: 0.4104 - accuracy: 0.81 - ETA: 1s - loss: 0.4121 - accuracy: 0.81 - ETA: 1s - loss: 0.4123 - accuracy: 0.81 - ETA: 1s - loss: 0.4130 - accuracy: 0.81 - ETA: 1s - loss: 0.4127 - accuracy: 0.81 - ETA: 1s - loss: 0.4080 - accuracy: 0.81 - ETA: 1s - loss: 0.4111 - accuracy: 0.81 - ETA: 1s - loss: 0.4097 - accuracy: 0.81 - ETA: 1s - loss: 0.4091 - accuracy: 0.81 - ETA: 1s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4093 - accuracy: 0.81 - ETA: 0s - loss: 0.4091 - accuracy: 0.81 - ETA: 0s - loss: 0.4093 - accuracy: 0.81 - ETA: 0s - loss: 0.4086 - accuracy: 0.81 - ETA: 0s - loss: 0.4097 - accuracy: 0.81 - ETA: 0s - loss: 0.4104 - accuracy: 0.81 - ETA: 0s - loss: 0.4113 - accuracy: 0.81 - ETA: 0s - loss: 0.4123 - accuracy: 0.81 - ETA: 0s - loss: 0.4121 - accuracy: 0.81 - ETA: 0s - loss: 0.4123 - accuracy: 0.81 - ETA: 0s - loss: 0.4137 - accuracy: 0.81 - ETA: 0s - loss: 0.4134 - accuracy: 0.81 - 3s 310us/step - loss: 0.4139 - accuracy: 0.8133 - val_loss: 0.3833 - val_accuracy: 0.8307\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.32433\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4217 - accuracy: 0.84 - ETA: 2s - loss: 0.3639 - accuracy: 0.84 - ETA: 2s - loss: 0.3564 - accuracy: 0.84 - ETA: 2s - loss: 0.3754 - accuracy: 0.83 - ETA: 2s - loss: 0.3801 - accuracy: 0.84 - ETA: 2s - loss: 0.3894 - accuracy: 0.83 - ETA: 2s - loss: 0.3924 - accuracy: 0.83 - ETA: 2s - loss: 0.3958 - accuracy: 0.83 - ETA: 2s - loss: 0.3988 - accuracy: 0.82 - ETA: 2s - loss: 0.4070 - accuracy: 0.81 - ETA: 2s - loss: 0.4073 - accuracy: 0.81 - ETA: 2s - loss: 0.4063 - accuracy: 0.81 - ETA: 2s - loss: 0.4080 - accuracy: 0.81 - ETA: 2s - loss: 0.4044 - accuracy: 0.81 - ETA: 2s - loss: 0.4038 - accuracy: 0.81 - ETA: 1s - loss: 0.3996 - accuracy: 0.81 - ETA: 1s - loss: 0.3983 - accuracy: 0.81 - ETA: 1s - loss: 0.3985 - accuracy: 0.81 - ETA: 1s - loss: 0.3980 - accuracy: 0.81 - ETA: 1s - loss: 0.3949 - accuracy: 0.81 - ETA: 1s - loss: 0.3953 - accuracy: 0.81 - ETA: 1s - loss: 0.3964 - accuracy: 0.81 - ETA: 1s - loss: 0.3977 - accuracy: 0.81 - ETA: 1s - loss: 0.3972 - accuracy: 0.81 - ETA: 1s - loss: 0.3981 - accuracy: 0.81 - ETA: 1s - loss: 0.3985 - accuracy: 0.81 - ETA: 1s - loss: 0.3984 - accuracy: 0.81 - ETA: 1s - loss: 0.3969 - accuracy: 0.81 - ETA: 0s - loss: 0.3968 - accuracy: 0.81 - ETA: 0s - loss: 0.3956 - accuracy: 0.81 - ETA: 0s - loss: 0.3968 - accuracy: 0.81 - ETA: 0s - loss: 0.3958 - accuracy: 0.81 - ETA: 0s - loss: 0.3946 - accuracy: 0.81 - ETA: 0s - loss: 0.3970 - accuracy: 0.81 - ETA: 0s - loss: 0.3985 - accuracy: 0.81 - ETA: 0s - loss: 0.3982 - accuracy: 0.81 - ETA: 0s - loss: 0.3999 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4002 - accuracy: 0.81 - ETA: 0s - loss: 0.3999 - accuracy: 0.81 - 3s 310us/step - loss: 0.3999 - accuracy: 0.8170 - val_loss: 0.3991 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.32433\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5093 - accuracy: 0.73 - ETA: 2s - loss: 0.4128 - accuracy: 0.81 - ETA: 2s - loss: 0.3937 - accuracy: 0.82 - ETA: 2s - loss: 0.4056 - accuracy: 0.81 - ETA: 2s - loss: 0.4112 - accuracy: 0.81 - ETA: 2s - loss: 0.4100 - accuracy: 0.81 - ETA: 2s - loss: 0.4077 - accuracy: 0.81 - ETA: 2s - loss: 0.4141 - accuracy: 0.81 - ETA: 2s - loss: 0.4108 - accuracy: 0.81 - ETA: 2s - loss: 0.4062 - accuracy: 0.81 - ETA: 2s - loss: 0.4002 - accuracy: 0.81 - ETA: 2s - loss: 0.4020 - accuracy: 0.81 - ETA: 2s - loss: 0.3995 - accuracy: 0.81 - ETA: 2s - loss: 0.3999 - accuracy: 0.81 - ETA: 2s - loss: 0.4028 - accuracy: 0.81 - ETA: 2s - loss: 0.4047 - accuracy: 0.81 - ETA: 2s - loss: 0.4060 - accuracy: 0.81 - ETA: 1s - loss: 0.4073 - accuracy: 0.81 - ETA: 1s - loss: 0.4062 - accuracy: 0.81 - ETA: 1s - loss: 0.4034 - accuracy: 0.81 - ETA: 1s - loss: 0.4054 - accuracy: 0.81 - ETA: 1s - loss: 0.4057 - accuracy: 0.81 - ETA: 1s - loss: 0.4035 - accuracy: 0.81 - ETA: 1s - loss: 0.4054 - accuracy: 0.81 - ETA: 1s - loss: 0.4057 - accuracy: 0.81 - ETA: 1s - loss: 0.4056 - accuracy: 0.81 - ETA: 1s - loss: 0.4053 - accuracy: 0.81 - ETA: 1s - loss: 0.4049 - accuracy: 0.81 - ETA: 1s - loss: 0.4060 - accuracy: 0.81 - ETA: 1s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.4027 - accuracy: 0.81 - ETA: 0s - loss: 0.4017 - accuracy: 0.81 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.4033 - accuracy: 0.81 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4038 - accuracy: 0.81 - ETA: 0s - loss: 0.4049 - accuracy: 0.81 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4035 - accuracy: 0.81 - 3s 316us/step - loss: 0.4031 - accuracy: 0.8164 - val_loss: 0.3617 - val_accuracy: 0.8357\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32433\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3848 - accuracy: 0.81 - ETA: 2s - loss: 0.4407 - accuracy: 0.80 - ETA: 2s - loss: 0.4272 - accuracy: 0.80 - ETA: 2s - loss: 0.4005 - accuracy: 0.82 - ETA: 2s - loss: 0.4065 - accuracy: 0.82 - ETA: 2s - loss: 0.4018 - accuracy: 0.82 - ETA: 2s - loss: 0.4012 - accuracy: 0.82 - ETA: 2s - loss: 0.3924 - accuracy: 0.82 - ETA: 2s - loss: 0.3940 - accuracy: 0.81 - ETA: 2s - loss: 0.3911 - accuracy: 0.82 - ETA: 2s - loss: 0.3884 - accuracy: 0.82 - ETA: 2s - loss: 0.3889 - accuracy: 0.82 - ETA: 2s - loss: 0.3938 - accuracy: 0.81 - ETA: 2s - loss: 0.3960 - accuracy: 0.81 - ETA: 2s - loss: 0.3987 - accuracy: 0.81 - ETA: 1s - loss: 0.3960 - accuracy: 0.81 - ETA: 1s - loss: 0.3979 - accuracy: 0.81 - ETA: 1s - loss: 0.3949 - accuracy: 0.81 - ETA: 1s - loss: 0.3974 - accuracy: 0.81 - ETA: 1s - loss: 0.3963 - accuracy: 0.81 - ETA: 1s - loss: 0.3965 - accuracy: 0.81 - ETA: 1s - loss: 0.3982 - accuracy: 0.81 - ETA: 1s - loss: 0.3983 - accuracy: 0.81 - ETA: 1s - loss: 0.3990 - accuracy: 0.81 - ETA: 1s - loss: 0.3983 - accuracy: 0.81 - ETA: 1s - loss: 0.3985 - accuracy: 0.81 - ETA: 1s - loss: 0.3999 - accuracy: 0.81 - ETA: 0s - loss: 0.4024 - accuracy: 0.81 - ETA: 0s - loss: 0.4016 - accuracy: 0.81 - ETA: 0s - loss: 0.3979 - accuracy: 0.81 - ETA: 0s - loss: 0.3976 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.3991 - accuracy: 0.81 - ETA: 0s - loss: 0.3972 - accuracy: 0.81 - ETA: 0s - loss: 0.3965 - accuracy: 0.81 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.81 - ETA: 0s - loss: 0.3962 - accuracy: 0.81 - 3s 310us/step - loss: 0.3956 - accuracy: 0.8191 - val_loss: 0.3838 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32433\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.6086 - accuracy: 0.76 - ETA: 2s - loss: 0.4568 - accuracy: 0.80 - ETA: 2s - loss: 0.4110 - accuracy: 0.81 - ETA: 2s - loss: 0.3870 - accuracy: 0.82 - ETA: 2s - loss: 0.3922 - accuracy: 0.82 - ETA: 2s - loss: 0.3885 - accuracy: 0.82 - ETA: 2s - loss: 0.3863 - accuracy: 0.82 - ETA: 2s - loss: 0.3837 - accuracy: 0.82 - ETA: 2s - loss: 0.3845 - accuracy: 0.82 - ETA: 2s - loss: 0.3869 - accuracy: 0.82 - ETA: 2s - loss: 0.3900 - accuracy: 0.81 - ETA: 2s - loss: 0.3897 - accuracy: 0.81 - ETA: 2s - loss: 0.3916 - accuracy: 0.81 - ETA: 2s - loss: 0.3906 - accuracy: 0.82 - ETA: 1s - loss: 0.3883 - accuracy: 0.82 - ETA: 1s - loss: 0.3897 - accuracy: 0.82 - ETA: 1s - loss: 0.3860 - accuracy: 0.82 - ETA: 1s - loss: 0.3830 - accuracy: 0.82 - ETA: 1s - loss: 0.3856 - accuracy: 0.82 - ETA: 1s - loss: 0.3862 - accuracy: 0.82 - ETA: 1s - loss: 0.3847 - accuracy: 0.82 - ETA: 1s - loss: 0.3843 - accuracy: 0.82 - ETA: 1s - loss: 0.3837 - accuracy: 0.82 - ETA: 1s - loss: 0.3833 - accuracy: 0.82 - ETA: 1s - loss: 0.3830 - accuracy: 0.82 - ETA: 1s - loss: 0.3827 - accuracy: 0.82 - ETA: 1s - loss: 0.3813 - accuracy: 0.82 - ETA: 0s - loss: 0.3828 - accuracy: 0.82 - ETA: 0s - loss: 0.3825 - accuracy: 0.82 - ETA: 0s - loss: 0.3820 - accuracy: 0.82 - ETA: 0s - loss: 0.3849 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.81 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - 3s 308us/step - loss: 0.3872 - accuracy: 0.8215 - val_loss: 0.3478 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32433\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3771 - accuracy: 0.86 - ETA: 2s - loss: 0.3598 - accuracy: 0.85 - ETA: 2s - loss: 0.3535 - accuracy: 0.84 - ETA: 2s - loss: 0.3621 - accuracy: 0.83 - ETA: 2s - loss: 0.3548 - accuracy: 0.83 - ETA: 2s - loss: 0.3508 - accuracy: 0.84 - ETA: 2s - loss: 0.3562 - accuracy: 0.84 - ETA: 2s - loss: 0.3640 - accuracy: 0.83 - ETA: 2s - loss: 0.3663 - accuracy: 0.83 - ETA: 2s - loss: 0.3661 - accuracy: 0.83 - ETA: 2s - loss: 0.3675 - accuracy: 0.83 - ETA: 2s - loss: 0.3660 - accuracy: 0.83 - ETA: 2s - loss: 0.3642 - accuracy: 0.83 - ETA: 2s - loss: 0.3662 - accuracy: 0.83 - ETA: 1s - loss: 0.3685 - accuracy: 0.83 - ETA: 1s - loss: 0.3724 - accuracy: 0.83 - ETA: 1s - loss: 0.3716 - accuracy: 0.83 - ETA: 1s - loss: 0.3677 - accuracy: 0.83 - ETA: 1s - loss: 0.3656 - accuracy: 0.83 - ETA: 1s - loss: 0.3694 - accuracy: 0.83 - ETA: 1s - loss: 0.3722 - accuracy: 0.83 - ETA: 1s - loss: 0.3721 - accuracy: 0.83 - ETA: 1s - loss: 0.3735 - accuracy: 0.83 - ETA: 1s - loss: 0.3757 - accuracy: 0.83 - ETA: 1s - loss: 0.3768 - accuracy: 0.83 - ETA: 1s - loss: 0.3768 - accuracy: 0.83 - ETA: 1s - loss: 0.3771 - accuracy: 0.83 - ETA: 1s - loss: 0.3777 - accuracy: 0.83 - ETA: 0s - loss: 0.3778 - accuracy: 0.83 - ETA: 0s - loss: 0.3795 - accuracy: 0.82 - ETA: 0s - loss: 0.3787 - accuracy: 0.82 - ETA: 0s - loss: 0.3790 - accuracy: 0.82 - ETA: 0s - loss: 0.3796 - accuracy: 0.82 - ETA: 0s - loss: 0.3802 - accuracy: 0.82 - ETA: 0s - loss: 0.3817 - accuracy: 0.82 - ETA: 0s - loss: 0.3808 - accuracy: 0.82 - ETA: 0s - loss: 0.3809 - accuracy: 0.82 - ETA: 0s - loss: 0.3812 - accuracy: 0.82 - ETA: 0s - loss: 0.3800 - accuracy: 0.82 - ETA: 0s - loss: 0.3814 - accuracy: 0.82 - 3s 309us/step - loss: 0.3838 - accuracy: 0.8262 - val_loss: 0.3528 - val_accuracy: 0.8298\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.32433\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3387 - accuracy: 0.82 - ETA: 2s - loss: 0.3255 - accuracy: 0.84 - ETA: 2s - loss: 0.3822 - accuracy: 0.82 - ETA: 2s - loss: 0.3621 - accuracy: 0.83 - ETA: 2s - loss: 0.3745 - accuracy: 0.82 - ETA: 2s - loss: 0.3808 - accuracy: 0.81 - ETA: 2s - loss: 0.3837 - accuracy: 0.81 - ETA: 2s - loss: 0.3901 - accuracy: 0.81 - ETA: 2s - loss: 0.3920 - accuracy: 0.81 - ETA: 2s - loss: 0.3882 - accuracy: 0.82 - ETA: 2s - loss: 0.3920 - accuracy: 0.81 - ETA: 2s - loss: 0.3958 - accuracy: 0.81 - ETA: 2s - loss: 0.3910 - accuracy: 0.81 - ETA: 2s - loss: 0.3886 - accuracy: 0.81 - ETA: 2s - loss: 0.3858 - accuracy: 0.81 - ETA: 2s - loss: 0.3826 - accuracy: 0.81 - ETA: 1s - loss: 0.3820 - accuracy: 0.81 - ETA: 1s - loss: 0.3819 - accuracy: 0.82 - ETA: 1s - loss: 0.3831 - accuracy: 0.82 - ETA: 1s - loss: 0.3836 - accuracy: 0.82 - ETA: 1s - loss: 0.3819 - accuracy: 0.82 - ETA: 1s - loss: 0.3811 - accuracy: 0.82 - ETA: 1s - loss: 0.3828 - accuracy: 0.82 - ETA: 1s - loss: 0.3811 - accuracy: 0.82 - ETA: 1s - loss: 0.3790 - accuracy: 0.82 - ETA: 1s - loss: 0.3769 - accuracy: 0.82 - ETA: 1s - loss: 0.3784 - accuracy: 0.82 - ETA: 1s - loss: 0.3781 - accuracy: 0.82 - ETA: 0s - loss: 0.3796 - accuracy: 0.82 - ETA: 0s - loss: 0.3807 - accuracy: 0.82 - ETA: 0s - loss: 0.3794 - accuracy: 0.82 - ETA: 0s - loss: 0.3799 - accuracy: 0.82 - ETA: 0s - loss: 0.3809 - accuracy: 0.82 - ETA: 0s - loss: 0.3805 - accuracy: 0.82 - ETA: 0s - loss: 0.3796 - accuracy: 0.82 - ETA: 0s - loss: 0.3812 - accuracy: 0.82 - ETA: 0s - loss: 0.3806 - accuracy: 0.82 - ETA: 0s - loss: 0.3811 - accuracy: 0.82 - ETA: 0s - loss: 0.3818 - accuracy: 0.82 - ETA: 0s - loss: 0.3820 - accuracy: 0.82 - 3s 309us/step - loss: 0.3819 - accuracy: 0.8251 - val_loss: 0.3473 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.32433\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3623 - accuracy: 0.86 - ETA: 2s - loss: 0.3836 - accuracy: 0.84 - ETA: 2s - loss: 0.3802 - accuracy: 0.83 - ETA: 2s - loss: 0.3727 - accuracy: 0.83 - ETA: 2s - loss: 0.3733 - accuracy: 0.83 - ETA: 2s - loss: 0.3657 - accuracy: 0.83 - ETA: 2s - loss: 0.3626 - accuracy: 0.83 - ETA: 2s - loss: 0.3708 - accuracy: 0.83 - ETA: 2s - loss: 0.3787 - accuracy: 0.82 - ETA: 2s - loss: 0.3756 - accuracy: 0.83 - ETA: 2s - loss: 0.3734 - accuracy: 0.83 - ETA: 2s - loss: 0.3730 - accuracy: 0.83 - ETA: 2s - loss: 0.3736 - accuracy: 0.83 - ETA: 2s - loss: 0.3758 - accuracy: 0.83 - ETA: 2s - loss: 0.3750 - accuracy: 0.83 - ETA: 1s - loss: 0.3730 - accuracy: 0.83 - ETA: 1s - loss: 0.3733 - accuracy: 0.83 - ETA: 1s - loss: 0.3705 - accuracy: 0.83 - ETA: 1s - loss: 0.3712 - accuracy: 0.83 - ETA: 1s - loss: 0.3715 - accuracy: 0.83 - ETA: 1s - loss: 0.3696 - accuracy: 0.83 - ETA: 1s - loss: 0.3731 - accuracy: 0.83 - ETA: 1s - loss: 0.3715 - accuracy: 0.83 - ETA: 1s - loss: 0.3717 - accuracy: 0.83 - ETA: 1s - loss: 0.3736 - accuracy: 0.83 - ETA: 1s - loss: 0.3743 - accuracy: 0.83 - ETA: 1s - loss: 0.3742 - accuracy: 0.83 - ETA: 0s - loss: 0.3748 - accuracy: 0.83 - ETA: 0s - loss: 0.3763 - accuracy: 0.82 - ETA: 0s - loss: 0.3756 - accuracy: 0.82 - ETA: 0s - loss: 0.3749 - accuracy: 0.83 - ETA: 0s - loss: 0.3738 - accuracy: 0.83 - ETA: 0s - loss: 0.3720 - accuracy: 0.83 - ETA: 0s - loss: 0.3727 - accuracy: 0.83 - ETA: 0s - loss: 0.3723 - accuracy: 0.83 - ETA: 0s - loss: 0.3725 - accuracy: 0.83 - ETA: 0s - loss: 0.3718 - accuracy: 0.83 - ETA: 0s - loss: 0.3726 - accuracy: 0.83 - ETA: 0s - loss: 0.3732 - accuracy: 0.83 - 3s 306us/step - loss: 0.3729 - accuracy: 0.8314 - val_loss: 0.4096 - val_accuracy: 0.7970\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.32433\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5570 - accuracy: 0.73 - ETA: 3s - loss: 0.5225 - accuracy: 0.76 - ETA: 2s - loss: 0.4486 - accuracy: 0.79 - ETA: 2s - loss: 0.4207 - accuracy: 0.81 - ETA: 2s - loss: 0.4224 - accuracy: 0.80 - ETA: 2s - loss: 0.4159 - accuracy: 0.81 - ETA: 2s - loss: 0.4220 - accuracy: 0.80 - ETA: 2s - loss: 0.4149 - accuracy: 0.81 - ETA: 2s - loss: 0.4128 - accuracy: 0.80 - ETA: 2s - loss: 0.4005 - accuracy: 0.81 - ETA: 2s - loss: 0.3965 - accuracy: 0.81 - ETA: 2s - loss: 0.3902 - accuracy: 0.81 - ETA: 2s - loss: 0.3882 - accuracy: 0.81 - ETA: 2s - loss: 0.3879 - accuracy: 0.81 - ETA: 2s - loss: 0.3868 - accuracy: 0.82 - ETA: 1s - loss: 0.3831 - accuracy: 0.82 - ETA: 1s - loss: 0.3857 - accuracy: 0.82 - ETA: 1s - loss: 0.3839 - accuracy: 0.82 - ETA: 1s - loss: 0.3833 - accuracy: 0.82 - ETA: 1s - loss: 0.3786 - accuracy: 0.82 - ETA: 1s - loss: 0.3794 - accuracy: 0.82 - ETA: 1s - loss: 0.3802 - accuracy: 0.82 - ETA: 1s - loss: 0.3810 - accuracy: 0.82 - ETA: 1s - loss: 0.3824 - accuracy: 0.82 - ETA: 1s - loss: 0.3816 - accuracy: 0.82 - ETA: 1s - loss: 0.3823 - accuracy: 0.82 - ETA: 1s - loss: 0.3792 - accuracy: 0.82 - ETA: 1s - loss: 0.3792 - accuracy: 0.82 - ETA: 0s - loss: 0.3800 - accuracy: 0.82 - ETA: 0s - loss: 0.3810 - accuracy: 0.82 - ETA: 0s - loss: 0.3791 - accuracy: 0.82 - ETA: 0s - loss: 0.3781 - accuracy: 0.82 - ETA: 0s - loss: 0.3772 - accuracy: 0.82 - ETA: 0s - loss: 0.3768 - accuracy: 0.82 - ETA: 0s - loss: 0.3764 - accuracy: 0.82 - ETA: 0s - loss: 0.3753 - accuracy: 0.82 - ETA: 0s - loss: 0.3742 - accuracy: 0.82 - ETA: 0s - loss: 0.3767 - accuracy: 0.82 - ETA: 0s - loss: 0.3749 - accuracy: 0.82 - ETA: 0s - loss: 0.3745 - accuracy: 0.82 - 3s 311us/step - loss: 0.3752 - accuracy: 0.8274 - val_loss: 0.3425 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.32433\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3548 - accuracy: 0.81 - ETA: 3s - loss: 0.3633 - accuracy: 0.83 - ETA: 2s - loss: 0.3769 - accuracy: 0.82 - ETA: 2s - loss: 0.3834 - accuracy: 0.83 - ETA: 2s - loss: 0.3652 - accuracy: 0.83 - ETA: 2s - loss: 0.3710 - accuracy: 0.83 - ETA: 2s - loss: 0.3765 - accuracy: 0.82 - ETA: 2s - loss: 0.3813 - accuracy: 0.82 - ETA: 2s - loss: 0.3785 - accuracy: 0.82 - ETA: 2s - loss: 0.3783 - accuracy: 0.82 - ETA: 2s - loss: 0.3759 - accuracy: 0.82 - ETA: 2s - loss: 0.3770 - accuracy: 0.82 - ETA: 2s - loss: 0.3751 - accuracy: 0.82 - ETA: 2s - loss: 0.3788 - accuracy: 0.82 - ETA: 2s - loss: 0.3772 - accuracy: 0.82 - ETA: 1s - loss: 0.3707 - accuracy: 0.82 - ETA: 1s - loss: 0.3714 - accuracy: 0.82 - ETA: 1s - loss: 0.3721 - accuracy: 0.82 - ETA: 1s - loss: 0.3708 - accuracy: 0.82 - ETA: 1s - loss: 0.3699 - accuracy: 0.82 - ETA: 1s - loss: 0.3718 - accuracy: 0.82 - ETA: 1s - loss: 0.3706 - accuracy: 0.82 - ETA: 1s - loss: 0.3721 - accuracy: 0.82 - ETA: 1s - loss: 0.3730 - accuracy: 0.82 - ETA: 1s - loss: 0.3731 - accuracy: 0.82 - ETA: 1s - loss: 0.3727 - accuracy: 0.82 - ETA: 0s - loss: 0.3742 - accuracy: 0.82 - ETA: 0s - loss: 0.3736 - accuracy: 0.82 - ETA: 0s - loss: 0.3714 - accuracy: 0.82 - ETA: 0s - loss: 0.3712 - accuracy: 0.82 - ETA: 0s - loss: 0.3700 - accuracy: 0.82 - ETA: 0s - loss: 0.3693 - accuracy: 0.82 - ETA: 0s - loss: 0.3692 - accuracy: 0.82 - ETA: 0s - loss: 0.3686 - accuracy: 0.82 - ETA: 0s - loss: 0.3673 - accuracy: 0.82 - ETA: 0s - loss: 0.3656 - accuracy: 0.83 - ETA: 0s - loss: 0.3651 - accuracy: 0.83 - ETA: 0s - loss: 0.3663 - accuracy: 0.83 - 3s 305us/step - loss: 0.3656 - accuracy: 0.8308 - val_loss: 0.3757 - val_accuracy: 0.8222\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.32433\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.3897 - accuracy: 0.82 - ETA: 4s - loss: 0.3526 - accuracy: 0.86 - ETA: 3s - loss: 0.3605 - accuracy: 0.84 - ETA: 3s - loss: 0.3823 - accuracy: 0.84 - ETA: 2s - loss: 0.3836 - accuracy: 0.83 - ETA: 2s - loss: 0.3801 - accuracy: 0.84 - ETA: 2s - loss: 0.3689 - accuracy: 0.84 - ETA: 2s - loss: 0.3759 - accuracy: 0.84 - ETA: 2s - loss: 0.3739 - accuracy: 0.84 - ETA: 2s - loss: 0.3681 - accuracy: 0.84 - ETA: 2s - loss: 0.3662 - accuracy: 0.84 - ETA: 2s - loss: 0.3644 - accuracy: 0.84 - ETA: 2s - loss: 0.3653 - accuracy: 0.84 - ETA: 2s - loss: 0.3699 - accuracy: 0.84 - ETA: 2s - loss: 0.3737 - accuracy: 0.83 - ETA: 1s - loss: 0.3760 - accuracy: 0.83 - ETA: 1s - loss: 0.3791 - accuracy: 0.83 - ETA: 1s - loss: 0.3759 - accuracy: 0.83 - ETA: 1s - loss: 0.3725 - accuracy: 0.83 - ETA: 1s - loss: 0.3717 - accuracy: 0.84 - ETA: 1s - loss: 0.3724 - accuracy: 0.83 - ETA: 1s - loss: 0.3716 - accuracy: 0.83 - ETA: 1s - loss: 0.3720 - accuracy: 0.83 - ETA: 1s - loss: 0.3687 - accuracy: 0.84 - ETA: 1s - loss: 0.3674 - accuracy: 0.84 - ETA: 1s - loss: 0.3692 - accuracy: 0.83 - ETA: 1s - loss: 0.3677 - accuracy: 0.84 - ETA: 0s - loss: 0.3683 - accuracy: 0.84 - ETA: 0s - loss: 0.3690 - accuracy: 0.83 - ETA: 0s - loss: 0.3688 - accuracy: 0.83 - ETA: 0s - loss: 0.3696 - accuracy: 0.83 - ETA: 0s - loss: 0.3682 - accuracy: 0.83 - ETA: 0s - loss: 0.3678 - accuracy: 0.83 - ETA: 0s - loss: 0.3653 - accuracy: 0.84 - ETA: 0s - loss: 0.3658 - accuracy: 0.83 - ETA: 0s - loss: 0.3655 - accuracy: 0.83 - ETA: 0s - loss: 0.3659 - accuracy: 0.83 - ETA: 0s - loss: 0.3665 - accuracy: 0.83 - ETA: 0s - loss: 0.3689 - accuracy: 0.83 - ETA: 0s - loss: 0.3691 - accuracy: 0.83 - 3s 307us/step - loss: 0.3699 - accuracy: 0.8369 - val_loss: 0.4034 - val_accuracy: 0.7995\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.32433\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3351 - accuracy: 0.88 - ETA: 3s - loss: 0.3673 - accuracy: 0.84 - ETA: 3s - loss: 0.3876 - accuracy: 0.83 - ETA: 3s - loss: 0.3840 - accuracy: 0.82 - ETA: 3s - loss: 0.3699 - accuracy: 0.83 - ETA: 2s - loss: 0.3641 - accuracy: 0.83 - ETA: 2s - loss: 0.3703 - accuracy: 0.83 - ETA: 2s - loss: 0.3687 - accuracy: 0.83 - ETA: 2s - loss: 0.3709 - accuracy: 0.83 - ETA: 2s - loss: 0.3731 - accuracy: 0.83 - ETA: 2s - loss: 0.3705 - accuracy: 0.83 - ETA: 2s - loss: 0.3677 - accuracy: 0.83 - ETA: 2s - loss: 0.3660 - accuracy: 0.83 - ETA: 2s - loss: 0.3668 - accuracy: 0.83 - ETA: 1s - loss: 0.3664 - accuracy: 0.83 - ETA: 2s - loss: 0.3676 - accuracy: 0.83 - ETA: 1s - loss: 0.3682 - accuracy: 0.83 - ETA: 1s - loss: 0.3651 - accuracy: 0.83 - ETA: 1s - loss: 0.3619 - accuracy: 0.83 - ETA: 1s - loss: 0.3612 - accuracy: 0.83 - ETA: 1s - loss: 0.3612 - accuracy: 0.83 - ETA: 1s - loss: 0.3621 - accuracy: 0.83 - ETA: 1s - loss: 0.3627 - accuracy: 0.83 - ETA: 1s - loss: 0.3623 - accuracy: 0.83 - ETA: 1s - loss: 0.3644 - accuracy: 0.83 - ETA: 1s - loss: 0.3653 - accuracy: 0.83 - ETA: 1s - loss: 0.3638 - accuracy: 0.83 - ETA: 1s - loss: 0.3627 - accuracy: 0.83 - ETA: 1s - loss: 0.3610 - accuracy: 0.83 - ETA: 0s - loss: 0.3599 - accuracy: 0.83 - ETA: 0s - loss: 0.3597 - accuracy: 0.83 - ETA: 0s - loss: 0.3595 - accuracy: 0.83 - ETA: 0s - loss: 0.3585 - accuracy: 0.83 - ETA: 0s - loss: 0.3571 - accuracy: 0.83 - ETA: 0s - loss: 0.3585 - accuracy: 0.83 - ETA: 0s - loss: 0.3560 - accuracy: 0.83 - ETA: 0s - loss: 0.3569 - accuracy: 0.83 - ETA: 0s - loss: 0.3556 - accuracy: 0.83 - ETA: 0s - loss: 0.3552 - accuracy: 0.83 - ETA: 0s - loss: 0.3550 - accuracy: 0.83 - ETA: 0s - loss: 0.3556 - accuracy: 0.83 - ETA: 0s - loss: 0.3559 - accuracy: 0.83 - ETA: 0s - loss: 0.3553 - accuracy: 0.83 - ETA: 0s - loss: 0.3550 - accuracy: 0.83 - 3s 318us/step - loss: 0.3554 - accuracy: 0.8387 - val_loss: 0.3812 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.32433\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.3461 - accuracy: 0.86 - ETA: 3s - loss: 0.3244 - accuracy: 0.85 - ETA: 3s - loss: 0.3410 - accuracy: 0.84 - ETA: 3s - loss: 0.3395 - accuracy: 0.84 - ETA: 3s - loss: 0.3496 - accuracy: 0.83 - ETA: 3s - loss: 0.3557 - accuracy: 0.83 - ETA: 2s - loss: 0.3636 - accuracy: 0.83 - ETA: 2s - loss: 0.3630 - accuracy: 0.83 - ETA: 2s - loss: 0.3572 - accuracy: 0.83 - ETA: 2s - loss: 0.3511 - accuracy: 0.83 - ETA: 2s - loss: 0.3483 - accuracy: 0.84 - ETA: 2s - loss: 0.3546 - accuracy: 0.83 - ETA: 2s - loss: 0.3579 - accuracy: 0.83 - ETA: 2s - loss: 0.3586 - accuracy: 0.83 - ETA: 2s - loss: 0.3588 - accuracy: 0.83 - ETA: 2s - loss: 0.3590 - accuracy: 0.83 - ETA: 1s - loss: 0.3628 - accuracy: 0.83 - ETA: 1s - loss: 0.3611 - accuracy: 0.83 - ETA: 1s - loss: 0.3600 - accuracy: 0.83 - ETA: 1s - loss: 0.3589 - accuracy: 0.83 - ETA: 1s - loss: 0.3584 - accuracy: 0.83 - ETA: 1s - loss: 0.3575 - accuracy: 0.83 - ETA: 1s - loss: 0.3565 - accuracy: 0.83 - ETA: 1s - loss: 0.3545 - accuracy: 0.83 - ETA: 1s - loss: 0.3543 - accuracy: 0.83 - ETA: 1s - loss: 0.3557 - accuracy: 0.83 - ETA: 1s - loss: 0.3556 - accuracy: 0.83 - ETA: 1s - loss: 0.3551 - accuracy: 0.83 - ETA: 1s - loss: 0.3553 - accuracy: 0.83 - ETA: 1s - loss: 0.3559 - accuracy: 0.83 - ETA: 1s - loss: 0.3550 - accuracy: 0.83 - ETA: 0s - loss: 0.3555 - accuracy: 0.83 - ETA: 0s - loss: 0.3541 - accuracy: 0.83 - ETA: 0s - loss: 0.3541 - accuracy: 0.83 - ETA: 0s - loss: 0.3560 - accuracy: 0.83 - ETA: 0s - loss: 0.3574 - accuracy: 0.83 - ETA: 0s - loss: 0.3574 - accuracy: 0.83 - ETA: 0s - loss: 0.3564 - accuracy: 0.83 - ETA: 0s - loss: 0.3587 - accuracy: 0.83 - ETA: 0s - loss: 0.3578 - accuracy: 0.83 - ETA: 0s - loss: 0.3572 - accuracy: 0.83 - ETA: 0s - loss: 0.3564 - accuracy: 0.83 - ETA: 0s - loss: 0.3564 - accuracy: 0.83 - 3s 322us/step - loss: 0.3552 - accuracy: 0.8373 - val_loss: 0.3347 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.32433\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3792 - accuracy: 0.84 - ETA: 3s - loss: 0.3721 - accuracy: 0.83 - ETA: 2s - loss: 0.3385 - accuracy: 0.86 - ETA: 2s - loss: 0.3454 - accuracy: 0.85 - ETA: 2s - loss: 0.3414 - accuracy: 0.85 - ETA: 2s - loss: 0.3498 - accuracy: 0.84 - ETA: 2s - loss: 0.3507 - accuracy: 0.84 - ETA: 2s - loss: 0.3549 - accuracy: 0.84 - ETA: 2s - loss: 0.3526 - accuracy: 0.84 - ETA: 2s - loss: 0.3542 - accuracy: 0.84 - ETA: 2s - loss: 0.3465 - accuracy: 0.84 - ETA: 2s - loss: 0.3466 - accuracy: 0.84 - ETA: 2s - loss: 0.3489 - accuracy: 0.84 - ETA: 2s - loss: 0.3495 - accuracy: 0.84 - ETA: 2s - loss: 0.3476 - accuracy: 0.84 - ETA: 2s - loss: 0.3491 - accuracy: 0.84 - ETA: 2s - loss: 0.3502 - accuracy: 0.84 - ETA: 1s - loss: 0.3481 - accuracy: 0.84 - ETA: 1s - loss: 0.3499 - accuracy: 0.84 - ETA: 1s - loss: 0.3514 - accuracy: 0.84 - ETA: 1s - loss: 0.3498 - accuracy: 0.84 - ETA: 1s - loss: 0.3466 - accuracy: 0.84 - ETA: 1s - loss: 0.3494 - accuracy: 0.84 - ETA: 1s - loss: 0.3494 - accuracy: 0.84 - ETA: 1s - loss: 0.3485 - accuracy: 0.84 - ETA: 1s - loss: 0.3499 - accuracy: 0.84 - ETA: 1s - loss: 0.3523 - accuracy: 0.84 - ETA: 1s - loss: 0.3513 - accuracy: 0.84 - ETA: 1s - loss: 0.3501 - accuracy: 0.84 - ETA: 1s - loss: 0.3505 - accuracy: 0.84 - ETA: 1s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.3491 - accuracy: 0.84 - ETA: 0s - loss: 0.3489 - accuracy: 0.84 - ETA: 0s - loss: 0.3494 - accuracy: 0.84 - ETA: 0s - loss: 0.3498 - accuracy: 0.84 - ETA: 0s - loss: 0.3529 - accuracy: 0.84 - ETA: 0s - loss: 0.3522 - accuracy: 0.84 - ETA: 0s - loss: 0.3522 - accuracy: 0.84 - ETA: 0s - loss: 0.3522 - accuracy: 0.84 - ETA: 0s - loss: 0.3509 - accuracy: 0.84 - ETA: 0s - loss: 0.3516 - accuracy: 0.84 - ETA: 0s - loss: 0.3513 - accuracy: 0.84 - ETA: 0s - loss: 0.3515 - accuracy: 0.84 - ETA: 0s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3527 - accuracy: 0.84 - 4s 329us/step - loss: 0.3517 - accuracy: 0.8417 - val_loss: 0.3531 - val_accuracy: 0.8391\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.32433\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2455 - accuracy: 0.88 - ETA: 3s - loss: 0.3287 - accuracy: 0.84 - ETA: 3s - loss: 0.3483 - accuracy: 0.84 - ETA: 2s - loss: 0.3674 - accuracy: 0.82 - ETA: 2s - loss: 0.3625 - accuracy: 0.83 - ETA: 2s - loss: 0.3573 - accuracy: 0.83 - ETA: 2s - loss: 0.3520 - accuracy: 0.84 - ETA: 2s - loss: 0.3505 - accuracy: 0.84 - ETA: 2s - loss: 0.3428 - accuracy: 0.84 - ETA: 2s - loss: 0.3446 - accuracy: 0.84 - ETA: 2s - loss: 0.3421 - accuracy: 0.84 - ETA: 2s - loss: 0.3410 - accuracy: 0.84 - ETA: 2s - loss: 0.3453 - accuracy: 0.84 - ETA: 2s - loss: 0.3458 - accuracy: 0.84 - ETA: 2s - loss: 0.3487 - accuracy: 0.84 - ETA: 2s - loss: 0.3474 - accuracy: 0.84 - ETA: 2s - loss: 0.3494 - accuracy: 0.84 - ETA: 1s - loss: 0.3494 - accuracy: 0.84 - ETA: 1s - loss: 0.3515 - accuracy: 0.83 - ETA: 1s - loss: 0.3503 - accuracy: 0.84 - ETA: 1s - loss: 0.3493 - accuracy: 0.84 - ETA: 1s - loss: 0.3499 - accuracy: 0.84 - ETA: 1s - loss: 0.3471 - accuracy: 0.84 - ETA: 1s - loss: 0.3460 - accuracy: 0.84 - ETA: 1s - loss: 0.3478 - accuracy: 0.84 - ETA: 1s - loss: 0.3471 - accuracy: 0.84 - ETA: 1s - loss: 0.3457 - accuracy: 0.84 - ETA: 1s - loss: 0.3467 - accuracy: 0.84 - ETA: 1s - loss: 0.3449 - accuracy: 0.84 - ETA: 1s - loss: 0.3435 - accuracy: 0.84 - ETA: 0s - loss: 0.3423 - accuracy: 0.84 - ETA: 0s - loss: 0.3420 - accuracy: 0.84 - ETA: 0s - loss: 0.3422 - accuracy: 0.84 - ETA: 0s - loss: 0.3416 - accuracy: 0.84 - ETA: 0s - loss: 0.3414 - accuracy: 0.84 - ETA: 0s - loss: 0.3421 - accuracy: 0.84 - ETA: 0s - loss: 0.3410 - accuracy: 0.84 - ETA: 0s - loss: 0.3415 - accuracy: 0.84 - ETA: 0s - loss: 0.3414 - accuracy: 0.84 - ETA: 0s - loss: 0.3430 - accuracy: 0.84 - ETA: 0s - loss: 0.3429 - accuracy: 0.84 - ETA: 0s - loss: 0.3435 - accuracy: 0.84 - ETA: 0s - loss: 0.3430 - accuracy: 0.84 - 4s 329us/step - loss: 0.3427 - accuracy: 0.8446 - val_loss: 0.3249 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.32433\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3664 - accuracy: 0.86 - ETA: 2s - loss: 0.3890 - accuracy: 0.84 - ETA: 2s - loss: 0.3906 - accuracy: 0.82 - ETA: 2s - loss: 0.3647 - accuracy: 0.84 - ETA: 2s - loss: 0.3577 - accuracy: 0.84 - ETA: 2s - loss: 0.3608 - accuracy: 0.84 - ETA: 2s - loss: 0.3569 - accuracy: 0.83 - ETA: 2s - loss: 0.3503 - accuracy: 0.84 - ETA: 2s - loss: 0.3529 - accuracy: 0.83 - ETA: 2s - loss: 0.3519 - accuracy: 0.84 - ETA: 2s - loss: 0.3442 - accuracy: 0.84 - ETA: 2s - loss: 0.3432 - accuracy: 0.84 - ETA: 2s - loss: 0.3382 - accuracy: 0.84 - ETA: 2s - loss: 0.3432 - accuracy: 0.84 - ETA: 2s - loss: 0.3458 - accuracy: 0.84 - ETA: 2s - loss: 0.3436 - accuracy: 0.84 - ETA: 2s - loss: 0.3437 - accuracy: 0.84 - ETA: 1s - loss: 0.3447 - accuracy: 0.84 - ETA: 1s - loss: 0.3440 - accuracy: 0.84 - ETA: 1s - loss: 0.3431 - accuracy: 0.84 - ETA: 1s - loss: 0.3436 - accuracy: 0.84 - ETA: 1s - loss: 0.3470 - accuracy: 0.84 - ETA: 1s - loss: 0.3466 - accuracy: 0.84 - ETA: 1s - loss: 0.3466 - accuracy: 0.84 - ETA: 1s - loss: 0.3459 - accuracy: 0.84 - ETA: 1s - loss: 0.3461 - accuracy: 0.84 - ETA: 1s - loss: 0.3453 - accuracy: 0.84 - ETA: 1s - loss: 0.3468 - accuracy: 0.84 - ETA: 1s - loss: 0.3464 - accuracy: 0.84 - ETA: 1s - loss: 0.3457 - accuracy: 0.84 - ETA: 0s - loss: 0.3460 - accuracy: 0.84 - ETA: 0s - loss: 0.3474 - accuracy: 0.84 - ETA: 0s - loss: 0.3468 - accuracy: 0.84 - ETA: 0s - loss: 0.3474 - accuracy: 0.84 - ETA: 0s - loss: 0.3472 - accuracy: 0.84 - ETA: 0s - loss: 0.3476 - accuracy: 0.84 - ETA: 0s - loss: 0.3476 - accuracy: 0.84 - ETA: 0s - loss: 0.3469 - accuracy: 0.84 - ETA: 0s - loss: 0.3483 - accuracy: 0.84 - ETA: 0s - loss: 0.3488 - accuracy: 0.84 - ETA: 0s - loss: 0.3483 - accuracy: 0.84 - ETA: 0s - loss: 0.3478 - accuracy: 0.84 - ETA: 0s - loss: 0.3480 - accuracy: 0.84 - ETA: 0s - loss: 0.3490 - accuracy: 0.84 - 3s 325us/step - loss: 0.3490 - accuracy: 0.8435 - val_loss: 0.3344 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.32433\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3322 - accuracy: 0.88 - ETA: 2s - loss: 0.3404 - accuracy: 0.85 - ETA: 3s - loss: 0.3305 - accuracy: 0.86 - ETA: 2s - loss: 0.3331 - accuracy: 0.85 - ETA: 2s - loss: 0.3430 - accuracy: 0.84 - ETA: 2s - loss: 0.3353 - accuracy: 0.84 - ETA: 2s - loss: 0.3283 - accuracy: 0.85 - ETA: 2s - loss: 0.3342 - accuracy: 0.84 - ETA: 2s - loss: 0.3314 - accuracy: 0.85 - ETA: 2s - loss: 0.3332 - accuracy: 0.85 - ETA: 2s - loss: 0.3287 - accuracy: 0.85 - ETA: 2s - loss: 0.3323 - accuracy: 0.85 - ETA: 2s - loss: 0.3361 - accuracy: 0.85 - ETA: 2s - loss: 0.3451 - accuracy: 0.84 - ETA: 2s - loss: 0.3449 - accuracy: 0.84 - ETA: 2s - loss: 0.3438 - accuracy: 0.84 - ETA: 2s - loss: 0.3483 - accuracy: 0.84 - ETA: 2s - loss: 0.3496 - accuracy: 0.84 - ETA: 1s - loss: 0.3500 - accuracy: 0.84 - ETA: 1s - loss: 0.3507 - accuracy: 0.84 - ETA: 1s - loss: 0.3467 - accuracy: 0.84 - ETA: 1s - loss: 0.3495 - accuracy: 0.84 - ETA: 1s - loss: 0.3461 - accuracy: 0.84 - ETA: 1s - loss: 0.3452 - accuracy: 0.84 - ETA: 1s - loss: 0.3449 - accuracy: 0.84 - ETA: 1s - loss: 0.3439 - accuracy: 0.84 - ETA: 1s - loss: 0.3442 - accuracy: 0.84 - ETA: 1s - loss: 0.3455 - accuracy: 0.84 - ETA: 1s - loss: 0.3446 - accuracy: 0.84 - ETA: 1s - loss: 0.3449 - accuracy: 0.84 - ETA: 0s - loss: 0.3459 - accuracy: 0.84 - ETA: 0s - loss: 0.3456 - accuracy: 0.84 - ETA: 0s - loss: 0.3462 - accuracy: 0.84 - ETA: 0s - loss: 0.3484 - accuracy: 0.84 - ETA: 0s - loss: 0.3498 - accuracy: 0.84 - ETA: 0s - loss: 0.3492 - accuracy: 0.84 - ETA: 0s - loss: 0.3489 - accuracy: 0.84 - ETA: 0s - loss: 0.3515 - accuracy: 0.84 - ETA: 0s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3522 - accuracy: 0.84 - ETA: 0s - loss: 0.3517 - accuracy: 0.84 - 3s 317us/step - loss: 0.3492 - accuracy: 0.8441 - val_loss: 0.3286 - val_accuracy: 0.8399\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.32433\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4619 - accuracy: 0.80 - ETA: 2s - loss: 0.4334 - accuracy: 0.79 - ETA: 2s - loss: 0.3989 - accuracy: 0.81 - ETA: 2s - loss: 0.3687 - accuracy: 0.83 - ETA: 2s - loss: 0.3730 - accuracy: 0.82 - ETA: 2s - loss: 0.3672 - accuracy: 0.83 - ETA: 2s - loss: 0.3665 - accuracy: 0.83 - ETA: 2s - loss: 0.3735 - accuracy: 0.82 - ETA: 2s - loss: 0.3697 - accuracy: 0.82 - ETA: 2s - loss: 0.3686 - accuracy: 0.83 - ETA: 2s - loss: 0.3629 - accuracy: 0.83 - ETA: 2s - loss: 0.3648 - accuracy: 0.83 - ETA: 2s - loss: 0.3583 - accuracy: 0.83 - ETA: 2s - loss: 0.3583 - accuracy: 0.83 - ETA: 2s - loss: 0.3554 - accuracy: 0.83 - ETA: 1s - loss: 0.3524 - accuracy: 0.83 - ETA: 1s - loss: 0.3529 - accuracy: 0.83 - ETA: 1s - loss: 0.3484 - accuracy: 0.83 - ETA: 1s - loss: 0.3484 - accuracy: 0.83 - ETA: 1s - loss: 0.3474 - accuracy: 0.84 - ETA: 1s - loss: 0.3462 - accuracy: 0.84 - ETA: 1s - loss: 0.3475 - accuracy: 0.84 - ETA: 1s - loss: 0.3446 - accuracy: 0.84 - ETA: 1s - loss: 0.3441 - accuracy: 0.84 - ETA: 1s - loss: 0.3421 - accuracy: 0.84 - ETA: 1s - loss: 0.3422 - accuracy: 0.84 - ETA: 1s - loss: 0.3423 - accuracy: 0.84 - ETA: 1s - loss: 0.3418 - accuracy: 0.84 - ETA: 0s - loss: 0.3417 - accuracy: 0.84 - ETA: 0s - loss: 0.3422 - accuracy: 0.84 - ETA: 0s - loss: 0.3425 - accuracy: 0.84 - ETA: 0s - loss: 0.3406 - accuracy: 0.84 - ETA: 0s - loss: 0.3417 - accuracy: 0.84 - ETA: 0s - loss: 0.3418 - accuracy: 0.84 - ETA: 0s - loss: 0.3407 - accuracy: 0.84 - ETA: 0s - loss: 0.3392 - accuracy: 0.84 - ETA: 0s - loss: 0.3444 - accuracy: 0.84 - ETA: 0s - loss: 0.3438 - accuracy: 0.84 - ETA: 0s - loss: 0.3439 - accuracy: 0.84 - ETA: 0s - loss: 0.3433 - accuracy: 0.84 - ETA: 0s - loss: 0.3426 - accuracy: 0.84 - 3s 310us/step - loss: 0.3421 - accuracy: 0.8429 - val_loss: 0.3134 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.32433 to 0.31344, saving model to ttbox_deep_model.h5\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.2813 - accuracy: 0.90 - ETA: 2s - loss: 0.2841 - accuracy: 0.89 - ETA: 2s - loss: 0.3065 - accuracy: 0.88 - ETA: 2s - loss: 0.3127 - accuracy: 0.86 - ETA: 2s - loss: 0.3139 - accuracy: 0.86 - ETA: 2s - loss: 0.3283 - accuracy: 0.85 - ETA: 2s - loss: 0.3250 - accuracy: 0.85 - ETA: 2s - loss: 0.3254 - accuracy: 0.85 - ETA: 2s - loss: 0.3313 - accuracy: 0.84 - ETA: 2s - loss: 0.3399 - accuracy: 0.84 - ETA: 2s - loss: 0.3368 - accuracy: 0.84 - ETA: 2s - loss: 0.3306 - accuracy: 0.85 - ETA: 2s - loss: 0.3272 - accuracy: 0.85 - ETA: 2s - loss: 0.3282 - accuracy: 0.85 - ETA: 2s - loss: 0.3291 - accuracy: 0.84 - ETA: 2s - loss: 0.3297 - accuracy: 0.85 - ETA: 1s - loss: 0.3310 - accuracy: 0.85 - ETA: 1s - loss: 0.3336 - accuracy: 0.84 - ETA: 1s - loss: 0.3339 - accuracy: 0.84 - ETA: 1s - loss: 0.3338 - accuracy: 0.84 - ETA: 1s - loss: 0.3334 - accuracy: 0.84 - ETA: 1s - loss: 0.3344 - accuracy: 0.84 - ETA: 1s - loss: 0.3338 - accuracy: 0.84 - ETA: 1s - loss: 0.3335 - accuracy: 0.84 - ETA: 1s - loss: 0.3336 - accuracy: 0.84 - ETA: 1s - loss: 0.3335 - accuracy: 0.84 - ETA: 1s - loss: 0.3356 - accuracy: 0.84 - ETA: 1s - loss: 0.3371 - accuracy: 0.84 - ETA: 0s - loss: 0.3346 - accuracy: 0.85 - ETA: 0s - loss: 0.3338 - accuracy: 0.85 - ETA: 0s - loss: 0.3360 - accuracy: 0.85 - ETA: 0s - loss: 0.3364 - accuracy: 0.84 - ETA: 0s - loss: 0.3379 - accuracy: 0.84 - ETA: 0s - loss: 0.3383 - accuracy: 0.84 - ETA: 0s - loss: 0.3377 - accuracy: 0.84 - ETA: 0s - loss: 0.3384 - accuracy: 0.84 - ETA: 0s - loss: 0.3383 - accuracy: 0.84 - ETA: 0s - loss: 0.3373 - accuracy: 0.85 - ETA: 0s - loss: 0.3378 - accuracy: 0.84 - ETA: 0s - loss: 0.3387 - accuracy: 0.84 - ETA: 0s - loss: 0.3387 - accuracy: 0.84 - 3s 310us/step - loss: 0.3390 - accuracy: 0.8488 - val_loss: 0.4266 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.31344\n",
      "Epoch 27/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5187 - accuracy: 0.77 - ETA: 2s - loss: 0.3916 - accuracy: 0.82 - ETA: 2s - loss: 0.3648 - accuracy: 0.84 - ETA: 2s - loss: 0.3571 - accuracy: 0.84 - ETA: 2s - loss: 0.3546 - accuracy: 0.84 - ETA: 2s - loss: 0.3434 - accuracy: 0.84 - ETA: 2s - loss: 0.3460 - accuracy: 0.84 - ETA: 2s - loss: 0.3446 - accuracy: 0.84 - ETA: 2s - loss: 0.3464 - accuracy: 0.84 - ETA: 2s - loss: 0.3441 - accuracy: 0.84 - ETA: 2s - loss: 0.3419 - accuracy: 0.84 - ETA: 2s - loss: 0.3337 - accuracy: 0.85 - ETA: 2s - loss: 0.3392 - accuracy: 0.84 - ETA: 2s - loss: 0.3389 - accuracy: 0.84 - ETA: 2s - loss: 0.3385 - accuracy: 0.84 - ETA: 2s - loss: 0.3380 - accuracy: 0.84 - ETA: 1s - loss: 0.3403 - accuracy: 0.84 - ETA: 1s - loss: 0.3420 - accuracy: 0.84 - ETA: 1s - loss: 0.3446 - accuracy: 0.84 - ETA: 1s - loss: 0.3428 - accuracy: 0.84 - ETA: 1s - loss: 0.3429 - accuracy: 0.84 - ETA: 1s - loss: 0.3399 - accuracy: 0.84 - ETA: 1s - loss: 0.3404 - accuracy: 0.84 - ETA: 1s - loss: 0.3367 - accuracy: 0.84 - ETA: 1s - loss: 0.3376 - accuracy: 0.84 - ETA: 1s - loss: 0.3339 - accuracy: 0.84 - ETA: 1s - loss: 0.3326 - accuracy: 0.84 - ETA: 1s - loss: 0.3357 - accuracy: 0.84 - ETA: 0s - loss: 0.3367 - accuracy: 0.84 - ETA: 0s - loss: 0.3381 - accuracy: 0.84 - ETA: 0s - loss: 0.3381 - accuracy: 0.84 - ETA: 0s - loss: 0.3398 - accuracy: 0.84 - ETA: 0s - loss: 0.3396 - accuracy: 0.84 - ETA: 0s - loss: 0.3387 - accuracy: 0.84 - ETA: 0s - loss: 0.3376 - accuracy: 0.84 - ETA: 0s - loss: 0.3361 - accuracy: 0.84 - ETA: 0s - loss: 0.3357 - accuracy: 0.84 - ETA: 0s - loss: 0.3352 - accuracy: 0.84 - ETA: 0s - loss: 0.3358 - accuracy: 0.84 - ETA: 0s - loss: 0.3355 - accuracy: 0.84 - ETA: 0s - loss: 0.3345 - accuracy: 0.85 - 3s 310us/step - loss: 0.3341 - accuracy: 0.8504 - val_loss: 0.3315 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.31344\n",
      "Epoch 28/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3538 - accuracy: 0.82 - ETA: 2s - loss: 0.3231 - accuracy: 0.85 - ETA: 2s - loss: 0.3102 - accuracy: 0.85 - ETA: 2s - loss: 0.3287 - accuracy: 0.85 - ETA: 2s - loss: 0.3271 - accuracy: 0.85 - ETA: 2s - loss: 0.3288 - accuracy: 0.85 - ETA: 2s - loss: 0.3300 - accuracy: 0.85 - ETA: 2s - loss: 0.3290 - accuracy: 0.85 - ETA: 2s - loss: 0.3321 - accuracy: 0.85 - ETA: 2s - loss: 0.3293 - accuracy: 0.85 - ETA: 2s - loss: 0.3322 - accuracy: 0.85 - ETA: 2s - loss: 0.3312 - accuracy: 0.85 - ETA: 2s - loss: 0.3378 - accuracy: 0.85 - ETA: 2s - loss: 0.3335 - accuracy: 0.85 - ETA: 2s - loss: 0.3338 - accuracy: 0.85 - ETA: 1s - loss: 0.3300 - accuracy: 0.85 - ETA: 1s - loss: 0.3284 - accuracy: 0.85 - ETA: 1s - loss: 0.3275 - accuracy: 0.85 - ETA: 1s - loss: 0.3262 - accuracy: 0.85 - ETA: 1s - loss: 0.3264 - accuracy: 0.85 - ETA: 1s - loss: 0.3259 - accuracy: 0.85 - ETA: 1s - loss: 0.3287 - accuracy: 0.85 - ETA: 1s - loss: 0.3270 - accuracy: 0.85 - ETA: 1s - loss: 0.3283 - accuracy: 0.85 - ETA: 1s - loss: 0.3267 - accuracy: 0.85 - ETA: 1s - loss: 0.3284 - accuracy: 0.85 - ETA: 1s - loss: 0.3291 - accuracy: 0.85 - ETA: 0s - loss: 0.3259 - accuracy: 0.85 - ETA: 0s - loss: 0.3277 - accuracy: 0.85 - ETA: 0s - loss: 0.3271 - accuracy: 0.85 - ETA: 0s - loss: 0.3278 - accuracy: 0.85 - ETA: 0s - loss: 0.3267 - accuracy: 0.85 - ETA: 0s - loss: 0.3276 - accuracy: 0.85 - ETA: 0s - loss: 0.3260 - accuracy: 0.85 - ETA: 0s - loss: 0.3259 - accuracy: 0.85 - ETA: 0s - loss: 0.3284 - accuracy: 0.85 - ETA: 0s - loss: 0.3280 - accuracy: 0.85 - ETA: 0s - loss: 0.3272 - accuracy: 0.85 - ETA: 0s - loss: 0.3269 - accuracy: 0.85 - ETA: 0s - loss: 0.3269 - accuracy: 0.85 - ETA: 0s - loss: 0.3266 - accuracy: 0.85 - 3s 315us/step - loss: 0.3265 - accuracy: 0.8559 - val_loss: 0.3168 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.31344\n",
      "Epoch 29/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3433 - accuracy: 0.84 - ETA: 2s - loss: 0.3783 - accuracy: 0.84 - ETA: 2s - loss: 0.3596 - accuracy: 0.83 - ETA: 2s - loss: 0.3640 - accuracy: 0.83 - ETA: 2s - loss: 0.3653 - accuracy: 0.83 - ETA: 2s - loss: 0.3683 - accuracy: 0.82 - ETA: 2s - loss: 0.3597 - accuracy: 0.83 - ETA: 2s - loss: 0.3537 - accuracy: 0.83 - ETA: 2s - loss: 0.3461 - accuracy: 0.84 - ETA: 2s - loss: 0.3438 - accuracy: 0.84 - ETA: 2s - loss: 0.3461 - accuracy: 0.84 - ETA: 2s - loss: 0.3456 - accuracy: 0.84 - ETA: 2s - loss: 0.3450 - accuracy: 0.84 - ETA: 2s - loss: 0.3429 - accuracy: 0.84 - ETA: 2s - loss: 0.3406 - accuracy: 0.84 - ETA: 2s - loss: 0.3400 - accuracy: 0.84 - ETA: 1s - loss: 0.3385 - accuracy: 0.84 - ETA: 1s - loss: 0.3381 - accuracy: 0.84 - ETA: 1s - loss: 0.3344 - accuracy: 0.84 - ETA: 1s - loss: 0.3349 - accuracy: 0.84 - ETA: 1s - loss: 0.3329 - accuracy: 0.84 - ETA: 1s - loss: 0.3304 - accuracy: 0.84 - ETA: 1s - loss: 0.3321 - accuracy: 0.84 - ETA: 1s - loss: 0.3312 - accuracy: 0.85 - ETA: 1s - loss: 0.3310 - accuracy: 0.85 - ETA: 1s - loss: 0.3312 - accuracy: 0.85 - ETA: 1s - loss: 0.3317 - accuracy: 0.85 - ETA: 1s - loss: 0.3305 - accuracy: 0.85 - ETA: 1s - loss: 0.3296 - accuracy: 0.85 - ETA: 0s - loss: 0.3281 - accuracy: 0.85 - ETA: 0s - loss: 0.3290 - accuracy: 0.85 - ETA: 0s - loss: 0.3266 - accuracy: 0.85 - ETA: 0s - loss: 0.3250 - accuracy: 0.85 - ETA: 0s - loss: 0.3252 - accuracy: 0.85 - ETA: 0s - loss: 0.3244 - accuracy: 0.85 - ETA: 0s - loss: 0.3251 - accuracy: 0.85 - ETA: 0s - loss: 0.3244 - accuracy: 0.85 - ETA: 0s - loss: 0.3249 - accuracy: 0.85 - ETA: 0s - loss: 0.3250 - accuracy: 0.85 - ETA: 0s - loss: 0.3251 - accuracy: 0.85 - ETA: 0s - loss: 0.3245 - accuracy: 0.85 - 3s 311us/step - loss: 0.3244 - accuracy: 0.8542 - val_loss: 0.3263 - val_accuracy: 0.8382\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.31344\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2656 - accuracy: 0.87 - ETA: 2s - loss: 0.3602 - accuracy: 0.82 - ETA: 2s - loss: 0.3523 - accuracy: 0.83 - ETA: 2s - loss: 0.3468 - accuracy: 0.83 - ETA: 2s - loss: 0.3476 - accuracy: 0.83 - ETA: 2s - loss: 0.3494 - accuracy: 0.84 - ETA: 2s - loss: 0.3433 - accuracy: 0.84 - ETA: 2s - loss: 0.3402 - accuracy: 0.84 - ETA: 2s - loss: 0.3422 - accuracy: 0.84 - ETA: 2s - loss: 0.3435 - accuracy: 0.84 - ETA: 2s - loss: 0.3417 - accuracy: 0.84 - ETA: 2s - loss: 0.3414 - accuracy: 0.84 - ETA: 2s - loss: 0.3390 - accuracy: 0.84 - ETA: 2s - loss: 0.3372 - accuracy: 0.84 - ETA: 1s - loss: 0.3363 - accuracy: 0.84 - ETA: 1s - loss: 0.3373 - accuracy: 0.84 - ETA: 1s - loss: 0.3384 - accuracy: 0.84 - ETA: 1s - loss: 0.3402 - accuracy: 0.84 - ETA: 1s - loss: 0.3377 - accuracy: 0.84 - ETA: 1s - loss: 0.3329 - accuracy: 0.84 - ETA: 1s - loss: 0.3295 - accuracy: 0.85 - ETA: 1s - loss: 0.3285 - accuracy: 0.85 - ETA: 1s - loss: 0.3277 - accuracy: 0.85 - ETA: 1s - loss: 0.3299 - accuracy: 0.85 - ETA: 1s - loss: 0.3295 - accuracy: 0.85 - ETA: 1s - loss: 0.3305 - accuracy: 0.85 - ETA: 1s - loss: 0.3313 - accuracy: 0.85 - ETA: 0s - loss: 0.3310 - accuracy: 0.85 - ETA: 0s - loss: 0.3321 - accuracy: 0.85 - ETA: 0s - loss: 0.3334 - accuracy: 0.85 - ETA: 0s - loss: 0.3321 - accuracy: 0.85 - ETA: 0s - loss: 0.3325 - accuracy: 0.85 - ETA: 0s - loss: 0.3319 - accuracy: 0.85 - ETA: 0s - loss: 0.3315 - accuracy: 0.85 - ETA: 0s - loss: 0.3301 - accuracy: 0.85 - ETA: 0s - loss: 0.3294 - accuracy: 0.85 - ETA: 0s - loss: 0.3320 - accuracy: 0.85 - ETA: 0s - loss: 0.3314 - accuracy: 0.85 - ETA: 0s - loss: 0.3299 - accuracy: 0.85 - ETA: 0s - loss: 0.3301 - accuracy: 0.85 - 3s 310us/step - loss: 0.3300 - accuracy: 0.8516 - val_loss: 0.3197 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.31344\n",
      "Epoch 00030: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 166us/step\n",
      "[2020-05-18 15:58:21 RAM63.8% 0.57GB] Val Score : [0.29353905472003483, 0.8680818676948547]\n",
      "[2020-05-18 15:58:22 RAM63.8% 0.57GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 15:58:22 RAM63.8% 0.57GB] Training on Fold : 3\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 31s - loss: 0.8566 - accuracy: 0.669 - ETA: 17s - loss: 0.8087 - accuracy: 0.600 - ETA: 12s - loss: 0.9882 - accuracy: 0.583 - ETA: 10s - loss: 0.8984 - accuracy: 0.606 - ETA: 7s - loss: 0.8139 - accuracy: 0.614 - ETA: 6s - loss: 0.7662 - accuracy: 0.62 - ETA: 5s - loss: 0.7368 - accuracy: 0.62 - ETA: 4s - loss: 0.7122 - accuracy: 0.63 - ETA: 4s - loss: 0.7028 - accuracy: 0.63 - ETA: 4s - loss: 0.6991 - accuracy: 0.63 - ETA: 3s - loss: 0.6838 - accuracy: 0.64 - ETA: 3s - loss: 0.6787 - accuracy: 0.64 - ETA: 3s - loss: 0.6724 - accuracy: 0.64 - ETA: 3s - loss: 0.6634 - accuracy: 0.65 - ETA: 3s - loss: 0.6572 - accuracy: 0.65 - ETA: 3s - loss: 0.6453 - accuracy: 0.66 - ETA: 2s - loss: 0.6363 - accuracy: 0.66 - ETA: 2s - loss: 0.6327 - accuracy: 0.67 - ETA: 2s - loss: 0.6291 - accuracy: 0.67 - ETA: 2s - loss: 0.6234 - accuracy: 0.68 - ETA: 2s - loss: 0.6207 - accuracy: 0.68 - ETA: 2s - loss: 0.6187 - accuracy: 0.68 - ETA: 1s - loss: 0.6159 - accuracy: 0.68 - ETA: 1s - loss: 0.6146 - accuracy: 0.68 - ETA: 1s - loss: 0.6135 - accuracy: 0.68 - ETA: 1s - loss: 0.6104 - accuracy: 0.68 - ETA: 1s - loss: 0.6097 - accuracy: 0.69 - ETA: 1s - loss: 0.6082 - accuracy: 0.69 - ETA: 1s - loss: 0.6055 - accuracy: 0.69 - ETA: 1s - loss: 0.6046 - accuracy: 0.69 - ETA: 1s - loss: 0.6037 - accuracy: 0.69 - ETA: 1s - loss: 0.6022 - accuracy: 0.69 - ETA: 0s - loss: 0.6013 - accuracy: 0.69 - ETA: 0s - loss: 0.5987 - accuracy: 0.70 - ETA: 0s - loss: 0.5983 - accuracy: 0.70 - ETA: 0s - loss: 0.5965 - accuracy: 0.70 - ETA: 0s - loss: 0.5946 - accuracy: 0.70 - ETA: 0s - loss: 0.5935 - accuracy: 0.70 - ETA: 0s - loss: 0.5937 - accuracy: 0.70 - ETA: 0s - loss: 0.5934 - accuracy: 0.70 - ETA: 0s - loss: 0.5932 - accuracy: 0.70 - ETA: 0s - loss: 0.5924 - accuracy: 0.70 - ETA: 0s - loss: 0.5911 - accuracy: 0.70 - ETA: 0s - loss: 0.5906 - accuracy: 0.70 - ETA: 0s - loss: 0.5898 - accuracy: 0.70 - ETA: 0s - loss: 0.5898 - accuracy: 0.70 - ETA: 0s - loss: 0.5889 - accuracy: 0.70 - ETA: 0s - loss: 0.5885 - accuracy: 0.70 - ETA: 0s - loss: 0.5876 - accuracy: 0.70 - 4s 381us/step - loss: 0.5873 - accuracy: 0.7064 - val_loss: 0.4982 - val_accuracy: 0.7961\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31344\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5326 - accuracy: 0.73 - ETA: 2s - loss: 0.5549 - accuracy: 0.72 - ETA: 2s - loss: 0.5536 - accuracy: 0.72 - ETA: 2s - loss: 0.5478 - accuracy: 0.73 - ETA: 2s - loss: 0.5390 - accuracy: 0.73 - ETA: 2s - loss: 0.5388 - accuracy: 0.73 - ETA: 2s - loss: 0.5370 - accuracy: 0.73 - ETA: 2s - loss: 0.5363 - accuracy: 0.74 - ETA: 2s - loss: 0.5415 - accuracy: 0.74 - ETA: 2s - loss: 0.5389 - accuracy: 0.74 - ETA: 2s - loss: 0.5399 - accuracy: 0.74 - ETA: 2s - loss: 0.5404 - accuracy: 0.74 - ETA: 2s - loss: 0.5372 - accuracy: 0.74 - ETA: 2s - loss: 0.5394 - accuracy: 0.74 - ETA: 2s - loss: 0.5385 - accuracy: 0.74 - ETA: 2s - loss: 0.5397 - accuracy: 0.74 - ETA: 1s - loss: 0.5385 - accuracy: 0.74 - ETA: 1s - loss: 0.5407 - accuracy: 0.74 - ETA: 1s - loss: 0.5376 - accuracy: 0.74 - ETA: 1s - loss: 0.5378 - accuracy: 0.74 - ETA: 1s - loss: 0.5364 - accuracy: 0.74 - ETA: 1s - loss: 0.5338 - accuracy: 0.74 - ETA: 1s - loss: 0.5344 - accuracy: 0.74 - ETA: 1s - loss: 0.5318 - accuracy: 0.74 - ETA: 1s - loss: 0.5351 - accuracy: 0.74 - ETA: 1s - loss: 0.5341 - accuracy: 0.74 - ETA: 1s - loss: 0.5341 - accuracy: 0.74 - ETA: 1s - loss: 0.5326 - accuracy: 0.74 - ETA: 1s - loss: 0.5339 - accuracy: 0.74 - ETA: 1s - loss: 0.5341 - accuracy: 0.74 - ETA: 1s - loss: 0.5335 - accuracy: 0.74 - ETA: 0s - loss: 0.5323 - accuracy: 0.74 - ETA: 0s - loss: 0.5318 - accuracy: 0.74 - ETA: 0s - loss: 0.5337 - accuracy: 0.74 - ETA: 0s - loss: 0.5343 - accuracy: 0.74 - ETA: 0s - loss: 0.5341 - accuracy: 0.74 - ETA: 0s - loss: 0.5337 - accuracy: 0.74 - ETA: 0s - loss: 0.5339 - accuracy: 0.74 - ETA: 0s - loss: 0.5334 - accuracy: 0.74 - ETA: 0s - loss: 0.5313 - accuracy: 0.75 - ETA: 0s - loss: 0.5306 - accuracy: 0.75 - ETA: 0s - loss: 0.5294 - accuracy: 0.75 - ETA: 0s - loss: 0.5284 - accuracy: 0.75 - ETA: 0s - loss: 0.5266 - accuracy: 0.75 - ETA: 0s - loss: 0.5251 - accuracy: 0.75 - 3s 326us/step - loss: 0.5240 - accuracy: 0.7561 - val_loss: 0.4625 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31344\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5237 - accuracy: 0.72 - ETA: 3s - loss: 0.5165 - accuracy: 0.74 - ETA: 3s - loss: 0.4896 - accuracy: 0.76 - ETA: 3s - loss: 0.5099 - accuracy: 0.75 - ETA: 3s - loss: 0.5074 - accuracy: 0.75 - ETA: 3s - loss: 0.5042 - accuracy: 0.75 - ETA: 3s - loss: 0.4978 - accuracy: 0.76 - ETA: 3s - loss: 0.5003 - accuracy: 0.76 - ETA: 2s - loss: 0.5018 - accuracy: 0.76 - ETA: 2s - loss: 0.5003 - accuracy: 0.76 - ETA: 2s - loss: 0.4998 - accuracy: 0.76 - ETA: 2s - loss: 0.5048 - accuracy: 0.75 - ETA: 2s - loss: 0.5082 - accuracy: 0.75 - ETA: 2s - loss: 0.5078 - accuracy: 0.75 - ETA: 2s - loss: 0.5059 - accuracy: 0.75 - ETA: 2s - loss: 0.5034 - accuracy: 0.75 - ETA: 2s - loss: 0.4990 - accuracy: 0.76 - ETA: 2s - loss: 0.5001 - accuracy: 0.76 - ETA: 2s - loss: 0.4961 - accuracy: 0.76 - ETA: 2s - loss: 0.4953 - accuracy: 0.76 - ETA: 1s - loss: 0.4964 - accuracy: 0.76 - ETA: 1s - loss: 0.4955 - accuracy: 0.76 - ETA: 1s - loss: 0.4941 - accuracy: 0.76 - ETA: 1s - loss: 0.4951 - accuracy: 0.76 - ETA: 1s - loss: 0.4971 - accuracy: 0.76 - ETA: 1s - loss: 0.4983 - accuracy: 0.76 - ETA: 1s - loss: 0.4948 - accuracy: 0.76 - ETA: 1s - loss: 0.4942 - accuracy: 0.76 - ETA: 1s - loss: 0.4935 - accuracy: 0.76 - ETA: 1s - loss: 0.4926 - accuracy: 0.76 - ETA: 1s - loss: 0.4920 - accuracy: 0.76 - ETA: 0s - loss: 0.4922 - accuracy: 0.76 - ETA: 0s - loss: 0.4915 - accuracy: 0.76 - ETA: 0s - loss: 0.4923 - accuracy: 0.76 - ETA: 0s - loss: 0.4922 - accuracy: 0.76 - ETA: 0s - loss: 0.4926 - accuracy: 0.76 - ETA: 0s - loss: 0.4929 - accuracy: 0.76 - ETA: 0s - loss: 0.4915 - accuracy: 0.76 - ETA: 0s - loss: 0.4933 - accuracy: 0.76 - ETA: 0s - loss: 0.4947 - accuracy: 0.76 - ETA: 0s - loss: 0.4940 - accuracy: 0.76 - ETA: 0s - loss: 0.4944 - accuracy: 0.76 - ETA: 0s - loss: 0.4934 - accuracy: 0.76 - ETA: 0s - loss: 0.4940 - accuracy: 0.76 - 4s 328us/step - loss: 0.4939 - accuracy: 0.7674 - val_loss: 0.4291 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31344\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4601 - accuracy: 0.73 - ETA: 3s - loss: 0.4773 - accuracy: 0.74 - ETA: 3s - loss: 0.4672 - accuracy: 0.75 - ETA: 3s - loss: 0.4815 - accuracy: 0.75 - ETA: 3s - loss: 0.4835 - accuracy: 0.75 - ETA: 3s - loss: 0.4894 - accuracy: 0.76 - ETA: 3s - loss: 0.4871 - accuracy: 0.76 - ETA: 3s - loss: 0.4905 - accuracy: 0.76 - ETA: 2s - loss: 0.4990 - accuracy: 0.76 - ETA: 2s - loss: 0.4939 - accuracy: 0.76 - ETA: 2s - loss: 0.4944 - accuracy: 0.76 - ETA: 2s - loss: 0.4963 - accuracy: 0.76 - ETA: 2s - loss: 0.4929 - accuracy: 0.76 - ETA: 2s - loss: 0.4894 - accuracy: 0.76 - ETA: 2s - loss: 0.4949 - accuracy: 0.76 - ETA: 2s - loss: 0.4943 - accuracy: 0.76 - ETA: 2s - loss: 0.4950 - accuracy: 0.76 - ETA: 2s - loss: 0.4922 - accuracy: 0.76 - ETA: 2s - loss: 0.4948 - accuracy: 0.76 - ETA: 2s - loss: 0.4943 - accuracy: 0.76 - ETA: 1s - loss: 0.4928 - accuracy: 0.76 - ETA: 1s - loss: 0.4926 - accuracy: 0.76 - ETA: 1s - loss: 0.4895 - accuracy: 0.76 - ETA: 1s - loss: 0.4925 - accuracy: 0.76 - ETA: 1s - loss: 0.4928 - accuracy: 0.76 - ETA: 1s - loss: 0.4925 - accuracy: 0.76 - ETA: 1s - loss: 0.4909 - accuracy: 0.76 - ETA: 1s - loss: 0.4898 - accuracy: 0.76 - ETA: 1s - loss: 0.4878 - accuracy: 0.77 - ETA: 1s - loss: 0.4872 - accuracy: 0.77 - ETA: 1s - loss: 0.4876 - accuracy: 0.77 - ETA: 0s - loss: 0.4875 - accuracy: 0.77 - ETA: 0s - loss: 0.4855 - accuracy: 0.77 - ETA: 0s - loss: 0.4853 - accuracy: 0.77 - ETA: 0s - loss: 0.4856 - accuracy: 0.77 - ETA: 0s - loss: 0.4856 - accuracy: 0.77 - ETA: 0s - loss: 0.4857 - accuracy: 0.77 - ETA: 0s - loss: 0.4860 - accuracy: 0.77 - ETA: 0s - loss: 0.4865 - accuracy: 0.77 - ETA: 0s - loss: 0.4850 - accuracy: 0.77 - ETA: 0s - loss: 0.4854 - accuracy: 0.77 - ETA: 0s - loss: 0.4851 - accuracy: 0.77 - ETA: 0s - loss: 0.4817 - accuracy: 0.77 - ETA: 0s - loss: 0.4822 - accuracy: 0.77 - ETA: 0s - loss: 0.4811 - accuracy: 0.77 - ETA: 0s - loss: 0.4819 - accuracy: 0.77 - 4s 329us/step - loss: 0.4823 - accuracy: 0.7748 - val_loss: 0.4376 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31344\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4534 - accuracy: 0.77 - ETA: 3s - loss: 0.4591 - accuracy: 0.78 - ETA: 3s - loss: 0.4625 - accuracy: 0.78 - ETA: 3s - loss: 0.4706 - accuracy: 0.78 - ETA: 3s - loss: 0.4711 - accuracy: 0.78 - ETA: 2s - loss: 0.4662 - accuracy: 0.78 - ETA: 2s - loss: 0.4656 - accuracy: 0.78 - ETA: 2s - loss: 0.4800 - accuracy: 0.77 - ETA: 2s - loss: 0.4708 - accuracy: 0.77 - ETA: 2s - loss: 0.4763 - accuracy: 0.77 - ETA: 2s - loss: 0.4760 - accuracy: 0.77 - ETA: 2s - loss: 0.4753 - accuracy: 0.77 - ETA: 2s - loss: 0.4769 - accuracy: 0.77 - ETA: 2s - loss: 0.4783 - accuracy: 0.77 - ETA: 2s - loss: 0.4787 - accuracy: 0.77 - ETA: 2s - loss: 0.4768 - accuracy: 0.77 - ETA: 2s - loss: 0.4746 - accuracy: 0.77 - ETA: 2s - loss: 0.4736 - accuracy: 0.77 - ETA: 2s - loss: 0.4740 - accuracy: 0.78 - ETA: 2s - loss: 0.4697 - accuracy: 0.78 - ETA: 2s - loss: 0.4688 - accuracy: 0.78 - ETA: 1s - loss: 0.4697 - accuracy: 0.78 - ETA: 1s - loss: 0.4710 - accuracy: 0.78 - ETA: 1s - loss: 0.4707 - accuracy: 0.78 - ETA: 1s - loss: 0.4695 - accuracy: 0.78 - ETA: 1s - loss: 0.4715 - accuracy: 0.78 - ETA: 1s - loss: 0.4707 - accuracy: 0.78 - ETA: 1s - loss: 0.4701 - accuracy: 0.78 - ETA: 1s - loss: 0.4709 - accuracy: 0.78 - ETA: 1s - loss: 0.4703 - accuracy: 0.78 - ETA: 1s - loss: 0.4724 - accuracy: 0.78 - ETA: 1s - loss: 0.4709 - accuracy: 0.78 - ETA: 1s - loss: 0.4685 - accuracy: 0.78 - ETA: 1s - loss: 0.4692 - accuracy: 0.78 - ETA: 1s - loss: 0.4671 - accuracy: 0.78 - ETA: 0s - loss: 0.4660 - accuracy: 0.78 - ETA: 0s - loss: 0.4660 - accuracy: 0.78 - ETA: 0s - loss: 0.4673 - accuracy: 0.78 - ETA: 0s - loss: 0.4669 - accuracy: 0.78 - ETA: 0s - loss: 0.4676 - accuracy: 0.78 - ETA: 0s - loss: 0.4647 - accuracy: 0.78 - ETA: 0s - loss: 0.4652 - accuracy: 0.78 - ETA: 0s - loss: 0.4650 - accuracy: 0.78 - ETA: 0s - loss: 0.4665 - accuracy: 0.78 - ETA: 0s - loss: 0.4674 - accuracy: 0.78 - ETA: 0s - loss: 0.4664 - accuracy: 0.78 - ETA: 0s - loss: 0.4665 - accuracy: 0.78 - ETA: 0s - loss: 0.4671 - accuracy: 0.78 - ETA: 0s - loss: 0.4680 - accuracy: 0.78 - 4s 339us/step - loss: 0.4680 - accuracy: 0.7838 - val_loss: 0.4388 - val_accuracy: 0.8096\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31344\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5339 - accuracy: 0.74 - ETA: 3s - loss: 0.5193 - accuracy: 0.74 - ETA: 3s - loss: 0.4941 - accuracy: 0.76 - ETA: 2s - loss: 0.4748 - accuracy: 0.77 - ETA: 2s - loss: 0.4770 - accuracy: 0.78 - ETA: 2s - loss: 0.4771 - accuracy: 0.78 - ETA: 2s - loss: 0.4804 - accuracy: 0.77 - ETA: 2s - loss: 0.4771 - accuracy: 0.78 - ETA: 2s - loss: 0.4755 - accuracy: 0.78 - ETA: 2s - loss: 0.4787 - accuracy: 0.78 - ETA: 2s - loss: 0.4765 - accuracy: 0.78 - ETA: 2s - loss: 0.4727 - accuracy: 0.78 - ETA: 2s - loss: 0.4673 - accuracy: 0.79 - ETA: 2s - loss: 0.4667 - accuracy: 0.79 - ETA: 2s - loss: 0.4598 - accuracy: 0.79 - ETA: 2s - loss: 0.4619 - accuracy: 0.79 - ETA: 2s - loss: 0.4602 - accuracy: 0.79 - ETA: 1s - loss: 0.4554 - accuracy: 0.79 - ETA: 1s - loss: 0.4562 - accuracy: 0.79 - ETA: 1s - loss: 0.4589 - accuracy: 0.79 - ETA: 1s - loss: 0.4591 - accuracy: 0.79 - ETA: 1s - loss: 0.4582 - accuracy: 0.79 - ETA: 1s - loss: 0.4571 - accuracy: 0.79 - ETA: 1s - loss: 0.4562 - accuracy: 0.79 - ETA: 1s - loss: 0.4563 - accuracy: 0.79 - ETA: 1s - loss: 0.4556 - accuracy: 0.79 - ETA: 1s - loss: 0.4536 - accuracy: 0.79 - ETA: 1s - loss: 0.4524 - accuracy: 0.79 - ETA: 1s - loss: 0.4517 - accuracy: 0.79 - ETA: 1s - loss: 0.4520 - accuracy: 0.79 - ETA: 0s - loss: 0.4528 - accuracy: 0.79 - ETA: 0s - loss: 0.4545 - accuracy: 0.79 - ETA: 0s - loss: 0.4547 - accuracy: 0.79 - ETA: 0s - loss: 0.4541 - accuracy: 0.79 - ETA: 0s - loss: 0.4545 - accuracy: 0.79 - ETA: 0s - loss: 0.4539 - accuracy: 0.79 - ETA: 0s - loss: 0.4541 - accuracy: 0.79 - ETA: 0s - loss: 0.4529 - accuracy: 0.79 - ETA: 0s - loss: 0.4530 - accuracy: 0.79 - ETA: 0s - loss: 0.4536 - accuracy: 0.79 - ETA: 0s - loss: 0.4542 - accuracy: 0.79 - ETA: 0s - loss: 0.4536 - accuracy: 0.79 - ETA: 0s - loss: 0.4537 - accuracy: 0.79 - ETA: 0s - loss: 0.4538 - accuracy: 0.79 - 4s 329us/step - loss: 0.4534 - accuracy: 0.7944 - val_loss: 0.4409 - val_accuracy: 0.7953\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31344\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3781 - accuracy: 0.85 - ETA: 3s - loss: 0.4536 - accuracy: 0.80 - ETA: 3s - loss: 0.4541 - accuracy: 0.80 - ETA: 3s - loss: 0.4526 - accuracy: 0.80 - ETA: 3s - loss: 0.4517 - accuracy: 0.80 - ETA: 2s - loss: 0.4508 - accuracy: 0.80 - ETA: 2s - loss: 0.4507 - accuracy: 0.80 - ETA: 2s - loss: 0.4526 - accuracy: 0.80 - ETA: 2s - loss: 0.4488 - accuracy: 0.80 - ETA: 2s - loss: 0.4510 - accuracy: 0.80 - ETA: 2s - loss: 0.4477 - accuracy: 0.80 - ETA: 2s - loss: 0.4485 - accuracy: 0.79 - ETA: 2s - loss: 0.4537 - accuracy: 0.79 - ETA: 2s - loss: 0.4505 - accuracy: 0.79 - ETA: 2s - loss: 0.4540 - accuracy: 0.79 - ETA: 2s - loss: 0.4539 - accuracy: 0.79 - ETA: 2s - loss: 0.4547 - accuracy: 0.79 - ETA: 2s - loss: 0.4549 - accuracy: 0.79 - ETA: 1s - loss: 0.4452 - accuracy: 0.79 - ETA: 1s - loss: 0.4400 - accuracy: 0.79 - ETA: 1s - loss: 0.4411 - accuracy: 0.79 - ETA: 1s - loss: 0.4445 - accuracy: 0.79 - ETA: 1s - loss: 0.4454 - accuracy: 0.79 - ETA: 1s - loss: 0.4442 - accuracy: 0.79 - ETA: 1s - loss: 0.4440 - accuracy: 0.79 - ETA: 1s - loss: 0.4443 - accuracy: 0.79 - ETA: 1s - loss: 0.4432 - accuracy: 0.79 - ETA: 1s - loss: 0.4417 - accuracy: 0.79 - ETA: 1s - loss: 0.4441 - accuracy: 0.79 - ETA: 1s - loss: 0.4427 - accuracy: 0.79 - ETA: 1s - loss: 0.4421 - accuracy: 0.79 - ETA: 1s - loss: 0.4455 - accuracy: 0.79 - ETA: 0s - loss: 0.4450 - accuracy: 0.79 - ETA: 0s - loss: 0.4446 - accuracy: 0.79 - ETA: 0s - loss: 0.4448 - accuracy: 0.79 - ETA: 0s - loss: 0.4450 - accuracy: 0.79 - ETA: 0s - loss: 0.4466 - accuracy: 0.79 - ETA: 0s - loss: 0.4462 - accuracy: 0.79 - ETA: 0s - loss: 0.4447 - accuracy: 0.79 - ETA: 0s - loss: 0.4438 - accuracy: 0.79 - ETA: 0s - loss: 0.4431 - accuracy: 0.79 - ETA: 0s - loss: 0.4440 - accuracy: 0.79 - ETA: 0s - loss: 0.4434 - accuracy: 0.79 - ETA: 0s - loss: 0.4416 - accuracy: 0.79 - ETA: 0s - loss: 0.4436 - accuracy: 0.79 - 3s 326us/step - loss: 0.4438 - accuracy: 0.7975 - val_loss: 0.4240 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31344\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4564 - accuracy: 0.77 - ETA: 3s - loss: 0.4519 - accuracy: 0.76 - ETA: 2s - loss: 0.4407 - accuracy: 0.79 - ETA: 2s - loss: 0.4317 - accuracy: 0.80 - ETA: 2s - loss: 0.4252 - accuracy: 0.80 - ETA: 2s - loss: 0.4160 - accuracy: 0.81 - ETA: 2s - loss: 0.4178 - accuracy: 0.81 - ETA: 2s - loss: 0.4268 - accuracy: 0.80 - ETA: 2s - loss: 0.4322 - accuracy: 0.80 - ETA: 2s - loss: 0.4298 - accuracy: 0.80 - ETA: 2s - loss: 0.4263 - accuracy: 0.80 - ETA: 2s - loss: 0.4298 - accuracy: 0.80 - ETA: 2s - loss: 0.4314 - accuracy: 0.80 - ETA: 2s - loss: 0.4301 - accuracy: 0.80 - ETA: 2s - loss: 0.4353 - accuracy: 0.80 - ETA: 2s - loss: 0.4366 - accuracy: 0.80 - ETA: 2s - loss: 0.4396 - accuracy: 0.80 - ETA: 2s - loss: 0.4388 - accuracy: 0.80 - ETA: 2s - loss: 0.4406 - accuracy: 0.80 - ETA: 2s - loss: 0.4399 - accuracy: 0.80 - ETA: 1s - loss: 0.4430 - accuracy: 0.79 - ETA: 1s - loss: 0.4432 - accuracy: 0.79 - ETA: 1s - loss: 0.4447 - accuracy: 0.79 - ETA: 1s - loss: 0.4441 - accuracy: 0.79 - ETA: 1s - loss: 0.4413 - accuracy: 0.79 - ETA: 1s - loss: 0.4427 - accuracy: 0.79 - ETA: 1s - loss: 0.4414 - accuracy: 0.79 - ETA: 1s - loss: 0.4440 - accuracy: 0.79 - ETA: 1s - loss: 0.4449 - accuracy: 0.79 - ETA: 1s - loss: 0.4444 - accuracy: 0.79 - ETA: 1s - loss: 0.4441 - accuracy: 0.79 - ETA: 1s - loss: 0.4438 - accuracy: 0.79 - ETA: 0s - loss: 0.4426 - accuracy: 0.79 - ETA: 0s - loss: 0.4410 - accuracy: 0.79 - ETA: 0s - loss: 0.4413 - accuracy: 0.79 - ETA: 0s - loss: 0.4400 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4383 - accuracy: 0.80 - ETA: 0s - loss: 0.4379 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4387 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4393 - accuracy: 0.80 - ETA: 0s - loss: 0.4387 - accuracy: 0.80 - 4s 329us/step - loss: 0.4385 - accuracy: 0.8012 - val_loss: 0.3779 - val_accuracy: 0.8340\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31344\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4056 - accuracy: 0.79 - ETA: 3s - loss: 0.3642 - accuracy: 0.83 - ETA: 3s - loss: 0.3707 - accuracy: 0.83 - ETA: 3s - loss: 0.3772 - accuracy: 0.83 - ETA: 3s - loss: 0.3897 - accuracy: 0.82 - ETA: 3s - loss: 0.4282 - accuracy: 0.80 - ETA: 2s - loss: 0.4274 - accuracy: 0.81 - ETA: 2s - loss: 0.4248 - accuracy: 0.80 - ETA: 2s - loss: 0.4239 - accuracy: 0.80 - ETA: 2s - loss: 0.4270 - accuracy: 0.80 - ETA: 2s - loss: 0.4209 - accuracy: 0.81 - ETA: 2s - loss: 0.4153 - accuracy: 0.81 - ETA: 2s - loss: 0.4156 - accuracy: 0.81 - ETA: 2s - loss: 0.4169 - accuracy: 0.81 - ETA: 2s - loss: 0.4215 - accuracy: 0.81 - ETA: 2s - loss: 0.4257 - accuracy: 0.81 - ETA: 2s - loss: 0.4252 - accuracy: 0.81 - ETA: 2s - loss: 0.4258 - accuracy: 0.81 - ETA: 2s - loss: 0.4280 - accuracy: 0.81 - ETA: 2s - loss: 0.4296 - accuracy: 0.81 - ETA: 2s - loss: 0.4351 - accuracy: 0.80 - ETA: 2s - loss: 0.4352 - accuracy: 0.80 - ETA: 2s - loss: 0.4352 - accuracy: 0.80 - ETA: 2s - loss: 0.4371 - accuracy: 0.80 - ETA: 2s - loss: 0.4364 - accuracy: 0.80 - ETA: 1s - loss: 0.4342 - accuracy: 0.80 - ETA: 1s - loss: 0.4339 - accuracy: 0.80 - ETA: 1s - loss: 0.4335 - accuracy: 0.80 - ETA: 1s - loss: 0.4335 - accuracy: 0.80 - ETA: 1s - loss: 0.4309 - accuracy: 0.80 - ETA: 1s - loss: 0.4274 - accuracy: 0.80 - ETA: 1s - loss: 0.4275 - accuracy: 0.80 - ETA: 1s - loss: 0.4270 - accuracy: 0.80 - ETA: 1s - loss: 0.4285 - accuracy: 0.80 - ETA: 1s - loss: 0.4311 - accuracy: 0.80 - ETA: 1s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4311 - accuracy: 0.80 - ETA: 0s - loss: 0.4327 - accuracy: 0.80 - ETA: 0s - loss: 0.4337 - accuracy: 0.80 - ETA: 0s - loss: 0.4331 - accuracy: 0.80 - ETA: 0s - loss: 0.4301 - accuracy: 0.80 - ETA: 0s - loss: 0.4326 - accuracy: 0.80 - ETA: 0s - loss: 0.4322 - accuracy: 0.80 - ETA: 0s - loss: 0.4316 - accuracy: 0.80 - ETA: 0s - loss: 0.4308 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.4314 - accuracy: 0.80 - 4s 341us/step - loss: 0.4313 - accuracy: 0.8057 - val_loss: 0.4052 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31344\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4330 - accuracy: 0.77 - ETA: 2s - loss: 0.4034 - accuracy: 0.81 - ETA: 2s - loss: 0.3865 - accuracy: 0.82 - ETA: 2s - loss: 0.3914 - accuracy: 0.82 - ETA: 2s - loss: 0.4137 - accuracy: 0.81 - ETA: 2s - loss: 0.4242 - accuracy: 0.80 - ETA: 2s - loss: 0.4159 - accuracy: 0.80 - ETA: 2s - loss: 0.4300 - accuracy: 0.80 - ETA: 2s - loss: 0.4279 - accuracy: 0.80 - ETA: 2s - loss: 0.4241 - accuracy: 0.80 - ETA: 2s - loss: 0.4222 - accuracy: 0.80 - ETA: 2s - loss: 0.4276 - accuracy: 0.80 - ETA: 2s - loss: 0.4298 - accuracy: 0.80 - ETA: 2s - loss: 0.4268 - accuracy: 0.80 - ETA: 2s - loss: 0.4266 - accuracy: 0.80 - ETA: 2s - loss: 0.4221 - accuracy: 0.80 - ETA: 2s - loss: 0.4230 - accuracy: 0.80 - ETA: 1s - loss: 0.4277 - accuracy: 0.80 - ETA: 1s - loss: 0.4285 - accuracy: 0.80 - ETA: 1s - loss: 0.4284 - accuracy: 0.80 - ETA: 1s - loss: 0.4268 - accuracy: 0.80 - ETA: 1s - loss: 0.4324 - accuracy: 0.80 - ETA: 1s - loss: 0.4308 - accuracy: 0.80 - ETA: 1s - loss: 0.4287 - accuracy: 0.80 - ETA: 1s - loss: 0.4270 - accuracy: 0.80 - ETA: 1s - loss: 0.4286 - accuracy: 0.80 - ETA: 1s - loss: 0.4270 - accuracy: 0.80 - ETA: 1s - loss: 0.4263 - accuracy: 0.80 - ETA: 1s - loss: 0.4253 - accuracy: 0.80 - ETA: 1s - loss: 0.4246 - accuracy: 0.80 - ETA: 0s - loss: 0.4253 - accuracy: 0.80 - ETA: 0s - loss: 0.4259 - accuracy: 0.80 - ETA: 0s - loss: 0.4277 - accuracy: 0.80 - ETA: 0s - loss: 0.4265 - accuracy: 0.80 - ETA: 0s - loss: 0.4265 - accuracy: 0.80 - ETA: 0s - loss: 0.4263 - accuracy: 0.80 - ETA: 0s - loss: 0.4260 - accuracy: 0.80 - ETA: 0s - loss: 0.4259 - accuracy: 0.80 - ETA: 0s - loss: 0.4272 - accuracy: 0.80 - ETA: 0s - loss: 0.4263 - accuracy: 0.80 - ETA: 0s - loss: 0.4270 - accuracy: 0.80 - ETA: 0s - loss: 0.4275 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4267 - accuracy: 0.80 - 3s 320us/step - loss: 0.4265 - accuracy: 0.8073 - val_loss: 0.4089 - val_accuracy: 0.8222\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31344\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3398 - accuracy: 0.83 - ETA: 2s - loss: 0.3686 - accuracy: 0.83 - ETA: 2s - loss: 0.4015 - accuracy: 0.82 - ETA: 2s - loss: 0.4002 - accuracy: 0.82 - ETA: 2s - loss: 0.4008 - accuracy: 0.83 - ETA: 2s - loss: 0.4152 - accuracy: 0.82 - ETA: 2s - loss: 0.4171 - accuracy: 0.81 - ETA: 2s - loss: 0.4193 - accuracy: 0.81 - ETA: 2s - loss: 0.4204 - accuracy: 0.81 - ETA: 2s - loss: 0.4139 - accuracy: 0.81 - ETA: 2s - loss: 0.4136 - accuracy: 0.81 - ETA: 2s - loss: 0.4147 - accuracy: 0.81 - ETA: 2s - loss: 0.4152 - accuracy: 0.81 - ETA: 2s - loss: 0.4160 - accuracy: 0.81 - ETA: 2s - loss: 0.4160 - accuracy: 0.81 - ETA: 2s - loss: 0.4182 - accuracy: 0.81 - ETA: 2s - loss: 0.4209 - accuracy: 0.81 - ETA: 1s - loss: 0.4248 - accuracy: 0.81 - ETA: 1s - loss: 0.4233 - accuracy: 0.81 - ETA: 1s - loss: 0.4227 - accuracy: 0.81 - ETA: 1s - loss: 0.4233 - accuracy: 0.81 - ETA: 1s - loss: 0.4223 - accuracy: 0.81 - ETA: 1s - loss: 0.4227 - accuracy: 0.81 - ETA: 1s - loss: 0.4209 - accuracy: 0.81 - ETA: 1s - loss: 0.4205 - accuracy: 0.81 - ETA: 1s - loss: 0.4207 - accuracy: 0.81 - ETA: 1s - loss: 0.4207 - accuracy: 0.81 - ETA: 1s - loss: 0.4212 - accuracy: 0.81 - ETA: 1s - loss: 0.4224 - accuracy: 0.81 - ETA: 1s - loss: 0.4231 - accuracy: 0.80 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - ETA: 0s - loss: 0.4260 - accuracy: 0.80 - ETA: 0s - loss: 0.4236 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - ETA: 0s - loss: 0.4226 - accuracy: 0.80 - ETA: 0s - loss: 0.4226 - accuracy: 0.80 - ETA: 0s - loss: 0.4211 - accuracy: 0.81 - ETA: 0s - loss: 0.4216 - accuracy: 0.80 - ETA: 0s - loss: 0.4210 - accuracy: 0.81 - ETA: 0s - loss: 0.4195 - accuracy: 0.81 - ETA: 0s - loss: 0.4198 - accuracy: 0.81 - ETA: 0s - loss: 0.4200 - accuracy: 0.81 - ETA: 0s - loss: 0.4195 - accuracy: 0.81 - ETA: 0s - loss: 0.4204 - accuracy: 0.81 - 3s 312us/step - loss: 0.4205 - accuracy: 0.8109 - val_loss: 0.3945 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31344\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4452 - accuracy: 0.82 - ETA: 2s - loss: 0.4326 - accuracy: 0.80 - ETA: 2s - loss: 0.4094 - accuracy: 0.81 - ETA: 2s - loss: 0.4226 - accuracy: 0.80 - ETA: 2s - loss: 0.4311 - accuracy: 0.80 - ETA: 2s - loss: 0.4300 - accuracy: 0.80 - ETA: 2s - loss: 0.4247 - accuracy: 0.81 - ETA: 2s - loss: 0.4287 - accuracy: 0.80 - ETA: 2s - loss: 0.4345 - accuracy: 0.80 - ETA: 2s - loss: 0.4335 - accuracy: 0.80 - ETA: 2s - loss: 0.4326 - accuracy: 0.80 - ETA: 2s - loss: 0.4264 - accuracy: 0.80 - ETA: 2s - loss: 0.4271 - accuracy: 0.80 - ETA: 2s - loss: 0.4223 - accuracy: 0.81 - ETA: 2s - loss: 0.4278 - accuracy: 0.80 - ETA: 2s - loss: 0.4255 - accuracy: 0.80 - ETA: 1s - loss: 0.4245 - accuracy: 0.80 - ETA: 1s - loss: 0.4248 - accuracy: 0.81 - ETA: 1s - loss: 0.4213 - accuracy: 0.81 - ETA: 1s - loss: 0.4216 - accuracy: 0.81 - ETA: 1s - loss: 0.4210 - accuracy: 0.81 - ETA: 1s - loss: 0.4214 - accuracy: 0.81 - ETA: 1s - loss: 0.4207 - accuracy: 0.81 - ETA: 1s - loss: 0.4199 - accuracy: 0.81 - ETA: 1s - loss: 0.4194 - accuracy: 0.81 - ETA: 1s - loss: 0.4198 - accuracy: 0.81 - ETA: 1s - loss: 0.4193 - accuracy: 0.81 - ETA: 1s - loss: 0.4206 - accuracy: 0.81 - ETA: 1s - loss: 0.4207 - accuracy: 0.80 - ETA: 0s - loss: 0.4221 - accuracy: 0.80 - ETA: 0s - loss: 0.4213 - accuracy: 0.81 - ETA: 0s - loss: 0.4218 - accuracy: 0.81 - ETA: 0s - loss: 0.4210 - accuracy: 0.80 - ETA: 0s - loss: 0.4218 - accuracy: 0.81 - ETA: 0s - loss: 0.4203 - accuracy: 0.81 - ETA: 0s - loss: 0.4190 - accuracy: 0.81 - ETA: 0s - loss: 0.4177 - accuracy: 0.81 - ETA: 0s - loss: 0.4177 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - ETA: 0s - loss: 0.4167 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4157 - accuracy: 0.81 - 3s 310us/step - loss: 0.4147 - accuracy: 0.8118 - val_loss: 0.3693 - val_accuracy: 0.8366\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: val_loss did not improve from 0.31344\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5084 - accuracy: 0.78 - ETA: 2s - loss: 0.4255 - accuracy: 0.80 - ETA: 2s - loss: 0.4163 - accuracy: 0.81 - ETA: 2s - loss: 0.4110 - accuracy: 0.81 - ETA: 2s - loss: 0.4100 - accuracy: 0.81 - ETA: 2s - loss: 0.4125 - accuracy: 0.81 - ETA: 2s - loss: 0.4046 - accuracy: 0.81 - ETA: 2s - loss: 0.4102 - accuracy: 0.81 - ETA: 2s - loss: 0.4070 - accuracy: 0.81 - ETA: 2s - loss: 0.4098 - accuracy: 0.81 - ETA: 2s - loss: 0.4097 - accuracy: 0.81 - ETA: 2s - loss: 0.4110 - accuracy: 0.81 - ETA: 2s - loss: 0.4094 - accuracy: 0.81 - ETA: 2s - loss: 0.4096 - accuracy: 0.81 - ETA: 2s - loss: 0.4106 - accuracy: 0.81 - ETA: 1s - loss: 0.4114 - accuracy: 0.81 - ETA: 1s - loss: 0.4097 - accuracy: 0.81 - ETA: 1s - loss: 0.4100 - accuracy: 0.81 - ETA: 1s - loss: 0.4107 - accuracy: 0.81 - ETA: 1s - loss: 0.4117 - accuracy: 0.81 - ETA: 1s - loss: 0.4100 - accuracy: 0.81 - ETA: 1s - loss: 0.4096 - accuracy: 0.81 - ETA: 1s - loss: 0.4107 - accuracy: 0.81 - ETA: 1s - loss: 0.4111 - accuracy: 0.81 - ETA: 1s - loss: 0.4105 - accuracy: 0.81 - ETA: 1s - loss: 0.4104 - accuracy: 0.81 - ETA: 1s - loss: 0.4094 - accuracy: 0.81 - ETA: 1s - loss: 0.4088 - accuracy: 0.81 - ETA: 1s - loss: 0.4097 - accuracy: 0.81 - ETA: 0s - loss: 0.4101 - accuracy: 0.81 - ETA: 0s - loss: 0.4105 - accuracy: 0.81 - ETA: 0s - loss: 0.4097 - accuracy: 0.81 - ETA: 0s - loss: 0.4092 - accuracy: 0.81 - ETA: 0s - loss: 0.4102 - accuracy: 0.81 - ETA: 0s - loss: 0.4100 - accuracy: 0.81 - ETA: 0s - loss: 0.4112 - accuracy: 0.81 - ETA: 0s - loss: 0.4104 - accuracy: 0.81 - ETA: 0s - loss: 0.4110 - accuracy: 0.81 - ETA: 0s - loss: 0.4125 - accuracy: 0.81 - ETA: 0s - loss: 0.4127 - accuracy: 0.81 - ETA: 0s - loss: 0.4131 - accuracy: 0.81 - ETA: 0s - loss: 0.4136 - accuracy: 0.81 - ETA: 0s - loss: 0.4136 - accuracy: 0.81 - 3s 316us/step - loss: 0.4133 - accuracy: 0.8114 - val_loss: 0.4218 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31344\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3939 - accuracy: 0.82 - ETA: 2s - loss: 0.4100 - accuracy: 0.81 - ETA: 2s - loss: 0.3875 - accuracy: 0.82 - ETA: 2s - loss: 0.4002 - accuracy: 0.81 - ETA: 2s - loss: 0.3939 - accuracy: 0.82 - ETA: 2s - loss: 0.4072 - accuracy: 0.81 - ETA: 2s - loss: 0.3992 - accuracy: 0.82 - ETA: 2s - loss: 0.3959 - accuracy: 0.82 - ETA: 2s - loss: 0.3959 - accuracy: 0.82 - ETA: 2s - loss: 0.3998 - accuracy: 0.81 - ETA: 2s - loss: 0.4068 - accuracy: 0.81 - ETA: 2s - loss: 0.4072 - accuracy: 0.81 - ETA: 2s - loss: 0.4103 - accuracy: 0.81 - ETA: 2s - loss: 0.4082 - accuracy: 0.81 - ETA: 2s - loss: 0.4047 - accuracy: 0.81 - ETA: 1s - loss: 0.4022 - accuracy: 0.81 - ETA: 1s - loss: 0.4026 - accuracy: 0.81 - ETA: 1s - loss: 0.3995 - accuracy: 0.81 - ETA: 1s - loss: 0.4012 - accuracy: 0.81 - ETA: 1s - loss: 0.4008 - accuracy: 0.81 - ETA: 1s - loss: 0.4013 - accuracy: 0.81 - ETA: 1s - loss: 0.4052 - accuracy: 0.81 - ETA: 1s - loss: 0.4041 - accuracy: 0.81 - ETA: 1s - loss: 0.4023 - accuracy: 0.81 - ETA: 1s - loss: 0.4005 - accuracy: 0.82 - ETA: 1s - loss: 0.4009 - accuracy: 0.82 - ETA: 1s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.4008 - accuracy: 0.81 - ETA: 0s - loss: 0.4016 - accuracy: 0.81 - ETA: 0s - loss: 0.4008 - accuracy: 0.81 - ETA: 0s - loss: 0.4008 - accuracy: 0.82 - ETA: 0s - loss: 0.4014 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.4017 - accuracy: 0.82 - ETA: 0s - loss: 0.4015 - accuracy: 0.82 - ETA: 0s - loss: 0.4037 - accuracy: 0.81 - ETA: 0s - loss: 0.4050 - accuracy: 0.81 - ETA: 0s - loss: 0.4053 - accuracy: 0.81 - ETA: 0s - loss: 0.4058 - accuracy: 0.81 - ETA: 0s - loss: 0.4061 - accuracy: 0.81 - 3s 312us/step - loss: 0.4058 - accuracy: 0.8187 - val_loss: 0.4011 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31344\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4195 - accuracy: 0.82 - ETA: 2s - loss: 0.4548 - accuracy: 0.77 - ETA: 2s - loss: 0.4228 - accuracy: 0.79 - ETA: 2s - loss: 0.4073 - accuracy: 0.81 - ETA: 2s - loss: 0.4011 - accuracy: 0.81 - ETA: 2s - loss: 0.3955 - accuracy: 0.82 - ETA: 2s - loss: 0.3922 - accuracy: 0.82 - ETA: 2s - loss: 0.3933 - accuracy: 0.82 - ETA: 2s - loss: 0.3971 - accuracy: 0.82 - ETA: 2s - loss: 0.3953 - accuracy: 0.82 - ETA: 2s - loss: 0.3923 - accuracy: 0.82 - ETA: 2s - loss: 0.3933 - accuracy: 0.82 - ETA: 2s - loss: 0.4015 - accuracy: 0.82 - ETA: 2s - loss: 0.3983 - accuracy: 0.82 - ETA: 2s - loss: 0.4004 - accuracy: 0.82 - ETA: 2s - loss: 0.3991 - accuracy: 0.82 - ETA: 1s - loss: 0.3984 - accuracy: 0.82 - ETA: 1s - loss: 0.3986 - accuracy: 0.82 - ETA: 1s - loss: 0.3985 - accuracy: 0.82 - ETA: 1s - loss: 0.3984 - accuracy: 0.82 - ETA: 1s - loss: 0.3947 - accuracy: 0.82 - ETA: 1s - loss: 0.3952 - accuracy: 0.82 - ETA: 1s - loss: 0.3950 - accuracy: 0.82 - ETA: 1s - loss: 0.3934 - accuracy: 0.82 - ETA: 1s - loss: 0.3959 - accuracy: 0.82 - ETA: 1s - loss: 0.3965 - accuracy: 0.82 - ETA: 1s - loss: 0.3981 - accuracy: 0.82 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3956 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3961 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3965 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - 3s 318us/step - loss: 0.3973 - accuracy: 0.8204 - val_loss: 0.3709 - val_accuracy: 0.8349\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31344\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3663 - accuracy: 0.83 - ETA: 3s - loss: 0.3741 - accuracy: 0.84 - ETA: 2s - loss: 0.3732 - accuracy: 0.83 - ETA: 2s - loss: 0.3616 - accuracy: 0.84 - ETA: 3s - loss: 0.3653 - accuracy: 0.84 - ETA: 2s - loss: 0.3734 - accuracy: 0.83 - ETA: 2s - loss: 0.3863 - accuracy: 0.82 - ETA: 2s - loss: 0.3885 - accuracy: 0.82 - ETA: 2s - loss: 0.3950 - accuracy: 0.82 - ETA: 2s - loss: 0.3941 - accuracy: 0.82 - ETA: 2s - loss: 0.3935 - accuracy: 0.82 - ETA: 2s - loss: 0.3912 - accuracy: 0.82 - ETA: 2s - loss: 0.3912 - accuracy: 0.82 - ETA: 2s - loss: 0.3909 - accuracy: 0.82 - ETA: 2s - loss: 0.3938 - accuracy: 0.82 - ETA: 2s - loss: 0.3933 - accuracy: 0.82 - ETA: 1s - loss: 0.3920 - accuracy: 0.82 - ETA: 1s - loss: 0.3945 - accuracy: 0.82 - ETA: 1s - loss: 0.3950 - accuracy: 0.82 - ETA: 1s - loss: 0.3954 - accuracy: 0.82 - ETA: 1s - loss: 0.3974 - accuracy: 0.81 - ETA: 1s - loss: 0.3983 - accuracy: 0.81 - ETA: 1s - loss: 0.3996 - accuracy: 0.81 - ETA: 1s - loss: 0.4001 - accuracy: 0.81 - ETA: 1s - loss: 0.4018 - accuracy: 0.81 - ETA: 1s - loss: 0.4003 - accuracy: 0.81 - ETA: 1s - loss: 0.4011 - accuracy: 0.81 - ETA: 1s - loss: 0.3991 - accuracy: 0.81 - ETA: 1s - loss: 0.3981 - accuracy: 0.81 - ETA: 1s - loss: 0.3967 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3961 - accuracy: 0.82 - ETA: 0s - loss: 0.3965 - accuracy: 0.82 - ETA: 0s - loss: 0.3976 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.3976 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.81 - ETA: 0s - loss: 0.3971 - accuracy: 0.81 - ETA: 0s - loss: 0.3982 - accuracy: 0.81 - ETA: 0s - loss: 0.3977 - accuracy: 0.81 - 4s 361us/step - loss: 0.3982 - accuracy: 0.8191 - val_loss: 0.3834 - val_accuracy: 0.8239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31344\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3622 - accuracy: 0.84 - ETA: 3s - loss: 0.4428 - accuracy: 0.79 - ETA: 3s - loss: 0.4311 - accuracy: 0.81 - ETA: 3s - loss: 0.4291 - accuracy: 0.81 - ETA: 3s - loss: 0.4130 - accuracy: 0.81 - ETA: 3s - loss: 0.4132 - accuracy: 0.81 - ETA: 3s - loss: 0.4159 - accuracy: 0.81 - ETA: 3s - loss: 0.4165 - accuracy: 0.81 - ETA: 3s - loss: 0.4117 - accuracy: 0.81 - ETA: 3s - loss: 0.4024 - accuracy: 0.82 - ETA: 3s - loss: 0.4067 - accuracy: 0.81 - ETA: 3s - loss: 0.4021 - accuracy: 0.82 - ETA: 3s - loss: 0.4063 - accuracy: 0.81 - ETA: 3s - loss: 0.4043 - accuracy: 0.81 - ETA: 3s - loss: 0.4025 - accuracy: 0.81 - ETA: 3s - loss: 0.4122 - accuracy: 0.81 - ETA: 3s - loss: 0.4059 - accuracy: 0.82 - ETA: 3s - loss: 0.4039 - accuracy: 0.82 - ETA: 3s - loss: 0.4001 - accuracy: 0.82 - ETA: 3s - loss: 0.4085 - accuracy: 0.81 - ETA: 2s - loss: 0.4121 - accuracy: 0.81 - ETA: 2s - loss: 0.4077 - accuracy: 0.81 - ETA: 2s - loss: 0.4088 - accuracy: 0.81 - ETA: 2s - loss: 0.4078 - accuracy: 0.81 - ETA: 2s - loss: 0.4074 - accuracy: 0.81 - ETA: 2s - loss: 0.4108 - accuracy: 0.81 - ETA: 2s - loss: 0.4116 - accuracy: 0.81 - ETA: 2s - loss: 0.4091 - accuracy: 0.81 - ETA: 2s - loss: 0.4094 - accuracy: 0.81 - ETA: 2s - loss: 0.4070 - accuracy: 0.81 - ETA: 2s - loss: 0.4065 - accuracy: 0.81 - ETA: 2s - loss: 0.4070 - accuracy: 0.81 - ETA: 2s - loss: 0.4053 - accuracy: 0.81 - ETA: 1s - loss: 0.4060 - accuracy: 0.81 - ETA: 1s - loss: 0.4037 - accuracy: 0.81 - ETA: 1s - loss: 0.4047 - accuracy: 0.81 - ETA: 1s - loss: 0.4053 - accuracy: 0.81 - ETA: 1s - loss: 0.4044 - accuracy: 0.81 - ETA: 1s - loss: 0.4041 - accuracy: 0.81 - ETA: 1s - loss: 0.4042 - accuracy: 0.81 - ETA: 1s - loss: 0.4043 - accuracy: 0.81 - ETA: 1s - loss: 0.4045 - accuracy: 0.81 - ETA: 1s - loss: 0.4030 - accuracy: 0.81 - ETA: 1s - loss: 0.4020 - accuracy: 0.81 - ETA: 1s - loss: 0.4020 - accuracy: 0.82 - ETA: 1s - loss: 0.4018 - accuracy: 0.82 - ETA: 1s - loss: 0.4031 - accuracy: 0.81 - ETA: 1s - loss: 0.4028 - accuracy: 0.81 - ETA: 0s - loss: 0.4032 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4021 - accuracy: 0.81 - ETA: 0s - loss: 0.4014 - accuracy: 0.81 - ETA: 0s - loss: 0.4011 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.81 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.3983 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.82 - 4s 385us/step - loss: 0.3926 - accuracy: 0.8225 - val_loss: 0.3950 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31344\n",
      "Epoch 00017: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 201us/step\n",
      "[2020-05-18 15:59:24 RAM64.7% 0.6GB] Val Score : [0.3628210893763657, 0.8415466547012329]\n",
      "[2020-05-18 15:59:24 RAM64.7% 0.6GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 15:59:24 RAM64.7% 0.6GB] Training on Fold : 4\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 31s - loss: 3.8315 - accuracy: 0.448 - ETA: 12s - loss: 2.2839 - accuracy: 0.524 - ETA: 8s - loss: 1.6922 - accuracy: 0.532 - ETA: 7s - loss: 1.5326 - accuracy: 0.52 - ETA: 6s - loss: 1.4252 - accuracy: 0.52 - ETA: 5s - loss: 1.2663 - accuracy: 0.53 - ETA: 5s - loss: 1.2119 - accuracy: 0.53 - ETA: 5s - loss: 1.1619 - accuracy: 0.54 - ETA: 5s - loss: 1.1159 - accuracy: 0.55 - ETA: 4s - loss: 1.0796 - accuracy: 0.55 - ETA: 4s - loss: 1.0515 - accuracy: 0.55 - ETA: 4s - loss: 1.0003 - accuracy: 0.56 - ETA: 4s - loss: 0.9758 - accuracy: 0.56 - ETA: 3s - loss: 0.9363 - accuracy: 0.57 - ETA: 3s - loss: 0.9215 - accuracy: 0.57 - ETA: 3s - loss: 0.9072 - accuracy: 0.58 - ETA: 3s - loss: 0.8808 - accuracy: 0.58 - ETA: 3s - loss: 0.8575 - accuracy: 0.59 - ETA: 2s - loss: 0.8386 - accuracy: 0.60 - ETA: 2s - loss: 0.8306 - accuracy: 0.60 - ETA: 2s - loss: 0.8156 - accuracy: 0.61 - ETA: 2s - loss: 0.8125 - accuracy: 0.61 - ETA: 2s - loss: 0.8064 - accuracy: 0.61 - ETA: 2s - loss: 0.7993 - accuracy: 0.61 - ETA: 2s - loss: 0.7934 - accuracy: 0.61 - ETA: 2s - loss: 0.7867 - accuracy: 0.62 - ETA: 2s - loss: 0.7816 - accuracy: 0.62 - ETA: 2s - loss: 0.7715 - accuracy: 0.62 - ETA: 2s - loss: 0.7673 - accuracy: 0.63 - ETA: 1s - loss: 0.7570 - accuracy: 0.63 - ETA: 1s - loss: 0.7523 - accuracy: 0.64 - ETA: 1s - loss: 0.7492 - accuracy: 0.64 - ETA: 1s - loss: 0.7448 - accuracy: 0.64 - ETA: 1s - loss: 0.7385 - accuracy: 0.64 - ETA: 1s - loss: 0.7289 - accuracy: 0.65 - ETA: 1s - loss: 0.7230 - accuracy: 0.65 - ETA: 1s - loss: 0.7191 - accuracy: 0.66 - ETA: 1s - loss: 0.7167 - accuracy: 0.66 - ETA: 1s - loss: 0.7148 - accuracy: 0.66 - ETA: 1s - loss: 0.7121 - accuracy: 0.66 - ETA: 1s - loss: 0.7097 - accuracy: 0.66 - ETA: 1s - loss: 0.7073 - accuracy: 0.66 - ETA: 0s - loss: 0.7006 - accuracy: 0.66 - ETA: 0s - loss: 0.6989 - accuracy: 0.66 - ETA: 0s - loss: 0.6968 - accuracy: 0.67 - ETA: 0s - loss: 0.6945 - accuracy: 0.67 - ETA: 0s - loss: 0.6901 - accuracy: 0.67 - ETA: 0s - loss: 0.6834 - accuracy: 0.67 - ETA: 0s - loss: 0.6808 - accuracy: 0.67 - ETA: 0s - loss: 0.6788 - accuracy: 0.68 - ETA: 0s - loss: 0.6765 - accuracy: 0.68 - ETA: 0s - loss: 0.6747 - accuracy: 0.68 - ETA: 0s - loss: 0.6725 - accuracy: 0.68 - ETA: 0s - loss: 0.6706 - accuracy: 0.68 - ETA: 0s - loss: 0.6693 - accuracy: 0.68 - 4s 404us/step - loss: 0.6680 - accuracy: 0.6861 - val_loss: 0.5379 - val_accuracy: 0.7422\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31344\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5282 - accuracy: 0.74 - ETA: 3s - loss: 0.5025 - accuracy: 0.75 - ETA: 3s - loss: 0.5188 - accuracy: 0.76 - ETA: 3s - loss: 0.5203 - accuracy: 0.76 - ETA: 3s - loss: 0.5150 - accuracy: 0.77 - ETA: 3s - loss: 0.5195 - accuracy: 0.76 - ETA: 3s - loss: 0.5244 - accuracy: 0.76 - ETA: 2s - loss: 0.5225 - accuracy: 0.76 - ETA: 2s - loss: 0.5325 - accuracy: 0.75 - ETA: 2s - loss: 0.5267 - accuracy: 0.76 - ETA: 2s - loss: 0.5251 - accuracy: 0.76 - ETA: 2s - loss: 0.5205 - accuracy: 0.76 - ETA: 2s - loss: 0.5214 - accuracy: 0.76 - ETA: 2s - loss: 0.5221 - accuracy: 0.76 - ETA: 2s - loss: 0.5246 - accuracy: 0.76 - ETA: 2s - loss: 0.5243 - accuracy: 0.76 - ETA: 2s - loss: 0.5258 - accuracy: 0.76 - ETA: 2s - loss: 0.5245 - accuracy: 0.76 - ETA: 2s - loss: 0.5226 - accuracy: 0.76 - ETA: 2s - loss: 0.5244 - accuracy: 0.76 - ETA: 2s - loss: 0.5267 - accuracy: 0.76 - ETA: 2s - loss: 0.5269 - accuracy: 0.76 - ETA: 2s - loss: 0.5276 - accuracy: 0.75 - ETA: 2s - loss: 0.5279 - accuracy: 0.76 - ETA: 2s - loss: 0.5267 - accuracy: 0.76 - ETA: 2s - loss: 0.5278 - accuracy: 0.76 - ETA: 2s - loss: 0.5263 - accuracy: 0.76 - ETA: 2s - loss: 0.5263 - accuracy: 0.76 - ETA: 2s - loss: 0.5262 - accuracy: 0.76 - ETA: 1s - loss: 0.5252 - accuracy: 0.76 - ETA: 1s - loss: 0.5252 - accuracy: 0.76 - ETA: 1s - loss: 0.5251 - accuracy: 0.76 - ETA: 1s - loss: 0.5255 - accuracy: 0.76 - ETA: 1s - loss: 0.5255 - accuracy: 0.76 - ETA: 1s - loss: 0.5263 - accuracy: 0.76 - ETA: 1s - loss: 0.5238 - accuracy: 0.76 - ETA: 1s - loss: 0.5229 - accuracy: 0.76 - ETA: 1s - loss: 0.5212 - accuracy: 0.76 - ETA: 1s - loss: 0.5210 - accuracy: 0.76 - ETA: 1s - loss: 0.5203 - accuracy: 0.76 - ETA: 1s - loss: 0.5200 - accuracy: 0.76 - ETA: 1s - loss: 0.5210 - accuracy: 0.76 - ETA: 1s - loss: 0.5219 - accuracy: 0.76 - ETA: 1s - loss: 0.5229 - accuracy: 0.76 - ETA: 0s - loss: 0.5227 - accuracy: 0.76 - ETA: 0s - loss: 0.5232 - accuracy: 0.76 - ETA: 0s - loss: 0.5238 - accuracy: 0.76 - ETA: 0s - loss: 0.5245 - accuracy: 0.75 - ETA: 0s - loss: 0.5223 - accuracy: 0.76 - ETA: 0s - loss: 0.5220 - accuracy: 0.76 - ETA: 0s - loss: 0.5210 - accuracy: 0.76 - ETA: 0s - loss: 0.5206 - accuracy: 0.76 - ETA: 0s - loss: 0.5208 - accuracy: 0.76 - ETA: 0s - loss: 0.5204 - accuracy: 0.76 - ETA: 0s - loss: 0.5198 - accuracy: 0.76 - ETA: 0s - loss: 0.5205 - accuracy: 0.76 - ETA: 0s - loss: 0.5198 - accuracy: 0.76 - ETA: 0s - loss: 0.5211 - accuracy: 0.76 - ETA: 0s - loss: 0.5210 - accuracy: 0.76 - ETA: 0s - loss: 0.5199 - accuracy: 0.76 - ETA: 0s - loss: 0.5191 - accuracy: 0.76 - 4s 363us/step - loss: 0.5183 - accuracy: 0.7652 - val_loss: 0.4777 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31344\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4828 - accuracy: 0.76 - ETA: 3s - loss: 0.5132 - accuracy: 0.77 - ETA: 3s - loss: 0.4981 - accuracy: 0.77 - ETA: 3s - loss: 0.5031 - accuracy: 0.77 - ETA: 3s - loss: 0.4969 - accuracy: 0.78 - ETA: 3s - loss: 0.5060 - accuracy: 0.78 - ETA: 3s - loss: 0.5046 - accuracy: 0.78 - ETA: 3s - loss: 0.5043 - accuracy: 0.78 - ETA: 3s - loss: 0.5123 - accuracy: 0.78 - ETA: 3s - loss: 0.5173 - accuracy: 0.77 - ETA: 3s - loss: 0.5192 - accuracy: 0.77 - ETA: 3s - loss: 0.5194 - accuracy: 0.77 - ETA: 2s - loss: 0.5139 - accuracy: 0.77 - ETA: 2s - loss: 0.5148 - accuracy: 0.77 - ETA: 2s - loss: 0.5183 - accuracy: 0.76 - ETA: 2s - loss: 0.5155 - accuracy: 0.77 - ETA: 2s - loss: 0.5131 - accuracy: 0.77 - ETA: 2s - loss: 0.5135 - accuracy: 0.77 - ETA: 2s - loss: 0.5115 - accuracy: 0.77 - ETA: 2s - loss: 0.5082 - accuracy: 0.77 - ETA: 2s - loss: 0.5063 - accuracy: 0.77 - ETA: 2s - loss: 0.5033 - accuracy: 0.77 - ETA: 2s - loss: 0.5056 - accuracy: 0.77 - ETA: 2s - loss: 0.5059 - accuracy: 0.77 - ETA: 2s - loss: 0.5087 - accuracy: 0.77 - ETA: 2s - loss: 0.5083 - accuracy: 0.77 - ETA: 1s - loss: 0.5069 - accuracy: 0.77 - ETA: 1s - loss: 0.5049 - accuracy: 0.77 - ETA: 1s - loss: 0.5052 - accuracy: 0.77 - ETA: 1s - loss: 0.5040 - accuracy: 0.77 - ETA: 1s - loss: 0.5036 - accuracy: 0.77 - ETA: 1s - loss: 0.5023 - accuracy: 0.77 - ETA: 1s - loss: 0.5039 - accuracy: 0.77 - ETA: 1s - loss: 0.5040 - accuracy: 0.77 - ETA: 1s - loss: 0.5048 - accuracy: 0.77 - ETA: 1s - loss: 0.5026 - accuracy: 0.77 - ETA: 1s - loss: 0.5031 - accuracy: 0.77 - ETA: 1s - loss: 0.5051 - accuracy: 0.77 - ETA: 1s - loss: 0.5068 - accuracy: 0.77 - ETA: 1s - loss: 0.5051 - accuracy: 0.77 - ETA: 0s - loss: 0.5046 - accuracy: 0.77 - ETA: 0s - loss: 0.5038 - accuracy: 0.77 - ETA: 0s - loss: 0.5040 - accuracy: 0.77 - ETA: 0s - loss: 0.5041 - accuracy: 0.77 - ETA: 0s - loss: 0.5026 - accuracy: 0.77 - ETA: 0s - loss: 0.5015 - accuracy: 0.77 - ETA: 0s - loss: 0.4996 - accuracy: 0.77 - ETA: 0s - loss: 0.5003 - accuracy: 0.77 - ETA: 0s - loss: 0.5004 - accuracy: 0.77 - ETA: 0s - loss: 0.5000 - accuracy: 0.77 - ETA: 0s - loss: 0.5004 - accuracy: 0.77 - ETA: 0s - loss: 0.5006 - accuracy: 0.77 - ETA: 0s - loss: 0.5010 - accuracy: 0.77 - ETA: 0s - loss: 0.5006 - accuracy: 0.77 - ETA: 0s - loss: 0.4994 - accuracy: 0.77 - 4s 358us/step - loss: 0.4989 - accuracy: 0.7751 - val_loss: 0.4480 - val_accuracy: 0.8020\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31344\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4297 - accuracy: 0.84 - ETA: 3s - loss: 0.4708 - accuracy: 0.79 - ETA: 3s - loss: 0.4626 - accuracy: 0.80 - ETA: 3s - loss: 0.4917 - accuracy: 0.77 - ETA: 3s - loss: 0.4909 - accuracy: 0.78 - ETA: 3s - loss: 0.4923 - accuracy: 0.78 - ETA: 3s - loss: 0.4997 - accuracy: 0.77 - ETA: 2s - loss: 0.5058 - accuracy: 0.77 - ETA: 2s - loss: 0.5040 - accuracy: 0.77 - ETA: 2s - loss: 0.5011 - accuracy: 0.78 - ETA: 2s - loss: 0.4953 - accuracy: 0.78 - ETA: 2s - loss: 0.4931 - accuracy: 0.78 - ETA: 2s - loss: 0.4931 - accuracy: 0.78 - ETA: 2s - loss: 0.4862 - accuracy: 0.78 - ETA: 2s - loss: 0.4870 - accuracy: 0.78 - ETA: 2s - loss: 0.4873 - accuracy: 0.78 - ETA: 2s - loss: 0.4901 - accuracy: 0.78 - ETA: 2s - loss: 0.4849 - accuracy: 0.78 - ETA: 2s - loss: 0.4853 - accuracy: 0.78 - ETA: 2s - loss: 0.4836 - accuracy: 0.78 - ETA: 2s - loss: 0.4812 - accuracy: 0.78 - ETA: 2s - loss: 0.4822 - accuracy: 0.78 - ETA: 2s - loss: 0.4798 - accuracy: 0.78 - ETA: 2s - loss: 0.4792 - accuracy: 0.78 - ETA: 2s - loss: 0.4781 - accuracy: 0.78 - ETA: 2s - loss: 0.4772 - accuracy: 0.79 - ETA: 1s - loss: 0.4763 - accuracy: 0.79 - ETA: 1s - loss: 0.4760 - accuracy: 0.79 - ETA: 1s - loss: 0.4765 - accuracy: 0.79 - ETA: 1s - loss: 0.4759 - accuracy: 0.79 - ETA: 1s - loss: 0.4778 - accuracy: 0.79 - ETA: 1s - loss: 0.4792 - accuracy: 0.78 - ETA: 1s - loss: 0.4795 - accuracy: 0.78 - ETA: 1s - loss: 0.4782 - accuracy: 0.78 - ETA: 1s - loss: 0.4780 - accuracy: 0.79 - ETA: 1s - loss: 0.4790 - accuracy: 0.79 - ETA: 1s - loss: 0.4781 - accuracy: 0.79 - ETA: 1s - loss: 0.4780 - accuracy: 0.79 - ETA: 1s - loss: 0.4777 - accuracy: 0.79 - ETA: 1s - loss: 0.4765 - accuracy: 0.79 - ETA: 0s - loss: 0.4775 - accuracy: 0.79 - ETA: 0s - loss: 0.4784 - accuracy: 0.78 - ETA: 0s - loss: 0.4784 - accuracy: 0.78 - ETA: 0s - loss: 0.4790 - accuracy: 0.78 - ETA: 0s - loss: 0.4798 - accuracy: 0.78 - ETA: 0s - loss: 0.4803 - accuracy: 0.78 - ETA: 0s - loss: 0.4787 - accuracy: 0.78 - ETA: 0s - loss: 0.4776 - accuracy: 0.78 - ETA: 0s - loss: 0.4780 - accuracy: 0.78 - ETA: 0s - loss: 0.4770 - accuracy: 0.79 - ETA: 0s - loss: 0.4773 - accuracy: 0.78 - ETA: 0s - loss: 0.4781 - accuracy: 0.78 - ETA: 0s - loss: 0.4778 - accuracy: 0.78 - ETA: 0s - loss: 0.4784 - accuracy: 0.78 - ETA: 0s - loss: 0.4767 - accuracy: 0.78 - ETA: 0s - loss: 0.4777 - accuracy: 0.78 - 4s 357us/step - loss: 0.4774 - accuracy: 0.7886 - val_loss: 0.4448 - val_accuracy: 0.8113\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31344\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5750 - accuracy: 0.71 - ETA: 3s - loss: 0.5352 - accuracy: 0.74 - ETA: 3s - loss: 0.5452 - accuracy: 0.74 - ETA: 3s - loss: 0.4976 - accuracy: 0.77 - ETA: 3s - loss: 0.4974 - accuracy: 0.78 - ETA: 3s - loss: 0.4910 - accuracy: 0.78 - ETA: 3s - loss: 0.4752 - accuracy: 0.78 - ETA: 3s - loss: 0.4768 - accuracy: 0.78 - ETA: 3s - loss: 0.4745 - accuracy: 0.78 - ETA: 3s - loss: 0.4761 - accuracy: 0.78 - ETA: 3s - loss: 0.4760 - accuracy: 0.78 - ETA: 2s - loss: 0.4796 - accuracy: 0.77 - ETA: 2s - loss: 0.4783 - accuracy: 0.77 - ETA: 2s - loss: 0.4773 - accuracy: 0.78 - ETA: 2s - loss: 0.4754 - accuracy: 0.78 - ETA: 2s - loss: 0.4710 - accuracy: 0.78 - ETA: 2s - loss: 0.4671 - accuracy: 0.78 - ETA: 2s - loss: 0.4658 - accuracy: 0.78 - ETA: 2s - loss: 0.4685 - accuracy: 0.78 - ETA: 2s - loss: 0.4697 - accuracy: 0.78 - ETA: 2s - loss: 0.4683 - accuracy: 0.78 - ETA: 2s - loss: 0.4693 - accuracy: 0.78 - ETA: 2s - loss: 0.4691 - accuracy: 0.78 - ETA: 2s - loss: 0.4695 - accuracy: 0.78 - ETA: 2s - loss: 0.4680 - accuracy: 0.78 - ETA: 2s - loss: 0.4676 - accuracy: 0.78 - ETA: 2s - loss: 0.4675 - accuracy: 0.78 - ETA: 1s - loss: 0.4662 - accuracy: 0.79 - ETA: 1s - loss: 0.4621 - accuracy: 0.79 - ETA: 1s - loss: 0.4614 - accuracy: 0.79 - ETA: 1s - loss: 0.4596 - accuracy: 0.79 - ETA: 1s - loss: 0.4583 - accuracy: 0.79 - ETA: 1s - loss: 0.4586 - accuracy: 0.79 - ETA: 1s - loss: 0.4577 - accuracy: 0.79 - ETA: 1s - loss: 0.4572 - accuracy: 0.79 - ETA: 1s - loss: 0.4572 - accuracy: 0.79 - ETA: 1s - loss: 0.4567 - accuracy: 0.79 - ETA: 1s - loss: 0.4571 - accuracy: 0.79 - ETA: 1s - loss: 0.4579 - accuracy: 0.79 - ETA: 1s - loss: 0.4570 - accuracy: 0.79 - ETA: 1s - loss: 0.4566 - accuracy: 0.79 - ETA: 1s - loss: 0.4542 - accuracy: 0.79 - ETA: 1s - loss: 0.4544 - accuracy: 0.79 - ETA: 0s - loss: 0.4543 - accuracy: 0.79 - ETA: 0s - loss: 0.4557 - accuracy: 0.79 - ETA: 0s - loss: 0.4558 - accuracy: 0.79 - ETA: 0s - loss: 0.4566 - accuracy: 0.79 - ETA: 0s - loss: 0.4568 - accuracy: 0.79 - ETA: 0s - loss: 0.4564 - accuracy: 0.79 - ETA: 0s - loss: 0.4573 - accuracy: 0.79 - ETA: 0s - loss: 0.4571 - accuracy: 0.79 - ETA: 0s - loss: 0.4580 - accuracy: 0.79 - ETA: 0s - loss: 0.4581 - accuracy: 0.79 - ETA: 0s - loss: 0.4575 - accuracy: 0.79 - ETA: 0s - loss: 0.4560 - accuracy: 0.79 - ETA: 0s - loss: 0.4565 - accuracy: 0.79 - ETA: 0s - loss: 0.4558 - accuracy: 0.79 - ETA: 0s - loss: 0.4560 - accuracy: 0.79 - ETA: 0s - loss: 0.4561 - accuracy: 0.79 - 4s 359us/step - loss: 0.4556 - accuracy: 0.7955 - val_loss: 0.4221 - val_accuracy: 0.8096\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31344\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.4181 - accuracy: 0.81 - ETA: 4s - loss: 0.4409 - accuracy: 0.80 - ETA: 3s - loss: 0.4582 - accuracy: 0.79 - ETA: 3s - loss: 0.4340 - accuracy: 0.81 - ETA: 3s - loss: 0.4355 - accuracy: 0.80 - ETA: 3s - loss: 0.4352 - accuracy: 0.80 - ETA: 3s - loss: 0.4269 - accuracy: 0.81 - ETA: 3s - loss: 0.4324 - accuracy: 0.81 - ETA: 3s - loss: 0.4317 - accuracy: 0.81 - ETA: 3s - loss: 0.4414 - accuracy: 0.81 - ETA: 3s - loss: 0.4458 - accuracy: 0.80 - ETA: 3s - loss: 0.4492 - accuracy: 0.80 - ETA: 3s - loss: 0.4525 - accuracy: 0.80 - ETA: 3s - loss: 0.4534 - accuracy: 0.79 - ETA: 3s - loss: 0.4480 - accuracy: 0.80 - ETA: 2s - loss: 0.4504 - accuracy: 0.79 - ETA: 2s - loss: 0.4469 - accuracy: 0.80 - ETA: 2s - loss: 0.4451 - accuracy: 0.80 - ETA: 2s - loss: 0.4444 - accuracy: 0.80 - ETA: 2s - loss: 0.4458 - accuracy: 0.80 - ETA: 2s - loss: 0.4474 - accuracy: 0.80 - ETA: 2s - loss: 0.4475 - accuracy: 0.80 - ETA: 2s - loss: 0.4429 - accuracy: 0.80 - ETA: 2s - loss: 0.4389 - accuracy: 0.80 - ETA: 2s - loss: 0.4391 - accuracy: 0.80 - ETA: 2s - loss: 0.4414 - accuracy: 0.80 - ETA: 2s - loss: 0.4398 - accuracy: 0.80 - ETA: 2s - loss: 0.4407 - accuracy: 0.80 - ETA: 2s - loss: 0.4428 - accuracy: 0.80 - ETA: 2s - loss: 0.4454 - accuracy: 0.80 - ETA: 2s - loss: 0.4438 - accuracy: 0.80 - ETA: 1s - loss: 0.4409 - accuracy: 0.80 - ETA: 1s - loss: 0.4428 - accuracy: 0.80 - ETA: 1s - loss: 0.4427 - accuracy: 0.80 - ETA: 1s - loss: 0.4434 - accuracy: 0.80 - ETA: 1s - loss: 0.4420 - accuracy: 0.80 - ETA: 1s - loss: 0.4438 - accuracy: 0.80 - ETA: 1s - loss: 0.4435 - accuracy: 0.80 - ETA: 1s - loss: 0.4442 - accuracy: 0.80 - ETA: 1s - loss: 0.4439 - accuracy: 0.80 - ETA: 1s - loss: 0.4435 - accuracy: 0.80 - ETA: 1s - loss: 0.4421 - accuracy: 0.80 - ETA: 1s - loss: 0.4416 - accuracy: 0.80 - ETA: 1s - loss: 0.4416 - accuracy: 0.80 - ETA: 1s - loss: 0.4436 - accuracy: 0.80 - ETA: 0s - loss: 0.4423 - accuracy: 0.80 - ETA: 0s - loss: 0.4417 - accuracy: 0.80 - ETA: 0s - loss: 0.4421 - accuracy: 0.80 - ETA: 0s - loss: 0.4435 - accuracy: 0.80 - ETA: 0s - loss: 0.4437 - accuracy: 0.80 - ETA: 0s - loss: 0.4428 - accuracy: 0.80 - ETA: 0s - loss: 0.4425 - accuracy: 0.80 - ETA: 0s - loss: 0.4426 - accuracy: 0.80 - ETA: 0s - loss: 0.4414 - accuracy: 0.80 - ETA: 0s - loss: 0.4412 - accuracy: 0.80 - ETA: 0s - loss: 0.4399 - accuracy: 0.80 - ETA: 0s - loss: 0.4396 - accuracy: 0.80 - ETA: 0s - loss: 0.4392 - accuracy: 0.80 - ETA: 0s - loss: 0.4393 - accuracy: 0.80 - ETA: 0s - loss: 0.4400 - accuracy: 0.80 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - 4s 365us/step - loss: 0.4403 - accuracy: 0.8040 - val_loss: 0.4307 - val_accuracy: 0.8045\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31344\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4256 - accuracy: 0.81 - ETA: 3s - loss: 0.4506 - accuracy: 0.79 - ETA: 3s - loss: 0.4317 - accuracy: 0.81 - ETA: 3s - loss: 0.4441 - accuracy: 0.80 - ETA: 3s - loss: 0.4454 - accuracy: 0.80 - ETA: 3s - loss: 0.4454 - accuracy: 0.80 - ETA: 3s - loss: 0.4368 - accuracy: 0.80 - ETA: 3s - loss: 0.4327 - accuracy: 0.80 - ETA: 3s - loss: 0.4251 - accuracy: 0.81 - ETA: 3s - loss: 0.4226 - accuracy: 0.81 - ETA: 3s - loss: 0.4200 - accuracy: 0.81 - ETA: 3s - loss: 0.4196 - accuracy: 0.81 - ETA: 3s - loss: 0.4297 - accuracy: 0.81 - ETA: 2s - loss: 0.4240 - accuracy: 0.81 - ETA: 2s - loss: 0.4251 - accuracy: 0.81 - ETA: 2s - loss: 0.4358 - accuracy: 0.80 - ETA: 2s - loss: 0.4336 - accuracy: 0.80 - ETA: 2s - loss: 0.4355 - accuracy: 0.80 - ETA: 2s - loss: 0.4372 - accuracy: 0.80 - ETA: 2s - loss: 0.4384 - accuracy: 0.80 - ETA: 2s - loss: 0.4381 - accuracy: 0.80 - ETA: 2s - loss: 0.4360 - accuracy: 0.80 - ETA: 2s - loss: 0.4328 - accuracy: 0.80 - ETA: 2s - loss: 0.4314 - accuracy: 0.80 - ETA: 2s - loss: 0.4324 - accuracy: 0.80 - ETA: 2s - loss: 0.4309 - accuracy: 0.80 - ETA: 2s - loss: 0.4337 - accuracy: 0.80 - ETA: 2s - loss: 0.4327 - accuracy: 0.80 - ETA: 2s - loss: 0.4334 - accuracy: 0.80 - ETA: 1s - loss: 0.4339 - accuracy: 0.80 - ETA: 1s - loss: 0.4342 - accuracy: 0.80 - ETA: 1s - loss: 0.4330 - accuracy: 0.80 - ETA: 1s - loss: 0.4301 - accuracy: 0.80 - ETA: 1s - loss: 0.4302 - accuracy: 0.80 - ETA: 1s - loss: 0.4312 - accuracy: 0.80 - ETA: 1s - loss: 0.4312 - accuracy: 0.80 - ETA: 1s - loss: 0.4337 - accuracy: 0.80 - ETA: 1s - loss: 0.4323 - accuracy: 0.80 - ETA: 1s - loss: 0.4307 - accuracy: 0.80 - ETA: 1s - loss: 0.4301 - accuracy: 0.80 - ETA: 1s - loss: 0.4299 - accuracy: 0.80 - ETA: 1s - loss: 0.4302 - accuracy: 0.80 - ETA: 1s - loss: 0.4291 - accuracy: 0.80 - ETA: 1s - loss: 0.4292 - accuracy: 0.80 - ETA: 1s - loss: 0.4279 - accuracy: 0.80 - ETA: 1s - loss: 0.4288 - accuracy: 0.80 - ETA: 1s - loss: 0.4304 - accuracy: 0.80 - ETA: 0s - loss: 0.4312 - accuracy: 0.80 - ETA: 0s - loss: 0.4313 - accuracy: 0.80 - ETA: 0s - loss: 0.4303 - accuracy: 0.80 - ETA: 0s - loss: 0.4299 - accuracy: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4284 - accuracy: 0.80 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4267 - accuracy: 0.80 - ETA: 0s - loss: 0.4261 - accuracy: 0.80 - ETA: 0s - loss: 0.4269 - accuracy: 0.80 - ETA: 0s - loss: 0.4288 - accuracy: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4287 - accuracy: 0.80 - ETA: 0s - loss: 0.4286 - accuracy: 0.80 - ETA: 0s - loss: 0.4280 - accuracy: 0.80 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - 4s 371us/step - loss: 0.4279 - accuracy: 0.8028 - val_loss: 0.3842 - val_accuracy: 0.8265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31344\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5317 - accuracy: 0.75 - ETA: 3s - loss: 0.4806 - accuracy: 0.77 - ETA: 3s - loss: 0.4587 - accuracy: 0.79 - ETA: 3s - loss: 0.4357 - accuracy: 0.79 - ETA: 3s - loss: 0.4207 - accuracy: 0.80 - ETA: 3s - loss: 0.4201 - accuracy: 0.80 - ETA: 3s - loss: 0.4132 - accuracy: 0.80 - ETA: 3s - loss: 0.4221 - accuracy: 0.80 - ETA: 3s - loss: 0.4199 - accuracy: 0.80 - ETA: 3s - loss: 0.4207 - accuracy: 0.80 - ETA: 3s - loss: 0.4164 - accuracy: 0.80 - ETA: 3s - loss: 0.4188 - accuracy: 0.80 - ETA: 3s - loss: 0.4220 - accuracy: 0.80 - ETA: 2s - loss: 0.4272 - accuracy: 0.79 - ETA: 2s - loss: 0.4292 - accuracy: 0.79 - ETA: 2s - loss: 0.4291 - accuracy: 0.80 - ETA: 2s - loss: 0.4298 - accuracy: 0.80 - ETA: 2s - loss: 0.4297 - accuracy: 0.80 - ETA: 2s - loss: 0.4289 - accuracy: 0.80 - ETA: 2s - loss: 0.4299 - accuracy: 0.80 - ETA: 2s - loss: 0.4284 - accuracy: 0.80 - ETA: 2s - loss: 0.4281 - accuracy: 0.80 - ETA: 2s - loss: 0.4282 - accuracy: 0.80 - ETA: 2s - loss: 0.4297 - accuracy: 0.80 - ETA: 2s - loss: 0.4278 - accuracy: 0.80 - ETA: 2s - loss: 0.4290 - accuracy: 0.80 - ETA: 2s - loss: 0.4314 - accuracy: 0.79 - ETA: 2s - loss: 0.4317 - accuracy: 0.80 - ETA: 2s - loss: 0.4295 - accuracy: 0.80 - ETA: 2s - loss: 0.4295 - accuracy: 0.80 - ETA: 2s - loss: 0.4301 - accuracy: 0.80 - ETA: 1s - loss: 0.4300 - accuracy: 0.80 - ETA: 1s - loss: 0.4308 - accuracy: 0.79 - ETA: 1s - loss: 0.4291 - accuracy: 0.80 - ETA: 1s - loss: 0.4284 - accuracy: 0.80 - ETA: 1s - loss: 0.4262 - accuracy: 0.80 - ETA: 1s - loss: 0.4254 - accuracy: 0.80 - ETA: 1s - loss: 0.4249 - accuracy: 0.80 - ETA: 1s - loss: 0.4247 - accuracy: 0.80 - ETA: 1s - loss: 0.4243 - accuracy: 0.80 - ETA: 1s - loss: 0.4247 - accuracy: 0.80 - ETA: 1s - loss: 0.4250 - accuracy: 0.80 - ETA: 1s - loss: 0.4233 - accuracy: 0.80 - ETA: 1s - loss: 0.4223 - accuracy: 0.80 - ETA: 1s - loss: 0.4204 - accuracy: 0.80 - ETA: 1s - loss: 0.4196 - accuracy: 0.80 - ETA: 1s - loss: 0.4184 - accuracy: 0.80 - ETA: 1s - loss: 0.4173 - accuracy: 0.80 - ETA: 1s - loss: 0.4181 - accuracy: 0.80 - ETA: 0s - loss: 0.4178 - accuracy: 0.80 - ETA: 0s - loss: 0.4189 - accuracy: 0.80 - ETA: 0s - loss: 0.4185 - accuracy: 0.80 - ETA: 0s - loss: 0.4180 - accuracy: 0.80 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - ETA: 0s - loss: 0.4172 - accuracy: 0.81 - ETA: 0s - loss: 0.4173 - accuracy: 0.80 - ETA: 0s - loss: 0.4160 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4166 - accuracy: 0.80 - ETA: 0s - loss: 0.4169 - accuracy: 0.80 - ETA: 0s - loss: 0.4184 - accuracy: 0.80 - ETA: 0s - loss: 0.4184 - accuracy: 0.80 - ETA: 0s - loss: 0.4186 - accuracy: 0.80 - ETA: 0s - loss: 0.4184 - accuracy: 0.80 - ETA: 0s - loss: 0.4185 - accuracy: 0.80 - 4s 374us/step - loss: 0.4190 - accuracy: 0.8071 - val_loss: 0.3913 - val_accuracy: 0.8290\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31344\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3872 - accuracy: 0.80 - ETA: 3s - loss: 0.4157 - accuracy: 0.78 - ETA: 3s - loss: 0.4206 - accuracy: 0.79 - ETA: 3s - loss: 0.4261 - accuracy: 0.79 - ETA: 3s - loss: 0.4196 - accuracy: 0.79 - ETA: 3s - loss: 0.4150 - accuracy: 0.80 - ETA: 3s - loss: 0.4121 - accuracy: 0.80 - ETA: 3s - loss: 0.4145 - accuracy: 0.80 - ETA: 3s - loss: 0.4056 - accuracy: 0.80 - ETA: 3s - loss: 0.4103 - accuracy: 0.80 - ETA: 3s - loss: 0.4085 - accuracy: 0.80 - ETA: 3s - loss: 0.4019 - accuracy: 0.81 - ETA: 3s - loss: 0.3976 - accuracy: 0.81 - ETA: 3s - loss: 0.4098 - accuracy: 0.80 - ETA: 3s - loss: 0.4049 - accuracy: 0.81 - ETA: 3s - loss: 0.4085 - accuracy: 0.81 - ETA: 3s - loss: 0.4073 - accuracy: 0.81 - ETA: 2s - loss: 0.4052 - accuracy: 0.81 - ETA: 2s - loss: 0.4082 - accuracy: 0.81 - ETA: 2s - loss: 0.4062 - accuracy: 0.81 - ETA: 2s - loss: 0.4097 - accuracy: 0.81 - ETA: 2s - loss: 0.4107 - accuracy: 0.81 - ETA: 2s - loss: 0.4098 - accuracy: 0.81 - ETA: 2s - loss: 0.4113 - accuracy: 0.81 - ETA: 2s - loss: 0.4116 - accuracy: 0.81 - ETA: 2s - loss: 0.4139 - accuracy: 0.81 - ETA: 2s - loss: 0.4134 - accuracy: 0.81 - ETA: 2s - loss: 0.4119 - accuracy: 0.81 - ETA: 2s - loss: 0.4135 - accuracy: 0.81 - ETA: 2s - loss: 0.4114 - accuracy: 0.81 - ETA: 2s - loss: 0.4123 - accuracy: 0.81 - ETA: 2s - loss: 0.4133 - accuracy: 0.81 - ETA: 2s - loss: 0.4128 - accuracy: 0.81 - ETA: 2s - loss: 0.4129 - accuracy: 0.81 - ETA: 2s - loss: 0.4117 - accuracy: 0.81 - ETA: 2s - loss: 0.4124 - accuracy: 0.81 - ETA: 1s - loss: 0.4105 - accuracy: 0.81 - ETA: 1s - loss: 0.4115 - accuracy: 0.81 - ETA: 1s - loss: 0.4122 - accuracy: 0.80 - ETA: 1s - loss: 0.4141 - accuracy: 0.80 - ETA: 1s - loss: 0.4134 - accuracy: 0.80 - ETA: 1s - loss: 0.4126 - accuracy: 0.80 - ETA: 1s - loss: 0.4139 - accuracy: 0.80 - ETA: 1s - loss: 0.4139 - accuracy: 0.80 - ETA: 1s - loss: 0.4128 - accuracy: 0.80 - ETA: 1s - loss: 0.4148 - accuracy: 0.80 - ETA: 1s - loss: 0.4142 - accuracy: 0.80 - ETA: 1s - loss: 0.4123 - accuracy: 0.80 - ETA: 1s - loss: 0.4131 - accuracy: 0.80 - ETA: 1s - loss: 0.4122 - accuracy: 0.80 - ETA: 1s - loss: 0.4135 - accuracy: 0.80 - ETA: 0s - loss: 0.4138 - accuracy: 0.80 - ETA: 0s - loss: 0.4137 - accuracy: 0.80 - ETA: 0s - loss: 0.4123 - accuracy: 0.81 - ETA: 0s - loss: 0.4111 - accuracy: 0.81 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - ETA: 0s - loss: 0.4115 - accuracy: 0.81 - ETA: 0s - loss: 0.4106 - accuracy: 0.81 - ETA: 0s - loss: 0.4105 - accuracy: 0.81 - ETA: 0s - loss: 0.4099 - accuracy: 0.81 - ETA: 0s - loss: 0.4113 - accuracy: 0.81 - ETA: 0s - loss: 0.4115 - accuracy: 0.81 - ETA: 0s - loss: 0.4115 - accuracy: 0.81 - ETA: 0s - loss: 0.4106 - accuracy: 0.81 - ETA: 0s - loss: 0.4105 - accuracy: 0.81 - ETA: 0s - loss: 0.4111 - accuracy: 0.81 - 4s 378us/step - loss: 0.4112 - accuracy: 0.8118 - val_loss: 0.3867 - val_accuracy: 0.8222\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31344\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3984 - accuracy: 0.82 - ETA: 3s - loss: 0.4094 - accuracy: 0.81 - ETA: 3s - loss: 0.4310 - accuracy: 0.80 - ETA: 3s - loss: 0.4170 - accuracy: 0.81 - ETA: 3s - loss: 0.4056 - accuracy: 0.81 - ETA: 3s - loss: 0.4092 - accuracy: 0.81 - ETA: 3s - loss: 0.4115 - accuracy: 0.81 - ETA: 3s - loss: 0.4118 - accuracy: 0.81 - ETA: 3s - loss: 0.4131 - accuracy: 0.81 - ETA: 3s - loss: 0.4102 - accuracy: 0.81 - ETA: 3s - loss: 0.4088 - accuracy: 0.81 - ETA: 2s - loss: 0.4047 - accuracy: 0.82 - ETA: 2s - loss: 0.3971 - accuracy: 0.82 - ETA: 2s - loss: 0.3943 - accuracy: 0.82 - ETA: 2s - loss: 0.3946 - accuracy: 0.82 - ETA: 2s - loss: 0.3943 - accuracy: 0.82 - ETA: 2s - loss: 0.3960 - accuracy: 0.82 - ETA: 2s - loss: 0.3966 - accuracy: 0.81 - ETA: 2s - loss: 0.3967 - accuracy: 0.81 - ETA: 2s - loss: 0.3983 - accuracy: 0.81 - ETA: 2s - loss: 0.3986 - accuracy: 0.81 - ETA: 2s - loss: 0.3966 - accuracy: 0.81 - ETA: 2s - loss: 0.3962 - accuracy: 0.81 - ETA: 2s - loss: 0.3939 - accuracy: 0.81 - ETA: 2s - loss: 0.3965 - accuracy: 0.81 - ETA: 2s - loss: 0.3977 - accuracy: 0.81 - ETA: 2s - loss: 0.3983 - accuracy: 0.81 - ETA: 2s - loss: 0.3975 - accuracy: 0.81 - ETA: 1s - loss: 0.3976 - accuracy: 0.81 - ETA: 1s - loss: 0.3985 - accuracy: 0.81 - ETA: 1s - loss: 0.3977 - accuracy: 0.81 - ETA: 1s - loss: 0.3979 - accuracy: 0.81 - ETA: 1s - loss: 0.3984 - accuracy: 0.81 - ETA: 1s - loss: 0.3995 - accuracy: 0.81 - ETA: 1s - loss: 0.3993 - accuracy: 0.81 - ETA: 1s - loss: 0.3991 - accuracy: 0.81 - ETA: 1s - loss: 0.3984 - accuracy: 0.81 - ETA: 1s - loss: 0.3989 - accuracy: 0.81 - ETA: 1s - loss: 0.3983 - accuracy: 0.81 - ETA: 1s - loss: 0.3993 - accuracy: 0.81 - ETA: 1s - loss: 0.3988 - accuracy: 0.81 - ETA: 1s - loss: 0.3984 - accuracy: 0.81 - ETA: 1s - loss: 0.3996 - accuracy: 0.81 - ETA: 1s - loss: 0.3992 - accuracy: 0.81 - ETA: 1s - loss: 0.4003 - accuracy: 0.81 - ETA: 1s - loss: 0.4004 - accuracy: 0.81 - ETA: 0s - loss: 0.4012 - accuracy: 0.81 - ETA: 0s - loss: 0.4002 - accuracy: 0.81 - ETA: 0s - loss: 0.4015 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.3999 - accuracy: 0.81 - ETA: 0s - loss: 0.3998 - accuracy: 0.81 - ETA: 0s - loss: 0.3993 - accuracy: 0.81 - ETA: 0s - loss: 0.3996 - accuracy: 0.81 - ETA: 0s - loss: 0.4002 - accuracy: 0.81 - ETA: 0s - loss: 0.4005 - accuracy: 0.81 - ETA: 0s - loss: 0.4017 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.81 - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - 4s 363us/step - loss: 0.4048 - accuracy: 0.8102 - val_loss: 0.3781 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31344\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3926 - accuracy: 0.82 - ETA: 3s - loss: 0.4151 - accuracy: 0.80 - ETA: 3s - loss: 0.4097 - accuracy: 0.80 - ETA: 3s - loss: 0.4010 - accuracy: 0.82 - ETA: 3s - loss: 0.3941 - accuracy: 0.81 - ETA: 3s - loss: 0.3905 - accuracy: 0.82 - ETA: 3s - loss: 0.3871 - accuracy: 0.82 - ETA: 3s - loss: 0.3868 - accuracy: 0.82 - ETA: 3s - loss: 0.3929 - accuracy: 0.82 - ETA: 3s - loss: 0.3929 - accuracy: 0.82 - ETA: 3s - loss: 0.3926 - accuracy: 0.82 - ETA: 3s - loss: 0.3904 - accuracy: 0.82 - ETA: 3s - loss: 0.3957 - accuracy: 0.81 - ETA: 3s - loss: 0.3973 - accuracy: 0.81 - ETA: 3s - loss: 0.4011 - accuracy: 0.81 - ETA: 2s - loss: 0.3976 - accuracy: 0.81 - ETA: 2s - loss: 0.3935 - accuracy: 0.82 - ETA: 2s - loss: 0.3981 - accuracy: 0.82 - ETA: 2s - loss: 0.3965 - accuracy: 0.82 - ETA: 2s - loss: 0.3964 - accuracy: 0.82 - ETA: 2s - loss: 0.4002 - accuracy: 0.81 - ETA: 2s - loss: 0.3992 - accuracy: 0.81 - ETA: 2s - loss: 0.3992 - accuracy: 0.81 - ETA: 2s - loss: 0.3977 - accuracy: 0.82 - ETA: 2s - loss: 0.3978 - accuracy: 0.82 - ETA: 2s - loss: 0.3971 - accuracy: 0.82 - ETA: 2s - loss: 0.3954 - accuracy: 0.82 - ETA: 2s - loss: 0.3944 - accuracy: 0.82 - ETA: 2s - loss: 0.3933 - accuracy: 0.82 - ETA: 2s - loss: 0.3938 - accuracy: 0.82 - ETA: 2s - loss: 0.3927 - accuracy: 0.82 - ETA: 1s - loss: 0.3919 - accuracy: 0.82 - ETA: 1s - loss: 0.3907 - accuracy: 0.82 - ETA: 1s - loss: 0.3891 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3866 - accuracy: 0.82 - ETA: 1s - loss: 0.3873 - accuracy: 0.82 - ETA: 1s - loss: 0.3883 - accuracy: 0.82 - ETA: 1s - loss: 0.3901 - accuracy: 0.82 - ETA: 1s - loss: 0.3895 - accuracy: 0.82 - ETA: 1s - loss: 0.3882 - accuracy: 0.82 - ETA: 1s - loss: 0.3875 - accuracy: 0.82 - ETA: 1s - loss: 0.3895 - accuracy: 0.82 - ETA: 1s - loss: 0.3882 - accuracy: 0.82 - ETA: 1s - loss: 0.3894 - accuracy: 0.82 - ETA: 1s - loss: 0.3889 - accuracy: 0.82 - ETA: 1s - loss: 0.3889 - accuracy: 0.82 - ETA: 1s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3907 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - ETA: 0s - loss: 0.3943 - accuracy: 0.81 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3925 - accuracy: 0.82 - ETA: 0s - loss: 0.3925 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3932 - accuracy: 0.82 - ETA: 0s - loss: 0.3932 - accuracy: 0.82 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3925 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3947 - accuracy: 0.81 - ETA: 0s - loss: 0.3959 - accuracy: 0.81 - 4s 371us/step - loss: 0.3964 - accuracy: 0.8176 - val_loss: 0.3691 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31344\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3552 - accuracy: 0.86 - ETA: 3s - loss: 0.3835 - accuracy: 0.83 - ETA: 3s - loss: 0.3812 - accuracy: 0.83 - ETA: 3s - loss: 0.3753 - accuracy: 0.83 - ETA: 3s - loss: 0.3943 - accuracy: 0.82 - ETA: 3s - loss: 0.3969 - accuracy: 0.81 - ETA: 3s - loss: 0.3976 - accuracy: 0.81 - ETA: 3s - loss: 0.3921 - accuracy: 0.81 - ETA: 3s - loss: 0.3926 - accuracy: 0.81 - ETA: 3s - loss: 0.3877 - accuracy: 0.82 - ETA: 2s - loss: 0.3954 - accuracy: 0.81 - ETA: 2s - loss: 0.3974 - accuracy: 0.81 - ETA: 2s - loss: 0.3998 - accuracy: 0.81 - ETA: 2s - loss: 0.3969 - accuracy: 0.81 - ETA: 2s - loss: 0.4010 - accuracy: 0.81 - ETA: 2s - loss: 0.3984 - accuracy: 0.81 - ETA: 2s - loss: 0.3972 - accuracy: 0.81 - ETA: 2s - loss: 0.3989 - accuracy: 0.81 - ETA: 2s - loss: 0.3965 - accuracy: 0.81 - ETA: 2s - loss: 0.4002 - accuracy: 0.81 - ETA: 2s - loss: 0.3968 - accuracy: 0.81 - ETA: 2s - loss: 0.3955 - accuracy: 0.81 - ETA: 2s - loss: 0.3957 - accuracy: 0.81 - ETA: 2s - loss: 0.3978 - accuracy: 0.81 - ETA: 2s - loss: 0.3975 - accuracy: 0.81 - ETA: 2s - loss: 0.3978 - accuracy: 0.81 - ETA: 1s - loss: 0.3968 - accuracy: 0.81 - ETA: 1s - loss: 0.3952 - accuracy: 0.81 - ETA: 1s - loss: 0.3934 - accuracy: 0.81 - ETA: 1s - loss: 0.3930 - accuracy: 0.81 - ETA: 1s - loss: 0.3924 - accuracy: 0.81 - ETA: 1s - loss: 0.3920 - accuracy: 0.81 - ETA: 1s - loss: 0.3939 - accuracy: 0.81 - ETA: 1s - loss: 0.3946 - accuracy: 0.81 - ETA: 1s - loss: 0.3962 - accuracy: 0.81 - ETA: 1s - loss: 0.3966 - accuracy: 0.81 - ETA: 1s - loss: 0.3969 - accuracy: 0.81 - ETA: 1s - loss: 0.3966 - accuracy: 0.81 - ETA: 1s - loss: 0.3958 - accuracy: 0.81 - ETA: 1s - loss: 0.3958 - accuracy: 0.81 - ETA: 1s - loss: 0.3951 - accuracy: 0.81 - ETA: 1s - loss: 0.3957 - accuracy: 0.81 - ETA: 1s - loss: 0.3953 - accuracy: 0.81 - ETA: 1s - loss: 0.3968 - accuracy: 0.81 - ETA: 1s - loss: 0.3972 - accuracy: 0.81 - ETA: 0s - loss: 0.3983 - accuracy: 0.81 - ETA: 0s - loss: 0.3987 - accuracy: 0.81 - ETA: 0s - loss: 0.3984 - accuracy: 0.81 - ETA: 0s - loss: 0.3995 - accuracy: 0.81 - ETA: 0s - loss: 0.3987 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.81 - ETA: 0s - loss: 0.3993 - accuracy: 0.81 - ETA: 0s - loss: 0.3998 - accuracy: 0.81 - ETA: 0s - loss: 0.4010 - accuracy: 0.81 - ETA: 0s - loss: 0.4008 - accuracy: 0.81 - ETA: 0s - loss: 0.3997 - accuracy: 0.81 - ETA: 0s - loss: 0.3989 - accuracy: 0.81 - ETA: 0s - loss: 0.3983 - accuracy: 0.81 - ETA: 0s - loss: 0.3987 - accuracy: 0.81 - ETA: 0s - loss: 0.3992 - accuracy: 0.81 - ETA: 0s - loss: 0.3994 - accuracy: 0.81 - 4s 364us/step - loss: 0.3990 - accuracy: 0.8153 - val_loss: 0.3969 - val_accuracy: 0.8121\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31344\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4697 - accuracy: 0.75 - ETA: 3s - loss: 0.4607 - accuracy: 0.78 - ETA: 3s - loss: 0.4441 - accuracy: 0.79 - ETA: 3s - loss: 0.4408 - accuracy: 0.79 - ETA: 3s - loss: 0.4186 - accuracy: 0.80 - ETA: 3s - loss: 0.4195 - accuracy: 0.80 - ETA: 3s - loss: 0.4176 - accuracy: 0.80 - ETA: 3s - loss: 0.4136 - accuracy: 0.80 - ETA: 3s - loss: 0.4067 - accuracy: 0.80 - ETA: 3s - loss: 0.4066 - accuracy: 0.81 - ETA: 3s - loss: 0.4081 - accuracy: 0.80 - ETA: 3s - loss: 0.4101 - accuracy: 0.80 - ETA: 3s - loss: 0.4116 - accuracy: 0.80 - ETA: 3s - loss: 0.4164 - accuracy: 0.80 - ETA: 3s - loss: 0.4173 - accuracy: 0.80 - ETA: 2s - loss: 0.4192 - accuracy: 0.80 - ETA: 2s - loss: 0.4199 - accuracy: 0.80 - ETA: 2s - loss: 0.4176 - accuracy: 0.80 - ETA: 2s - loss: 0.4153 - accuracy: 0.80 - ETA: 2s - loss: 0.4133 - accuracy: 0.80 - ETA: 2s - loss: 0.4080 - accuracy: 0.81 - ETA: 2s - loss: 0.4098 - accuracy: 0.80 - ETA: 2s - loss: 0.4074 - accuracy: 0.81 - ETA: 2s - loss: 0.4067 - accuracy: 0.81 - ETA: 2s - loss: 0.4095 - accuracy: 0.80 - ETA: 2s - loss: 0.4069 - accuracy: 0.81 - ETA: 2s - loss: 0.4067 - accuracy: 0.81 - ETA: 2s - loss: 0.4088 - accuracy: 0.80 - ETA: 2s - loss: 0.4073 - accuracy: 0.80 - ETA: 2s - loss: 0.4056 - accuracy: 0.81 - ETA: 2s - loss: 0.4076 - accuracy: 0.80 - ETA: 2s - loss: 0.4080 - accuracy: 0.80 - ETA: 2s - loss: 0.4056 - accuracy: 0.80 - ETA: 1s - loss: 0.4021 - accuracy: 0.81 - ETA: 1s - loss: 0.4027 - accuracy: 0.81 - ETA: 1s - loss: 0.4035 - accuracy: 0.80 - ETA: 1s - loss: 0.4020 - accuracy: 0.81 - ETA: 1s - loss: 0.4033 - accuracy: 0.80 - ETA: 1s - loss: 0.4025 - accuracy: 0.80 - ETA: 1s - loss: 0.4023 - accuracy: 0.81 - ETA: 1s - loss: 0.4034 - accuracy: 0.80 - ETA: 1s - loss: 0.4030 - accuracy: 0.80 - ETA: 1s - loss: 0.4024 - accuracy: 0.80 - ETA: 1s - loss: 0.4030 - accuracy: 0.80 - ETA: 1s - loss: 0.4026 - accuracy: 0.80 - ETA: 1s - loss: 0.4023 - accuracy: 0.80 - ETA: 1s - loss: 0.4025 - accuracy: 0.80 - ETA: 1s - loss: 0.4043 - accuracy: 0.80 - ETA: 0s - loss: 0.4032 - accuracy: 0.80 - ETA: 0s - loss: 0.4032 - accuracy: 0.80 - ETA: 0s - loss: 0.4030 - accuracy: 0.80 - ETA: 0s - loss: 0.4030 - accuracy: 0.80 - ETA: 0s - loss: 0.4023 - accuracy: 0.80 - ETA: 0s - loss: 0.3997 - accuracy: 0.80 - ETA: 0s - loss: 0.3984 - accuracy: 0.80 - ETA: 0s - loss: 0.3974 - accuracy: 0.80 - ETA: 0s - loss: 0.3975 - accuracy: 0.81 - ETA: 0s - loss: 0.3974 - accuracy: 0.81 - ETA: 0s - loss: 0.3974 - accuracy: 0.81 - ETA: 0s - loss: 0.3962 - accuracy: 0.81 - ETA: 0s - loss: 0.3963 - accuracy: 0.81 - ETA: 0s - loss: 0.3963 - accuracy: 0.81 - ETA: 0s - loss: 0.3947 - accuracy: 0.81 - ETA: 0s - loss: 0.3935 - accuracy: 0.81 - 4s 370us/step - loss: 0.3929 - accuracy: 0.8131 - val_loss: 0.3474 - val_accuracy: 0.8450\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31344\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4304 - accuracy: 0.82 - ETA: 3s - loss: 0.4173 - accuracy: 0.80 - ETA: 3s - loss: 0.4269 - accuracy: 0.79 - ETA: 3s - loss: 0.4366 - accuracy: 0.79 - ETA: 3s - loss: 0.4290 - accuracy: 0.79 - ETA: 3s - loss: 0.4324 - accuracy: 0.79 - ETA: 3s - loss: 0.4275 - accuracy: 0.80 - ETA: 3s - loss: 0.4310 - accuracy: 0.79 - ETA: 3s - loss: 0.4051 - accuracy: 0.81 - ETA: 3s - loss: 0.3984 - accuracy: 0.81 - ETA: 3s - loss: 0.3990 - accuracy: 0.81 - ETA: 3s - loss: 0.3993 - accuracy: 0.81 - ETA: 3s - loss: 0.4022 - accuracy: 0.81 - ETA: 2s - loss: 0.4014 - accuracy: 0.81 - ETA: 2s - loss: 0.4001 - accuracy: 0.81 - ETA: 2s - loss: 0.3975 - accuracy: 0.81 - ETA: 2s - loss: 0.3948 - accuracy: 0.81 - ETA: 2s - loss: 0.3953 - accuracy: 0.81 - ETA: 2s - loss: 0.3957 - accuracy: 0.81 - ETA: 2s - loss: 0.3887 - accuracy: 0.82 - ETA: 2s - loss: 0.3882 - accuracy: 0.82 - ETA: 2s - loss: 0.3896 - accuracy: 0.81 - ETA: 2s - loss: 0.3892 - accuracy: 0.81 - ETA: 2s - loss: 0.3868 - accuracy: 0.82 - ETA: 2s - loss: 0.3848 - accuracy: 0.82 - ETA: 2s - loss: 0.3855 - accuracy: 0.82 - ETA: 2s - loss: 0.3860 - accuracy: 0.82 - ETA: 2s - loss: 0.3873 - accuracy: 0.82 - ETA: 2s - loss: 0.3900 - accuracy: 0.81 - ETA: 2s - loss: 0.3887 - accuracy: 0.82 - ETA: 1s - loss: 0.3902 - accuracy: 0.82 - ETA: 1s - loss: 0.3904 - accuracy: 0.82 - ETA: 1s - loss: 0.3931 - accuracy: 0.81 - ETA: 1s - loss: 0.3933 - accuracy: 0.81 - ETA: 1s - loss: 0.3932 - accuracy: 0.81 - ETA: 1s - loss: 0.3945 - accuracy: 0.81 - ETA: 1s - loss: 0.3952 - accuracy: 0.81 - ETA: 1s - loss: 0.3951 - accuracy: 0.81 - ETA: 1s - loss: 0.3951 - accuracy: 0.81 - ETA: 1s - loss: 0.3948 - accuracy: 0.81 - ETA: 1s - loss: 0.3951 - accuracy: 0.81 - ETA: 1s - loss: 0.3959 - accuracy: 0.81 - ETA: 1s - loss: 0.3966 - accuracy: 0.81 - ETA: 1s - loss: 0.3945 - accuracy: 0.81 - ETA: 1s - loss: 0.3925 - accuracy: 0.81 - ETA: 1s - loss: 0.3919 - accuracy: 0.81 - ETA: 1s - loss: 0.3914 - accuracy: 0.81 - ETA: 1s - loss: 0.3921 - accuracy: 0.81 - ETA: 0s - loss: 0.3909 - accuracy: 0.81 - ETA: 0s - loss: 0.3898 - accuracy: 0.81 - ETA: 0s - loss: 0.3889 - accuracy: 0.81 - ETA: 0s - loss: 0.3886 - accuracy: 0.81 - ETA: 0s - loss: 0.3896 - accuracy: 0.81 - ETA: 0s - loss: 0.3889 - accuracy: 0.81 - ETA: 0s - loss: 0.3882 - accuracy: 0.81 - ETA: 0s - loss: 0.3873 - accuracy: 0.81 - ETA: 0s - loss: 0.3880 - accuracy: 0.81 - ETA: 0s - loss: 0.3893 - accuracy: 0.81 - ETA: 0s - loss: 0.3885 - accuracy: 0.81 - ETA: 0s - loss: 0.3876 - accuracy: 0.81 - ETA: 0s - loss: 0.3883 - accuracy: 0.81 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3866 - accuracy: 0.82 - ETA: 0s - loss: 0.3867 - accuracy: 0.82 - 4s 368us/step - loss: 0.3863 - accuracy: 0.8205 - val_loss: 0.3697 - val_accuracy: 0.8180\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31344\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3569 - accuracy: 0.81 - ETA: 3s - loss: 0.3958 - accuracy: 0.82 - ETA: 3s - loss: 0.4247 - accuracy: 0.81 - ETA: 3s - loss: 0.4179 - accuracy: 0.81 - ETA: 3s - loss: 0.4181 - accuracy: 0.81 - ETA: 3s - loss: 0.4126 - accuracy: 0.81 - ETA: 3s - loss: 0.4072 - accuracy: 0.81 - ETA: 3s - loss: 0.4097 - accuracy: 0.81 - ETA: 3s - loss: 0.4025 - accuracy: 0.81 - ETA: 3s - loss: 0.4081 - accuracy: 0.81 - ETA: 3s - loss: 0.4044 - accuracy: 0.81 - ETA: 3s - loss: 0.3998 - accuracy: 0.81 - ETA: 3s - loss: 0.3967 - accuracy: 0.82 - ETA: 2s - loss: 0.4023 - accuracy: 0.81 - ETA: 2s - loss: 0.3993 - accuracy: 0.82 - ETA: 2s - loss: 0.3962 - accuracy: 0.82 - ETA: 2s - loss: 0.3961 - accuracy: 0.81 - ETA: 2s - loss: 0.3975 - accuracy: 0.81 - ETA: 2s - loss: 0.3986 - accuracy: 0.81 - ETA: 2s - loss: 0.3933 - accuracy: 0.82 - ETA: 2s - loss: 0.3946 - accuracy: 0.82 - ETA: 2s - loss: 0.3915 - accuracy: 0.82 - ETA: 2s - loss: 0.3923 - accuracy: 0.82 - ETA: 2s - loss: 0.3925 - accuracy: 0.82 - ETA: 2s - loss: 0.3946 - accuracy: 0.82 - ETA: 2s - loss: 0.3943 - accuracy: 0.82 - ETA: 2s - loss: 0.3913 - accuracy: 0.82 - ETA: 2s - loss: 0.3903 - accuracy: 0.82 - ETA: 2s - loss: 0.3901 - accuracy: 0.82 - ETA: 1s - loss: 0.3897 - accuracy: 0.82 - ETA: 1s - loss: 0.3884 - accuracy: 0.82 - ETA: 1s - loss: 0.3866 - accuracy: 0.82 - ETA: 1s - loss: 0.3892 - accuracy: 0.82 - ETA: 1s - loss: 0.3893 - accuracy: 0.82 - ETA: 1s - loss: 0.3906 - accuracy: 0.82 - ETA: 1s - loss: 0.3909 - accuracy: 0.82 - ETA: 1s - loss: 0.3901 - accuracy: 0.82 - ETA: 1s - loss: 0.3900 - accuracy: 0.82 - ETA: 1s - loss: 0.3900 - accuracy: 0.82 - ETA: 1s - loss: 0.3895 - accuracy: 0.82 - ETA: 1s - loss: 0.3885 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3887 - accuracy: 0.82 - ETA: 1s - loss: 0.3882 - accuracy: 0.82 - ETA: 1s - loss: 0.3864 - accuracy: 0.82 - ETA: 1s - loss: 0.3857 - accuracy: 0.82 - ETA: 0s - loss: 0.3862 - accuracy: 0.82 - ETA: 0s - loss: 0.3852 - accuracy: 0.82 - ETA: 0s - loss: 0.3841 - accuracy: 0.82 - ETA: 0s - loss: 0.3847 - accuracy: 0.82 - ETA: 0s - loss: 0.3846 - accuracy: 0.82 - ETA: 0s - loss: 0.3852 - accuracy: 0.82 - ETA: 0s - loss: 0.3859 - accuracy: 0.82 - ETA: 0s - loss: 0.3863 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3869 - accuracy: 0.82 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3881 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3867 - accuracy: 0.82 - ETA: 0s - loss: 0.3857 - accuracy: 0.82 - ETA: 0s - loss: 0.3841 - accuracy: 0.82 - ETA: 0s - loss: 0.3850 - accuracy: 0.82 - ETA: 0s - loss: 0.3846 - accuracy: 0.82 - 4s 369us/step - loss: 0.3839 - accuracy: 0.8235 - val_loss: 0.3783 - val_accuracy: 0.8206\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31344\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3298 - accuracy: 0.88 - ETA: 3s - loss: 0.3041 - accuracy: 0.88 - ETA: 3s - loss: 0.3441 - accuracy: 0.87 - ETA: 3s - loss: 0.3601 - accuracy: 0.86 - ETA: 3s - loss: 0.3743 - accuracy: 0.85 - ETA: 3s - loss: 0.3799 - accuracy: 0.84 - ETA: 3s - loss: 0.3854 - accuracy: 0.84 - ETA: 3s - loss: 0.3913 - accuracy: 0.83 - ETA: 3s - loss: 0.3802 - accuracy: 0.83 - ETA: 3s - loss: 0.3842 - accuracy: 0.83 - ETA: 3s - loss: 0.3804 - accuracy: 0.83 - ETA: 3s - loss: 0.3793 - accuracy: 0.83 - ETA: 3s - loss: 0.3774 - accuracy: 0.83 - ETA: 3s - loss: 0.3726 - accuracy: 0.84 - ETA: 3s - loss: 0.3835 - accuracy: 0.83 - ETA: 3s - loss: 0.3800 - accuracy: 0.83 - ETA: 3s - loss: 0.3791 - accuracy: 0.83 - ETA: 2s - loss: 0.3753 - accuracy: 0.83 - ETA: 2s - loss: 0.3736 - accuracy: 0.83 - ETA: 2s - loss: 0.3733 - accuracy: 0.83 - ETA: 2s - loss: 0.3766 - accuracy: 0.83 - ETA: 2s - loss: 0.3776 - accuracy: 0.83 - ETA: 2s - loss: 0.3761 - accuracy: 0.83 - ETA: 2s - loss: 0.3756 - accuracy: 0.83 - ETA: 2s - loss: 0.3773 - accuracy: 0.83 - ETA: 2s - loss: 0.3761 - accuracy: 0.83 - ETA: 2s - loss: 0.3752 - accuracy: 0.83 - ETA: 2s - loss: 0.3742 - accuracy: 0.83 - ETA: 2s - loss: 0.3746 - accuracy: 0.83 - ETA: 2s - loss: 0.3751 - accuracy: 0.83 - ETA: 2s - loss: 0.3777 - accuracy: 0.83 - ETA: 2s - loss: 0.3776 - accuracy: 0.83 - ETA: 1s - loss: 0.3799 - accuracy: 0.83 - ETA: 1s - loss: 0.3791 - accuracy: 0.83 - ETA: 1s - loss: 0.3779 - accuracy: 0.83 - ETA: 1s - loss: 0.3799 - accuracy: 0.83 - ETA: 1s - loss: 0.3813 - accuracy: 0.83 - ETA: 1s - loss: 0.3799 - accuracy: 0.83 - ETA: 1s - loss: 0.3795 - accuracy: 0.83 - ETA: 1s - loss: 0.3773 - accuracy: 0.83 - ETA: 1s - loss: 0.3762 - accuracy: 0.83 - ETA: 1s - loss: 0.3786 - accuracy: 0.83 - ETA: 1s - loss: 0.3771 - accuracy: 0.83 - ETA: 1s - loss: 0.3787 - accuracy: 0.83 - ETA: 1s - loss: 0.3791 - accuracy: 0.83 - ETA: 1s - loss: 0.3786 - accuracy: 0.83 - ETA: 1s - loss: 0.3784 - accuracy: 0.83 - ETA: 1s - loss: 0.3803 - accuracy: 0.82 - ETA: 0s - loss: 0.3800 - accuracy: 0.82 - ETA: 0s - loss: 0.3788 - accuracy: 0.82 - ETA: 0s - loss: 0.3794 - accuracy: 0.82 - ETA: 0s - loss: 0.3790 - accuracy: 0.82 - ETA: 0s - loss: 0.3810 - accuracy: 0.82 - ETA: 0s - loss: 0.3815 - accuracy: 0.82 - ETA: 0s - loss: 0.3815 - accuracy: 0.82 - ETA: 0s - loss: 0.3809 - accuracy: 0.82 - ETA: 0s - loss: 0.3792 - accuracy: 0.82 - ETA: 0s - loss: 0.3785 - accuracy: 0.82 - ETA: 0s - loss: 0.3785 - accuracy: 0.82 - ETA: 0s - loss: 0.3795 - accuracy: 0.82 - ETA: 0s - loss: 0.3812 - accuracy: 0.82 - ETA: 0s - loss: 0.3813 - accuracy: 0.82 - ETA: 0s - loss: 0.3813 - accuracy: 0.82 - ETA: 0s - loss: 0.3810 - accuracy: 0.82 - ETA: 0s - loss: 0.3812 - accuracy: 0.82 - 4s 368us/step - loss: 0.3809 - accuracy: 0.8266 - val_loss: 0.3480 - val_accuracy: 0.8366\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31344\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4525 - accuracy: 0.77 - ETA: 3s - loss: 0.4270 - accuracy: 0.78 - ETA: 3s - loss: 0.4038 - accuracy: 0.81 - ETA: 3s - loss: 0.4122 - accuracy: 0.80 - ETA: 3s - loss: 0.4019 - accuracy: 0.81 - ETA: 3s - loss: 0.3928 - accuracy: 0.82 - ETA: 3s - loss: 0.3960 - accuracy: 0.82 - ETA: 3s - loss: 0.3966 - accuracy: 0.82 - ETA: 3s - loss: 0.3932 - accuracy: 0.82 - ETA: 3s - loss: 0.4034 - accuracy: 0.81 - ETA: 3s - loss: 0.4017 - accuracy: 0.82 - ETA: 3s - loss: 0.3955 - accuracy: 0.82 - ETA: 3s - loss: 0.3892 - accuracy: 0.82 - ETA: 3s - loss: 0.3923 - accuracy: 0.82 - ETA: 2s - loss: 0.3938 - accuracy: 0.82 - ETA: 2s - loss: 0.3877 - accuracy: 0.82 - ETA: 2s - loss: 0.3858 - accuracy: 0.82 - ETA: 2s - loss: 0.3856 - accuracy: 0.82 - ETA: 2s - loss: 0.3836 - accuracy: 0.82 - ETA: 2s - loss: 0.3818 - accuracy: 0.82 - ETA: 2s - loss: 0.3810 - accuracy: 0.82 - ETA: 2s - loss: 0.3811 - accuracy: 0.82 - ETA: 2s - loss: 0.3797 - accuracy: 0.82 - ETA: 2s - loss: 0.3767 - accuracy: 0.82 - ETA: 2s - loss: 0.3754 - accuracy: 0.82 - ETA: 2s - loss: 0.3738 - accuracy: 0.82 - ETA: 2s - loss: 0.3751 - accuracy: 0.82 - ETA: 2s - loss: 0.3734 - accuracy: 0.82 - ETA: 2s - loss: 0.3753 - accuracy: 0.82 - ETA: 2s - loss: 0.3758 - accuracy: 0.82 - ETA: 2s - loss: 0.3749 - accuracy: 0.82 - ETA: 1s - loss: 0.3751 - accuracy: 0.82 - ETA: 1s - loss: 0.3754 - accuracy: 0.82 - ETA: 1s - loss: 0.3733 - accuracy: 0.83 - ETA: 1s - loss: 0.3744 - accuracy: 0.83 - ETA: 1s - loss: 0.3746 - accuracy: 0.83 - ETA: 1s - loss: 0.3748 - accuracy: 0.82 - ETA: 1s - loss: 0.3783 - accuracy: 0.82 - ETA: 1s - loss: 0.3765 - accuracy: 0.82 - ETA: 1s - loss: 0.3750 - accuracy: 0.82 - ETA: 1s - loss: 0.3753 - accuracy: 0.82 - ETA: 1s - loss: 0.3758 - accuracy: 0.82 - ETA: 1s - loss: 0.3753 - accuracy: 0.82 - ETA: 1s - loss: 0.3747 - accuracy: 0.82 - ETA: 1s - loss: 0.3726 - accuracy: 0.83 - ETA: 1s - loss: 0.3735 - accuracy: 0.83 - ETA: 1s - loss: 0.3729 - accuracy: 0.83 - ETA: 0s - loss: 0.3719 - accuracy: 0.83 - ETA: 0s - loss: 0.3703 - accuracy: 0.83 - ETA: 0s - loss: 0.3716 - accuracy: 0.83 - ETA: 0s - loss: 0.3713 - accuracy: 0.83 - ETA: 0s - loss: 0.3725 - accuracy: 0.83 - ETA: 0s - loss: 0.3728 - accuracy: 0.83 - ETA: 0s - loss: 0.3724 - accuracy: 0.83 - ETA: 0s - loss: 0.3735 - accuracy: 0.83 - ETA: 0s - loss: 0.3727 - accuracy: 0.83 - ETA: 0s - loss: 0.3724 - accuracy: 0.83 - ETA: 0s - loss: 0.3719 - accuracy: 0.83 - ETA: 0s - loss: 0.3712 - accuracy: 0.83 - ETA: 0s - loss: 0.3718 - accuracy: 0.83 - ETA: 0s - loss: 0.3730 - accuracy: 0.83 - 4s 367us/step - loss: 0.3732 - accuracy: 0.8300 - val_loss: 0.3796 - val_accuracy: 0.8340\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31344\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3954 - accuracy: 0.80 - ETA: 3s - loss: 0.3477 - accuracy: 0.85 - ETA: 3s - loss: 0.3664 - accuracy: 0.84 - ETA: 3s - loss: 0.3656 - accuracy: 0.84 - ETA: 3s - loss: 0.3628 - accuracy: 0.84 - ETA: 3s - loss: 0.3727 - accuracy: 0.83 - ETA: 3s - loss: 0.3754 - accuracy: 0.83 - ETA: 3s - loss: 0.3683 - accuracy: 0.83 - ETA: 3s - loss: 0.3787 - accuracy: 0.83 - ETA: 3s - loss: 0.3766 - accuracy: 0.83 - ETA: 2s - loss: 0.3760 - accuracy: 0.83 - ETA: 2s - loss: 0.3767 - accuracy: 0.83 - ETA: 2s - loss: 0.3810 - accuracy: 0.82 - ETA: 2s - loss: 0.3828 - accuracy: 0.82 - ETA: 2s - loss: 0.3817 - accuracy: 0.82 - ETA: 2s - loss: 0.3752 - accuracy: 0.83 - ETA: 2s - loss: 0.3743 - accuracy: 0.83 - ETA: 2s - loss: 0.3722 - accuracy: 0.83 - ETA: 2s - loss: 0.3691 - accuracy: 0.83 - ETA: 2s - loss: 0.3674 - accuracy: 0.83 - ETA: 2s - loss: 0.3688 - accuracy: 0.83 - ETA: 2s - loss: 0.3683 - accuracy: 0.83 - ETA: 2s - loss: 0.3680 - accuracy: 0.83 - ETA: 2s - loss: 0.3692 - accuracy: 0.83 - ETA: 2s - loss: 0.3680 - accuracy: 0.83 - ETA: 2s - loss: 0.3693 - accuracy: 0.82 - ETA: 2s - loss: 0.3683 - accuracy: 0.82 - ETA: 2s - loss: 0.3698 - accuracy: 0.82 - ETA: 2s - loss: 0.3706 - accuracy: 0.82 - ETA: 2s - loss: 0.3690 - accuracy: 0.82 - ETA: 1s - loss: 0.3677 - accuracy: 0.82 - ETA: 1s - loss: 0.3689 - accuracy: 0.82 - ETA: 1s - loss: 0.3666 - accuracy: 0.82 - ETA: 1s - loss: 0.3668 - accuracy: 0.82 - ETA: 1s - loss: 0.3668 - accuracy: 0.82 - ETA: 1s - loss: 0.3664 - accuracy: 0.82 - ETA: 1s - loss: 0.3654 - accuracy: 0.82 - ETA: 1s - loss: 0.3651 - accuracy: 0.82 - ETA: 1s - loss: 0.3647 - accuracy: 0.82 - ETA: 1s - loss: 0.3642 - accuracy: 0.82 - ETA: 1s - loss: 0.3622 - accuracy: 0.83 - ETA: 1s - loss: 0.3636 - accuracy: 0.82 - ETA: 1s - loss: 0.3668 - accuracy: 0.82 - ETA: 1s - loss: 0.3670 - accuracy: 0.82 - ETA: 1s - loss: 0.3658 - accuracy: 0.82 - ETA: 1s - loss: 0.3680 - accuracy: 0.82 - ETA: 1s - loss: 0.3681 - accuracy: 0.82 - ETA: 1s - loss: 0.3686 - accuracy: 0.82 - ETA: 0s - loss: 0.3715 - accuracy: 0.82 - ETA: 0s - loss: 0.3711 - accuracy: 0.82 - ETA: 0s - loss: 0.3713 - accuracy: 0.82 - ETA: 0s - loss: 0.3717 - accuracy: 0.82 - ETA: 0s - loss: 0.3709 - accuracy: 0.82 - ETA: 0s - loss: 0.3708 - accuracy: 0.82 - ETA: 0s - loss: 0.3707 - accuracy: 0.82 - ETA: 0s - loss: 0.3716 - accuracy: 0.82 - ETA: 0s - loss: 0.3716 - accuracy: 0.82 - ETA: 0s - loss: 0.3728 - accuracy: 0.82 - ETA: 0s - loss: 0.3737 - accuracy: 0.82 - ETA: 0s - loss: 0.3721 - accuracy: 0.82 - ETA: 0s - loss: 0.3728 - accuracy: 0.82 - ETA: 0s - loss: 0.3732 - accuracy: 0.82 - ETA: 0s - loss: 0.3727 - accuracy: 0.82 - 4s 369us/step - loss: 0.3726 - accuracy: 0.8266 - val_loss: 0.3628 - val_accuracy: 0.8315\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31344\n",
      "Epoch 00018: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 222us/step\n",
      "[2020-05-18 16:00:37 RAM64.7% 0.63GB] Val Score : [0.3416585911056867, 0.8377558588981628]\n",
      "[2020-05-18 16:00:37 RAM64.7% 0.63GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:00:37 RAM64.7% 0.63GB] Training on Fold : 5\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 42s - loss: 2.2928 - accuracy: 0.379 - ETA: 22s - loss: 2.1128 - accuracy: 0.458 - ETA: 16s - loss: 1.7176 - accuracy: 0.487 - ETA: 12s - loss: 1.4837 - accuracy: 0.500 - ETA: 10s - loss: 1.3359 - accuracy: 0.513 - ETA: 9s - loss: 1.2268 - accuracy: 0.529 - ETA: 8s - loss: 1.1464 - accuracy: 0.54 - ETA: 7s - loss: 1.0699 - accuracy: 0.56 - ETA: 6s - loss: 0.9853 - accuracy: 0.57 - ETA: 6s - loss: 0.9568 - accuracy: 0.58 - ETA: 5s - loss: 0.9291 - accuracy: 0.59 - ETA: 5s - loss: 0.9055 - accuracy: 0.59 - ETA: 5s - loss: 0.8844 - accuracy: 0.60 - ETA: 5s - loss: 0.8671 - accuracy: 0.60 - ETA: 4s - loss: 0.8517 - accuracy: 0.61 - ETA: 4s - loss: 0.8404 - accuracy: 0.61 - ETA: 4s - loss: 0.8266 - accuracy: 0.61 - ETA: 4s - loss: 0.8126 - accuracy: 0.62 - ETA: 4s - loss: 0.7906 - accuracy: 0.63 - ETA: 3s - loss: 0.7806 - accuracy: 0.63 - ETA: 3s - loss: 0.7701 - accuracy: 0.64 - ETA: 3s - loss: 0.7643 - accuracy: 0.64 - ETA: 3s - loss: 0.7608 - accuracy: 0.64 - ETA: 3s - loss: 0.7474 - accuracy: 0.64 - ETA: 3s - loss: 0.7394 - accuracy: 0.65 - ETA: 3s - loss: 0.7276 - accuracy: 0.65 - ETA: 2s - loss: 0.7148 - accuracy: 0.66 - ETA: 2s - loss: 0.7097 - accuracy: 0.66 - ETA: 2s - loss: 0.7067 - accuracy: 0.66 - ETA: 2s - loss: 0.7035 - accuracy: 0.66 - ETA: 2s - loss: 0.7020 - accuracy: 0.66 - ETA: 2s - loss: 0.7002 - accuracy: 0.66 - ETA: 2s - loss: 0.6937 - accuracy: 0.67 - ETA: 2s - loss: 0.6907 - accuracy: 0.67 - ETA: 2s - loss: 0.6875 - accuracy: 0.67 - ETA: 2s - loss: 0.6826 - accuracy: 0.67 - ETA: 1s - loss: 0.6808 - accuracy: 0.68 - ETA: 1s - loss: 0.6789 - accuracy: 0.67 - ETA: 1s - loss: 0.6772 - accuracy: 0.68 - ETA: 1s - loss: 0.6760 - accuracy: 0.68 - ETA: 1s - loss: 0.6736 - accuracy: 0.68 - ETA: 1s - loss: 0.6702 - accuracy: 0.68 - ETA: 1s - loss: 0.6651 - accuracy: 0.68 - ETA: 1s - loss: 0.6613 - accuracy: 0.68 - ETA: 1s - loss: 0.6609 - accuracy: 0.68 - ETA: 1s - loss: 0.6598 - accuracy: 0.69 - ETA: 1s - loss: 0.6569 - accuracy: 0.69 - ETA: 1s - loss: 0.6552 - accuracy: 0.69 - ETA: 1s - loss: 0.6535 - accuracy: 0.69 - ETA: 1s - loss: 0.6519 - accuracy: 0.69 - ETA: 0s - loss: 0.6488 - accuracy: 0.69 - ETA: 0s - loss: 0.6473 - accuracy: 0.69 - ETA: 0s - loss: 0.6449 - accuracy: 0.69 - ETA: 0s - loss: 0.6424 - accuracy: 0.69 - ETA: 0s - loss: 0.6404 - accuracy: 0.70 - ETA: 0s - loss: 0.6381 - accuracy: 0.70 - ETA: 0s - loss: 0.6383 - accuracy: 0.70 - ETA: 0s - loss: 0.6364 - accuracy: 0.70 - ETA: 0s - loss: 0.6345 - accuracy: 0.70 - ETA: 0s - loss: 0.6324 - accuracy: 0.70 - ETA: 0s - loss: 0.6325 - accuracy: 0.70 - ETA: 0s - loss: 0.6309 - accuracy: 0.70 - ETA: 0s - loss: 0.6290 - accuracy: 0.70 - ETA: 0s - loss: 0.6273 - accuracy: 0.70 - ETA: 0s - loss: 0.6267 - accuracy: 0.70 - ETA: 0s - loss: 0.6253 - accuracy: 0.71 - 5s 437us/step - loss: 0.6240 - accuracy: 0.7112 - val_loss: 0.4917 - val_accuracy: 0.8045\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31344\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4781 - accuracy: 0.78 - ETA: 3s - loss: 0.5105 - accuracy: 0.78 - ETA: 3s - loss: 0.5190 - accuracy: 0.78 - ETA: 3s - loss: 0.5177 - accuracy: 0.78 - ETA: 3s - loss: 0.5200 - accuracy: 0.78 - ETA: 3s - loss: 0.5241 - accuracy: 0.78 - ETA: 3s - loss: 0.5277 - accuracy: 0.78 - ETA: 3s - loss: 0.5304 - accuracy: 0.78 - ETA: 3s - loss: 0.5280 - accuracy: 0.78 - ETA: 3s - loss: 0.5273 - accuracy: 0.78 - ETA: 3s - loss: 0.5267 - accuracy: 0.78 - ETA: 3s - loss: 0.5276 - accuracy: 0.77 - ETA: 3s - loss: 0.5287 - accuracy: 0.77 - ETA: 2s - loss: 0.5335 - accuracy: 0.77 - ETA: 2s - loss: 0.5272 - accuracy: 0.77 - ETA: 2s - loss: 0.5246 - accuracy: 0.77 - ETA: 2s - loss: 0.5203 - accuracy: 0.77 - ETA: 2s - loss: 0.5238 - accuracy: 0.77 - ETA: 2s - loss: 0.5224 - accuracy: 0.77 - ETA: 2s - loss: 0.5217 - accuracy: 0.77 - ETA: 2s - loss: 0.5206 - accuracy: 0.77 - ETA: 2s - loss: 0.5193 - accuracy: 0.77 - ETA: 2s - loss: 0.5172 - accuracy: 0.77 - ETA: 2s - loss: 0.5164 - accuracy: 0.77 - ETA: 2s - loss: 0.5155 - accuracy: 0.77 - ETA: 2s - loss: 0.5178 - accuracy: 0.77 - ETA: 2s - loss: 0.5177 - accuracy: 0.77 - ETA: 2s - loss: 0.5170 - accuracy: 0.77 - ETA: 2s - loss: 0.5164 - accuracy: 0.77 - ETA: 2s - loss: 0.5138 - accuracy: 0.77 - ETA: 1s - loss: 0.5115 - accuracy: 0.77 - ETA: 1s - loss: 0.5106 - accuracy: 0.77 - ETA: 1s - loss: 0.5097 - accuracy: 0.77 - ETA: 1s - loss: 0.5107 - accuracy: 0.77 - ETA: 1s - loss: 0.5107 - accuracy: 0.77 - ETA: 1s - loss: 0.5107 - accuracy: 0.77 - ETA: 1s - loss: 0.5115 - accuracy: 0.77 - ETA: 1s - loss: 0.5112 - accuracy: 0.77 - ETA: 1s - loss: 0.5117 - accuracy: 0.77 - ETA: 1s - loss: 0.5092 - accuracy: 0.77 - ETA: 1s - loss: 0.5085 - accuracy: 0.77 - ETA: 1s - loss: 0.5084 - accuracy: 0.77 - ETA: 1s - loss: 0.5084 - accuracy: 0.77 - ETA: 1s - loss: 0.5078 - accuracy: 0.77 - ETA: 1s - loss: 0.5063 - accuracy: 0.77 - ETA: 1s - loss: 0.5070 - accuracy: 0.77 - ETA: 1s - loss: 0.5065 - accuracy: 0.77 - ETA: 0s - loss: 0.5065 - accuracy: 0.77 - ETA: 0s - loss: 0.5060 - accuracy: 0.77 - ETA: 0s - loss: 0.5047 - accuracy: 0.77 - ETA: 0s - loss: 0.5060 - accuracy: 0.77 - ETA: 0s - loss: 0.5055 - accuracy: 0.77 - ETA: 0s - loss: 0.5044 - accuracy: 0.77 - ETA: 0s - loss: 0.5058 - accuracy: 0.77 - ETA: 0s - loss: 0.5061 - accuracy: 0.77 - ETA: 0s - loss: 0.5068 - accuracy: 0.77 - ETA: 0s - loss: 0.5064 - accuracy: 0.77 - ETA: 0s - loss: 0.5068 - accuracy: 0.77 - ETA: 0s - loss: 0.5063 - accuracy: 0.77 - ETA: 0s - loss: 0.5061 - accuracy: 0.77 - ETA: 0s - loss: 0.5066 - accuracy: 0.77 - ETA: 0s - loss: 0.5065 - accuracy: 0.77 - ETA: 0s - loss: 0.5062 - accuracy: 0.77 - 4s 370us/step - loss: 0.5069 - accuracy: 0.7769 - val_loss: 0.4870 - val_accuracy: 0.7599\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31344\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5735 - accuracy: 0.71 - ETA: 3s - loss: 0.5169 - accuracy: 0.76 - ETA: 3s - loss: 0.5119 - accuracy: 0.76 - ETA: 3s - loss: 0.5040 - accuracy: 0.76 - ETA: 3s - loss: 0.4952 - accuracy: 0.76 - ETA: 3s - loss: 0.4944 - accuracy: 0.77 - ETA: 3s - loss: 0.4892 - accuracy: 0.77 - ETA: 3s - loss: 0.4854 - accuracy: 0.77 - ETA: 3s - loss: 0.4833 - accuracy: 0.77 - ETA: 3s - loss: 0.4901 - accuracy: 0.77 - ETA: 2s - loss: 0.4943 - accuracy: 0.77 - ETA: 2s - loss: 0.4955 - accuracy: 0.77 - ETA: 2s - loss: 0.4992 - accuracy: 0.77 - ETA: 2s - loss: 0.4954 - accuracy: 0.77 - ETA: 2s - loss: 0.4939 - accuracy: 0.77 - ETA: 2s - loss: 0.4972 - accuracy: 0.77 - ETA: 2s - loss: 0.4958 - accuracy: 0.77 - ETA: 2s - loss: 0.4965 - accuracy: 0.77 - ETA: 2s - loss: 0.4941 - accuracy: 0.77 - ETA: 2s - loss: 0.4907 - accuracy: 0.77 - ETA: 2s - loss: 0.4921 - accuracy: 0.77 - ETA: 2s - loss: 0.4930 - accuracy: 0.77 - ETA: 2s - loss: 0.4927 - accuracy: 0.77 - ETA: 2s - loss: 0.4915 - accuracy: 0.77 - ETA: 2s - loss: 0.4904 - accuracy: 0.77 - ETA: 2s - loss: 0.4891 - accuracy: 0.78 - ETA: 2s - loss: 0.4901 - accuracy: 0.77 - ETA: 2s - loss: 0.4897 - accuracy: 0.77 - ETA: 1s - loss: 0.4870 - accuracy: 0.78 - ETA: 1s - loss: 0.4857 - accuracy: 0.78 - ETA: 1s - loss: 0.4857 - accuracy: 0.78 - ETA: 1s - loss: 0.4878 - accuracy: 0.77 - ETA: 1s - loss: 0.4892 - accuracy: 0.77 - ETA: 1s - loss: 0.4898 - accuracy: 0.78 - ETA: 1s - loss: 0.4910 - accuracy: 0.78 - ETA: 1s - loss: 0.4911 - accuracy: 0.78 - ETA: 1s - loss: 0.4903 - accuracy: 0.78 - ETA: 1s - loss: 0.4894 - accuracy: 0.78 - ETA: 1s - loss: 0.4875 - accuracy: 0.78 - ETA: 1s - loss: 0.4865 - accuracy: 0.78 - ETA: 1s - loss: 0.4870 - accuracy: 0.78 - ETA: 1s - loss: 0.4860 - accuracy: 0.78 - ETA: 1s - loss: 0.4859 - accuracy: 0.78 - ETA: 1s - loss: 0.4866 - accuracy: 0.78 - ETA: 1s - loss: 0.4869 - accuracy: 0.78 - ETA: 1s - loss: 0.4885 - accuracy: 0.78 - ETA: 0s - loss: 0.4880 - accuracy: 0.78 - ETA: 0s - loss: 0.4869 - accuracy: 0.78 - ETA: 0s - loss: 0.4886 - accuracy: 0.78 - ETA: 0s - loss: 0.4881 - accuracy: 0.78 - ETA: 0s - loss: 0.4868 - accuracy: 0.78 - ETA: 0s - loss: 0.4871 - accuracy: 0.78 - ETA: 0s - loss: 0.4859 - accuracy: 0.78 - ETA: 0s - loss: 0.4867 - accuracy: 0.78 - ETA: 0s - loss: 0.4878 - accuracy: 0.78 - ETA: 0s - loss: 0.4879 - accuracy: 0.78 - ETA: 0s - loss: 0.4863 - accuracy: 0.78 - ETA: 0s - loss: 0.4862 - accuracy: 0.78 - ETA: 0s - loss: 0.4859 - accuracy: 0.78 - ETA: 0s - loss: 0.4857 - accuracy: 0.78 - ETA: 0s - loss: 0.4853 - accuracy: 0.78 - ETA: 0s - loss: 0.4846 - accuracy: 0.78 - 4s 368us/step - loss: 0.4848 - accuracy: 0.7823 - val_loss: 0.4634 - val_accuracy: 0.8206\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31344\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4908 - accuracy: 0.77 - ETA: 3s - loss: 0.5067 - accuracy: 0.77 - ETA: 3s - loss: 0.4810 - accuracy: 0.78 - ETA: 3s - loss: 0.4714 - accuracy: 0.78 - ETA: 3s - loss: 0.4647 - accuracy: 0.79 - ETA: 3s - loss: 0.4576 - accuracy: 0.79 - ETA: 3s - loss: 0.4701 - accuracy: 0.78 - ETA: 3s - loss: 0.4620 - accuracy: 0.79 - ETA: 3s - loss: 0.4642 - accuracy: 0.79 - ETA: 3s - loss: 0.4722 - accuracy: 0.78 - ETA: 3s - loss: 0.4726 - accuracy: 0.78 - ETA: 3s - loss: 0.4724 - accuracy: 0.78 - ETA: 3s - loss: 0.4700 - accuracy: 0.78 - ETA: 2s - loss: 0.4643 - accuracy: 0.78 - ETA: 2s - loss: 0.4655 - accuracy: 0.78 - ETA: 2s - loss: 0.4656 - accuracy: 0.78 - ETA: 2s - loss: 0.4630 - accuracy: 0.78 - ETA: 2s - loss: 0.4625 - accuracy: 0.78 - ETA: 2s - loss: 0.4599 - accuracy: 0.78 - ETA: 2s - loss: 0.4628 - accuracy: 0.78 - ETA: 2s - loss: 0.4630 - accuracy: 0.78 - ETA: 2s - loss: 0.4604 - accuracy: 0.79 - ETA: 2s - loss: 0.4598 - accuracy: 0.79 - ETA: 2s - loss: 0.4588 - accuracy: 0.79 - ETA: 2s - loss: 0.4631 - accuracy: 0.78 - ETA: 2s - loss: 0.4646 - accuracy: 0.79 - ETA: 2s - loss: 0.4657 - accuracy: 0.78 - ETA: 2s - loss: 0.4666 - accuracy: 0.78 - ETA: 2s - loss: 0.4663 - accuracy: 0.78 - ETA: 2s - loss: 0.4669 - accuracy: 0.78 - ETA: 2s - loss: 0.4660 - accuracy: 0.78 - ETA: 2s - loss: 0.4654 - accuracy: 0.78 - ETA: 2s - loss: 0.4635 - accuracy: 0.78 - ETA: 1s - loss: 0.4605 - accuracy: 0.79 - ETA: 1s - loss: 0.4604 - accuracy: 0.79 - ETA: 1s - loss: 0.4610 - accuracy: 0.78 - ETA: 1s - loss: 0.4613 - accuracy: 0.79 - ETA: 1s - loss: 0.4621 - accuracy: 0.78 - ETA: 1s - loss: 0.4607 - accuracy: 0.79 - ETA: 1s - loss: 0.4604 - accuracy: 0.79 - ETA: 1s - loss: 0.4604 - accuracy: 0.79 - ETA: 1s - loss: 0.4595 - accuracy: 0.79 - ETA: 1s - loss: 0.4583 - accuracy: 0.79 - ETA: 1s - loss: 0.4597 - accuracy: 0.79 - ETA: 1s - loss: 0.4597 - accuracy: 0.79 - ETA: 1s - loss: 0.4598 - accuracy: 0.79 - ETA: 1s - loss: 0.4589 - accuracy: 0.79 - ETA: 1s - loss: 0.4586 - accuracy: 0.79 - ETA: 1s - loss: 0.4594 - accuracy: 0.79 - ETA: 0s - loss: 0.4597 - accuracy: 0.79 - ETA: 0s - loss: 0.4597 - accuracy: 0.79 - ETA: 0s - loss: 0.4592 - accuracy: 0.79 - ETA: 0s - loss: 0.4604 - accuracy: 0.79 - ETA: 0s - loss: 0.4599 - accuracy: 0.79 - ETA: 0s - loss: 0.4587 - accuracy: 0.79 - ETA: 0s - loss: 0.4589 - accuracy: 0.79 - ETA: 0s - loss: 0.4582 - accuracy: 0.79 - ETA: 0s - loss: 0.4590 - accuracy: 0.79 - ETA: 0s - loss: 0.4583 - accuracy: 0.79 - ETA: 0s - loss: 0.4579 - accuracy: 0.79 - ETA: 0s - loss: 0.4571 - accuracy: 0.79 - ETA: 0s - loss: 0.4570 - accuracy: 0.79 - ETA: 0s - loss: 0.4570 - accuracy: 0.79 - ETA: 0s - loss: 0.4576 - accuracy: 0.79 - ETA: 0s - loss: 0.4575 - accuracy: 0.79 - ETA: 0s - loss: 0.4579 - accuracy: 0.79 - 4s 371us/step - loss: 0.4577 - accuracy: 0.7924 - val_loss: 0.3924 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31344\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4565 - accuracy: 0.78 - ETA: 3s - loss: 0.4594 - accuracy: 0.78 - ETA: 3s - loss: 0.4678 - accuracy: 0.78 - ETA: 3s - loss: 0.4724 - accuracy: 0.79 - ETA: 3s - loss: 0.4651 - accuracy: 0.79 - ETA: 3s - loss: 0.4593 - accuracy: 0.79 - ETA: 3s - loss: 0.4452 - accuracy: 0.79 - ETA: 3s - loss: 0.4383 - accuracy: 0.80 - ETA: 3s - loss: 0.4429 - accuracy: 0.79 - ETA: 3s - loss: 0.4369 - accuracy: 0.79 - ETA: 2s - loss: 0.4372 - accuracy: 0.79 - ETA: 2s - loss: 0.4455 - accuracy: 0.79 - ETA: 2s - loss: 0.4437 - accuracy: 0.79 - ETA: 2s - loss: 0.4425 - accuracy: 0.79 - ETA: 2s - loss: 0.4447 - accuracy: 0.79 - ETA: 2s - loss: 0.4436 - accuracy: 0.79 - ETA: 2s - loss: 0.4454 - accuracy: 0.79 - ETA: 2s - loss: 0.4433 - accuracy: 0.79 - ETA: 2s - loss: 0.4416 - accuracy: 0.79 - ETA: 2s - loss: 0.4448 - accuracy: 0.79 - ETA: 2s - loss: 0.4478 - accuracy: 0.79 - ETA: 2s - loss: 0.4478 - accuracy: 0.79 - ETA: 2s - loss: 0.4489 - accuracy: 0.79 - ETA: 2s - loss: 0.4493 - accuracy: 0.79 - ETA: 2s - loss: 0.4494 - accuracy: 0.79 - ETA: 2s - loss: 0.4490 - accuracy: 0.79 - ETA: 1s - loss: 0.4496 - accuracy: 0.79 - ETA: 1s - loss: 0.4488 - accuracy: 0.79 - ETA: 1s - loss: 0.4513 - accuracy: 0.79 - ETA: 1s - loss: 0.4504 - accuracy: 0.79 - ETA: 1s - loss: 0.4497 - accuracy: 0.79 - ETA: 1s - loss: 0.4492 - accuracy: 0.79 - ETA: 1s - loss: 0.4522 - accuracy: 0.79 - ETA: 1s - loss: 0.4517 - accuracy: 0.79 - ETA: 1s - loss: 0.4509 - accuracy: 0.79 - ETA: 1s - loss: 0.4502 - accuracy: 0.79 - ETA: 1s - loss: 0.4500 - accuracy: 0.79 - ETA: 1s - loss: 0.4496 - accuracy: 0.79 - ETA: 1s - loss: 0.4497 - accuracy: 0.79 - ETA: 1s - loss: 0.4475 - accuracy: 0.79 - ETA: 1s - loss: 0.4472 - accuracy: 0.79 - ETA: 1s - loss: 0.4462 - accuracy: 0.79 - ETA: 0s - loss: 0.4460 - accuracy: 0.79 - ETA: 0s - loss: 0.4464 - accuracy: 0.79 - ETA: 0s - loss: 0.4455 - accuracy: 0.79 - ETA: 0s - loss: 0.4455 - accuracy: 0.79 - ETA: 0s - loss: 0.4449 - accuracy: 0.79 - ETA: 0s - loss: 0.4443 - accuracy: 0.79 - ETA: 0s - loss: 0.4445 - accuracy: 0.79 - ETA: 0s - loss: 0.4448 - accuracy: 0.79 - ETA: 0s - loss: 0.4453 - accuracy: 0.79 - ETA: 0s - loss: 0.4440 - accuracy: 0.79 - ETA: 0s - loss: 0.4436 - accuracy: 0.79 - ETA: 0s - loss: 0.4431 - accuracy: 0.79 - ETA: 0s - loss: 0.4438 - accuracy: 0.79 - ETA: 0s - loss: 0.4432 - accuracy: 0.79 - ETA: 0s - loss: 0.4436 - accuracy: 0.79 - ETA: 0s - loss: 0.4434 - accuracy: 0.79 - ETA: 0s - loss: 0.4429 - accuracy: 0.79 - 4s 365us/step - loss: 0.4429 - accuracy: 0.7962 - val_loss: 0.3694 - val_accuracy: 0.8425\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31344\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4197 - accuracy: 0.79 - ETA: 3s - loss: 0.3823 - accuracy: 0.80 - ETA: 3s - loss: 0.4152 - accuracy: 0.79 - ETA: 3s - loss: 0.4127 - accuracy: 0.79 - ETA: 3s - loss: 0.4242 - accuracy: 0.79 - ETA: 3s - loss: 0.4277 - accuracy: 0.80 - ETA: 3s - loss: 0.4307 - accuracy: 0.79 - ETA: 3s - loss: 0.4303 - accuracy: 0.79 - ETA: 3s - loss: 0.4280 - accuracy: 0.80 - ETA: 3s - loss: 0.4269 - accuracy: 0.80 - ETA: 3s - loss: 0.4274 - accuracy: 0.79 - ETA: 3s - loss: 0.4300 - accuracy: 0.80 - ETA: 3s - loss: 0.4297 - accuracy: 0.80 - ETA: 3s - loss: 0.4257 - accuracy: 0.80 - ETA: 2s - loss: 0.4331 - accuracy: 0.79 - ETA: 2s - loss: 0.4369 - accuracy: 0.79 - ETA: 2s - loss: 0.4346 - accuracy: 0.80 - ETA: 2s - loss: 0.4329 - accuracy: 0.80 - ETA: 2s - loss: 0.4289 - accuracy: 0.80 - ETA: 2s - loss: 0.4305 - accuracy: 0.80 - ETA: 2s - loss: 0.4343 - accuracy: 0.80 - ETA: 2s - loss: 0.4311 - accuracy: 0.80 - ETA: 2s - loss: 0.4300 - accuracy: 0.80 - ETA: 2s - loss: 0.4272 - accuracy: 0.80 - ETA: 2s - loss: 0.4269 - accuracy: 0.80 - ETA: 2s - loss: 0.4255 - accuracy: 0.80 - ETA: 2s - loss: 0.4268 - accuracy: 0.80 - ETA: 2s - loss: 0.4277 - accuracy: 0.80 - ETA: 2s - loss: 0.4326 - accuracy: 0.79 - ETA: 2s - loss: 0.4334 - accuracy: 0.79 - ETA: 2s - loss: 0.4344 - accuracy: 0.79 - ETA: 1s - loss: 0.4330 - accuracy: 0.79 - ETA: 1s - loss: 0.4323 - accuracy: 0.79 - ETA: 1s - loss: 0.4319 - accuracy: 0.79 - ETA: 1s - loss: 0.4325 - accuracy: 0.79 - ETA: 1s - loss: 0.4344 - accuracy: 0.79 - ETA: 1s - loss: 0.4321 - accuracy: 0.79 - ETA: 1s - loss: 0.4310 - accuracy: 0.79 - ETA: 1s - loss: 0.4295 - accuracy: 0.80 - ETA: 1s - loss: 0.4297 - accuracy: 0.80 - ETA: 1s - loss: 0.4285 - accuracy: 0.80 - ETA: 1s - loss: 0.4267 - accuracy: 0.80 - ETA: 1s - loss: 0.4271 - accuracy: 0.80 - ETA: 1s - loss: 0.4275 - accuracy: 0.80 - ETA: 1s - loss: 0.4272 - accuracy: 0.80 - ETA: 1s - loss: 0.4284 - accuracy: 0.80 - ETA: 1s - loss: 0.4282 - accuracy: 0.80 - ETA: 0s - loss: 0.4277 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4307 - accuracy: 0.80 - ETA: 0s - loss: 0.4310 - accuracy: 0.80 - ETA: 0s - loss: 0.4306 - accuracy: 0.80 - ETA: 0s - loss: 0.4306 - accuracy: 0.80 - ETA: 0s - loss: 0.4308 - accuracy: 0.80 - ETA: 0s - loss: 0.4301 - accuracy: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4269 - accuracy: 0.80 - ETA: 0s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4266 - accuracy: 0.80 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4266 - accuracy: 0.80 - ETA: 0s - loss: 0.4264 - accuracy: 0.80 - 4s 370us/step - loss: 0.4263 - accuracy: 0.8036 - val_loss: 0.3717 - val_accuracy: 0.8315\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31344\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4523 - accuracy: 0.80 - ETA: 3s - loss: 0.4173 - accuracy: 0.81 - ETA: 3s - loss: 0.4641 - accuracy: 0.80 - ETA: 3s - loss: 0.4512 - accuracy: 0.80 - ETA: 3s - loss: 0.4380 - accuracy: 0.81 - ETA: 3s - loss: 0.4262 - accuracy: 0.82 - ETA: 3s - loss: 0.4261 - accuracy: 0.81 - ETA: 3s - loss: 0.4234 - accuracy: 0.81 - ETA: 3s - loss: 0.4260 - accuracy: 0.81 - ETA: 3s - loss: 0.4244 - accuracy: 0.81 - ETA: 3s - loss: 0.4246 - accuracy: 0.81 - ETA: 3s - loss: 0.4222 - accuracy: 0.81 - ETA: 3s - loss: 0.4175 - accuracy: 0.81 - ETA: 3s - loss: 0.4143 - accuracy: 0.81 - ETA: 3s - loss: 0.4127 - accuracy: 0.81 - ETA: 2s - loss: 0.4089 - accuracy: 0.82 - ETA: 2s - loss: 0.4091 - accuracy: 0.82 - ETA: 2s - loss: 0.4067 - accuracy: 0.82 - ETA: 2s - loss: 0.4137 - accuracy: 0.81 - ETA: 2s - loss: 0.4176 - accuracy: 0.81 - ETA: 2s - loss: 0.4210 - accuracy: 0.81 - ETA: 2s - loss: 0.4216 - accuracy: 0.81 - ETA: 2s - loss: 0.4179 - accuracy: 0.81 - ETA: 2s - loss: 0.4171 - accuracy: 0.81 - ETA: 2s - loss: 0.4175 - accuracy: 0.81 - ETA: 2s - loss: 0.4169 - accuracy: 0.81 - ETA: 2s - loss: 0.4164 - accuracy: 0.81 - ETA: 2s - loss: 0.4154 - accuracy: 0.81 - ETA: 2s - loss: 0.4154 - accuracy: 0.81 - ETA: 2s - loss: 0.4159 - accuracy: 0.81 - ETA: 2s - loss: 0.4184 - accuracy: 0.81 - ETA: 2s - loss: 0.4170 - accuracy: 0.81 - ETA: 1s - loss: 0.4192 - accuracy: 0.81 - ETA: 1s - loss: 0.4202 - accuracy: 0.81 - ETA: 1s - loss: 0.4200 - accuracy: 0.81 - ETA: 1s - loss: 0.4178 - accuracy: 0.81 - ETA: 1s - loss: 0.4160 - accuracy: 0.81 - ETA: 1s - loss: 0.4138 - accuracy: 0.81 - ETA: 1s - loss: 0.4136 - accuracy: 0.81 - ETA: 1s - loss: 0.4154 - accuracy: 0.81 - ETA: 1s - loss: 0.4138 - accuracy: 0.81 - ETA: 1s - loss: 0.4152 - accuracy: 0.81 - ETA: 1s - loss: 0.4149 - accuracy: 0.81 - ETA: 1s - loss: 0.4145 - accuracy: 0.81 - ETA: 1s - loss: 0.4142 - accuracy: 0.81 - ETA: 1s - loss: 0.4148 - accuracy: 0.81 - ETA: 1s - loss: 0.4130 - accuracy: 0.81 - ETA: 1s - loss: 0.4132 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.81 - ETA: 0s - loss: 0.4138 - accuracy: 0.81 - ETA: 0s - loss: 0.4139 - accuracy: 0.81 - ETA: 0s - loss: 0.4135 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4152 - accuracy: 0.81 - ETA: 0s - loss: 0.4163 - accuracy: 0.81 - ETA: 0s - loss: 0.4160 - accuracy: 0.81 - ETA: 0s - loss: 0.4154 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4146 - accuracy: 0.81 - ETA: 0s - loss: 0.4138 - accuracy: 0.81 - ETA: 0s - loss: 0.4131 - accuracy: 0.81 - ETA: 0s - loss: 0.4131 - accuracy: 0.81 - ETA: 0s - loss: 0.4124 - accuracy: 0.81 - 4s 375us/step - loss: 0.4130 - accuracy: 0.8136 - val_loss: 0.3652 - val_accuracy: 0.8458\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31344\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4063 - accuracy: 0.84 - ETA: 3s - loss: 0.3924 - accuracy: 0.83 - ETA: 3s - loss: 0.3978 - accuracy: 0.83 - ETA: 3s - loss: 0.4042 - accuracy: 0.82 - ETA: 3s - loss: 0.4036 - accuracy: 0.82 - ETA: 3s - loss: 0.3948 - accuracy: 0.83 - ETA: 3s - loss: 0.3863 - accuracy: 0.83 - ETA: 3s - loss: 0.3879 - accuracy: 0.83 - ETA: 3s - loss: 0.3911 - accuracy: 0.83 - ETA: 3s - loss: 0.3922 - accuracy: 0.82 - ETA: 3s - loss: 0.3850 - accuracy: 0.83 - ETA: 3s - loss: 0.3862 - accuracy: 0.83 - ETA: 3s - loss: 0.3925 - accuracy: 0.82 - ETA: 3s - loss: 0.3912 - accuracy: 0.82 - ETA: 2s - loss: 0.3968 - accuracy: 0.82 - ETA: 2s - loss: 0.4016 - accuracy: 0.81 - ETA: 2s - loss: 0.4033 - accuracy: 0.81 - ETA: 2s - loss: 0.4052 - accuracy: 0.81 - ETA: 2s - loss: 0.4021 - accuracy: 0.81 - ETA: 2s - loss: 0.4001 - accuracy: 0.81 - ETA: 2s - loss: 0.3996 - accuracy: 0.81 - ETA: 2s - loss: 0.4005 - accuracy: 0.81 - ETA: 2s - loss: 0.4011 - accuracy: 0.81 - ETA: 2s - loss: 0.4015 - accuracy: 0.81 - ETA: 2s - loss: 0.3991 - accuracy: 0.81 - ETA: 2s - loss: 0.3993 - accuracy: 0.81 - ETA: 2s - loss: 0.4012 - accuracy: 0.81 - ETA: 2s - loss: 0.4028 - accuracy: 0.81 - ETA: 2s - loss: 0.4030 - accuracy: 0.81 - ETA: 2s - loss: 0.4017 - accuracy: 0.81 - ETA: 1s - loss: 0.4022 - accuracy: 0.81 - ETA: 1s - loss: 0.4017 - accuracy: 0.81 - ETA: 1s - loss: 0.4011 - accuracy: 0.81 - ETA: 1s - loss: 0.4015 - accuracy: 0.81 - ETA: 1s - loss: 0.4019 - accuracy: 0.81 - ETA: 1s - loss: 0.4012 - accuracy: 0.81 - ETA: 1s - loss: 0.3997 - accuracy: 0.81 - ETA: 1s - loss: 0.4022 - accuracy: 0.81 - ETA: 1s - loss: 0.4036 - accuracy: 0.81 - ETA: 1s - loss: 0.4045 - accuracy: 0.81 - ETA: 1s - loss: 0.4051 - accuracy: 0.81 - ETA: 1s - loss: 0.4069 - accuracy: 0.81 - ETA: 1s - loss: 0.4078 - accuracy: 0.81 - ETA: 1s - loss: 0.4088 - accuracy: 0.81 - ETA: 1s - loss: 0.4078 - accuracy: 0.81 - ETA: 1s - loss: 0.4079 - accuracy: 0.81 - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.4035 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4039 - accuracy: 0.81 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4035 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4018 - accuracy: 0.81 - ETA: 0s - loss: 0.4012 - accuracy: 0.81 - ETA: 0s - loss: 0.3997 - accuracy: 0.81 - ETA: 0s - loss: 0.4004 - accuracy: 0.81 - ETA: 0s - loss: 0.3998 - accuracy: 0.81 - ETA: 0s - loss: 0.4013 - accuracy: 0.81 - ETA: 0s - loss: 0.4030 - accuracy: 0.81 - ETA: 0s - loss: 0.4026 - accuracy: 0.81 - 4s 366us/step - loss: 0.4028 - accuracy: 0.8134 - val_loss: 0.3896 - val_accuracy: 0.8315\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31344\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.4329 - accuracy: 0.77 - ETA: 4s - loss: 0.3949 - accuracy: 0.81 - ETA: 4s - loss: 0.3919 - accuracy: 0.82 - ETA: 3s - loss: 0.3947 - accuracy: 0.82 - ETA: 3s - loss: 0.3911 - accuracy: 0.82 - ETA: 3s - loss: 0.3979 - accuracy: 0.81 - ETA: 3s - loss: 0.3962 - accuracy: 0.81 - ETA: 3s - loss: 0.3935 - accuracy: 0.81 - ETA: 3s - loss: 0.3973 - accuracy: 0.81 - ETA: 3s - loss: 0.3974 - accuracy: 0.81 - ETA: 3s - loss: 0.3917 - accuracy: 0.81 - ETA: 3s - loss: 0.3890 - accuracy: 0.82 - ETA: 3s - loss: 0.3911 - accuracy: 0.82 - ETA: 3s - loss: 0.3868 - accuracy: 0.82 - ETA: 3s - loss: 0.3901 - accuracy: 0.82 - ETA: 2s - loss: 0.3917 - accuracy: 0.82 - ETA: 2s - loss: 0.3904 - accuracy: 0.82 - ETA: 2s - loss: 0.3867 - accuracy: 0.82 - ETA: 2s - loss: 0.3871 - accuracy: 0.82 - ETA: 2s - loss: 0.3882 - accuracy: 0.82 - ETA: 2s - loss: 0.3877 - accuracy: 0.82 - ETA: 2s - loss: 0.3917 - accuracy: 0.81 - ETA: 2s - loss: 0.3946 - accuracy: 0.81 - ETA: 2s - loss: 0.3983 - accuracy: 0.81 - ETA: 2s - loss: 0.3974 - accuracy: 0.81 - ETA: 2s - loss: 0.3992 - accuracy: 0.81 - ETA: 2s - loss: 0.3983 - accuracy: 0.81 - ETA: 2s - loss: 0.3946 - accuracy: 0.81 - ETA: 2s - loss: 0.3928 - accuracy: 0.82 - ETA: 2s - loss: 0.3965 - accuracy: 0.81 - ETA: 2s - loss: 0.3958 - accuracy: 0.81 - ETA: 1s - loss: 0.3971 - accuracy: 0.81 - ETA: 1s - loss: 0.3986 - accuracy: 0.81 - ETA: 1s - loss: 0.3987 - accuracy: 0.81 - ETA: 1s - loss: 0.4002 - accuracy: 0.81 - ETA: 1s - loss: 0.3984 - accuracy: 0.81 - ETA: 1s - loss: 0.3975 - accuracy: 0.81 - ETA: 1s - loss: 0.3989 - accuracy: 0.81 - ETA: 1s - loss: 0.3980 - accuracy: 0.81 - ETA: 1s - loss: 0.3976 - accuracy: 0.81 - ETA: 1s - loss: 0.3984 - accuracy: 0.81 - ETA: 1s - loss: 0.4007 - accuracy: 0.81 - ETA: 1s - loss: 0.4002 - accuracy: 0.81 - ETA: 1s - loss: 0.4009 - accuracy: 0.81 - ETA: 1s - loss: 0.4003 - accuracy: 0.81 - ETA: 0s - loss: 0.3990 - accuracy: 0.81 - ETA: 0s - loss: 0.3971 - accuracy: 0.81 - ETA: 0s - loss: 0.3962 - accuracy: 0.81 - ETA: 0s - loss: 0.3959 - accuracy: 0.81 - ETA: 0s - loss: 0.3945 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.81 - ETA: 0s - loss: 0.3958 - accuracy: 0.81 - ETA: 0s - loss: 0.3951 - accuracy: 0.81 - ETA: 0s - loss: 0.3950 - accuracy: 0.82 - ETA: 0s - loss: 0.3957 - accuracy: 0.81 - ETA: 0s - loss: 0.3955 - accuracy: 0.81 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.81 - ETA: 0s - loss: 0.3956 - accuracy: 0.81 - 4s 367us/step - loss: 0.3964 - accuracy: 0.8178 - val_loss: 0.3697 - val_accuracy: 0.8475\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31344\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3952 - accuracy: 0.86 - ETA: 3s - loss: 0.4040 - accuracy: 0.84 - ETA: 3s - loss: 0.3980 - accuracy: 0.83 - ETA: 3s - loss: 0.4056 - accuracy: 0.83 - ETA: 3s - loss: 0.3979 - accuracy: 0.83 - ETA: 3s - loss: 0.4023 - accuracy: 0.82 - ETA: 3s - loss: 0.4015 - accuracy: 0.82 - ETA: 3s - loss: 0.3985 - accuracy: 0.82 - ETA: 3s - loss: 0.3954 - accuracy: 0.82 - ETA: 3s - loss: 0.3930 - accuracy: 0.82 - ETA: 3s - loss: 0.3914 - accuracy: 0.82 - ETA: 2s - loss: 0.3879 - accuracy: 0.82 - ETA: 2s - loss: 0.3859 - accuracy: 0.82 - ETA: 2s - loss: 0.3834 - accuracy: 0.82 - ETA: 2s - loss: 0.3833 - accuracy: 0.82 - ETA: 2s - loss: 0.3854 - accuracy: 0.82 - ETA: 2s - loss: 0.3850 - accuracy: 0.82 - ETA: 2s - loss: 0.3868 - accuracy: 0.82 - ETA: 2s - loss: 0.3869 - accuracy: 0.82 - ETA: 2s - loss: 0.3857 - accuracy: 0.82 - ETA: 2s - loss: 0.3917 - accuracy: 0.82 - ETA: 2s - loss: 0.3908 - accuracy: 0.82 - ETA: 2s - loss: 0.3909 - accuracy: 0.82 - ETA: 2s - loss: 0.3912 - accuracy: 0.82 - ETA: 2s - loss: 0.3912 - accuracy: 0.82 - ETA: 1s - loss: 0.3929 - accuracy: 0.81 - ETA: 1s - loss: 0.3923 - accuracy: 0.81 - ETA: 1s - loss: 0.3907 - accuracy: 0.82 - ETA: 1s - loss: 0.3920 - accuracy: 0.81 - ETA: 1s - loss: 0.3902 - accuracy: 0.82 - ETA: 1s - loss: 0.3906 - accuracy: 0.82 - ETA: 1s - loss: 0.3939 - accuracy: 0.81 - ETA: 1s - loss: 0.3907 - accuracy: 0.82 - ETA: 1s - loss: 0.3913 - accuracy: 0.82 - ETA: 1s - loss: 0.3918 - accuracy: 0.82 - ETA: 1s - loss: 0.3913 - accuracy: 0.82 - ETA: 1s - loss: 0.3898 - accuracy: 0.82 - ETA: 1s - loss: 0.3889 - accuracy: 0.82 - ETA: 1s - loss: 0.3869 - accuracy: 0.82 - ETA: 1s - loss: 0.3903 - accuracy: 0.82 - ETA: 1s - loss: 0.3901 - accuracy: 0.82 - ETA: 1s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3870 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - 4s 362us/step - loss: 0.3896 - accuracy: 0.8216 - val_loss: 0.4152 - val_accuracy: 0.8012\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: val_loss did not improve from 0.31344\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.4407 - accuracy: 0.78 - ETA: 4s - loss: 0.4053 - accuracy: 0.81 - ETA: 4s - loss: 0.4021 - accuracy: 0.80 - ETA: 3s - loss: 0.4079 - accuracy: 0.80 - ETA: 3s - loss: 0.4187 - accuracy: 0.80 - ETA: 3s - loss: 0.4125 - accuracy: 0.80 - ETA: 3s - loss: 0.4092 - accuracy: 0.80 - ETA: 3s - loss: 0.4109 - accuracy: 0.80 - ETA: 3s - loss: 0.4048 - accuracy: 0.81 - ETA: 3s - loss: 0.3924 - accuracy: 0.81 - ETA: 3s - loss: 0.3993 - accuracy: 0.81 - ETA: 3s - loss: 0.4017 - accuracy: 0.80 - ETA: 3s - loss: 0.3982 - accuracy: 0.80 - ETA: 2s - loss: 0.3954 - accuracy: 0.81 - ETA: 2s - loss: 0.3923 - accuracy: 0.81 - ETA: 2s - loss: 0.3922 - accuracy: 0.81 - ETA: 2s - loss: 0.3939 - accuracy: 0.81 - ETA: 2s - loss: 0.3919 - accuracy: 0.81 - ETA: 2s - loss: 0.3912 - accuracy: 0.81 - ETA: 2s - loss: 0.3906 - accuracy: 0.81 - ETA: 2s - loss: 0.3923 - accuracy: 0.81 - ETA: 2s - loss: 0.3932 - accuracy: 0.81 - ETA: 2s - loss: 0.3954 - accuracy: 0.81 - ETA: 2s - loss: 0.3934 - accuracy: 0.81 - ETA: 2s - loss: 0.3913 - accuracy: 0.82 - ETA: 2s - loss: 0.3892 - accuracy: 0.82 - ETA: 2s - loss: 0.3888 - accuracy: 0.82 - ETA: 2s - loss: 0.3892 - accuracy: 0.82 - ETA: 1s - loss: 0.3853 - accuracy: 0.82 - ETA: 1s - loss: 0.3845 - accuracy: 0.82 - ETA: 1s - loss: 0.3850 - accuracy: 0.82 - ETA: 1s - loss: 0.3849 - accuracy: 0.82 - ETA: 1s - loss: 0.3858 - accuracy: 0.82 - ETA: 1s - loss: 0.3850 - accuracy: 0.82 - ETA: 1s - loss: 0.3834 - accuracy: 0.82 - ETA: 1s - loss: 0.3831 - accuracy: 0.82 - ETA: 1s - loss: 0.3843 - accuracy: 0.82 - ETA: 1s - loss: 0.3852 - accuracy: 0.82 - ETA: 1s - loss: 0.3840 - accuracy: 0.82 - ETA: 1s - loss: 0.3845 - accuracy: 0.82 - ETA: 1s - loss: 0.3833 - accuracy: 0.82 - ETA: 1s - loss: 0.3854 - accuracy: 0.82 - ETA: 1s - loss: 0.3860 - accuracy: 0.82 - ETA: 1s - loss: 0.3844 - accuracy: 0.82 - ETA: 1s - loss: 0.3861 - accuracy: 0.82 - ETA: 1s - loss: 0.3861 - accuracy: 0.82 - ETA: 0s - loss: 0.3859 - accuracy: 0.82 - ETA: 0s - loss: 0.3863 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.82 - ETA: 0s - loss: 0.3857 - accuracy: 0.82 - ETA: 0s - loss: 0.3856 - accuracy: 0.82 - ETA: 0s - loss: 0.3849 - accuracy: 0.82 - ETA: 0s - loss: 0.3849 - accuracy: 0.82 - ETA: 0s - loss: 0.3851 - accuracy: 0.82 - ETA: 0s - loss: 0.3845 - accuracy: 0.82 - ETA: 0s - loss: 0.3841 - accuracy: 0.82 - ETA: 0s - loss: 0.3846 - accuracy: 0.82 - ETA: 0s - loss: 0.3838 - accuracy: 0.82 - ETA: 0s - loss: 0.3856 - accuracy: 0.82 - ETA: 0s - loss: 0.3852 - accuracy: 0.82 - ETA: 0s - loss: 0.3850 - accuracy: 0.82 - 4s 366us/step - loss: 0.3854 - accuracy: 0.8253 - val_loss: 0.3645 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31344\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4036 - accuracy: 0.81 - ETA: 3s - loss: 0.3571 - accuracy: 0.85 - ETA: 3s - loss: 0.3529 - accuracy: 0.85 - ETA: 3s - loss: 0.3469 - accuracy: 0.84 - ETA: 3s - loss: 0.3516 - accuracy: 0.83 - ETA: 3s - loss: 0.3517 - accuracy: 0.84 - ETA: 3s - loss: 0.3518 - accuracy: 0.84 - ETA: 3s - loss: 0.3533 - accuracy: 0.84 - ETA: 3s - loss: 0.3613 - accuracy: 0.84 - ETA: 3s - loss: 0.3606 - accuracy: 0.84 - ETA: 3s - loss: 0.3593 - accuracy: 0.84 - ETA: 3s - loss: 0.3598 - accuracy: 0.84 - ETA: 3s - loss: 0.3604 - accuracy: 0.84 - ETA: 2s - loss: 0.3604 - accuracy: 0.84 - ETA: 2s - loss: 0.3629 - accuracy: 0.84 - ETA: 2s - loss: 0.3611 - accuracy: 0.83 - ETA: 2s - loss: 0.3625 - accuracy: 0.83 - ETA: 2s - loss: 0.3691 - accuracy: 0.83 - ETA: 2s - loss: 0.3696 - accuracy: 0.83 - ETA: 2s - loss: 0.3753 - accuracy: 0.83 - ETA: 2s - loss: 0.3735 - accuracy: 0.83 - ETA: 2s - loss: 0.3719 - accuracy: 0.83 - ETA: 2s - loss: 0.3693 - accuracy: 0.83 - ETA: 2s - loss: 0.3705 - accuracy: 0.83 - ETA: 2s - loss: 0.3712 - accuracy: 0.83 - ETA: 2s - loss: 0.3722 - accuracy: 0.83 - ETA: 2s - loss: 0.3746 - accuracy: 0.83 - ETA: 1s - loss: 0.3751 - accuracy: 0.83 - ETA: 1s - loss: 0.3747 - accuracy: 0.83 - ETA: 1s - loss: 0.3747 - accuracy: 0.83 - ETA: 1s - loss: 0.3740 - accuracy: 0.83 - ETA: 1s - loss: 0.3743 - accuracy: 0.83 - ETA: 1s - loss: 0.3741 - accuracy: 0.83 - ETA: 1s - loss: 0.3738 - accuracy: 0.83 - ETA: 1s - loss: 0.3760 - accuracy: 0.83 - ETA: 1s - loss: 0.3741 - accuracy: 0.83 - ETA: 1s - loss: 0.3710 - accuracy: 0.83 - ETA: 1s - loss: 0.3718 - accuracy: 0.83 - ETA: 1s - loss: 0.3722 - accuracy: 0.83 - ETA: 1s - loss: 0.3723 - accuracy: 0.83 - ETA: 1s - loss: 0.3730 - accuracy: 0.83 - ETA: 1s - loss: 0.3744 - accuracy: 0.83 - ETA: 0s - loss: 0.3747 - accuracy: 0.83 - ETA: 0s - loss: 0.3756 - accuracy: 0.83 - ETA: 0s - loss: 0.3757 - accuracy: 0.83 - ETA: 0s - loss: 0.3765 - accuracy: 0.83 - ETA: 0s - loss: 0.3771 - accuracy: 0.83 - ETA: 0s - loss: 0.3774 - accuracy: 0.83 - ETA: 0s - loss: 0.3773 - accuracy: 0.83 - ETA: 0s - loss: 0.3771 - accuracy: 0.83 - ETA: 0s - loss: 0.3780 - accuracy: 0.83 - ETA: 0s - loss: 0.3778 - accuracy: 0.83 - ETA: 0s - loss: 0.3772 - accuracy: 0.83 - ETA: 0s - loss: 0.3765 - accuracy: 0.83 - ETA: 0s - loss: 0.3759 - accuracy: 0.83 - ETA: 0s - loss: 0.3761 - accuracy: 0.83 - ETA: 0s - loss: 0.3756 - accuracy: 0.83 - ETA: 0s - loss: 0.3745 - accuracy: 0.83 - ETA: 0s - loss: 0.3746 - accuracy: 0.83 - 4s 363us/step - loss: 0.3743 - accuracy: 0.8343 - val_loss: 0.3384 - val_accuracy: 0.8441\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31344\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.4103 - accuracy: 0.78 - ETA: 4s - loss: 0.4277 - accuracy: 0.78 - ETA: 4s - loss: 0.4063 - accuracy: 0.79 - ETA: 4s - loss: 0.3879 - accuracy: 0.80 - ETA: 3s - loss: 0.3698 - accuracy: 0.81 - ETA: 3s - loss: 0.3700 - accuracy: 0.81 - ETA: 3s - loss: 0.3679 - accuracy: 0.82 - ETA: 3s - loss: 0.3771 - accuracy: 0.82 - ETA: 3s - loss: 0.3883 - accuracy: 0.81 - ETA: 3s - loss: 0.3848 - accuracy: 0.81 - ETA: 3s - loss: 0.3815 - accuracy: 0.82 - ETA: 3s - loss: 0.3815 - accuracy: 0.82 - ETA: 3s - loss: 0.3753 - accuracy: 0.82 - ETA: 2s - loss: 0.3729 - accuracy: 0.82 - ETA: 2s - loss: 0.3776 - accuracy: 0.82 - ETA: 2s - loss: 0.3815 - accuracy: 0.82 - ETA: 2s - loss: 0.3785 - accuracy: 0.82 - ETA: 2s - loss: 0.3755 - accuracy: 0.82 - ETA: 2s - loss: 0.3721 - accuracy: 0.83 - ETA: 2s - loss: 0.3734 - accuracy: 0.82 - ETA: 2s - loss: 0.3737 - accuracy: 0.82 - ETA: 2s - loss: 0.3717 - accuracy: 0.82 - ETA: 2s - loss: 0.3713 - accuracy: 0.83 - ETA: 2s - loss: 0.3684 - accuracy: 0.83 - ETA: 2s - loss: 0.3691 - accuracy: 0.83 - ETA: 2s - loss: 0.3704 - accuracy: 0.83 - ETA: 2s - loss: 0.3708 - accuracy: 0.83 - ETA: 2s - loss: 0.3687 - accuracy: 0.83 - ETA: 1s - loss: 0.3683 - accuracy: 0.83 - ETA: 1s - loss: 0.3711 - accuracy: 0.83 - ETA: 1s - loss: 0.3710 - accuracy: 0.82 - ETA: 1s - loss: 0.3735 - accuracy: 0.82 - ETA: 1s - loss: 0.3740 - accuracy: 0.82 - ETA: 1s - loss: 0.3738 - accuracy: 0.82 - ETA: 1s - loss: 0.3734 - accuracy: 0.82 - ETA: 1s - loss: 0.3741 - accuracy: 0.82 - ETA: 1s - loss: 0.3735 - accuracy: 0.82 - ETA: 1s - loss: 0.3738 - accuracy: 0.82 - ETA: 1s - loss: 0.3736 - accuracy: 0.82 - ETA: 1s - loss: 0.3745 - accuracy: 0.82 - ETA: 1s - loss: 0.3737 - accuracy: 0.82 - ETA: 1s - loss: 0.3742 - accuracy: 0.82 - ETA: 1s - loss: 0.3741 - accuracy: 0.82 - ETA: 1s - loss: 0.3737 - accuracy: 0.82 - ETA: 1s - loss: 0.3726 - accuracy: 0.82 - ETA: 1s - loss: 0.3719 - accuracy: 0.82 - ETA: 1s - loss: 0.3725 - accuracy: 0.82 - ETA: 0s - loss: 0.3729 - accuracy: 0.82 - ETA: 0s - loss: 0.3733 - accuracy: 0.82 - ETA: 0s - loss: 0.3718 - accuracy: 0.82 - ETA: 0s - loss: 0.3731 - accuracy: 0.82 - ETA: 0s - loss: 0.3732 - accuracy: 0.82 - ETA: 0s - loss: 0.3735 - accuracy: 0.82 - ETA: 0s - loss: 0.3724 - accuracy: 0.82 - ETA: 0s - loss: 0.3720 - accuracy: 0.82 - ETA: 0s - loss: 0.3712 - accuracy: 0.83 - ETA: 0s - loss: 0.3715 - accuracy: 0.82 - ETA: 0s - loss: 0.3736 - accuracy: 0.82 - ETA: 0s - loss: 0.3727 - accuracy: 0.82 - ETA: 0s - loss: 0.3734 - accuracy: 0.82 - ETA: 0s - loss: 0.3742 - accuracy: 0.82 - ETA: 0s - loss: 0.3736 - accuracy: 0.82 - 4s 366us/step - loss: 0.3732 - accuracy: 0.8294 - val_loss: 0.3348 - val_accuracy: 0.8576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31344\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3655 - accuracy: 0.81 - ETA: 3s - loss: 0.3690 - accuracy: 0.82 - ETA: 3s - loss: 0.3683 - accuracy: 0.82 - ETA: 3s - loss: 0.3812 - accuracy: 0.82 - ETA: 3s - loss: 0.3718 - accuracy: 0.82 - ETA: 3s - loss: 0.3617 - accuracy: 0.82 - ETA: 3s - loss: 0.3628 - accuracy: 0.82 - ETA: 3s - loss: 0.3656 - accuracy: 0.82 - ETA: 3s - loss: 0.3596 - accuracy: 0.82 - ETA: 3s - loss: 0.3601 - accuracy: 0.82 - ETA: 3s - loss: 0.3603 - accuracy: 0.82 - ETA: 2s - loss: 0.3618 - accuracy: 0.82 - ETA: 2s - loss: 0.3663 - accuracy: 0.82 - ETA: 2s - loss: 0.3653 - accuracy: 0.82 - ETA: 2s - loss: 0.3603 - accuracy: 0.83 - ETA: 2s - loss: 0.3632 - accuracy: 0.82 - ETA: 2s - loss: 0.3647 - accuracy: 0.82 - ETA: 2s - loss: 0.3651 - accuracy: 0.82 - ETA: 2s - loss: 0.3658 - accuracy: 0.82 - ETA: 2s - loss: 0.3679 - accuracy: 0.82 - ETA: 2s - loss: 0.3702 - accuracy: 0.82 - ETA: 2s - loss: 0.3684 - accuracy: 0.82 - ETA: 2s - loss: 0.3679 - accuracy: 0.82 - ETA: 2s - loss: 0.3670 - accuracy: 0.82 - ETA: 2s - loss: 0.3668 - accuracy: 0.82 - ETA: 2s - loss: 0.3691 - accuracy: 0.82 - ETA: 2s - loss: 0.3675 - accuracy: 0.82 - ETA: 2s - loss: 0.3691 - accuracy: 0.82 - ETA: 2s - loss: 0.3705 - accuracy: 0.82 - ETA: 2s - loss: 0.3714 - accuracy: 0.82 - ETA: 2s - loss: 0.3709 - accuracy: 0.82 - ETA: 2s - loss: 0.3717 - accuracy: 0.82 - ETA: 1s - loss: 0.3754 - accuracy: 0.82 - ETA: 1s - loss: 0.3757 - accuracy: 0.82 - ETA: 1s - loss: 0.3749 - accuracy: 0.82 - ETA: 1s - loss: 0.3756 - accuracy: 0.82 - ETA: 1s - loss: 0.3740 - accuracy: 0.82 - ETA: 1s - loss: 0.3750 - accuracy: 0.82 - ETA: 1s - loss: 0.3752 - accuracy: 0.82 - ETA: 1s - loss: 0.3728 - accuracy: 0.82 - ETA: 1s - loss: 0.3728 - accuracy: 0.82 - ETA: 1s - loss: 0.3720 - accuracy: 0.82 - ETA: 1s - loss: 0.3707 - accuracy: 0.82 - ETA: 1s - loss: 0.3702 - accuracy: 0.82 - ETA: 1s - loss: 0.3690 - accuracy: 0.82 - ETA: 1s - loss: 0.3681 - accuracy: 0.82 - ETA: 0s - loss: 0.3672 - accuracy: 0.82 - ETA: 0s - loss: 0.3691 - accuracy: 0.82 - ETA: 0s - loss: 0.3698 - accuracy: 0.82 - ETA: 0s - loss: 0.3686 - accuracy: 0.82 - ETA: 0s - loss: 0.3676 - accuracy: 0.82 - ETA: 0s - loss: 0.3679 - accuracy: 0.82 - ETA: 0s - loss: 0.3681 - accuracy: 0.82 - ETA: 0s - loss: 0.3687 - accuracy: 0.82 - ETA: 0s - loss: 0.3685 - accuracy: 0.82 - ETA: 0s - loss: 0.3681 - accuracy: 0.82 - ETA: 0s - loss: 0.3678 - accuracy: 0.82 - ETA: 0s - loss: 0.3663 - accuracy: 0.82 - ETA: 0s - loss: 0.3655 - accuracy: 0.83 - ETA: 0s - loss: 0.3662 - accuracy: 0.82 - ETA: 0s - loss: 0.3665 - accuracy: 0.82 - 4s 370us/step - loss: 0.3663 - accuracy: 0.8300 - val_loss: 0.3297 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31344\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.3016 - accuracy: 0.84 - ETA: 3s - loss: 0.3028 - accuracy: 0.85 - ETA: 3s - loss: 0.3170 - accuracy: 0.86 - ETA: 3s - loss: 0.3309 - accuracy: 0.84 - ETA: 3s - loss: 0.3462 - accuracy: 0.84 - ETA: 3s - loss: 0.3605 - accuracy: 0.83 - ETA: 3s - loss: 0.3671 - accuracy: 0.83 - ETA: 3s - loss: 0.3611 - accuracy: 0.83 - ETA: 3s - loss: 0.3832 - accuracy: 0.82 - ETA: 3s - loss: 0.3825 - accuracy: 0.82 - ETA: 2s - loss: 0.3775 - accuracy: 0.82 - ETA: 2s - loss: 0.3745 - accuracy: 0.82 - ETA: 2s - loss: 0.3722 - accuracy: 0.82 - ETA: 2s - loss: 0.3722 - accuracy: 0.83 - ETA: 2s - loss: 0.3676 - accuracy: 0.83 - ETA: 2s - loss: 0.3654 - accuracy: 0.83 - ETA: 2s - loss: 0.3670 - accuracy: 0.83 - ETA: 2s - loss: 0.3609 - accuracy: 0.83 - ETA: 2s - loss: 0.3612 - accuracy: 0.83 - ETA: 2s - loss: 0.3594 - accuracy: 0.83 - ETA: 2s - loss: 0.3611 - accuracy: 0.83 - ETA: 2s - loss: 0.3606 - accuracy: 0.83 - ETA: 2s - loss: 0.3599 - accuracy: 0.83 - ETA: 2s - loss: 0.3588 - accuracy: 0.84 - ETA: 2s - loss: 0.3565 - accuracy: 0.84 - ETA: 2s - loss: 0.3541 - accuracy: 0.84 - ETA: 2s - loss: 0.3557 - accuracy: 0.84 - ETA: 2s - loss: 0.3564 - accuracy: 0.84 - ETA: 2s - loss: 0.3599 - accuracy: 0.83 - ETA: 2s - loss: 0.3597 - accuracy: 0.83 - ETA: 2s - loss: 0.3620 - accuracy: 0.83 - ETA: 1s - loss: 0.3620 - accuracy: 0.83 - ETA: 1s - loss: 0.3616 - accuracy: 0.83 - ETA: 1s - loss: 0.3625 - accuracy: 0.83 - ETA: 1s - loss: 0.3633 - accuracy: 0.83 - ETA: 1s - loss: 0.3617 - accuracy: 0.83 - ETA: 1s - loss: 0.3620 - accuracy: 0.83 - ETA: 1s - loss: 0.3616 - accuracy: 0.83 - ETA: 1s - loss: 0.3633 - accuracy: 0.83 - ETA: 1s - loss: 0.3637 - accuracy: 0.83 - ETA: 1s - loss: 0.3642 - accuracy: 0.83 - ETA: 1s - loss: 0.3630 - accuracy: 0.83 - ETA: 1s - loss: 0.3624 - accuracy: 0.83 - ETA: 1s - loss: 0.3616 - accuracy: 0.83 - ETA: 1s - loss: 0.3617 - accuracy: 0.83 - ETA: 1s - loss: 0.3614 - accuracy: 0.83 - ETA: 1s - loss: 0.3637 - accuracy: 0.83 - ETA: 1s - loss: 0.3654 - accuracy: 0.83 - ETA: 0s - loss: 0.3659 - accuracy: 0.83 - ETA: 0s - loss: 0.3657 - accuracy: 0.83 - ETA: 0s - loss: 0.3659 - accuracy: 0.83 - ETA: 0s - loss: 0.3662 - accuracy: 0.83 - ETA: 0s - loss: 0.3670 - accuracy: 0.82 - ETA: 0s - loss: 0.3659 - accuracy: 0.83 - ETA: 0s - loss: 0.3636 - accuracy: 0.83 - ETA: 0s - loss: 0.3654 - accuracy: 0.83 - ETA: 0s - loss: 0.3652 - accuracy: 0.83 - ETA: 0s - loss: 0.3651 - accuracy: 0.83 - ETA: 0s - loss: 0.3644 - accuracy: 0.83 - ETA: 0s - loss: 0.3657 - accuracy: 0.83 - ETA: 0s - loss: 0.3655 - accuracy: 0.83 - ETA: 0s - loss: 0.3648 - accuracy: 0.83 - ETA: 0s - loss: 0.3644 - accuracy: 0.83 - ETA: 0s - loss: 0.3646 - accuracy: 0.83 - 4s 372us/step - loss: 0.3653 - accuracy: 0.8312 - val_loss: 0.4250 - val_accuracy: 0.8045\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31344\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3866 - accuracy: 0.78 - ETA: 3s - loss: 0.3598 - accuracy: 0.82 - ETA: 3s - loss: 0.3607 - accuracy: 0.83 - ETA: 3s - loss: 0.3604 - accuracy: 0.83 - ETA: 3s - loss: 0.3711 - accuracy: 0.83 - ETA: 3s - loss: 0.3650 - accuracy: 0.83 - ETA: 3s - loss: 0.3825 - accuracy: 0.83 - ETA: 3s - loss: 0.3806 - accuracy: 0.82 - ETA: 3s - loss: 0.3844 - accuracy: 0.83 - ETA: 3s - loss: 0.3804 - accuracy: 0.83 - ETA: 3s - loss: 0.3789 - accuracy: 0.83 - ETA: 3s - loss: 0.3719 - accuracy: 0.83 - ETA: 3s - loss: 0.3715 - accuracy: 0.83 - ETA: 3s - loss: 0.3728 - accuracy: 0.83 - ETA: 3s - loss: 0.3705 - accuracy: 0.83 - ETA: 3s - loss: 0.3728 - accuracy: 0.83 - ETA: 3s - loss: 0.3717 - accuracy: 0.83 - ETA: 2s - loss: 0.3707 - accuracy: 0.83 - ETA: 2s - loss: 0.3696 - accuracy: 0.83 - ETA: 2s - loss: 0.3718 - accuracy: 0.83 - ETA: 2s - loss: 0.3741 - accuracy: 0.83 - ETA: 2s - loss: 0.3720 - accuracy: 0.83 - ETA: 2s - loss: 0.3705 - accuracy: 0.83 - ETA: 2s - loss: 0.3717 - accuracy: 0.83 - ETA: 2s - loss: 0.3720 - accuracy: 0.83 - ETA: 2s - loss: 0.3715 - accuracy: 0.83 - ETA: 2s - loss: 0.3698 - accuracy: 0.83 - ETA: 2s - loss: 0.3700 - accuracy: 0.83 - ETA: 2s - loss: 0.3704 - accuracy: 0.83 - ETA: 2s - loss: 0.3674 - accuracy: 0.83 - ETA: 2s - loss: 0.3679 - accuracy: 0.83 - ETA: 2s - loss: 0.3692 - accuracy: 0.83 - ETA: 2s - loss: 0.3686 - accuracy: 0.83 - ETA: 2s - loss: 0.3697 - accuracy: 0.83 - ETA: 2s - loss: 0.3677 - accuracy: 0.83 - ETA: 1s - loss: 0.3678 - accuracy: 0.83 - ETA: 1s - loss: 0.3690 - accuracy: 0.83 - ETA: 1s - loss: 0.3689 - accuracy: 0.83 - ETA: 1s - loss: 0.3689 - accuracy: 0.83 - ETA: 1s - loss: 0.3673 - accuracy: 0.83 - ETA: 1s - loss: 0.3666 - accuracy: 0.83 - ETA: 1s - loss: 0.3653 - accuracy: 0.83 - ETA: 1s - loss: 0.3654 - accuracy: 0.83 - ETA: 1s - loss: 0.3638 - accuracy: 0.83 - ETA: 1s - loss: 0.3632 - accuracy: 0.83 - ETA: 1s - loss: 0.3650 - accuracy: 0.83 - ETA: 1s - loss: 0.3669 - accuracy: 0.83 - ETA: 1s - loss: 0.3681 - accuracy: 0.83 - ETA: 1s - loss: 0.3668 - accuracy: 0.83 - ETA: 1s - loss: 0.3672 - accuracy: 0.83 - ETA: 1s - loss: 0.3663 - accuracy: 0.83 - ETA: 1s - loss: 0.3652 - accuracy: 0.83 - ETA: 1s - loss: 0.3655 - accuracy: 0.83 - ETA: 0s - loss: 0.3640 - accuracy: 0.83 - ETA: 0s - loss: 0.3640 - accuracy: 0.83 - ETA: 0s - loss: 0.3648 - accuracy: 0.83 - ETA: 0s - loss: 0.3642 - accuracy: 0.83 - ETA: 0s - loss: 0.3634 - accuracy: 0.83 - ETA: 0s - loss: 0.3628 - accuracy: 0.83 - ETA: 0s - loss: 0.3622 - accuracy: 0.83 - ETA: 0s - loss: 0.3622 - accuracy: 0.83 - ETA: 0s - loss: 0.3614 - accuracy: 0.83 - ETA: 0s - loss: 0.3606 - accuracy: 0.83 - ETA: 0s - loss: 0.3611 - accuracy: 0.83 - ETA: 0s - loss: 0.3614 - accuracy: 0.83 - ETA: 0s - loss: 0.3617 - accuracy: 0.83 - ETA: 0s - loss: 0.3631 - accuracy: 0.83 - ETA: 0s - loss: 0.3630 - accuracy: 0.83 - 4s 419us/step - loss: 0.3623 - accuracy: 0.8368 - val_loss: 0.3403 - val_accuracy: 0.8391\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31344\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.4704 - accuracy: 0.80 - ETA: 4s - loss: 0.4444 - accuracy: 0.79 - ETA: 4s - loss: 0.4386 - accuracy: 0.80 - ETA: 4s - loss: 0.3860 - accuracy: 0.83 - ETA: 3s - loss: 0.3690 - accuracy: 0.84 - ETA: 3s - loss: 0.3827 - accuracy: 0.83 - ETA: 3s - loss: 0.3720 - accuracy: 0.84 - ETA: 3s - loss: 0.3725 - accuracy: 0.84 - ETA: 4s - loss: 0.3614 - accuracy: 0.84 - ETA: 3s - loss: 0.3640 - accuracy: 0.84 - ETA: 3s - loss: 0.3625 - accuracy: 0.84 - ETA: 3s - loss: 0.3661 - accuracy: 0.84 - ETA: 3s - loss: 0.3684 - accuracy: 0.83 - ETA: 3s - loss: 0.3622 - accuracy: 0.84 - ETA: 2s - loss: 0.3592 - accuracy: 0.84 - ETA: 2s - loss: 0.3566 - accuracy: 0.84 - ETA: 2s - loss: 0.3562 - accuracy: 0.84 - ETA: 2s - loss: 0.3561 - accuracy: 0.84 - ETA: 2s - loss: 0.3554 - accuracy: 0.84 - ETA: 2s - loss: 0.3490 - accuracy: 0.84 - ETA: 2s - loss: 0.3470 - accuracy: 0.84 - ETA: 2s - loss: 0.3491 - accuracy: 0.84 - ETA: 2s - loss: 0.3508 - accuracy: 0.84 - ETA: 1s - loss: 0.3495 - accuracy: 0.84 - ETA: 1s - loss: 0.3499 - accuracy: 0.84 - ETA: 1s - loss: 0.3498 - accuracy: 0.84 - ETA: 1s - loss: 0.3512 - accuracy: 0.84 - ETA: 1s - loss: 0.3536 - accuracy: 0.84 - ETA: 1s - loss: 0.3529 - accuracy: 0.84 - ETA: 1s - loss: 0.3549 - accuracy: 0.84 - ETA: 1s - loss: 0.3559 - accuracy: 0.84 - ETA: 1s - loss: 0.3532 - accuracy: 0.84 - ETA: 1s - loss: 0.3526 - accuracy: 0.84 - ETA: 1s - loss: 0.3525 - accuracy: 0.84 - ETA: 0s - loss: 0.3518 - accuracy: 0.84 - ETA: 0s - loss: 0.3507 - accuracy: 0.84 - ETA: 0s - loss: 0.3502 - accuracy: 0.84 - ETA: 0s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3511 - accuracy: 0.84 - ETA: 0s - loss: 0.3510 - accuracy: 0.84 - ETA: 0s - loss: 0.3504 - accuracy: 0.84 - ETA: 0s - loss: 0.3514 - accuracy: 0.84 - ETA: 0s - loss: 0.3529 - accuracy: 0.84 - ETA: 0s - loss: 0.3534 - accuracy: 0.84 - ETA: 0s - loss: 0.3525 - accuracy: 0.84 - ETA: 0s - loss: 0.3529 - accuracy: 0.84 - 4s 344us/step - loss: 0.3533 - accuracy: 0.8432 - val_loss: 0.4158 - val_accuracy: 0.7877\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31344\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3998 - accuracy: 0.86 - ETA: 3s - loss: 0.3745 - accuracy: 0.85 - ETA: 3s - loss: 0.3774 - accuracy: 0.84 - ETA: 3s - loss: 0.3877 - accuracy: 0.83 - ETA: 3s - loss: 0.3859 - accuracy: 0.83 - ETA: 3s - loss: 0.3690 - accuracy: 0.84 - ETA: 2s - loss: 0.3611 - accuracy: 0.84 - ETA: 2s - loss: 0.3561 - accuracy: 0.84 - ETA: 2s - loss: 0.3610 - accuracy: 0.84 - ETA: 2s - loss: 0.3575 - accuracy: 0.84 - ETA: 2s - loss: 0.3571 - accuracy: 0.84 - ETA: 2s - loss: 0.3565 - accuracy: 0.84 - ETA: 2s - loss: 0.3621 - accuracy: 0.84 - ETA: 2s - loss: 0.3586 - accuracy: 0.84 - ETA: 2s - loss: 0.3564 - accuracy: 0.84 - ETA: 2s - loss: 0.3570 - accuracy: 0.84 - ETA: 1s - loss: 0.3594 - accuracy: 0.84 - ETA: 1s - loss: 0.3566 - accuracy: 0.84 - ETA: 1s - loss: 0.3583 - accuracy: 0.84 - ETA: 1s - loss: 0.3586 - accuracy: 0.84 - ETA: 1s - loss: 0.3568 - accuracy: 0.84 - ETA: 1s - loss: 0.3555 - accuracy: 0.84 - ETA: 1s - loss: 0.3549 - accuracy: 0.84 - ETA: 1s - loss: 0.3546 - accuracy: 0.84 - ETA: 1s - loss: 0.3538 - accuracy: 0.84 - ETA: 1s - loss: 0.3539 - accuracy: 0.84 - ETA: 1s - loss: 0.3521 - accuracy: 0.84 - ETA: 1s - loss: 0.3524 - accuracy: 0.84 - ETA: 1s - loss: 0.3526 - accuracy: 0.84 - ETA: 1s - loss: 0.3538 - accuracy: 0.84 - ETA: 1s - loss: 0.3536 - accuracy: 0.84 - ETA: 1s - loss: 0.3541 - accuracy: 0.84 - ETA: 1s - loss: 0.3557 - accuracy: 0.84 - ETA: 1s - loss: 0.3549 - accuracy: 0.84 - ETA: 0s - loss: 0.3553 - accuracy: 0.84 - ETA: 0s - loss: 0.3558 - accuracy: 0.84 - ETA: 0s - loss: 0.3550 - accuracy: 0.84 - ETA: 0s - loss: 0.3554 - accuracy: 0.84 - ETA: 0s - loss: 0.3542 - accuracy: 0.84 - ETA: 0s - loss: 0.3537 - accuracy: 0.84 - ETA: 0s - loss: 0.3552 - accuracy: 0.84 - ETA: 0s - loss: 0.3551 - accuracy: 0.84 - ETA: 0s - loss: 0.3543 - accuracy: 0.84 - ETA: 0s - loss: 0.3543 - accuracy: 0.84 - ETA: 0s - loss: 0.3547 - accuracy: 0.84 - ETA: 0s - loss: 0.3544 - accuracy: 0.84 - ETA: 0s - loss: 0.3542 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3543 - accuracy: 0.84 - ETA: 0s - loss: 0.3536 - accuracy: 0.84 - ETA: 0s - loss: 0.3529 - accuracy: 0.84 - ETA: 0s - loss: 0.3528 - accuracy: 0.84 - ETA: 0s - loss: 0.3535 - accuracy: 0.84 - 4s 353us/step - loss: 0.3532 - accuracy: 0.8435 - val_loss: 0.3446 - val_accuracy: 0.8517\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31344\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3462 - accuracy: 0.85 - ETA: 3s - loss: 0.3380 - accuracy: 0.84 - ETA: 3s - loss: 0.3443 - accuracy: 0.84 - ETA: 3s - loss: 0.3559 - accuracy: 0.84 - ETA: 3s - loss: 0.3542 - accuracy: 0.83 - ETA: 3s - loss: 0.3460 - accuracy: 0.84 - ETA: 3s - loss: 0.3494 - accuracy: 0.84 - ETA: 3s - loss: 0.3578 - accuracy: 0.83 - ETA: 3s - loss: 0.3547 - accuracy: 0.83 - ETA: 3s - loss: 0.3576 - accuracy: 0.83 - ETA: 3s - loss: 0.3557 - accuracy: 0.83 - ETA: 3s - loss: 0.3564 - accuracy: 0.84 - ETA: 3s - loss: 0.3525 - accuracy: 0.84 - ETA: 3s - loss: 0.3524 - accuracy: 0.84 - ETA: 2s - loss: 0.3561 - accuracy: 0.83 - ETA: 2s - loss: 0.3516 - accuracy: 0.84 - ETA: 2s - loss: 0.3491 - accuracy: 0.84 - ETA: 2s - loss: 0.3481 - accuracy: 0.84 - ETA: 2s - loss: 0.3449 - accuracy: 0.84 - ETA: 2s - loss: 0.3426 - accuracy: 0.84 - ETA: 2s - loss: 0.3395 - accuracy: 0.85 - ETA: 2s - loss: 0.3424 - accuracy: 0.84 - ETA: 2s - loss: 0.3424 - accuracy: 0.84 - ETA: 2s - loss: 0.3424 - accuracy: 0.84 - ETA: 2s - loss: 0.3435 - accuracy: 0.84 - ETA: 2s - loss: 0.3440 - accuracy: 0.84 - ETA: 2s - loss: 0.3406 - accuracy: 0.84 - ETA: 2s - loss: 0.3494 - accuracy: 0.84 - ETA: 2s - loss: 0.3488 - accuracy: 0.84 - ETA: 2s - loss: 0.3475 - accuracy: 0.84 - ETA: 2s - loss: 0.3462 - accuracy: 0.84 - ETA: 2s - loss: 0.3470 - accuracy: 0.84 - ETA: 1s - loss: 0.3464 - accuracy: 0.84 - ETA: 1s - loss: 0.3462 - accuracy: 0.84 - ETA: 1s - loss: 0.3476 - accuracy: 0.84 - ETA: 1s - loss: 0.3487 - accuracy: 0.84 - ETA: 1s - loss: 0.3489 - accuracy: 0.84 - ETA: 1s - loss: 0.3475 - accuracy: 0.84 - ETA: 1s - loss: 0.3487 - accuracy: 0.84 - ETA: 1s - loss: 0.3481 - accuracy: 0.84 - ETA: 1s - loss: 0.3488 - accuracy: 0.84 - ETA: 1s - loss: 0.3485 - accuracy: 0.84 - ETA: 1s - loss: 0.3485 - accuracy: 0.84 - ETA: 1s - loss: 0.3481 - accuracy: 0.84 - ETA: 1s - loss: 0.3482 - accuracy: 0.84 - ETA: 1s - loss: 0.3472 - accuracy: 0.84 - ETA: 1s - loss: 0.3464 - accuracy: 0.84 - ETA: 1s - loss: 0.3462 - accuracy: 0.84 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3484 - accuracy: 0.84 - ETA: 0s - loss: 0.3481 - accuracy: 0.84 - ETA: 0s - loss: 0.3482 - accuracy: 0.84 - ETA: 0s - loss: 0.3482 - accuracy: 0.84 - ETA: 0s - loss: 0.3486 - accuracy: 0.84 - ETA: 0s - loss: 0.3481 - accuracy: 0.84 - ETA: 0s - loss: 0.3489 - accuracy: 0.84 - ETA: 0s - loss: 0.3494 - accuracy: 0.84 - ETA: 0s - loss: 0.3482 - accuracy: 0.84 - ETA: 0s - loss: 0.3485 - accuracy: 0.84 - ETA: 0s - loss: 0.3480 - accuracy: 0.84 - ETA: 0s - loss: 0.3474 - accuracy: 0.84 - ETA: 0s - loss: 0.3465 - accuracy: 0.84 - ETA: 0s - loss: 0.3467 - accuracy: 0.84 - ETA: 0s - loss: 0.3470 - accuracy: 0.84 - ETA: 0s - loss: 0.3471 - accuracy: 0.84 - ETA: 0s - loss: 0.3473 - accuracy: 0.84 - 4s 381us/step - loss: 0.3476 - accuracy: 0.8426 - val_loss: 0.3447 - val_accuracy: 0.8425\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31344\n",
      "Epoch 00019: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 238us/step\n",
      "[2020-05-18 16:01:55 RAM64.4% 0.62GB] Val Score : [0.3260300976702261, 0.8544352054595947]\n",
      "[2020-05-18 16:01:55 RAM64.4% 0.62GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:01:55 RAM64.5% 0.62GB] Training on Fold : 6\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 43s - loss: 1.5888 - accuracy: 0.600 - ETA: 16s - loss: 1.3242 - accuracy: 0.583 - ETA: 13s - loss: 1.1676 - accuracy: 0.582 - ETA: 10s - loss: 1.0638 - accuracy: 0.593 - ETA: 9s - loss: 0.9993 - accuracy: 0.596 - ETA: 8s - loss: 0.9478 - accuracy: 0.59 - ETA: 7s - loss: 0.9022 - accuracy: 0.60 - ETA: 7s - loss: 0.8762 - accuracy: 0.60 - ETA: 6s - loss: 0.8548 - accuracy: 0.60 - ETA: 5s - loss: 0.8163 - accuracy: 0.60 - ETA: 5s - loss: 0.7983 - accuracy: 0.61 - ETA: 5s - loss: 0.7713 - accuracy: 0.62 - ETA: 4s - loss: 0.7630 - accuracy: 0.62 - ETA: 4s - loss: 0.7535 - accuracy: 0.62 - ETA: 4s - loss: 0.7422 - accuracy: 0.63 - ETA: 4s - loss: 0.7371 - accuracy: 0.63 - ETA: 4s - loss: 0.7285 - accuracy: 0.64 - ETA: 4s - loss: 0.7189 - accuracy: 0.64 - ETA: 3s - loss: 0.7148 - accuracy: 0.64 - ETA: 3s - loss: 0.7085 - accuracy: 0.65 - ETA: 3s - loss: 0.7014 - accuracy: 0.65 - ETA: 3s - loss: 0.6900 - accuracy: 0.66 - ETA: 3s - loss: 0.6872 - accuracy: 0.66 - ETA: 3s - loss: 0.6804 - accuracy: 0.66 - ETA: 2s - loss: 0.6766 - accuracy: 0.66 - ETA: 2s - loss: 0.6740 - accuracy: 0.66 - ETA: 2s - loss: 0.6718 - accuracy: 0.67 - ETA: 2s - loss: 0.6680 - accuracy: 0.67 - ETA: 2s - loss: 0.6646 - accuracy: 0.67 - ETA: 2s - loss: 0.6630 - accuracy: 0.67 - ETA: 2s - loss: 0.6590 - accuracy: 0.67 - ETA: 2s - loss: 0.6580 - accuracy: 0.67 - ETA: 2s - loss: 0.6582 - accuracy: 0.67 - ETA: 2s - loss: 0.6571 - accuracy: 0.67 - ETA: 2s - loss: 0.6520 - accuracy: 0.68 - ETA: 2s - loss: 0.6478 - accuracy: 0.68 - ETA: 1s - loss: 0.6433 - accuracy: 0.68 - ETA: 1s - loss: 0.6420 - accuracy: 0.68 - ETA: 1s - loss: 0.6391 - accuracy: 0.69 - ETA: 1s - loss: 0.6380 - accuracy: 0.69 - ETA: 1s - loss: 0.6357 - accuracy: 0.69 - ETA: 1s - loss: 0.6344 - accuracy: 0.69 - ETA: 1s - loss: 0.6313 - accuracy: 0.69 - ETA: 1s - loss: 0.6270 - accuracy: 0.69 - ETA: 1s - loss: 0.6259 - accuracy: 0.69 - ETA: 1s - loss: 0.6246 - accuracy: 0.70 - ETA: 1s - loss: 0.6230 - accuracy: 0.70 - ETA: 0s - loss: 0.6220 - accuracy: 0.70 - ETA: 0s - loss: 0.6215 - accuracy: 0.70 - ETA: 0s - loss: 0.6197 - accuracy: 0.70 - ETA: 0s - loss: 0.6177 - accuracy: 0.70 - ETA: 0s - loss: 0.6168 - accuracy: 0.70 - ETA: 0s - loss: 0.6152 - accuracy: 0.70 - ETA: 0s - loss: 0.6128 - accuracy: 0.70 - ETA: 0s - loss: 0.6132 - accuracy: 0.70 - ETA: 0s - loss: 0.6116 - accuracy: 0.70 - ETA: 0s - loss: 0.6112 - accuracy: 0.70 - ETA: 0s - loss: 0.6095 - accuracy: 0.70 - ETA: 0s - loss: 0.6076 - accuracy: 0.71 - ETA: 0s - loss: 0.6072 - accuracy: 0.71 - ETA: 0s - loss: 0.6070 - accuracy: 0.71 - ETA: 0s - loss: 0.6058 - accuracy: 0.71 - ETA: 0s - loss: 0.6048 - accuracy: 0.71 - 5s 437us/step - loss: 0.6044 - accuracy: 0.7118 - val_loss: 0.5499 - val_accuracy: 0.7338\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31344\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.6006 - accuracy: 0.64 - ETA: 3s - loss: 0.5905 - accuracy: 0.68 - ETA: 3s - loss: 0.6070 - accuracy: 0.67 - ETA: 3s - loss: 0.5933 - accuracy: 0.70 - ETA: 3s - loss: 0.5761 - accuracy: 0.71 - ETA: 3s - loss: 0.5616 - accuracy: 0.72 - ETA: 3s - loss: 0.5521 - accuracy: 0.73 - ETA: 3s - loss: 0.5445 - accuracy: 0.73 - ETA: 3s - loss: 0.5416 - accuracy: 0.74 - ETA: 3s - loss: 0.5383 - accuracy: 0.74 - ETA: 3s - loss: 0.5404 - accuracy: 0.74 - ETA: 3s - loss: 0.5379 - accuracy: 0.74 - ETA: 3s - loss: 0.5346 - accuracy: 0.75 - ETA: 2s - loss: 0.5355 - accuracy: 0.75 - ETA: 2s - loss: 0.5257 - accuracy: 0.75 - ETA: 2s - loss: 0.5256 - accuracy: 0.75 - ETA: 2s - loss: 0.5250 - accuracy: 0.75 - ETA: 2s - loss: 0.5238 - accuracy: 0.75 - ETA: 2s - loss: 0.5228 - accuracy: 0.75 - ETA: 2s - loss: 0.5216 - accuracy: 0.75 - ETA: 2s - loss: 0.5215 - accuracy: 0.76 - ETA: 2s - loss: 0.5214 - accuracy: 0.76 - ETA: 2s - loss: 0.5204 - accuracy: 0.76 - ETA: 2s - loss: 0.5224 - accuracy: 0.75 - ETA: 2s - loss: 0.5192 - accuracy: 0.76 - ETA: 2s - loss: 0.5175 - accuracy: 0.76 - ETA: 2s - loss: 0.5154 - accuracy: 0.76 - ETA: 2s - loss: 0.5155 - accuracy: 0.76 - ETA: 2s - loss: 0.5130 - accuracy: 0.76 - ETA: 2s - loss: 0.5140 - accuracy: 0.76 - ETA: 2s - loss: 0.5133 - accuracy: 0.76 - ETA: 2s - loss: 0.5123 - accuracy: 0.76 - ETA: 2s - loss: 0.5141 - accuracy: 0.76 - ETA: 1s - loss: 0.5142 - accuracy: 0.76 - ETA: 1s - loss: 0.5136 - accuracy: 0.76 - ETA: 1s - loss: 0.5124 - accuracy: 0.76 - ETA: 1s - loss: 0.5110 - accuracy: 0.76 - ETA: 1s - loss: 0.5100 - accuracy: 0.76 - ETA: 1s - loss: 0.5108 - accuracy: 0.76 - ETA: 1s - loss: 0.5097 - accuracy: 0.76 - ETA: 1s - loss: 0.5102 - accuracy: 0.76 - ETA: 1s - loss: 0.5117 - accuracy: 0.76 - ETA: 1s - loss: 0.5119 - accuracy: 0.76 - ETA: 1s - loss: 0.5107 - accuracy: 0.76 - ETA: 1s - loss: 0.5123 - accuracy: 0.76 - ETA: 1s - loss: 0.5108 - accuracy: 0.76 - ETA: 1s - loss: 0.5080 - accuracy: 0.76 - ETA: 1s - loss: 0.5081 - accuracy: 0.76 - ETA: 0s - loss: 0.5085 - accuracy: 0.76 - ETA: 0s - loss: 0.5100 - accuracy: 0.76 - ETA: 0s - loss: 0.5099 - accuracy: 0.76 - ETA: 0s - loss: 0.5098 - accuracy: 0.76 - ETA: 0s - loss: 0.5088 - accuracy: 0.76 - ETA: 0s - loss: 0.5085 - accuracy: 0.76 - ETA: 0s - loss: 0.5079 - accuracy: 0.76 - ETA: 0s - loss: 0.5083 - accuracy: 0.76 - ETA: 0s - loss: 0.5084 - accuracy: 0.76 - ETA: 0s - loss: 0.5090 - accuracy: 0.76 - ETA: 0s - loss: 0.5098 - accuracy: 0.76 - ETA: 0s - loss: 0.5095 - accuracy: 0.76 - ETA: 0s - loss: 0.5101 - accuracy: 0.76 - ETA: 0s - loss: 0.5095 - accuracy: 0.76 - ETA: 0s - loss: 0.5080 - accuracy: 0.76 - ETA: 0s - loss: 0.5075 - accuracy: 0.76 - ETA: 0s - loss: 0.5070 - accuracy: 0.76 - ETA: 0s - loss: 0.5075 - accuracy: 0.76 - 4s 394us/step - loss: 0.5069 - accuracy: 0.7685 - val_loss: 0.4659 - val_accuracy: 0.7953\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31344\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4630 - accuracy: 0.82 - ETA: 4s - loss: 0.4551 - accuracy: 0.77 - ETA: 4s - loss: 0.4701 - accuracy: 0.77 - ETA: 4s - loss: 0.4707 - accuracy: 0.77 - ETA: 3s - loss: 0.4746 - accuracy: 0.77 - ETA: 3s - loss: 0.4715 - accuracy: 0.77 - ETA: 3s - loss: 0.4711 - accuracy: 0.78 - ETA: 3s - loss: 0.4774 - accuracy: 0.78 - ETA: 3s - loss: 0.4780 - accuracy: 0.78 - ETA: 3s - loss: 0.4901 - accuracy: 0.77 - ETA: 2s - loss: 0.4828 - accuracy: 0.77 - ETA: 2s - loss: 0.4792 - accuracy: 0.77 - ETA: 2s - loss: 0.4769 - accuracy: 0.78 - ETA: 2s - loss: 0.4800 - accuracy: 0.77 - ETA: 2s - loss: 0.4853 - accuracy: 0.77 - ETA: 2s - loss: 0.4872 - accuracy: 0.77 - ETA: 2s - loss: 0.4869 - accuracy: 0.77 - ETA: 2s - loss: 0.4892 - accuracy: 0.77 - ETA: 2s - loss: 0.4880 - accuracy: 0.77 - ETA: 2s - loss: 0.4857 - accuracy: 0.77 - ETA: 1s - loss: 0.4841 - accuracy: 0.77 - ETA: 1s - loss: 0.4810 - accuracy: 0.77 - ETA: 1s - loss: 0.4808 - accuracy: 0.78 - ETA: 1s - loss: 0.4866 - accuracy: 0.77 - ETA: 1s - loss: 0.4851 - accuracy: 0.77 - ETA: 1s - loss: 0.4849 - accuracy: 0.77 - ETA: 1s - loss: 0.4846 - accuracy: 0.77 - ETA: 1s - loss: 0.4866 - accuracy: 0.77 - ETA: 1s - loss: 0.4851 - accuracy: 0.77 - ETA: 1s - loss: 0.4863 - accuracy: 0.77 - ETA: 1s - loss: 0.4871 - accuracy: 0.77 - ETA: 0s - loss: 0.4865 - accuracy: 0.77 - ETA: 0s - loss: 0.4866 - accuracy: 0.77 - ETA: 0s - loss: 0.4881 - accuracy: 0.77 - ETA: 0s - loss: 0.4894 - accuracy: 0.77 - ETA: 0s - loss: 0.4889 - accuracy: 0.77 - ETA: 0s - loss: 0.4881 - accuracy: 0.77 - ETA: 0s - loss: 0.4876 - accuracy: 0.77 - ETA: 0s - loss: 0.4875 - accuracy: 0.77 - ETA: 0s - loss: 0.4872 - accuracy: 0.77 - ETA: 0s - loss: 0.4860 - accuracy: 0.78 - ETA: 0s - loss: 0.4854 - accuracy: 0.78 - 4s 329us/step - loss: 0.4863 - accuracy: 0.7798 - val_loss: 0.4937 - val_accuracy: 0.7801\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31344\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4194 - accuracy: 0.82 - ETA: 3s - loss: 0.4913 - accuracy: 0.77 - ETA: 3s - loss: 0.5050 - accuracy: 0.76 - ETA: 2s - loss: 0.5046 - accuracy: 0.76 - ETA: 2s - loss: 0.5121 - accuracy: 0.76 - ETA: 2s - loss: 0.4997 - accuracy: 0.77 - ETA: 2s - loss: 0.4899 - accuracy: 0.77 - ETA: 2s - loss: 0.4902 - accuracy: 0.78 - ETA: 2s - loss: 0.4911 - accuracy: 0.77 - ETA: 2s - loss: 0.4912 - accuracy: 0.77 - ETA: 2s - loss: 0.4896 - accuracy: 0.78 - ETA: 2s - loss: 0.4899 - accuracy: 0.78 - ETA: 2s - loss: 0.4853 - accuracy: 0.78 - ETA: 2s - loss: 0.4837 - accuracy: 0.78 - ETA: 2s - loss: 0.4857 - accuracy: 0.78 - ETA: 2s - loss: 0.4780 - accuracy: 0.78 - ETA: 2s - loss: 0.4778 - accuracy: 0.79 - ETA: 2s - loss: 0.4766 - accuracy: 0.79 - ETA: 2s - loss: 0.4787 - accuracy: 0.78 - ETA: 2s - loss: 0.4791 - accuracy: 0.78 - ETA: 1s - loss: 0.4797 - accuracy: 0.78 - ETA: 1s - loss: 0.4810 - accuracy: 0.78 - ETA: 1s - loss: 0.4789 - accuracy: 0.78 - ETA: 1s - loss: 0.4810 - accuracy: 0.78 - ETA: 1s - loss: 0.4770 - accuracy: 0.78 - ETA: 1s - loss: 0.4766 - accuracy: 0.78 - ETA: 1s - loss: 0.4735 - accuracy: 0.78 - ETA: 1s - loss: 0.4735 - accuracy: 0.78 - ETA: 1s - loss: 0.4736 - accuracy: 0.78 - ETA: 1s - loss: 0.4740 - accuracy: 0.78 - ETA: 1s - loss: 0.4724 - accuracy: 0.78 - ETA: 1s - loss: 0.4710 - accuracy: 0.78 - ETA: 1s - loss: 0.4722 - accuracy: 0.78 - ETA: 1s - loss: 0.4726 - accuracy: 0.78 - ETA: 0s - loss: 0.4731 - accuracy: 0.78 - ETA: 0s - loss: 0.4725 - accuracy: 0.78 - ETA: 0s - loss: 0.4713 - accuracy: 0.78 - ETA: 0s - loss: 0.4712 - accuracy: 0.78 - ETA: 0s - loss: 0.4721 - accuracy: 0.78 - ETA: 0s - loss: 0.4726 - accuracy: 0.78 - ETA: 0s - loss: 0.4716 - accuracy: 0.78 - ETA: 0s - loss: 0.4708 - accuracy: 0.78 - ETA: 0s - loss: 0.4716 - accuracy: 0.78 - ETA: 0s - loss: 0.4725 - accuracy: 0.78 - ETA: 0s - loss: 0.4731 - accuracy: 0.78 - ETA: 0s - loss: 0.4719 - accuracy: 0.78 - ETA: 0s - loss: 0.4726 - accuracy: 0.78 - ETA: 0s - loss: 0.4725 - accuracy: 0.78 - ETA: 0s - loss: 0.4720 - accuracy: 0.78 - ETA: 0s - loss: 0.4723 - accuracy: 0.78 - ETA: 0s - loss: 0.4729 - accuracy: 0.78 - ETA: 0s - loss: 0.4718 - accuracy: 0.78 - ETA: 0s - loss: 0.4713 - accuracy: 0.78 - ETA: 0s - loss: 0.4714 - accuracy: 0.78 - 4s 350us/step - loss: 0.4717 - accuracy: 0.7866 - val_loss: 0.4273 - val_accuracy: 0.8180\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31344\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4283 - accuracy: 0.79 - ETA: 3s - loss: 0.4806 - accuracy: 0.76 - ETA: 3s - loss: 0.4884 - accuracy: 0.76 - ETA: 3s - loss: 0.4798 - accuracy: 0.77 - ETA: 3s - loss: 0.4751 - accuracy: 0.77 - ETA: 3s - loss: 0.4836 - accuracy: 0.77 - ETA: 3s - loss: 0.4813 - accuracy: 0.76 - ETA: 3s - loss: 0.4759 - accuracy: 0.77 - ETA: 3s - loss: 0.4826 - accuracy: 0.77 - ETA: 3s - loss: 0.4809 - accuracy: 0.77 - ETA: 3s - loss: 0.4796 - accuracy: 0.77 - ETA: 3s - loss: 0.4781 - accuracy: 0.77 - ETA: 3s - loss: 0.4733 - accuracy: 0.77 - ETA: 3s - loss: 0.4722 - accuracy: 0.77 - ETA: 3s - loss: 0.4717 - accuracy: 0.77 - ETA: 3s - loss: 0.4729 - accuracy: 0.77 - ETA: 3s - loss: 0.4647 - accuracy: 0.78 - ETA: 3s - loss: 0.4629 - accuracy: 0.78 - ETA: 3s - loss: 0.4629 - accuracy: 0.78 - ETA: 3s - loss: 0.4611 - accuracy: 0.78 - ETA: 3s - loss: 0.4617 - accuracy: 0.78 - ETA: 3s - loss: 0.4646 - accuracy: 0.78 - ETA: 2s - loss: 0.4663 - accuracy: 0.78 - ETA: 2s - loss: 0.4642 - accuracy: 0.78 - ETA: 2s - loss: 0.4648 - accuracy: 0.78 - ETA: 2s - loss: 0.4645 - accuracy: 0.78 - ETA: 2s - loss: 0.4656 - accuracy: 0.78 - ETA: 2s - loss: 0.4658 - accuracy: 0.78 - ETA: 2s - loss: 0.4668 - accuracy: 0.78 - ETA: 2s - loss: 0.4669 - accuracy: 0.78 - ETA: 2s - loss: 0.4674 - accuracy: 0.78 - ETA: 2s - loss: 0.4666 - accuracy: 0.78 - ETA: 2s - loss: 0.4646 - accuracy: 0.78 - ETA: 2s - loss: 0.4642 - accuracy: 0.78 - ETA: 2s - loss: 0.4642 - accuracy: 0.78 - ETA: 2s - loss: 0.4619 - accuracy: 0.78 - ETA: 2s - loss: 0.4634 - accuracy: 0.78 - ETA: 2s - loss: 0.4631 - accuracy: 0.78 - ETA: 1s - loss: 0.4630 - accuracy: 0.78 - ETA: 1s - loss: 0.4624 - accuracy: 0.78 - ETA: 1s - loss: 0.4603 - accuracy: 0.79 - ETA: 1s - loss: 0.4612 - accuracy: 0.79 - ETA: 1s - loss: 0.4599 - accuracy: 0.79 - ETA: 1s - loss: 0.4611 - accuracy: 0.79 - ETA: 1s - loss: 0.4603 - accuracy: 0.79 - ETA: 1s - loss: 0.4588 - accuracy: 0.79 - ETA: 1s - loss: 0.4592 - accuracy: 0.79 - ETA: 1s - loss: 0.4591 - accuracy: 0.79 - ETA: 1s - loss: 0.4594 - accuracy: 0.79 - ETA: 1s - loss: 0.4581 - accuracy: 0.79 - ETA: 1s - loss: 0.4587 - accuracy: 0.79 - ETA: 1s - loss: 0.4582 - accuracy: 0.79 - ETA: 1s - loss: 0.4599 - accuracy: 0.79 - ETA: 1s - loss: 0.4597 - accuracy: 0.79 - ETA: 1s - loss: 0.4606 - accuracy: 0.79 - ETA: 0s - loss: 0.4596 - accuracy: 0.79 - ETA: 0s - loss: 0.4582 - accuracy: 0.79 - ETA: 0s - loss: 0.4591 - accuracy: 0.79 - ETA: 0s - loss: 0.4601 - accuracy: 0.79 - ETA: 0s - loss: 0.4598 - accuracy: 0.79 - ETA: 0s - loss: 0.4596 - accuracy: 0.79 - ETA: 0s - loss: 0.4605 - accuracy: 0.79 - ETA: 0s - loss: 0.4612 - accuracy: 0.79 - ETA: 0s - loss: 0.4611 - accuracy: 0.79 - ETA: 0s - loss: 0.4611 - accuracy: 0.79 - ETA: 0s - loss: 0.4622 - accuracy: 0.79 - ETA: 0s - loss: 0.4621 - accuracy: 0.79 - ETA: 0s - loss: 0.4618 - accuracy: 0.79 - ETA: 0s - loss: 0.4614 - accuracy: 0.79 - ETA: 0s - loss: 0.4608 - accuracy: 0.79 - 4s 410us/step - loss: 0.4610 - accuracy: 0.7918 - val_loss: 0.4876 - val_accuracy: 0.7919\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31344\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5541 - accuracy: 0.76 - ETA: 4s - loss: 0.5216 - accuracy: 0.77 - ETA: 4s - loss: 0.4765 - accuracy: 0.80 - ETA: 4s - loss: 0.4752 - accuracy: 0.79 - ETA: 4s - loss: 0.4689 - accuracy: 0.79 - ETA: 4s - loss: 0.4764 - accuracy: 0.78 - ETA: 3s - loss: 0.4827 - accuracy: 0.79 - ETA: 3s - loss: 0.4737 - accuracy: 0.79 - ETA: 3s - loss: 0.4813 - accuracy: 0.79 - ETA: 3s - loss: 0.4792 - accuracy: 0.79 - ETA: 3s - loss: 0.4782 - accuracy: 0.79 - ETA: 3s - loss: 0.4734 - accuracy: 0.79 - ETA: 3s - loss: 0.4658 - accuracy: 0.79 - ETA: 3s - loss: 0.4664 - accuracy: 0.79 - ETA: 3s - loss: 0.4623 - accuracy: 0.79 - ETA: 3s - loss: 0.4577 - accuracy: 0.80 - ETA: 3s - loss: 0.4548 - accuracy: 0.80 - ETA: 3s - loss: 0.4543 - accuracy: 0.80 - ETA: 3s - loss: 0.4546 - accuracy: 0.79 - ETA: 3s - loss: 0.4555 - accuracy: 0.79 - ETA: 3s - loss: 0.4566 - accuracy: 0.79 - ETA: 3s - loss: 0.4583 - accuracy: 0.79 - ETA: 3s - loss: 0.4603 - accuracy: 0.79 - ETA: 2s - loss: 0.4613 - accuracy: 0.79 - ETA: 2s - loss: 0.4589 - accuracy: 0.79 - ETA: 2s - loss: 0.4585 - accuracy: 0.79 - ETA: 2s - loss: 0.4569 - accuracy: 0.79 - ETA: 2s - loss: 0.4580 - accuracy: 0.79 - ETA: 2s - loss: 0.4581 - accuracy: 0.79 - ETA: 2s - loss: 0.4570 - accuracy: 0.79 - ETA: 2s - loss: 0.4582 - accuracy: 0.79 - ETA: 2s - loss: 0.4599 - accuracy: 0.78 - ETA: 2s - loss: 0.4586 - accuracy: 0.79 - ETA: 2s - loss: 0.4565 - accuracy: 0.79 - ETA: 2s - loss: 0.4570 - accuracy: 0.79 - ETA: 2s - loss: 0.4599 - accuracy: 0.79 - ETA: 2s - loss: 0.4593 - accuracy: 0.79 - ETA: 2s - loss: 0.4580 - accuracy: 0.79 - ETA: 2s - loss: 0.4577 - accuracy: 0.79 - ETA: 1s - loss: 0.4580 - accuracy: 0.79 - ETA: 1s - loss: 0.4592 - accuracy: 0.79 - ETA: 1s - loss: 0.4573 - accuracy: 0.79 - ETA: 1s - loss: 0.4577 - accuracy: 0.79 - ETA: 1s - loss: 0.4575 - accuracy: 0.79 - ETA: 1s - loss: 0.4558 - accuracy: 0.79 - ETA: 1s - loss: 0.4553 - accuracy: 0.79 - ETA: 1s - loss: 0.4544 - accuracy: 0.79 - ETA: 1s - loss: 0.4546 - accuracy: 0.79 - ETA: 1s - loss: 0.4543 - accuracy: 0.79 - ETA: 1s - loss: 0.4535 - accuracy: 0.79 - ETA: 1s - loss: 0.4562 - accuracy: 0.79 - ETA: 1s - loss: 0.4562 - accuracy: 0.79 - ETA: 1s - loss: 0.4559 - accuracy: 0.79 - ETA: 1s - loss: 0.4566 - accuracy: 0.79 - ETA: 1s - loss: 0.4571 - accuracy: 0.79 - ETA: 1s - loss: 0.4560 - accuracy: 0.79 - ETA: 0s - loss: 0.4560 - accuracy: 0.79 - ETA: 0s - loss: 0.4559 - accuracy: 0.79 - ETA: 0s - loss: 0.4557 - accuracy: 0.79 - ETA: 0s - loss: 0.4545 - accuracy: 0.79 - ETA: 0s - loss: 0.4539 - accuracy: 0.79 - ETA: 0s - loss: 0.4528 - accuracy: 0.79 - ETA: 0s - loss: 0.4512 - accuracy: 0.79 - ETA: 0s - loss: 0.4532 - accuracy: 0.79 - ETA: 0s - loss: 0.4526 - accuracy: 0.79 - ETA: 0s - loss: 0.4527 - accuracy: 0.79 - ETA: 0s - loss: 0.4513 - accuracy: 0.79 - ETA: 0s - loss: 0.4501 - accuracy: 0.79 - ETA: 0s - loss: 0.4497 - accuracy: 0.79 - ETA: 0s - loss: 0.4501 - accuracy: 0.79 - ETA: 0s - loss: 0.4499 - accuracy: 0.79 - 5s 424us/step - loss: 0.4502 - accuracy: 0.7983 - val_loss: 0.4447 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31344\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.4893 - accuracy: 0.75 - ETA: 4s - loss: 0.4260 - accuracy: 0.79 - ETA: 4s - loss: 0.4630 - accuracy: 0.78 - ETA: 4s - loss: 0.4597 - accuracy: 0.79 - ETA: 4s - loss: 0.4615 - accuracy: 0.79 - ETA: 4s - loss: 0.4458 - accuracy: 0.80 - ETA: 4s - loss: 0.4447 - accuracy: 0.80 - ETA: 4s - loss: 0.4478 - accuracy: 0.80 - ETA: 4s - loss: 0.4399 - accuracy: 0.81 - ETA: 4s - loss: 0.4398 - accuracy: 0.80 - ETA: 4s - loss: 0.4421 - accuracy: 0.80 - ETA: 3s - loss: 0.4397 - accuracy: 0.80 - ETA: 3s - loss: 0.4407 - accuracy: 0.80 - ETA: 3s - loss: 0.4377 - accuracy: 0.80 - ETA: 3s - loss: 0.4357 - accuracy: 0.80 - ETA: 3s - loss: 0.4375 - accuracy: 0.80 - ETA: 3s - loss: 0.4326 - accuracy: 0.80 - ETA: 3s - loss: 0.4371 - accuracy: 0.80 - ETA: 3s - loss: 0.4410 - accuracy: 0.79 - ETA: 3s - loss: 0.4393 - accuracy: 0.79 - ETA: 3s - loss: 0.4389 - accuracy: 0.79 - ETA: 3s - loss: 0.4403 - accuracy: 0.79 - ETA: 3s - loss: 0.4380 - accuracy: 0.79 - ETA: 2s - loss: 0.4386 - accuracy: 0.79 - ETA: 2s - loss: 0.4376 - accuracy: 0.79 - ETA: 2s - loss: 0.4376 - accuracy: 0.79 - ETA: 2s - loss: 0.4373 - accuracy: 0.79 - ETA: 2s - loss: 0.4383 - accuracy: 0.79 - ETA: 2s - loss: 0.4423 - accuracy: 0.79 - ETA: 2s - loss: 0.4421 - accuracy: 0.79 - ETA: 2s - loss: 0.4413 - accuracy: 0.79 - ETA: 2s - loss: 0.4417 - accuracy: 0.79 - ETA: 2s - loss: 0.4399 - accuracy: 0.79 - ETA: 2s - loss: 0.4399 - accuracy: 0.79 - ETA: 2s - loss: 0.4415 - accuracy: 0.79 - ETA: 2s - loss: 0.4412 - accuracy: 0.79 - ETA: 2s - loss: 0.4420 - accuracy: 0.79 - ETA: 2s - loss: 0.4410 - accuracy: 0.79 - ETA: 1s - loss: 0.4392 - accuracy: 0.79 - ETA: 1s - loss: 0.4386 - accuracy: 0.79 - ETA: 1s - loss: 0.4369 - accuracy: 0.79 - ETA: 1s - loss: 0.4368 - accuracy: 0.79 - ETA: 1s - loss: 0.4364 - accuracy: 0.79 - ETA: 1s - loss: 0.4363 - accuracy: 0.79 - ETA: 1s - loss: 0.4366 - accuracy: 0.79 - ETA: 1s - loss: 0.4362 - accuracy: 0.79 - ETA: 1s - loss: 0.4351 - accuracy: 0.79 - ETA: 1s - loss: 0.4330 - accuracy: 0.79 - ETA: 1s - loss: 0.4322 - accuracy: 0.79 - ETA: 1s - loss: 0.4324 - accuracy: 0.79 - ETA: 1s - loss: 0.4331 - accuracy: 0.79 - ETA: 1s - loss: 0.4332 - accuracy: 0.79 - ETA: 1s - loss: 0.4335 - accuracy: 0.79 - ETA: 1s - loss: 0.4333 - accuracy: 0.79 - ETA: 1s - loss: 0.4323 - accuracy: 0.79 - ETA: 0s - loss: 0.4317 - accuracy: 0.79 - ETA: 0s - loss: 0.4343 - accuracy: 0.79 - ETA: 0s - loss: 0.4356 - accuracy: 0.79 - ETA: 0s - loss: 0.4363 - accuracy: 0.79 - ETA: 0s - loss: 0.4357 - accuracy: 0.79 - ETA: 0s - loss: 0.4363 - accuracy: 0.79 - ETA: 0s - loss: 0.4370 - accuracy: 0.79 - ETA: 0s - loss: 0.4368 - accuracy: 0.79 - ETA: 0s - loss: 0.4365 - accuracy: 0.79 - ETA: 0s - loss: 0.4365 - accuracy: 0.79 - ETA: 0s - loss: 0.4352 - accuracy: 0.79 - ETA: 0s - loss: 0.4357 - accuracy: 0.79 - ETA: 0s - loss: 0.4355 - accuracy: 0.79 - ETA: 0s - loss: 0.4355 - accuracy: 0.79 - ETA: 0s - loss: 0.4353 - accuracy: 0.79 - ETA: 0s - loss: 0.4355 - accuracy: 0.79 - 5s 441us/step - loss: 0.4374 - accuracy: 0.7965 - val_loss: 0.4330 - val_accuracy: 0.8248\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31344\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4902 - accuracy: 0.76 - ETA: 3s - loss: 0.4838 - accuracy: 0.78 - ETA: 3s - loss: 0.4504 - accuracy: 0.78 - ETA: 3s - loss: 0.4335 - accuracy: 0.79 - ETA: 3s - loss: 0.4235 - accuracy: 0.79 - ETA: 3s - loss: 0.4303 - accuracy: 0.79 - ETA: 3s - loss: 0.4321 - accuracy: 0.79 - ETA: 3s - loss: 0.4404 - accuracy: 0.79 - ETA: 4s - loss: 0.4356 - accuracy: 0.79 - ETA: 4s - loss: 0.4237 - accuracy: 0.80 - ETA: 4s - loss: 0.4206 - accuracy: 0.80 - ETA: 4s - loss: 0.4169 - accuracy: 0.80 - ETA: 4s - loss: 0.4090 - accuracy: 0.81 - ETA: 4s - loss: 0.4133 - accuracy: 0.81 - ETA: 4s - loss: 0.4133 - accuracy: 0.81 - ETA: 4s - loss: 0.4170 - accuracy: 0.81 - ETA: 3s - loss: 0.4175 - accuracy: 0.81 - ETA: 3s - loss: 0.4174 - accuracy: 0.81 - ETA: 3s - loss: 0.4185 - accuracy: 0.81 - ETA: 3s - loss: 0.4178 - accuracy: 0.81 - ETA: 3s - loss: 0.4148 - accuracy: 0.81 - ETA: 3s - loss: 0.4184 - accuracy: 0.81 - ETA: 3s - loss: 0.4213 - accuracy: 0.80 - ETA: 3s - loss: 0.4215 - accuracy: 0.81 - ETA: 3s - loss: 0.4242 - accuracy: 0.81 - ETA: 3s - loss: 0.4229 - accuracy: 0.81 - ETA: 3s - loss: 0.4260 - accuracy: 0.81 - ETA: 2s - loss: 0.4262 - accuracy: 0.80 - ETA: 2s - loss: 0.4276 - accuracy: 0.80 - ETA: 2s - loss: 0.4287 - accuracy: 0.80 - ETA: 2s - loss: 0.4267 - accuracy: 0.80 - ETA: 2s - loss: 0.4272 - accuracy: 0.80 - ETA: 2s - loss: 0.4261 - accuracy: 0.80 - ETA: 2s - loss: 0.4271 - accuracy: 0.80 - ETA: 2s - loss: 0.4280 - accuracy: 0.80 - ETA: 2s - loss: 0.4271 - accuracy: 0.80 - ETA: 2s - loss: 0.4270 - accuracy: 0.80 - ETA: 2s - loss: 0.4259 - accuracy: 0.80 - ETA: 2s - loss: 0.4262 - accuracy: 0.80 - ETA: 2s - loss: 0.4255 - accuracy: 0.80 - ETA: 2s - loss: 0.4254 - accuracy: 0.80 - ETA: 2s - loss: 0.4239 - accuracy: 0.80 - ETA: 2s - loss: 0.4233 - accuracy: 0.80 - ETA: 1s - loss: 0.4242 - accuracy: 0.80 - ETA: 1s - loss: 0.4245 - accuracy: 0.80 - ETA: 1s - loss: 0.4248 - accuracy: 0.80 - ETA: 1s - loss: 0.4237 - accuracy: 0.80 - ETA: 1s - loss: 0.4231 - accuracy: 0.80 - ETA: 1s - loss: 0.4226 - accuracy: 0.80 - ETA: 1s - loss: 0.4236 - accuracy: 0.80 - ETA: 1s - loss: 0.4232 - accuracy: 0.80 - ETA: 1s - loss: 0.4223 - accuracy: 0.80 - ETA: 1s - loss: 0.4221 - accuracy: 0.80 - ETA: 1s - loss: 0.4214 - accuracy: 0.80 - ETA: 1s - loss: 0.4215 - accuracy: 0.80 - ETA: 1s - loss: 0.4238 - accuracy: 0.80 - ETA: 1s - loss: 0.4243 - accuracy: 0.80 - ETA: 1s - loss: 0.4236 - accuracy: 0.80 - ETA: 0s - loss: 0.4242 - accuracy: 0.80 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - ETA: 0s - loss: 0.4245 - accuracy: 0.80 - ETA: 0s - loss: 0.4258 - accuracy: 0.80 - ETA: 0s - loss: 0.4260 - accuracy: 0.80 - ETA: 0s - loss: 0.4265 - accuracy: 0.80 - ETA: 0s - loss: 0.4261 - accuracy: 0.80 - ETA: 0s - loss: 0.4261 - accuracy: 0.80 - ETA: 0s - loss: 0.4263 - accuracy: 0.80 - ETA: 0s - loss: 0.4256 - accuracy: 0.80 - ETA: 0s - loss: 0.4251 - accuracy: 0.80 - ETA: 0s - loss: 0.4257 - accuracy: 0.80 - ETA: 0s - loss: 0.4260 - accuracy: 0.80 - ETA: 0s - loss: 0.4266 - accuracy: 0.80 - ETA: 0s - loss: 0.4259 - accuracy: 0.80 - 5s 475us/step - loss: 0.4247 - accuracy: 0.8068 - val_loss: 0.3767 - val_accuracy: 0.8298\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31344\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 4s - loss: 0.3578 - accuracy: 0.82 - ETA: 4s - loss: 0.3763 - accuracy: 0.82 - ETA: 5s - loss: 0.3921 - accuracy: 0.83 - ETA: 5s - loss: 0.4019 - accuracy: 0.82 - ETA: 5s - loss: 0.4135 - accuracy: 0.81 - ETA: 6s - loss: 0.4084 - accuracy: 0.81 - ETA: 6s - loss: 0.4105 - accuracy: 0.81 - ETA: 5s - loss: 0.4039 - accuracy: 0.82 - ETA: 5s - loss: 0.3993 - accuracy: 0.82 - ETA: 5s - loss: 0.3947 - accuracy: 0.82 - ETA: 5s - loss: 0.3914 - accuracy: 0.82 - ETA: 5s - loss: 0.3948 - accuracy: 0.82 - ETA: 5s - loss: 0.3986 - accuracy: 0.82 - ETA: 5s - loss: 0.3964 - accuracy: 0.82 - ETA: 4s - loss: 0.4040 - accuracy: 0.82 - ETA: 4s - loss: 0.4084 - accuracy: 0.81 - ETA: 4s - loss: 0.4120 - accuracy: 0.81 - ETA: 4s - loss: 0.4103 - accuracy: 0.81 - ETA: 4s - loss: 0.4112 - accuracy: 0.81 - ETA: 4s - loss: 0.4151 - accuracy: 0.81 - ETA: 4s - loss: 0.4150 - accuracy: 0.81 - ETA: 4s - loss: 0.4188 - accuracy: 0.81 - ETA: 4s - loss: 0.4177 - accuracy: 0.81 - ETA: 4s - loss: 0.4155 - accuracy: 0.81 - ETA: 3s - loss: 0.4144 - accuracy: 0.81 - ETA: 3s - loss: 0.4169 - accuracy: 0.81 - ETA: 3s - loss: 0.4191 - accuracy: 0.80 - ETA: 3s - loss: 0.4196 - accuracy: 0.80 - ETA: 3s - loss: 0.4202 - accuracy: 0.80 - ETA: 3s - loss: 0.4181 - accuracy: 0.80 - ETA: 3s - loss: 0.4181 - accuracy: 0.80 - ETA: 3s - loss: 0.4183 - accuracy: 0.80 - ETA: 3s - loss: 0.4184 - accuracy: 0.80 - ETA: 3s - loss: 0.4168 - accuracy: 0.81 - ETA: 2s - loss: 0.4184 - accuracy: 0.81 - ETA: 2s - loss: 0.4187 - accuracy: 0.81 - ETA: 2s - loss: 0.4181 - accuracy: 0.81 - ETA: 2s - loss: 0.4168 - accuracy: 0.81 - ETA: 2s - loss: 0.4161 - accuracy: 0.81 - ETA: 2s - loss: 0.4175 - accuracy: 0.81 - ETA: 2s - loss: 0.4177 - accuracy: 0.81 - ETA: 2s - loss: 0.4191 - accuracy: 0.81 - ETA: 2s - loss: 0.4167 - accuracy: 0.81 - ETA: 2s - loss: 0.4170 - accuracy: 0.81 - ETA: 2s - loss: 0.4193 - accuracy: 0.81 - ETA: 2s - loss: 0.4198 - accuracy: 0.81 - ETA: 1s - loss: 0.4184 - accuracy: 0.81 - ETA: 1s - loss: 0.4188 - accuracy: 0.81 - ETA: 1s - loss: 0.4178 - accuracy: 0.81 - ETA: 1s - loss: 0.4184 - accuracy: 0.81 - ETA: 1s - loss: 0.4193 - accuracy: 0.81 - ETA: 1s - loss: 0.4199 - accuracy: 0.81 - ETA: 1s - loss: 0.4203 - accuracy: 0.80 - ETA: 1s - loss: 0.4203 - accuracy: 0.80 - ETA: 1s - loss: 0.4202 - accuracy: 0.80 - ETA: 1s - loss: 0.4197 - accuracy: 0.80 - ETA: 1s - loss: 0.4192 - accuracy: 0.80 - ETA: 1s - loss: 0.4190 - accuracy: 0.80 - ETA: 1s - loss: 0.4214 - accuracy: 0.80 - ETA: 0s - loss: 0.4208 - accuracy: 0.80 - ETA: 0s - loss: 0.4210 - accuracy: 0.80 - ETA: 0s - loss: 0.4212 - accuracy: 0.80 - ETA: 0s - loss: 0.4208 - accuracy: 0.80 - ETA: 0s - loss: 0.4210 - accuracy: 0.80 - ETA: 0s - loss: 0.4208 - accuracy: 0.80 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - ETA: 0s - loss: 0.4244 - accuracy: 0.80 - ETA: 0s - loss: 0.4244 - accuracy: 0.80 - ETA: 0s - loss: 0.4245 - accuracy: 0.80 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - ETA: 0s - loss: 0.4233 - accuracy: 0.80 - ETA: 0s - loss: 0.4232 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - 5s 488us/step - loss: 0.4221 - accuracy: 0.8076 - val_loss: 0.3884 - val_accuracy: 0.8315\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31344\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 4s - loss: 0.3316 - accuracy: 0.87 - ETA: 4s - loss: 0.3807 - accuracy: 0.85 - ETA: 4s - loss: 0.4018 - accuracy: 0.82 - ETA: 4s - loss: 0.3984 - accuracy: 0.82 - ETA: 4s - loss: 0.4201 - accuracy: 0.81 - ETA: 4s - loss: 0.4369 - accuracy: 0.80 - ETA: 4s - loss: 0.4445 - accuracy: 0.79 - ETA: 4s - loss: 0.4404 - accuracy: 0.79 - ETA: 4s - loss: 0.4307 - accuracy: 0.80 - ETA: 4s - loss: 0.4323 - accuracy: 0.79 - ETA: 3s - loss: 0.4315 - accuracy: 0.79 - ETA: 3s - loss: 0.4295 - accuracy: 0.79 - ETA: 3s - loss: 0.4231 - accuracy: 0.80 - ETA: 3s - loss: 0.4230 - accuracy: 0.80 - ETA: 3s - loss: 0.4195 - accuracy: 0.80 - ETA: 3s - loss: 0.4193 - accuracy: 0.80 - ETA: 3s - loss: 0.4180 - accuracy: 0.80 - ETA: 3s - loss: 0.4211 - accuracy: 0.80 - ETA: 3s - loss: 0.4217 - accuracy: 0.80 - ETA: 3s - loss: 0.4196 - accuracy: 0.81 - ETA: 3s - loss: 0.4156 - accuracy: 0.81 - ETA: 3s - loss: 0.4145 - accuracy: 0.81 - ETA: 3s - loss: 0.4177 - accuracy: 0.81 - ETA: 3s - loss: 0.4156 - accuracy: 0.81 - ETA: 3s - loss: 0.4156 - accuracy: 0.81 - ETA: 3s - loss: 0.4182 - accuracy: 0.80 - ETA: 3s - loss: 0.4198 - accuracy: 0.80 - ETA: 2s - loss: 0.4156 - accuracy: 0.81 - ETA: 2s - loss: 0.4163 - accuracy: 0.81 - ETA: 2s - loss: 0.4170 - accuracy: 0.80 - ETA: 2s - loss: 0.4170 - accuracy: 0.80 - ETA: 2s - loss: 0.4160 - accuracy: 0.80 - ETA: 2s - loss: 0.4150 - accuracy: 0.81 - ETA: 2s - loss: 0.4169 - accuracy: 0.80 - ETA: 2s - loss: 0.4154 - accuracy: 0.81 - ETA: 2s - loss: 0.4166 - accuracy: 0.81 - ETA: 2s - loss: 0.4156 - accuracy: 0.81 - ETA: 2s - loss: 0.4173 - accuracy: 0.80 - ETA: 2s - loss: 0.4173 - accuracy: 0.80 - ETA: 2s - loss: 0.4167 - accuracy: 0.81 - ETA: 2s - loss: 0.4158 - accuracy: 0.81 - ETA: 2s - loss: 0.4155 - accuracy: 0.81 - ETA: 2s - loss: 0.4165 - accuracy: 0.81 - ETA: 2s - loss: 0.4167 - accuracy: 0.81 - ETA: 1s - loss: 0.4171 - accuracy: 0.81 - ETA: 1s - loss: 0.4174 - accuracy: 0.81 - ETA: 1s - loss: 0.4172 - accuracy: 0.81 - ETA: 1s - loss: 0.4170 - accuracy: 0.81 - ETA: 1s - loss: 0.4179 - accuracy: 0.81 - ETA: 1s - loss: 0.4188 - accuracy: 0.81 - ETA: 1s - loss: 0.4181 - accuracy: 0.81 - ETA: 1s - loss: 0.4180 - accuracy: 0.81 - ETA: 1s - loss: 0.4175 - accuracy: 0.81 - ETA: 1s - loss: 0.4178 - accuracy: 0.81 - ETA: 1s - loss: 0.4178 - accuracy: 0.81 - ETA: 1s - loss: 0.4187 - accuracy: 0.81 - ETA: 0s - loss: 0.4185 - accuracy: 0.81 - ETA: 0s - loss: 0.4175 - accuracy: 0.81 - ETA: 0s - loss: 0.4175 - accuracy: 0.81 - ETA: 0s - loss: 0.4179 - accuracy: 0.81 - ETA: 0s - loss: 0.4164 - accuracy: 0.81 - ETA: 0s - loss: 0.4157 - accuracy: 0.81 - ETA: 0s - loss: 0.4154 - accuracy: 0.81 - ETA: 0s - loss: 0.4152 - accuracy: 0.81 - ETA: 0s - loss: 0.4144 - accuracy: 0.81 - ETA: 0s - loss: 0.4154 - accuracy: 0.81 - ETA: 0s - loss: 0.4161 - accuracy: 0.81 - ETA: 0s - loss: 0.4157 - accuracy: 0.81 - ETA: 0s - loss: 0.4145 - accuracy: 0.81 - ETA: 0s - loss: 0.4148 - accuracy: 0.81 - 5s 455us/step - loss: 0.4139 - accuracy: 0.8136 - val_loss: 0.3750 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31344\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4187 - accuracy: 0.80 - ETA: 3s - loss: 0.4071 - accuracy: 0.81 - ETA: 3s - loss: 0.4029 - accuracy: 0.81 - ETA: 3s - loss: 0.3890 - accuracy: 0.82 - ETA: 3s - loss: 0.3934 - accuracy: 0.82 - ETA: 3s - loss: 0.3877 - accuracy: 0.82 - ETA: 3s - loss: 0.3978 - accuracy: 0.82 - ETA: 3s - loss: 0.3942 - accuracy: 0.82 - ETA: 3s - loss: 0.3995 - accuracy: 0.82 - ETA: 3s - loss: 0.4014 - accuracy: 0.82 - ETA: 3s - loss: 0.4017 - accuracy: 0.82 - ETA: 3s - loss: 0.3976 - accuracy: 0.82 - ETA: 3s - loss: 0.3960 - accuracy: 0.82 - ETA: 3s - loss: 0.3951 - accuracy: 0.82 - ETA: 3s - loss: 0.3926 - accuracy: 0.82 - ETA: 3s - loss: 0.3960 - accuracy: 0.82 - ETA: 2s - loss: 0.3951 - accuracy: 0.82 - ETA: 2s - loss: 0.3934 - accuracy: 0.82 - ETA: 2s - loss: 0.3912 - accuracy: 0.82 - ETA: 2s - loss: 0.3899 - accuracy: 0.82 - ETA: 2s - loss: 0.3946 - accuracy: 0.82 - ETA: 2s - loss: 0.3998 - accuracy: 0.82 - ETA: 2s - loss: 0.4068 - accuracy: 0.82 - ETA: 2s - loss: 0.4062 - accuracy: 0.82 - ETA: 2s - loss: 0.4063 - accuracy: 0.82 - ETA: 2s - loss: 0.4073 - accuracy: 0.82 - ETA: 2s - loss: 0.4079 - accuracy: 0.81 - ETA: 2s - loss: 0.4081 - accuracy: 0.81 - ETA: 2s - loss: 0.4092 - accuracy: 0.81 - ETA: 2s - loss: 0.4088 - accuracy: 0.81 - ETA: 2s - loss: 0.4076 - accuracy: 0.81 - ETA: 2s - loss: 0.4073 - accuracy: 0.81 - ETA: 2s - loss: 0.4069 - accuracy: 0.81 - ETA: 1s - loss: 0.4059 - accuracy: 0.81 - ETA: 1s - loss: 0.4054 - accuracy: 0.81 - ETA: 1s - loss: 0.4056 - accuracy: 0.81 - ETA: 1s - loss: 0.4068 - accuracy: 0.81 - ETA: 1s - loss: 0.4063 - accuracy: 0.81 - ETA: 1s - loss: 0.4057 - accuracy: 0.81 - ETA: 1s - loss: 0.4059 - accuracy: 0.81 - ETA: 1s - loss: 0.4072 - accuracy: 0.81 - ETA: 1s - loss: 0.4074 - accuracy: 0.81 - ETA: 1s - loss: 0.4091 - accuracy: 0.81 - ETA: 1s - loss: 0.4090 - accuracy: 0.81 - ETA: 1s - loss: 0.4099 - accuracy: 0.81 - ETA: 1s - loss: 0.4118 - accuracy: 0.81 - ETA: 1s - loss: 0.4113 - accuracy: 0.81 - ETA: 1s - loss: 0.4116 - accuracy: 0.81 - ETA: 1s - loss: 0.4114 - accuracy: 0.81 - ETA: 1s - loss: 0.4110 - accuracy: 0.81 - ETA: 1s - loss: 0.4108 - accuracy: 0.81 - ETA: 1s - loss: 0.4104 - accuracy: 0.81 - ETA: 1s - loss: 0.4120 - accuracy: 0.81 - ETA: 0s - loss: 0.4113 - accuracy: 0.81 - ETA: 0s - loss: 0.4129 - accuracy: 0.81 - ETA: 0s - loss: 0.4126 - accuracy: 0.81 - ETA: 0s - loss: 0.4136 - accuracy: 0.81 - ETA: 0s - loss: 0.4134 - accuracy: 0.81 - ETA: 0s - loss: 0.4135 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.81 - ETA: 0s - loss: 0.4137 - accuracy: 0.81 - ETA: 0s - loss: 0.4142 - accuracy: 0.81 - ETA: 0s - loss: 0.4135 - accuracy: 0.81 - ETA: 0s - loss: 0.4131 - accuracy: 0.81 - ETA: 0s - loss: 0.4114 - accuracy: 0.81 - ETA: 0s - loss: 0.4104 - accuracy: 0.81 - ETA: 0s - loss: 0.4097 - accuracy: 0.81 - 4s 405us/step - loss: 0.4102 - accuracy: 0.8134 - val_loss: 0.3800 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31344\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3640 - accuracy: 0.80 - ETA: 3s - loss: 0.3530 - accuracy: 0.82 - ETA: 3s - loss: 0.3383 - accuracy: 0.84 - ETA: 3s - loss: 0.3587 - accuracy: 0.83 - ETA: 3s - loss: 0.3637 - accuracy: 0.83 - ETA: 3s - loss: 0.3750 - accuracy: 0.83 - ETA: 2s - loss: 0.3947 - accuracy: 0.81 - ETA: 2s - loss: 0.3927 - accuracy: 0.81 - ETA: 2s - loss: 0.3915 - accuracy: 0.81 - ETA: 2s - loss: 0.3943 - accuracy: 0.81 - ETA: 2s - loss: 0.3950 - accuracy: 0.81 - ETA: 2s - loss: 0.3953 - accuracy: 0.81 - ETA: 2s - loss: 0.3947 - accuracy: 0.81 - ETA: 2s - loss: 0.4024 - accuracy: 0.81 - ETA: 2s - loss: 0.4037 - accuracy: 0.81 - ETA: 2s - loss: 0.4032 - accuracy: 0.81 - ETA: 2s - loss: 0.4000 - accuracy: 0.81 - ETA: 2s - loss: 0.4064 - accuracy: 0.81 - ETA: 2s - loss: 0.4097 - accuracy: 0.81 - ETA: 2s - loss: 0.4122 - accuracy: 0.80 - ETA: 2s - loss: 0.4115 - accuracy: 0.80 - ETA: 2s - loss: 0.4079 - accuracy: 0.81 - ETA: 2s - loss: 0.4086 - accuracy: 0.80 - ETA: 2s - loss: 0.4082 - accuracy: 0.81 - ETA: 1s - loss: 0.4089 - accuracy: 0.81 - ETA: 1s - loss: 0.4115 - accuracy: 0.80 - ETA: 1s - loss: 0.4125 - accuracy: 0.80 - ETA: 1s - loss: 0.4124 - accuracy: 0.80 - ETA: 1s - loss: 0.4120 - accuracy: 0.80 - ETA: 1s - loss: 0.4125 - accuracy: 0.80 - ETA: 1s - loss: 0.4123 - accuracy: 0.80 - ETA: 1s - loss: 0.4123 - accuracy: 0.81 - ETA: 1s - loss: 0.4116 - accuracy: 0.80 - ETA: 1s - loss: 0.4093 - accuracy: 0.81 - ETA: 1s - loss: 0.4090 - accuracy: 0.81 - ETA: 1s - loss: 0.4065 - accuracy: 0.81 - ETA: 1s - loss: 0.4062 - accuracy: 0.81 - ETA: 1s - loss: 0.4071 - accuracy: 0.81 - ETA: 1s - loss: 0.4053 - accuracy: 0.81 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4053 - accuracy: 0.81 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - ETA: 0s - loss: 0.4032 - accuracy: 0.81 - ETA: 0s - loss: 0.4045 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4054 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4046 - accuracy: 0.81 - ETA: 0s - loss: 0.4041 - accuracy: 0.81 - ETA: 0s - loss: 0.4048 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4034 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4031 - accuracy: 0.81 - ETA: 0s - loss: 0.4034 - accuracy: 0.81 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - 4s 370us/step - loss: 0.4042 - accuracy: 0.8158 - val_loss: 0.3643 - val_accuracy: 0.8357\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31344\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3971 - accuracy: 0.85 - ETA: 3s - loss: 0.3825 - accuracy: 0.86 - ETA: 3s - loss: 0.3690 - accuracy: 0.86 - ETA: 3s - loss: 0.3722 - accuracy: 0.85 - ETA: 3s - loss: 0.3794 - accuracy: 0.86 - ETA: 3s - loss: 0.3926 - accuracy: 0.84 - ETA: 3s - loss: 0.3912 - accuracy: 0.84 - ETA: 3s - loss: 0.3921 - accuracy: 0.83 - ETA: 3s - loss: 0.3968 - accuracy: 0.83 - ETA: 3s - loss: 0.3977 - accuracy: 0.83 - ETA: 3s - loss: 0.3990 - accuracy: 0.82 - ETA: 3s - loss: 0.3941 - accuracy: 0.82 - ETA: 3s - loss: 0.3946 - accuracy: 0.82 - ETA: 3s - loss: 0.3902 - accuracy: 0.83 - ETA: 3s - loss: 0.3904 - accuracy: 0.83 - ETA: 3s - loss: 0.3959 - accuracy: 0.82 - ETA: 3s - loss: 0.3930 - accuracy: 0.82 - ETA: 3s - loss: 0.3882 - accuracy: 0.83 - ETA: 3s - loss: 0.3868 - accuracy: 0.83 - ETA: 2s - loss: 0.3886 - accuracy: 0.82 - ETA: 2s - loss: 0.3910 - accuracy: 0.82 - ETA: 2s - loss: 0.3909 - accuracy: 0.82 - ETA: 2s - loss: 0.3935 - accuracy: 0.82 - ETA: 2s - loss: 0.3904 - accuracy: 0.83 - ETA: 2s - loss: 0.3929 - accuracy: 0.82 - ETA: 2s - loss: 0.3917 - accuracy: 0.82 - ETA: 2s - loss: 0.3898 - accuracy: 0.83 - ETA: 2s - loss: 0.3910 - accuracy: 0.82 - ETA: 2s - loss: 0.3947 - accuracy: 0.82 - ETA: 2s - loss: 0.3935 - accuracy: 0.82 - ETA: 2s - loss: 0.3958 - accuracy: 0.82 - ETA: 2s - loss: 0.3962 - accuracy: 0.82 - ETA: 2s - loss: 0.3992 - accuracy: 0.82 - ETA: 2s - loss: 0.3984 - accuracy: 0.82 - ETA: 2s - loss: 0.4006 - accuracy: 0.82 - ETA: 2s - loss: 0.4019 - accuracy: 0.82 - ETA: 2s - loss: 0.4003 - accuracy: 0.82 - ETA: 1s - loss: 0.4013 - accuracy: 0.82 - ETA: 1s - loss: 0.4008 - accuracy: 0.82 - ETA: 1s - loss: 0.4021 - accuracy: 0.82 - ETA: 1s - loss: 0.4017 - accuracy: 0.82 - ETA: 1s - loss: 0.4016 - accuracy: 0.82 - ETA: 1s - loss: 0.4010 - accuracy: 0.82 - ETA: 1s - loss: 0.4004 - accuracy: 0.82 - ETA: 1s - loss: 0.4001 - accuracy: 0.82 - ETA: 1s - loss: 0.4014 - accuracy: 0.82 - ETA: 1s - loss: 0.4019 - accuracy: 0.82 - ETA: 1s - loss: 0.3992 - accuracy: 0.82 - ETA: 1s - loss: 0.4002 - accuracy: 0.82 - ETA: 1s - loss: 0.3987 - accuracy: 0.82 - ETA: 1s - loss: 0.3984 - accuracy: 0.82 - ETA: 1s - loss: 0.3982 - accuracy: 0.82 - ETA: 1s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.4002 - accuracy: 0.82 - ETA: 0s - loss: 0.4011 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.3980 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.3975 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3981 - accuracy: 0.82 - 4s 398us/step - loss: 0.3981 - accuracy: 0.8221 - val_loss: 0.3452 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31344\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5111 - accuracy: 0.75 - ETA: 3s - loss: 0.4288 - accuracy: 0.80 - ETA: 2s - loss: 0.4227 - accuracy: 0.80 - ETA: 3s - loss: 0.4175 - accuracy: 0.80 - ETA: 3s - loss: 0.4115 - accuracy: 0.81 - ETA: 2s - loss: 0.3995 - accuracy: 0.81 - ETA: 2s - loss: 0.3903 - accuracy: 0.82 - ETA: 2s - loss: 0.3927 - accuracy: 0.82 - ETA: 2s - loss: 0.3944 - accuracy: 0.82 - ETA: 3s - loss: 0.3950 - accuracy: 0.82 - ETA: 3s - loss: 0.3925 - accuracy: 0.82 - ETA: 3s - loss: 0.3939 - accuracy: 0.82 - ETA: 3s - loss: 0.3990 - accuracy: 0.82 - ETA: 2s - loss: 0.3951 - accuracy: 0.82 - ETA: 2s - loss: 0.3976 - accuracy: 0.82 - ETA: 2s - loss: 0.3959 - accuracy: 0.82 - ETA: 2s - loss: 0.3968 - accuracy: 0.82 - ETA: 2s - loss: 0.4019 - accuracy: 0.82 - ETA: 2s - loss: 0.4057 - accuracy: 0.82 - ETA: 2s - loss: 0.4046 - accuracy: 0.82 - ETA: 2s - loss: 0.4040 - accuracy: 0.82 - ETA: 2s - loss: 0.4014 - accuracy: 0.82 - ETA: 2s - loss: 0.3976 - accuracy: 0.82 - ETA: 2s - loss: 0.3978 - accuracy: 0.82 - ETA: 2s - loss: 0.3971 - accuracy: 0.82 - ETA: 2s - loss: 0.3997 - accuracy: 0.82 - ETA: 2s - loss: 0.3994 - accuracy: 0.81 - ETA: 2s - loss: 0.3991 - accuracy: 0.82 - ETA: 2s - loss: 0.4004 - accuracy: 0.81 - ETA: 2s - loss: 0.4000 - accuracy: 0.81 - ETA: 2s - loss: 0.3997 - accuracy: 0.81 - ETA: 2s - loss: 0.3993 - accuracy: 0.81 - ETA: 1s - loss: 0.3980 - accuracy: 0.82 - ETA: 1s - loss: 0.3993 - accuracy: 0.82 - ETA: 1s - loss: 0.4018 - accuracy: 0.81 - ETA: 1s - loss: 0.4001 - accuracy: 0.81 - ETA: 1s - loss: 0.3993 - accuracy: 0.82 - ETA: 1s - loss: 0.3966 - accuracy: 0.82 - ETA: 1s - loss: 0.3949 - accuracy: 0.82 - ETA: 1s - loss: 0.3944 - accuracy: 0.82 - ETA: 1s - loss: 0.3928 - accuracy: 0.82 - ETA: 1s - loss: 0.3933 - accuracy: 0.82 - ETA: 1s - loss: 0.3958 - accuracy: 0.82 - ETA: 1s - loss: 0.3975 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.81 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.81 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.3965 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.81 - ETA: 0s - loss: 0.3977 - accuracy: 0.81 - ETA: 0s - loss: 0.3956 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.81 - 4s 362us/step - loss: 0.3964 - accuracy: 0.8199 - val_loss: 0.3688 - val_accuracy: 0.8458\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31344\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 5s - loss: 0.3577 - accuracy: 0.86 - ETA: 4s - loss: 0.3746 - accuracy: 0.84 - ETA: 4s - loss: 0.3586 - accuracy: 0.84 - ETA: 4s - loss: 0.3796 - accuracy: 0.83 - ETA: 4s - loss: 0.3779 - accuracy: 0.83 - ETA: 4s - loss: 0.3823 - accuracy: 0.83 - ETA: 3s - loss: 0.3902 - accuracy: 0.83 - ETA: 3s - loss: 0.4112 - accuracy: 0.81 - ETA: 3s - loss: 0.4101 - accuracy: 0.81 - ETA: 3s - loss: 0.4056 - accuracy: 0.81 - ETA: 3s - loss: 0.4090 - accuracy: 0.81 - ETA: 3s - loss: 0.4028 - accuracy: 0.82 - ETA: 3s - loss: 0.4002 - accuracy: 0.82 - ETA: 3s - loss: 0.3941 - accuracy: 0.82 - ETA: 3s - loss: 0.3944 - accuracy: 0.82 - ETA: 3s - loss: 0.3935 - accuracy: 0.82 - ETA: 3s - loss: 0.3893 - accuracy: 0.82 - ETA: 3s - loss: 0.3900 - accuracy: 0.82 - ETA: 2s - loss: 0.3907 - accuracy: 0.82 - ETA: 2s - loss: 0.3962 - accuracy: 0.82 - ETA: 2s - loss: 0.3971 - accuracy: 0.81 - ETA: 2s - loss: 0.4011 - accuracy: 0.81 - ETA: 2s - loss: 0.3994 - accuracy: 0.82 - ETA: 2s - loss: 0.4034 - accuracy: 0.81 - ETA: 2s - loss: 0.4037 - accuracy: 0.81 - ETA: 2s - loss: 0.4030 - accuracy: 0.81 - ETA: 2s - loss: 0.4032 - accuracy: 0.81 - ETA: 2s - loss: 0.4022 - accuracy: 0.81 - ETA: 2s - loss: 0.3996 - accuracy: 0.82 - ETA: 2s - loss: 0.3986 - accuracy: 0.82 - ETA: 2s - loss: 0.3966 - accuracy: 0.82 - ETA: 2s - loss: 0.3962 - accuracy: 0.82 - ETA: 2s - loss: 0.3954 - accuracy: 0.82 - ETA: 2s - loss: 0.3954 - accuracy: 0.82 - ETA: 2s - loss: 0.3950 - accuracy: 0.82 - ETA: 1s - loss: 0.3936 - accuracy: 0.82 - ETA: 1s - loss: 0.3938 - accuracy: 0.82 - ETA: 1s - loss: 0.3914 - accuracy: 0.82 - ETA: 1s - loss: 0.3922 - accuracy: 0.82 - ETA: 1s - loss: 0.3915 - accuracy: 0.82 - ETA: 1s - loss: 0.3939 - accuracy: 0.82 - ETA: 1s - loss: 0.3930 - accuracy: 0.82 - ETA: 1s - loss: 0.3926 - accuracy: 0.82 - ETA: 1s - loss: 0.3920 - accuracy: 0.82 - ETA: 1s - loss: 0.3897 - accuracy: 0.82 - ETA: 1s - loss: 0.3894 - accuracy: 0.82 - ETA: 1s - loss: 0.3889 - accuracy: 0.82 - ETA: 1s - loss: 0.3878 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3866 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3876 - accuracy: 0.82 - ETA: 0s - loss: 0.3872 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.82 - ETA: 0s - loss: 0.3902 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - 4s 390us/step - loss: 0.3884 - accuracy: 0.8238 - val_loss: 0.4253 - val_accuracy: 0.7970\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31344\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4080 - accuracy: 0.80 - ETA: 3s - loss: 0.4205 - accuracy: 0.81 - ETA: 3s - loss: 0.4020 - accuracy: 0.82 - ETA: 3s - loss: 0.3870 - accuracy: 0.83 - ETA: 3s - loss: 0.3875 - accuracy: 0.83 - ETA: 3s - loss: 0.3827 - accuracy: 0.83 - ETA: 3s - loss: 0.3824 - accuracy: 0.82 - ETA: 3s - loss: 0.3795 - accuracy: 0.83 - ETA: 3s - loss: 0.3747 - accuracy: 0.83 - ETA: 3s - loss: 0.3686 - accuracy: 0.83 - ETA: 3s - loss: 0.3731 - accuracy: 0.83 - ETA: 3s - loss: 0.3776 - accuracy: 0.83 - ETA: 3s - loss: 0.3936 - accuracy: 0.82 - ETA: 3s - loss: 0.3973 - accuracy: 0.82 - ETA: 3s - loss: 0.3989 - accuracy: 0.82 - ETA: 3s - loss: 0.4007 - accuracy: 0.81 - ETA: 2s - loss: 0.3958 - accuracy: 0.82 - ETA: 2s - loss: 0.3918 - accuracy: 0.82 - ETA: 2s - loss: 0.3940 - accuracy: 0.82 - ETA: 2s - loss: 0.3915 - accuracy: 0.82 - ETA: 2s - loss: 0.3927 - accuracy: 0.82 - ETA: 2s - loss: 0.3927 - accuracy: 0.82 - ETA: 2s - loss: 0.3948 - accuracy: 0.82 - ETA: 2s - loss: 0.3945 - accuracy: 0.82 - ETA: 2s - loss: 0.3934 - accuracy: 0.82 - ETA: 2s - loss: 0.3928 - accuracy: 0.82 - ETA: 2s - loss: 0.3931 - accuracy: 0.82 - ETA: 2s - loss: 0.3931 - accuracy: 0.82 - ETA: 1s - loss: 0.3931 - accuracy: 0.82 - ETA: 1s - loss: 0.3936 - accuracy: 0.82 - ETA: 1s - loss: 0.3945 - accuracy: 0.82 - ETA: 1s - loss: 0.3952 - accuracy: 0.82 - ETA: 1s - loss: 0.3955 - accuracy: 0.82 - ETA: 1s - loss: 0.3945 - accuracy: 0.82 - ETA: 1s - loss: 0.3924 - accuracy: 0.82 - ETA: 1s - loss: 0.3927 - accuracy: 0.82 - ETA: 1s - loss: 0.3935 - accuracy: 0.82 - ETA: 1s - loss: 0.3913 - accuracy: 0.82 - ETA: 1s - loss: 0.3903 - accuracy: 0.82 - ETA: 1s - loss: 0.3894 - accuracy: 0.82 - ETA: 1s - loss: 0.3901 - accuracy: 0.82 - ETA: 1s - loss: 0.3900 - accuracy: 0.82 - ETA: 1s - loss: 0.3898 - accuracy: 0.82 - ETA: 1s - loss: 0.3885 - accuracy: 0.82 - ETA: 1s - loss: 0.3868 - accuracy: 0.82 - ETA: 1s - loss: 0.3867 - accuracy: 0.82 - ETA: 1s - loss: 0.3869 - accuracy: 0.82 - ETA: 1s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3869 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3868 - accuracy: 0.82 - ETA: 0s - loss: 0.3867 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - 4s 390us/step - loss: 0.3895 - accuracy: 0.8218 - val_loss: 0.3705 - val_accuracy: 0.8298\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31344\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3805 - accuracy: 0.81 - ETA: 3s - loss: 0.3679 - accuracy: 0.83 - ETA: 3s - loss: 0.3569 - accuracy: 0.84 - ETA: 3s - loss: 0.3780 - accuracy: 0.83 - ETA: 3s - loss: 0.3823 - accuracy: 0.83 - ETA: 3s - loss: 0.3877 - accuracy: 0.82 - ETA: 3s - loss: 0.3834 - accuracy: 0.82 - ETA: 3s - loss: 0.3866 - accuracy: 0.82 - ETA: 3s - loss: 0.3793 - accuracy: 0.82 - ETA: 3s - loss: 0.3790 - accuracy: 0.82 - ETA: 3s - loss: 0.3767 - accuracy: 0.82 - ETA: 3s - loss: 0.3785 - accuracy: 0.82 - ETA: 2s - loss: 0.3765 - accuracy: 0.82 - ETA: 2s - loss: 0.3723 - accuracy: 0.82 - ETA: 3s - loss: 0.3720 - accuracy: 0.82 - ETA: 3s - loss: 0.3710 - accuracy: 0.83 - ETA: 3s - loss: 0.3711 - accuracy: 0.83 - ETA: 2s - loss: 0.3702 - accuracy: 0.83 - ETA: 2s - loss: 0.3689 - accuracy: 0.83 - ETA: 2s - loss: 0.3706 - accuracy: 0.83 - ETA: 2s - loss: 0.3691 - accuracy: 0.83 - ETA: 2s - loss: 0.3712 - accuracy: 0.82 - ETA: 2s - loss: 0.3722 - accuracy: 0.82 - ETA: 2s - loss: 0.3741 - accuracy: 0.82 - ETA: 2s - loss: 0.3752 - accuracy: 0.82 - ETA: 2s - loss: 0.3766 - accuracy: 0.82 - ETA: 2s - loss: 0.3786 - accuracy: 0.82 - ETA: 2s - loss: 0.3792 - accuracy: 0.82 - ETA: 2s - loss: 0.3769 - accuracy: 0.82 - ETA: 2s - loss: 0.3763 - accuracy: 0.82 - ETA: 2s - loss: 0.3747 - accuracy: 0.82 - ETA: 2s - loss: 0.3757 - accuracy: 0.82 - ETA: 2s - loss: 0.3771 - accuracy: 0.82 - ETA: 1s - loss: 0.3768 - accuracy: 0.82 - ETA: 1s - loss: 0.3767 - accuracy: 0.82 - ETA: 1s - loss: 0.3760 - accuracy: 0.82 - ETA: 1s - loss: 0.3789 - accuracy: 0.82 - ETA: 1s - loss: 0.3783 - accuracy: 0.82 - ETA: 1s - loss: 0.3793 - accuracy: 0.82 - ETA: 1s - loss: 0.3790 - accuracy: 0.82 - ETA: 1s - loss: 0.3802 - accuracy: 0.82 - ETA: 1s - loss: 0.3816 - accuracy: 0.82 - ETA: 1s - loss: 0.3827 - accuracy: 0.82 - ETA: 1s - loss: 0.3807 - accuracy: 0.82 - ETA: 1s - loss: 0.3818 - accuracy: 0.82 - ETA: 1s - loss: 0.3815 - accuracy: 0.82 - ETA: 1s - loss: 0.3810 - accuracy: 0.82 - ETA: 1s - loss: 0.3810 - accuracy: 0.82 - ETA: 1s - loss: 0.3802 - accuracy: 0.82 - ETA: 1s - loss: 0.3794 - accuracy: 0.82 - ETA: 1s - loss: 0.3801 - accuracy: 0.82 - ETA: 0s - loss: 0.3814 - accuracy: 0.82 - ETA: 0s - loss: 0.3819 - accuracy: 0.82 - ETA: 0s - loss: 0.3839 - accuracy: 0.82 - ETA: 0s - loss: 0.3846 - accuracy: 0.82 - ETA: 0s - loss: 0.3852 - accuracy: 0.82 - ETA: 0s - loss: 0.3839 - accuracy: 0.82 - ETA: 0s - loss: 0.3828 - accuracy: 0.82 - ETA: 0s - loss: 0.3828 - accuracy: 0.82 - ETA: 0s - loss: 0.3828 - accuracy: 0.82 - ETA: 0s - loss: 0.3823 - accuracy: 0.82 - ETA: 0s - loss: 0.3822 - accuracy: 0.82 - ETA: 0s - loss: 0.3820 - accuracy: 0.82 - ETA: 0s - loss: 0.3829 - accuracy: 0.82 - ETA: 0s - loss: 0.3831 - accuracy: 0.82 - 4s 384us/step - loss: 0.3825 - accuracy: 0.8245 - val_loss: 0.3625 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31344\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4010 - accuracy: 0.83 - ETA: 3s - loss: 0.3668 - accuracy: 0.85 - ETA: 3s - loss: 0.3643 - accuracy: 0.84 - ETA: 3s - loss: 0.3649 - accuracy: 0.84 - ETA: 3s - loss: 0.3637 - accuracy: 0.84 - ETA: 3s - loss: 0.3593 - accuracy: 0.84 - ETA: 3s - loss: 0.3601 - accuracy: 0.84 - ETA: 3s - loss: 0.3705 - accuracy: 0.83 - ETA: 3s - loss: 0.3694 - accuracy: 0.84 - ETA: 3s - loss: 0.3770 - accuracy: 0.83 - ETA: 3s - loss: 0.3721 - accuracy: 0.83 - ETA: 3s - loss: 0.3669 - accuracy: 0.83 - ETA: 3s - loss: 0.3656 - accuracy: 0.83 - ETA: 3s - loss: 0.3704 - accuracy: 0.83 - ETA: 3s - loss: 0.3733 - accuracy: 0.83 - ETA: 3s - loss: 0.3729 - accuracy: 0.83 - ETA: 3s - loss: 0.3704 - accuracy: 0.83 - ETA: 3s - loss: 0.3768 - accuracy: 0.83 - ETA: 2s - loss: 0.3774 - accuracy: 0.83 - ETA: 2s - loss: 0.3785 - accuracy: 0.82 - ETA: 2s - loss: 0.3788 - accuracy: 0.82 - ETA: 2s - loss: 0.3783 - accuracy: 0.82 - ETA: 2s - loss: 0.3783 - accuracy: 0.82 - ETA: 2s - loss: 0.3774 - accuracy: 0.82 - ETA: 2s - loss: 0.3783 - accuracy: 0.82 - ETA: 2s - loss: 0.3781 - accuracy: 0.82 - ETA: 2s - loss: 0.3792 - accuracy: 0.82 - ETA: 2s - loss: 0.3775 - accuracy: 0.83 - ETA: 2s - loss: 0.3797 - accuracy: 0.83 - ETA: 2s - loss: 0.3792 - accuracy: 0.83 - ETA: 2s - loss: 0.3806 - accuracy: 0.83 - ETA: 2s - loss: 0.3827 - accuracy: 0.82 - ETA: 2s - loss: 0.3817 - accuracy: 0.83 - ETA: 2s - loss: 0.3817 - accuracy: 0.83 - ETA: 2s - loss: 0.3803 - accuracy: 0.83 - ETA: 2s - loss: 0.3801 - accuracy: 0.83 - ETA: 1s - loss: 0.3770 - accuracy: 0.83 - ETA: 1s - loss: 0.3761 - accuracy: 0.83 - ETA: 1s - loss: 0.3794 - accuracy: 0.83 - ETA: 1s - loss: 0.3811 - accuracy: 0.83 - ETA: 1s - loss: 0.3805 - accuracy: 0.83 - ETA: 1s - loss: 0.3828 - accuracy: 0.83 - ETA: 1s - loss: 0.3811 - accuracy: 0.83 - ETA: 1s - loss: 0.3782 - accuracy: 0.83 - ETA: 1s - loss: 0.3778 - accuracy: 0.83 - ETA: 1s - loss: 0.3782 - accuracy: 0.83 - ETA: 1s - loss: 0.3780 - accuracy: 0.83 - ETA: 1s - loss: 0.3784 - accuracy: 0.83 - ETA: 1s - loss: 0.3775 - accuracy: 0.83 - ETA: 1s - loss: 0.3774 - accuracy: 0.83 - ETA: 1s - loss: 0.3772 - accuracy: 0.83 - ETA: 1s - loss: 0.3791 - accuracy: 0.83 - ETA: 1s - loss: 0.3791 - accuracy: 0.83 - ETA: 1s - loss: 0.3790 - accuracy: 0.83 - ETA: 0s - loss: 0.3797 - accuracy: 0.83 - ETA: 0s - loss: 0.3783 - accuracy: 0.83 - ETA: 0s - loss: 0.3790 - accuracy: 0.83 - ETA: 0s - loss: 0.3777 - accuracy: 0.83 - ETA: 0s - loss: 0.3768 - accuracy: 0.83 - ETA: 0s - loss: 0.3785 - accuracy: 0.83 - ETA: 0s - loss: 0.3808 - accuracy: 0.83 - ETA: 0s - loss: 0.3813 - accuracy: 0.83 - ETA: 0s - loss: 0.3802 - accuracy: 0.83 - ETA: 0s - loss: 0.3798 - accuracy: 0.83 - ETA: 0s - loss: 0.3794 - accuracy: 0.83 - ETA: 0s - loss: 0.3809 - accuracy: 0.83 - ETA: 0s - loss: 0.3814 - accuracy: 0.83 - ETA: 0s - loss: 0.3818 - accuracy: 0.83 - ETA: 0s - loss: 0.3817 - accuracy: 0.83 - ETA: 0s - loss: 0.3819 - accuracy: 0.83 - ETA: 0s - loss: 0.3812 - accuracy: 0.83 - 4s 407us/step - loss: 0.3811 - accuracy: 0.8322 - val_loss: 0.3726 - val_accuracy: 0.8391\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31344\n",
      "Epoch 00018: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 218us/step\n",
      "[2020-05-18 16:03:16 RAM67.7% 0.65GB] Val Score : [0.35745140069628234, 0.8438210487365723]\n",
      "[2020-05-18 16:03:16 RAM67.7% 0.65GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:03:16 RAM67.7% 0.65GB] Training on Fold : 7\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 43s - loss: 2.8075 - accuracy: 0.365 - ETA: 23s - loss: 2.2937 - accuracy: 0.544 - ETA: 16s - loss: 1.8766 - accuracy: 0.570 - ETA: 13s - loss: 1.6343 - accuracy: 0.569 - ETA: 9s - loss: 1.3311 - accuracy: 0.580 - ETA: 8s - loss: 1.2509 - accuracy: 0.57 - ETA: 7s - loss: 1.1703 - accuracy: 0.57 - ETA: 6s - loss: 1.0764 - accuracy: 0.57 - ETA: 5s - loss: 1.0060 - accuracy: 0.58 - ETA: 5s - loss: 0.9775 - accuracy: 0.58 - ETA: 5s - loss: 0.9525 - accuracy: 0.59 - ETA: 5s - loss: 0.9323 - accuracy: 0.60 - ETA: 4s - loss: 0.9169 - accuracy: 0.60 - ETA: 4s - loss: 0.9008 - accuracy: 0.60 - ETA: 4s - loss: 0.8841 - accuracy: 0.60 - ETA: 4s - loss: 0.8698 - accuracy: 0.61 - ETA: 4s - loss: 0.8610 - accuracy: 0.60 - ETA: 4s - loss: 0.8528 - accuracy: 0.60 - ETA: 3s - loss: 0.8300 - accuracy: 0.61 - ETA: 3s - loss: 0.8230 - accuracy: 0.61 - ETA: 3s - loss: 0.8024 - accuracy: 0.62 - ETA: 3s - loss: 0.7940 - accuracy: 0.62 - ETA: 3s - loss: 0.7838 - accuracy: 0.63 - ETA: 3s - loss: 0.7700 - accuracy: 0.63 - ETA: 2s - loss: 0.7563 - accuracy: 0.64 - ETA: 2s - loss: 0.7522 - accuracy: 0.64 - ETA: 2s - loss: 0.7473 - accuracy: 0.64 - ETA: 2s - loss: 0.7404 - accuracy: 0.65 - ETA: 2s - loss: 0.7371 - accuracy: 0.65 - ETA: 2s - loss: 0.7326 - accuracy: 0.65 - ETA: 2s - loss: 0.7246 - accuracy: 0.66 - ETA: 2s - loss: 0.7184 - accuracy: 0.66 - ETA: 2s - loss: 0.7191 - accuracy: 0.66 - ETA: 1s - loss: 0.7169 - accuracy: 0.66 - ETA: 1s - loss: 0.7136 - accuracy: 0.66 - ETA: 1s - loss: 0.7114 - accuracy: 0.66 - ETA: 1s - loss: 0.7079 - accuracy: 0.66 - ETA: 1s - loss: 0.7049 - accuracy: 0.66 - ETA: 1s - loss: 0.7020 - accuracy: 0.67 - ETA: 1s - loss: 0.6997 - accuracy: 0.67 - ETA: 1s - loss: 0.6952 - accuracy: 0.67 - ETA: 1s - loss: 0.6939 - accuracy: 0.67 - ETA: 1s - loss: 0.6905 - accuracy: 0.67 - ETA: 1s - loss: 0.6886 - accuracy: 0.67 - ETA: 1s - loss: 0.6864 - accuracy: 0.68 - ETA: 1s - loss: 0.6842 - accuracy: 0.68 - ETA: 1s - loss: 0.6818 - accuracy: 0.68 - ETA: 0s - loss: 0.6783 - accuracy: 0.68 - ETA: 0s - loss: 0.6753 - accuracy: 0.68 - ETA: 0s - loss: 0.6706 - accuracy: 0.68 - ETA: 0s - loss: 0.6692 - accuracy: 0.68 - ETA: 0s - loss: 0.6678 - accuracy: 0.68 - ETA: 0s - loss: 0.6661 - accuracy: 0.69 - ETA: 0s - loss: 0.6641 - accuracy: 0.69 - ETA: 0s - loss: 0.6615 - accuracy: 0.69 - ETA: 0s - loss: 0.6591 - accuracy: 0.69 - ETA: 0s - loss: 0.6565 - accuracy: 0.69 - ETA: 0s - loss: 0.6564 - accuracy: 0.69 - ETA: 0s - loss: 0.6554 - accuracy: 0.69 - ETA: 0s - loss: 0.6546 - accuracy: 0.69 - ETA: 0s - loss: 0.6525 - accuracy: 0.69 - 5s 434us/step - loss: 0.6525 - accuracy: 0.6973 - val_loss: 0.5129 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31344\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5450 - accuracy: 0.73 - ETA: 3s - loss: 0.5525 - accuracy: 0.73 - ETA: 3s - loss: 0.5336 - accuracy: 0.75 - ETA: 3s - loss: 0.5415 - accuracy: 0.74 - ETA: 3s - loss: 0.5487 - accuracy: 0.74 - ETA: 3s - loss: 0.5619 - accuracy: 0.73 - ETA: 3s - loss: 0.5585 - accuracy: 0.73 - ETA: 3s - loss: 0.5594 - accuracy: 0.73 - ETA: 3s - loss: 0.5590 - accuracy: 0.73 - ETA: 3s - loss: 0.5579 - accuracy: 0.73 - ETA: 3s - loss: 0.5526 - accuracy: 0.73 - ETA: 3s - loss: 0.5489 - accuracy: 0.74 - ETA: 3s - loss: 0.5491 - accuracy: 0.74 - ETA: 2s - loss: 0.5448 - accuracy: 0.74 - ETA: 2s - loss: 0.5460 - accuracy: 0.74 - ETA: 2s - loss: 0.5435 - accuracy: 0.74 - ETA: 2s - loss: 0.5418 - accuracy: 0.74 - ETA: 2s - loss: 0.5373 - accuracy: 0.74 - ETA: 2s - loss: 0.5389 - accuracy: 0.74 - ETA: 2s - loss: 0.5434 - accuracy: 0.74 - ETA: 2s - loss: 0.5417 - accuracy: 0.74 - ETA: 2s - loss: 0.5374 - accuracy: 0.74 - ETA: 2s - loss: 0.5400 - accuracy: 0.74 - ETA: 2s - loss: 0.5404 - accuracy: 0.74 - ETA: 2s - loss: 0.5392 - accuracy: 0.74 - ETA: 2s - loss: 0.5381 - accuracy: 0.74 - ETA: 2s - loss: 0.5360 - accuracy: 0.75 - ETA: 2s - loss: 0.5361 - accuracy: 0.75 - ETA: 2s - loss: 0.5346 - accuracy: 0.75 - ETA: 2s - loss: 0.5341 - accuracy: 0.75 - ETA: 1s - loss: 0.5341 - accuracy: 0.75 - ETA: 1s - loss: 0.5333 - accuracy: 0.75 - ETA: 1s - loss: 0.5317 - accuracy: 0.75 - ETA: 1s - loss: 0.5324 - accuracy: 0.75 - ETA: 1s - loss: 0.5312 - accuracy: 0.75 - ETA: 1s - loss: 0.5297 - accuracy: 0.75 - ETA: 1s - loss: 0.5287 - accuracy: 0.75 - ETA: 1s - loss: 0.5290 - accuracy: 0.75 - ETA: 1s - loss: 0.5291 - accuracy: 0.75 - ETA: 1s - loss: 0.5279 - accuracy: 0.75 - ETA: 1s - loss: 0.5267 - accuracy: 0.75 - ETA: 1s - loss: 0.5270 - accuracy: 0.75 - ETA: 1s - loss: 0.5281 - accuracy: 0.75 - ETA: 1s - loss: 0.5276 - accuracy: 0.75 - ETA: 1s - loss: 0.5286 - accuracy: 0.75 - ETA: 1s - loss: 0.5286 - accuracy: 0.75 - ETA: 1s - loss: 0.5284 - accuracy: 0.75 - ETA: 1s - loss: 0.5274 - accuracy: 0.75 - ETA: 0s - loss: 0.5259 - accuracy: 0.75 - ETA: 0s - loss: 0.5255 - accuracy: 0.75 - ETA: 0s - loss: 0.5248 - accuracy: 0.75 - ETA: 0s - loss: 0.5235 - accuracy: 0.75 - ETA: 0s - loss: 0.5243 - accuracy: 0.75 - ETA: 0s - loss: 0.5238 - accuracy: 0.75 - ETA: 0s - loss: 0.5235 - accuracy: 0.75 - ETA: 0s - loss: 0.5231 - accuracy: 0.75 - ETA: 0s - loss: 0.5225 - accuracy: 0.75 - ETA: 0s - loss: 0.5225 - accuracy: 0.75 - ETA: 0s - loss: 0.5232 - accuracy: 0.75 - ETA: 0s - loss: 0.5228 - accuracy: 0.75 - ETA: 0s - loss: 0.5214 - accuracy: 0.75 - ETA: 0s - loss: 0.5205 - accuracy: 0.75 - ETA: 0s - loss: 0.5213 - accuracy: 0.75 - ETA: 0s - loss: 0.5202 - accuracy: 0.75 - 4s 371us/step - loss: 0.5196 - accuracy: 0.7597 - val_loss: 0.4793 - val_accuracy: 0.7902\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31344\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5712 - accuracy: 0.72 - ETA: 3s - loss: 0.5889 - accuracy: 0.72 - ETA: 3s - loss: 0.5680 - accuracy: 0.74 - ETA: 3s - loss: 0.5464 - accuracy: 0.74 - ETA: 3s - loss: 0.5269 - accuracy: 0.75 - ETA: 3s - loss: 0.5213 - accuracy: 0.75 - ETA: 3s - loss: 0.5216 - accuracy: 0.75 - ETA: 3s - loss: 0.5136 - accuracy: 0.76 - ETA: 3s - loss: 0.5138 - accuracy: 0.76 - ETA: 3s - loss: 0.5156 - accuracy: 0.76 - ETA: 3s - loss: 0.5114 - accuracy: 0.76 - ETA: 3s - loss: 0.5120 - accuracy: 0.76 - ETA: 3s - loss: 0.5090 - accuracy: 0.76 - ETA: 3s - loss: 0.5107 - accuracy: 0.76 - ETA: 3s - loss: 0.5068 - accuracy: 0.76 - ETA: 2s - loss: 0.5052 - accuracy: 0.76 - ETA: 2s - loss: 0.5049 - accuracy: 0.76 - ETA: 2s - loss: 0.5017 - accuracy: 0.76 - ETA: 2s - loss: 0.5021 - accuracy: 0.76 - ETA: 2s - loss: 0.5024 - accuracy: 0.76 - ETA: 2s - loss: 0.5026 - accuracy: 0.76 - ETA: 2s - loss: 0.5024 - accuracy: 0.76 - ETA: 2s - loss: 0.5024 - accuracy: 0.76 - ETA: 2s - loss: 0.4984 - accuracy: 0.76 - ETA: 2s - loss: 0.4960 - accuracy: 0.77 - ETA: 2s - loss: 0.4971 - accuracy: 0.77 - ETA: 2s - loss: 0.4983 - accuracy: 0.77 - ETA: 2s - loss: 0.5015 - accuracy: 0.77 - ETA: 2s - loss: 0.5004 - accuracy: 0.77 - ETA: 2s - loss: 0.4993 - accuracy: 0.77 - ETA: 2s - loss: 0.4988 - accuracy: 0.77 - ETA: 2s - loss: 0.4988 - accuracy: 0.77 - ETA: 1s - loss: 0.4970 - accuracy: 0.77 - ETA: 1s - loss: 0.4958 - accuracy: 0.77 - ETA: 1s - loss: 0.4948 - accuracy: 0.77 - ETA: 1s - loss: 0.4945 - accuracy: 0.77 - ETA: 1s - loss: 0.4958 - accuracy: 0.77 - ETA: 1s - loss: 0.4950 - accuracy: 0.77 - ETA: 1s - loss: 0.4953 - accuracy: 0.77 - ETA: 1s - loss: 0.4948 - accuracy: 0.77 - ETA: 1s - loss: 0.4946 - accuracy: 0.77 - ETA: 1s - loss: 0.4944 - accuracy: 0.77 - ETA: 1s - loss: 0.4943 - accuracy: 0.77 - ETA: 1s - loss: 0.4924 - accuracy: 0.77 - ETA: 1s - loss: 0.4925 - accuracy: 0.77 - ETA: 1s - loss: 0.4920 - accuracy: 0.77 - ETA: 1s - loss: 0.4908 - accuracy: 0.77 - ETA: 1s - loss: 0.4901 - accuracy: 0.77 - ETA: 1s - loss: 0.4893 - accuracy: 0.77 - ETA: 1s - loss: 0.4898 - accuracy: 0.77 - ETA: 1s - loss: 0.4886 - accuracy: 0.77 - ETA: 0s - loss: 0.4891 - accuracy: 0.77 - ETA: 0s - loss: 0.4887 - accuracy: 0.77 - ETA: 0s - loss: 0.4900 - accuracy: 0.77 - ETA: 0s - loss: 0.4916 - accuracy: 0.77 - ETA: 0s - loss: 0.4924 - accuracy: 0.77 - ETA: 0s - loss: 0.4920 - accuracy: 0.77 - ETA: 0s - loss: 0.4927 - accuracy: 0.77 - ETA: 0s - loss: 0.4921 - accuracy: 0.77 - ETA: 0s - loss: 0.4922 - accuracy: 0.77 - ETA: 0s - loss: 0.4933 - accuracy: 0.77 - ETA: 0s - loss: 0.4928 - accuracy: 0.77 - ETA: 0s - loss: 0.4924 - accuracy: 0.77 - ETA: 0s - loss: 0.4918 - accuracy: 0.77 - ETA: 0s - loss: 0.4924 - accuracy: 0.77 - ETA: 0s - loss: 0.4925 - accuracy: 0.77 - ETA: 0s - loss: 0.4925 - accuracy: 0.77 - ETA: 0s - loss: 0.4919 - accuracy: 0.77 - 4s 404us/step - loss: 0.4921 - accuracy: 0.7744 - val_loss: 0.4356 - val_accuracy: 0.8088\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31344\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4865 - accuracy: 0.78 - ETA: 3s - loss: 0.4655 - accuracy: 0.78 - ETA: 3s - loss: 0.4545 - accuracy: 0.80 - ETA: 3s - loss: 0.4451 - accuracy: 0.81 - ETA: 2s - loss: 0.4542 - accuracy: 0.80 - ETA: 2s - loss: 0.4401 - accuracy: 0.80 - ETA: 2s - loss: 0.4468 - accuracy: 0.80 - ETA: 2s - loss: 0.4519 - accuracy: 0.80 - ETA: 2s - loss: 0.4564 - accuracy: 0.79 - ETA: 2s - loss: 0.4522 - accuracy: 0.79 - ETA: 2s - loss: 0.4570 - accuracy: 0.79 - ETA: 2s - loss: 0.4580 - accuracy: 0.79 - ETA: 2s - loss: 0.4616 - accuracy: 0.79 - ETA: 2s - loss: 0.4638 - accuracy: 0.79 - ETA: 2s - loss: 0.4682 - accuracy: 0.78 - ETA: 2s - loss: 0.4688 - accuracy: 0.78 - ETA: 2s - loss: 0.4704 - accuracy: 0.78 - ETA: 2s - loss: 0.4737 - accuracy: 0.78 - ETA: 1s - loss: 0.4748 - accuracy: 0.78 - ETA: 1s - loss: 0.4754 - accuracy: 0.78 - ETA: 1s - loss: 0.4735 - accuracy: 0.78 - ETA: 1s - loss: 0.4760 - accuracy: 0.78 - ETA: 1s - loss: 0.4778 - accuracy: 0.78 - ETA: 1s - loss: 0.4772 - accuracy: 0.78 - ETA: 1s - loss: 0.4765 - accuracy: 0.78 - ETA: 1s - loss: 0.4769 - accuracy: 0.78 - ETA: 1s - loss: 0.4772 - accuracy: 0.78 - ETA: 1s - loss: 0.4784 - accuracy: 0.78 - ETA: 1s - loss: 0.4778 - accuracy: 0.77 - ETA: 1s - loss: 0.4763 - accuracy: 0.78 - ETA: 1s - loss: 0.4751 - accuracy: 0.78 - ETA: 0s - loss: 0.4755 - accuracy: 0.78 - ETA: 0s - loss: 0.4761 - accuracy: 0.78 - ETA: 0s - loss: 0.4765 - accuracy: 0.78 - ETA: 0s - loss: 0.4757 - accuracy: 0.78 - ETA: 0s - loss: 0.4760 - accuracy: 0.78 - ETA: 0s - loss: 0.4761 - accuracy: 0.78 - ETA: 0s - loss: 0.4762 - accuracy: 0.78 - ETA: 0s - loss: 0.4755 - accuracy: 0.78 - ETA: 0s - loss: 0.4748 - accuracy: 0.78 - ETA: 0s - loss: 0.4747 - accuracy: 0.78 - ETA: 0s - loss: 0.4757 - accuracy: 0.78 - ETA: 0s - loss: 0.4745 - accuracy: 0.78 - ETA: 0s - loss: 0.4735 - accuracy: 0.78 - 3s 325us/step - loss: 0.4730 - accuracy: 0.7839 - val_loss: 0.4474 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31344\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4911 - accuracy: 0.76 - ETA: 2s - loss: 0.4437 - accuracy: 0.78 - ETA: 2s - loss: 0.4457 - accuracy: 0.80 - ETA: 2s - loss: 0.4559 - accuracy: 0.79 - ETA: 2s - loss: 0.4666 - accuracy: 0.78 - ETA: 2s - loss: 0.4802 - accuracy: 0.77 - ETA: 2s - loss: 0.4846 - accuracy: 0.76 - ETA: 2s - loss: 0.4928 - accuracy: 0.76 - ETA: 2s - loss: 0.4779 - accuracy: 0.77 - ETA: 2s - loss: 0.4688 - accuracy: 0.77 - ETA: 2s - loss: 0.4585 - accuracy: 0.78 - ETA: 2s - loss: 0.4612 - accuracy: 0.78 - ETA: 2s - loss: 0.4629 - accuracy: 0.78 - ETA: 2s - loss: 0.4594 - accuracy: 0.78 - ETA: 2s - loss: 0.4629 - accuracy: 0.78 - ETA: 1s - loss: 0.4656 - accuracy: 0.78 - ETA: 1s - loss: 0.4667 - accuracy: 0.78 - ETA: 1s - loss: 0.4694 - accuracy: 0.78 - ETA: 1s - loss: 0.4655 - accuracy: 0.78 - ETA: 1s - loss: 0.4645 - accuracy: 0.78 - ETA: 1s - loss: 0.4620 - accuracy: 0.78 - ETA: 1s - loss: 0.4644 - accuracy: 0.78 - ETA: 1s - loss: 0.4630 - accuracy: 0.78 - ETA: 1s - loss: 0.4649 - accuracy: 0.78 - ETA: 1s - loss: 0.4649 - accuracy: 0.78 - ETA: 1s - loss: 0.4653 - accuracy: 0.78 - ETA: 1s - loss: 0.4637 - accuracy: 0.78 - ETA: 1s - loss: 0.4616 - accuracy: 0.78 - ETA: 1s - loss: 0.4616 - accuracy: 0.78 - ETA: 0s - loss: 0.4643 - accuracy: 0.78 - ETA: 0s - loss: 0.4650 - accuracy: 0.78 - ETA: 0s - loss: 0.4646 - accuracy: 0.78 - ETA: 0s - loss: 0.4667 - accuracy: 0.78 - ETA: 0s - loss: 0.4665 - accuracy: 0.78 - ETA: 0s - loss: 0.4662 - accuracy: 0.78 - ETA: 0s - loss: 0.4648 - accuracy: 0.78 - ETA: 0s - loss: 0.4655 - accuracy: 0.78 - ETA: 0s - loss: 0.4645 - accuracy: 0.78 - ETA: 0s - loss: 0.4637 - accuracy: 0.78 - ETA: 0s - loss: 0.4631 - accuracy: 0.78 - ETA: 0s - loss: 0.4639 - accuracy: 0.78 - ETA: 0s - loss: 0.4625 - accuracy: 0.78 - ETA: 0s - loss: 0.4617 - accuracy: 0.79 - ETA: 0s - loss: 0.4613 - accuracy: 0.79 - ETA: 0s - loss: 0.4608 - accuracy: 0.79 - ETA: 0s - loss: 0.4614 - accuracy: 0.79 - 4s 331us/step - loss: 0.4608 - accuracy: 0.7905 - val_loss: 0.4183 - val_accuracy: 0.8239\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31344\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4717 - accuracy: 0.74 - ETA: 3s - loss: 0.5122 - accuracy: 0.73 - ETA: 3s - loss: 0.4878 - accuracy: 0.75 - ETA: 3s - loss: 0.4644 - accuracy: 0.76 - ETA: 3s - loss: 0.4580 - accuracy: 0.76 - ETA: 3s - loss: 0.4652 - accuracy: 0.76 - ETA: 3s - loss: 0.4560 - accuracy: 0.78 - ETA: 3s - loss: 0.4587 - accuracy: 0.78 - ETA: 3s - loss: 0.4518 - accuracy: 0.78 - ETA: 3s - loss: 0.4442 - accuracy: 0.79 - ETA: 2s - loss: 0.4366 - accuracy: 0.79 - ETA: 2s - loss: 0.4404 - accuracy: 0.79 - ETA: 2s - loss: 0.4416 - accuracy: 0.79 - ETA: 2s - loss: 0.4421 - accuracy: 0.80 - ETA: 2s - loss: 0.4414 - accuracy: 0.80 - ETA: 2s - loss: 0.4412 - accuracy: 0.80 - ETA: 2s - loss: 0.4392 - accuracy: 0.80 - ETA: 2s - loss: 0.4386 - accuracy: 0.80 - ETA: 2s - loss: 0.4419 - accuracy: 0.80 - ETA: 1s - loss: 0.4479 - accuracy: 0.79 - ETA: 1s - loss: 0.4476 - accuracy: 0.79 - ETA: 1s - loss: 0.4463 - accuracy: 0.79 - ETA: 1s - loss: 0.4447 - accuracy: 0.79 - ETA: 1s - loss: 0.4453 - accuracy: 0.79 - ETA: 1s - loss: 0.4456 - accuracy: 0.79 - ETA: 1s - loss: 0.4472 - accuracy: 0.79 - ETA: 1s - loss: 0.4484 - accuracy: 0.79 - ETA: 1s - loss: 0.4494 - accuracy: 0.79 - ETA: 1s - loss: 0.4482 - accuracy: 0.79 - ETA: 1s - loss: 0.4481 - accuracy: 0.79 - ETA: 0s - loss: 0.4461 - accuracy: 0.79 - ETA: 0s - loss: 0.4444 - accuracy: 0.79 - ETA: 0s - loss: 0.4442 - accuracy: 0.79 - ETA: 0s - loss: 0.4456 - accuracy: 0.79 - ETA: 0s - loss: 0.4459 - accuracy: 0.79 - ETA: 0s - loss: 0.4458 - accuracy: 0.79 - ETA: 0s - loss: 0.4452 - accuracy: 0.79 - ETA: 0s - loss: 0.4442 - accuracy: 0.79 - ETA: 0s - loss: 0.4432 - accuracy: 0.79 - ETA: 0s - loss: 0.4423 - accuracy: 0.79 - ETA: 0s - loss: 0.4431 - accuracy: 0.79 - ETA: 0s - loss: 0.4442 - accuracy: 0.79 - 3s 317us/step - loss: 0.4447 - accuracy: 0.7977 - val_loss: 0.3900 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31344\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4728 - accuracy: 0.75 - ETA: 2s - loss: 0.4524 - accuracy: 0.78 - ETA: 3s - loss: 0.4472 - accuracy: 0.79 - ETA: 3s - loss: 0.4493 - accuracy: 0.78 - ETA: 3s - loss: 0.4225 - accuracy: 0.80 - ETA: 2s - loss: 0.4190 - accuracy: 0.80 - ETA: 2s - loss: 0.4205 - accuracy: 0.81 - ETA: 2s - loss: 0.4324 - accuracy: 0.80 - ETA: 2s - loss: 0.4325 - accuracy: 0.80 - ETA: 2s - loss: 0.4324 - accuracy: 0.80 - ETA: 2s - loss: 0.4361 - accuracy: 0.80 - ETA: 2s - loss: 0.4382 - accuracy: 0.80 - ETA: 2s - loss: 0.4419 - accuracy: 0.79 - ETA: 2s - loss: 0.4423 - accuracy: 0.80 - ETA: 1s - loss: 0.4423 - accuracy: 0.80 - ETA: 1s - loss: 0.4408 - accuracy: 0.80 - ETA: 1s - loss: 0.4417 - accuracy: 0.79 - ETA: 1s - loss: 0.4394 - accuracy: 0.80 - ETA: 1s - loss: 0.4379 - accuracy: 0.80 - ETA: 1s - loss: 0.4362 - accuracy: 0.80 - ETA: 1s - loss: 0.4354 - accuracy: 0.80 - ETA: 1s - loss: 0.4332 - accuracy: 0.80 - ETA: 1s - loss: 0.4349 - accuracy: 0.80 - ETA: 1s - loss: 0.4379 - accuracy: 0.80 - ETA: 1s - loss: 0.4365 - accuracy: 0.80 - ETA: 1s - loss: 0.4358 - accuracy: 0.80 - ETA: 1s - loss: 0.4375 - accuracy: 0.79 - ETA: 0s - loss: 0.4360 - accuracy: 0.79 - ETA: 0s - loss: 0.4346 - accuracy: 0.80 - ETA: 0s - loss: 0.4364 - accuracy: 0.79 - ETA: 0s - loss: 0.4361 - accuracy: 0.79 - ETA: 0s - loss: 0.4357 - accuracy: 0.79 - ETA: 0s - loss: 0.4338 - accuracy: 0.79 - ETA: 0s - loss: 0.4340 - accuracy: 0.79 - ETA: 0s - loss: 0.4337 - accuracy: 0.80 - ETA: 0s - loss: 0.4343 - accuracy: 0.79 - ETA: 0s - loss: 0.4346 - accuracy: 0.79 - ETA: 0s - loss: 0.4333 - accuracy: 0.79 - ETA: 0s - loss: 0.4335 - accuracy: 0.79 - 3s 308us/step - loss: 0.4324 - accuracy: 0.7999 - val_loss: 0.3894 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31344\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4382 - accuracy: 0.82 - ETA: 2s - loss: 0.4338 - accuracy: 0.80 - ETA: 2s - loss: 0.4226 - accuracy: 0.81 - ETA: 2s - loss: 0.4286 - accuracy: 0.80 - ETA: 2s - loss: 0.4333 - accuracy: 0.80 - ETA: 2s - loss: 0.4228 - accuracy: 0.80 - ETA: 2s - loss: 0.4329 - accuracy: 0.80 - ETA: 2s - loss: 0.4372 - accuracy: 0.79 - ETA: 2s - loss: 0.4326 - accuracy: 0.80 - ETA: 2s - loss: 0.4314 - accuracy: 0.80 - ETA: 2s - loss: 0.4366 - accuracy: 0.79 - ETA: 2s - loss: 0.4362 - accuracy: 0.79 - ETA: 2s - loss: 0.4389 - accuracy: 0.79 - ETA: 2s - loss: 0.4378 - accuracy: 0.79 - ETA: 1s - loss: 0.4352 - accuracy: 0.79 - ETA: 1s - loss: 0.4379 - accuracy: 0.79 - ETA: 1s - loss: 0.4339 - accuracy: 0.79 - ETA: 1s - loss: 0.4361 - accuracy: 0.79 - ETA: 1s - loss: 0.4350 - accuracy: 0.79 - ETA: 1s - loss: 0.4325 - accuracy: 0.79 - ETA: 1s - loss: 0.4295 - accuracy: 0.80 - ETA: 1s - loss: 0.4310 - accuracy: 0.79 - ETA: 1s - loss: 0.4284 - accuracy: 0.80 - ETA: 1s - loss: 0.4286 - accuracy: 0.80 - ETA: 1s - loss: 0.4294 - accuracy: 0.80 - ETA: 1s - loss: 0.4307 - accuracy: 0.79 - ETA: 1s - loss: 0.4287 - accuracy: 0.80 - ETA: 0s - loss: 0.4273 - accuracy: 0.80 - ETA: 0s - loss: 0.4271 - accuracy: 0.80 - ETA: 0s - loss: 0.4273 - accuracy: 0.80 - ETA: 0s - loss: 0.4255 - accuracy: 0.80 - ETA: 0s - loss: 0.4252 - accuracy: 0.80 - ETA: 0s - loss: 0.4241 - accuracy: 0.80 - ETA: 0s - loss: 0.4226 - accuracy: 0.80 - ETA: 0s - loss: 0.4236 - accuracy: 0.80 - ETA: 0s - loss: 0.4225 - accuracy: 0.80 - ETA: 0s - loss: 0.4233 - accuracy: 0.80 - ETA: 0s - loss: 0.4249 - accuracy: 0.80 - ETA: 0s - loss: 0.4246 - accuracy: 0.80 - ETA: 0s - loss: 0.4252 - accuracy: 0.80 - 3s 309us/step - loss: 0.4252 - accuracy: 0.8035 - val_loss: 0.3860 - val_accuracy: 0.8222\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31344\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3508 - accuracy: 0.84 - ETA: 2s - loss: 0.4053 - accuracy: 0.81 - ETA: 2s - loss: 0.4053 - accuracy: 0.81 - ETA: 2s - loss: 0.3976 - accuracy: 0.82 - ETA: 2s - loss: 0.4024 - accuracy: 0.81 - ETA: 3s - loss: 0.4061 - accuracy: 0.81 - ETA: 2s - loss: 0.4123 - accuracy: 0.80 - ETA: 2s - loss: 0.4089 - accuracy: 0.81 - ETA: 2s - loss: 0.4238 - accuracy: 0.80 - ETA: 2s - loss: 0.4214 - accuracy: 0.80 - ETA: 2s - loss: 0.4175 - accuracy: 0.80 - ETA: 2s - loss: 0.4114 - accuracy: 0.80 - ETA: 2s - loss: 0.4099 - accuracy: 0.80 - ETA: 2s - loss: 0.4100 - accuracy: 0.80 - ETA: 2s - loss: 0.4099 - accuracy: 0.80 - ETA: 1s - loss: 0.4091 - accuracy: 0.81 - ETA: 1s - loss: 0.4145 - accuracy: 0.80 - ETA: 1s - loss: 0.4123 - accuracy: 0.81 - ETA: 1s - loss: 0.4111 - accuracy: 0.81 - ETA: 1s - loss: 0.4117 - accuracy: 0.81 - ETA: 1s - loss: 0.4126 - accuracy: 0.81 - ETA: 1s - loss: 0.4147 - accuracy: 0.80 - ETA: 1s - loss: 0.4151 - accuracy: 0.80 - ETA: 1s - loss: 0.4158 - accuracy: 0.80 - ETA: 1s - loss: 0.4160 - accuracy: 0.80 - ETA: 1s - loss: 0.4153 - accuracy: 0.80 - ETA: 1s - loss: 0.4174 - accuracy: 0.80 - ETA: 1s - loss: 0.4198 - accuracy: 0.80 - ETA: 0s - loss: 0.4194 - accuracy: 0.80 - ETA: 0s - loss: 0.4193 - accuracy: 0.80 - ETA: 0s - loss: 0.4161 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4157 - accuracy: 0.81 - ETA: 0s - loss: 0.4158 - accuracy: 0.81 - ETA: 0s - loss: 0.4187 - accuracy: 0.81 - ETA: 0s - loss: 0.4193 - accuracy: 0.80 - ETA: 0s - loss: 0.4186 - accuracy: 0.80 - ETA: 0s - loss: 0.4187 - accuracy: 0.80 - ETA: 0s - loss: 0.4179 - accuracy: 0.80 - 3s 310us/step - loss: 0.4181 - accuracy: 0.8086 - val_loss: 0.3950 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31344\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3815 - accuracy: 0.82 - ETA: 2s - loss: 0.4202 - accuracy: 0.80 - ETA: 2s - loss: 0.4314 - accuracy: 0.80 - ETA: 2s - loss: 0.4248 - accuracy: 0.80 - ETA: 2s - loss: 0.4291 - accuracy: 0.80 - ETA: 2s - loss: 0.4317 - accuracy: 0.79 - ETA: 2s - loss: 0.4299 - accuracy: 0.79 - ETA: 2s - loss: 0.4274 - accuracy: 0.79 - ETA: 2s - loss: 0.4303 - accuracy: 0.79 - ETA: 2s - loss: 0.4259 - accuracy: 0.80 - ETA: 2s - loss: 0.4224 - accuracy: 0.80 - ETA: 2s - loss: 0.4176 - accuracy: 0.81 - ETA: 2s - loss: 0.4189 - accuracy: 0.80 - ETA: 2s - loss: 0.4158 - accuracy: 0.81 - ETA: 2s - loss: 0.4161 - accuracy: 0.80 - ETA: 1s - loss: 0.4152 - accuracy: 0.80 - ETA: 1s - loss: 0.4128 - accuracy: 0.81 - ETA: 1s - loss: 0.4160 - accuracy: 0.80 - ETA: 1s - loss: 0.4167 - accuracy: 0.80 - ETA: 1s - loss: 0.4149 - accuracy: 0.80 - ETA: 1s - loss: 0.4154 - accuracy: 0.80 - ETA: 1s - loss: 0.4171 - accuracy: 0.80 - ETA: 1s - loss: 0.4179 - accuracy: 0.80 - ETA: 1s - loss: 0.4189 - accuracy: 0.80 - ETA: 1s - loss: 0.4185 - accuracy: 0.80 - ETA: 1s - loss: 0.4205 - accuracy: 0.80 - ETA: 1s - loss: 0.4210 - accuracy: 0.80 - ETA: 1s - loss: 0.4167 - accuracy: 0.80 - ETA: 0s - loss: 0.4165 - accuracy: 0.80 - ETA: 0s - loss: 0.4139 - accuracy: 0.80 - ETA: 0s - loss: 0.4124 - accuracy: 0.81 - ETA: 0s - loss: 0.4112 - accuracy: 0.81 - ETA: 0s - loss: 0.4110 - accuracy: 0.81 - ETA: 0s - loss: 0.4103 - accuracy: 0.81 - ETA: 0s - loss: 0.4111 - accuracy: 0.81 - ETA: 0s - loss: 0.4109 - accuracy: 0.81 - ETA: 0s - loss: 0.4102 - accuracy: 0.80 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4103 - accuracy: 0.80 - ETA: 0s - loss: 0.4104 - accuracy: 0.80 - ETA: 0s - loss: 0.4107 - accuracy: 0.80 - ETA: 0s - loss: 0.4095 - accuracy: 0.81 - 3s 312us/step - loss: 0.4100 - accuracy: 0.8096 - val_loss: 0.3909 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31344\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4815 - accuracy: 0.76 - ETA: 2s - loss: 0.4644 - accuracy: 0.77 - ETA: 2s - loss: 0.4532 - accuracy: 0.78 - ETA: 2s - loss: 0.4565 - accuracy: 0.78 - ETA: 2s - loss: 0.4537 - accuracy: 0.78 - ETA: 2s - loss: 0.4434 - accuracy: 0.79 - ETA: 3s - loss: 0.4384 - accuracy: 0.79 - ETA: 2s - loss: 0.4373 - accuracy: 0.79 - ETA: 2s - loss: 0.4353 - accuracy: 0.79 - ETA: 2s - loss: 0.4275 - accuracy: 0.79 - ETA: 2s - loss: 0.4267 - accuracy: 0.79 - ETA: 2s - loss: 0.4274 - accuracy: 0.79 - ETA: 2s - loss: 0.4298 - accuracy: 0.79 - ETA: 2s - loss: 0.4292 - accuracy: 0.79 - ETA: 2s - loss: 0.4289 - accuracy: 0.79 - ETA: 2s - loss: 0.4254 - accuracy: 0.80 - ETA: 1s - loss: 0.4265 - accuracy: 0.79 - ETA: 1s - loss: 0.4212 - accuracy: 0.80 - ETA: 1s - loss: 0.4181 - accuracy: 0.80 - ETA: 1s - loss: 0.4178 - accuracy: 0.80 - ETA: 1s - loss: 0.4157 - accuracy: 0.80 - ETA: 1s - loss: 0.4126 - accuracy: 0.80 - ETA: 1s - loss: 0.4131 - accuracy: 0.80 - ETA: 1s - loss: 0.4116 - accuracy: 0.80 - ETA: 1s - loss: 0.4102 - accuracy: 0.81 - ETA: 1s - loss: 0.4090 - accuracy: 0.81 - ETA: 1s - loss: 0.4106 - accuracy: 0.80 - ETA: 1s - loss: 0.4101 - accuracy: 0.80 - ETA: 1s - loss: 0.4099 - accuracy: 0.80 - ETA: 1s - loss: 0.4088 - accuracy: 0.81 - ETA: 1s - loss: 0.4108 - accuracy: 0.81 - ETA: 1s - loss: 0.4098 - accuracy: 0.81 - ETA: 1s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4075 - accuracy: 0.81 - ETA: 0s - loss: 0.4068 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - ETA: 0s - loss: 0.4044 - accuracy: 0.81 - ETA: 0s - loss: 0.4024 - accuracy: 0.81 - ETA: 0s - loss: 0.4021 - accuracy: 0.81 - ETA: 0s - loss: 0.4023 - accuracy: 0.81 - ETA: 0s - loss: 0.4012 - accuracy: 0.81 - ETA: 0s - loss: 0.4045 - accuracy: 0.81 - ETA: 0s - loss: 0.4036 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.81 - ETA: 0s - loss: 0.4011 - accuracy: 0.81 - ETA: 0s - loss: 0.4024 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - 4s 349us/step - loss: 0.4019 - accuracy: 0.8164 - val_loss: 0.3626 - val_accuracy: 0.8425\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31344\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3982 - accuracy: 0.82 - ETA: 3s - loss: 0.3597 - accuracy: 0.83 - ETA: 3s - loss: 0.4062 - accuracy: 0.81 - ETA: 3s - loss: 0.4016 - accuracy: 0.81 - ETA: 3s - loss: 0.3960 - accuracy: 0.82 - ETA: 3s - loss: 0.4000 - accuracy: 0.81 - ETA: 3s - loss: 0.4028 - accuracy: 0.81 - ETA: 3s - loss: 0.3912 - accuracy: 0.82 - ETA: 3s - loss: 0.3916 - accuracy: 0.82 - ETA: 3s - loss: 0.3877 - accuracy: 0.82 - ETA: 3s - loss: 0.3856 - accuracy: 0.82 - ETA: 3s - loss: 0.3815 - accuracy: 0.82 - ETA: 3s - loss: 0.3792 - accuracy: 0.82 - ETA: 3s - loss: 0.3766 - accuracy: 0.83 - ETA: 3s - loss: 0.3768 - accuracy: 0.82 - ETA: 3s - loss: 0.3818 - accuracy: 0.82 - ETA: 3s - loss: 0.3817 - accuracy: 0.82 - ETA: 3s - loss: 0.3819 - accuracy: 0.82 - ETA: 3s - loss: 0.3828 - accuracy: 0.82 - ETA: 3s - loss: 0.3850 - accuracy: 0.82 - ETA: 2s - loss: 0.3875 - accuracy: 0.82 - ETA: 2s - loss: 0.3904 - accuracy: 0.82 - ETA: 2s - loss: 0.3876 - accuracy: 0.82 - ETA: 2s - loss: 0.3890 - accuracy: 0.82 - ETA: 2s - loss: 0.3913 - accuracy: 0.82 - ETA: 2s - loss: 0.3896 - accuracy: 0.82 - ETA: 2s - loss: 0.3899 - accuracy: 0.82 - ETA: 2s - loss: 0.3892 - accuracy: 0.81 - ETA: 2s - loss: 0.3910 - accuracy: 0.81 - ETA: 1s - loss: 0.3893 - accuracy: 0.81 - ETA: 1s - loss: 0.3896 - accuracy: 0.81 - ETA: 1s - loss: 0.3882 - accuracy: 0.81 - ETA: 1s - loss: 0.3916 - accuracy: 0.81 - ETA: 1s - loss: 0.3919 - accuracy: 0.81 - ETA: 1s - loss: 0.3935 - accuracy: 0.81 - ETA: 1s - loss: 0.3934 - accuracy: 0.81 - ETA: 1s - loss: 0.3936 - accuracy: 0.81 - ETA: 1s - loss: 0.3940 - accuracy: 0.81 - ETA: 1s - loss: 0.3923 - accuracy: 0.81 - ETA: 1s - loss: 0.3937 - accuracy: 0.81 - ETA: 1s - loss: 0.3933 - accuracy: 0.81 - ETA: 1s - loss: 0.3948 - accuracy: 0.81 - ETA: 0s - loss: 0.3944 - accuracy: 0.81 - ETA: 0s - loss: 0.3944 - accuracy: 0.81 - ETA: 0s - loss: 0.3939 - accuracy: 0.81 - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - ETA: 0s - loss: 0.3939 - accuracy: 0.81 - ETA: 0s - loss: 0.3933 - accuracy: 0.81 - ETA: 0s - loss: 0.3933 - accuracy: 0.81 - ETA: 0s - loss: 0.3937 - accuracy: 0.81 - ETA: 0s - loss: 0.3946 - accuracy: 0.81 - ETA: 0s - loss: 0.3946 - accuracy: 0.81 - ETA: 0s - loss: 0.3940 - accuracy: 0.81 - ETA: 0s - loss: 0.3945 - accuracy: 0.81 - ETA: 0s - loss: 0.3947 - accuracy: 0.81 - ETA: 0s - loss: 0.3953 - accuracy: 0.81 - ETA: 0s - loss: 0.3945 - accuracy: 0.81 - ETA: 0s - loss: 0.3956 - accuracy: 0.81 - ETA: 0s - loss: 0.3953 - accuracy: 0.81 - 4s 381us/step - loss: 0.3950 - accuracy: 0.8161 - val_loss: 0.3500 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31344\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3980 - accuracy: 0.82 - ETA: 3s - loss: 0.4387 - accuracy: 0.80 - ETA: 3s - loss: 0.4300 - accuracy: 0.80 - ETA: 3s - loss: 0.4288 - accuracy: 0.80 - ETA: 3s - loss: 0.4379 - accuracy: 0.80 - ETA: 3s - loss: 0.4283 - accuracy: 0.80 - ETA: 3s - loss: 0.4106 - accuracy: 0.81 - ETA: 3s - loss: 0.4096 - accuracy: 0.81 - ETA: 3s - loss: 0.4112 - accuracy: 0.80 - ETA: 3s - loss: 0.4102 - accuracy: 0.80 - ETA: 2s - loss: 0.4073 - accuracy: 0.81 - ETA: 2s - loss: 0.4073 - accuracy: 0.81 - ETA: 2s - loss: 0.4019 - accuracy: 0.81 - ETA: 2s - loss: 0.3991 - accuracy: 0.81 - ETA: 2s - loss: 0.3992 - accuracy: 0.81 - ETA: 2s - loss: 0.3970 - accuracy: 0.81 - ETA: 2s - loss: 0.3964 - accuracy: 0.81 - ETA: 2s - loss: 0.3962 - accuracy: 0.81 - ETA: 2s - loss: 0.3936 - accuracy: 0.81 - ETA: 2s - loss: 0.3923 - accuracy: 0.82 - ETA: 2s - loss: 0.3948 - accuracy: 0.81 - ETA: 2s - loss: 0.3956 - accuracy: 0.81 - ETA: 1s - loss: 0.3974 - accuracy: 0.81 - ETA: 1s - loss: 0.3981 - accuracy: 0.81 - ETA: 1s - loss: 0.3991 - accuracy: 0.81 - ETA: 1s - loss: 0.3977 - accuracy: 0.82 - ETA: 1s - loss: 0.3975 - accuracy: 0.81 - ETA: 1s - loss: 0.3965 - accuracy: 0.81 - ETA: 1s - loss: 0.3944 - accuracy: 0.82 - ETA: 1s - loss: 0.4001 - accuracy: 0.81 - ETA: 1s - loss: 0.3969 - accuracy: 0.82 - ETA: 1s - loss: 0.3965 - accuracy: 0.82 - ETA: 1s - loss: 0.3960 - accuracy: 0.82 - ETA: 1s - loss: 0.3966 - accuracy: 0.82 - ETA: 1s - loss: 0.3959 - accuracy: 0.82 - ETA: 1s - loss: 0.3949 - accuracy: 0.82 - ETA: 1s - loss: 0.3955 - accuracy: 0.82 - ETA: 1s - loss: 0.3953 - accuracy: 0.82 - ETA: 1s - loss: 0.3965 - accuracy: 0.82 - ETA: 1s - loss: 0.3961 - accuracy: 0.82 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3947 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3913 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3909 - accuracy: 0.82 - 4s 356us/step - loss: 0.3898 - accuracy: 0.8225 - val_loss: 0.3589 - val_accuracy: 0.8349\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31344\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4184 - accuracy: 0.80 - ETA: 3s - loss: 0.3672 - accuracy: 0.83 - ETA: 3s - loss: 0.3691 - accuracy: 0.84 - ETA: 3s - loss: 0.3571 - accuracy: 0.85 - ETA: 3s - loss: 0.3752 - accuracy: 0.83 - ETA: 3s - loss: 0.3688 - accuracy: 0.83 - ETA: 3s - loss: 0.3752 - accuracy: 0.83 - ETA: 3s - loss: 0.3759 - accuracy: 0.83 - ETA: 3s - loss: 0.3722 - accuracy: 0.83 - ETA: 3s - loss: 0.3740 - accuracy: 0.83 - ETA: 3s - loss: 0.3707 - accuracy: 0.84 - ETA: 3s - loss: 0.3741 - accuracy: 0.83 - ETA: 2s - loss: 0.3737 - accuracy: 0.83 - ETA: 2s - loss: 0.3740 - accuracy: 0.83 - ETA: 2s - loss: 0.3746 - accuracy: 0.83 - ETA: 2s - loss: 0.3761 - accuracy: 0.83 - ETA: 2s - loss: 0.3733 - accuracy: 0.83 - ETA: 2s - loss: 0.3771 - accuracy: 0.83 - ETA: 2s - loss: 0.3835 - accuracy: 0.83 - ETA: 2s - loss: 0.3873 - accuracy: 0.82 - ETA: 2s - loss: 0.3891 - accuracy: 0.82 - ETA: 2s - loss: 0.3884 - accuracy: 0.82 - ETA: 2s - loss: 0.3884 - accuracy: 0.82 - ETA: 2s - loss: 0.3905 - accuracy: 0.82 - ETA: 2s - loss: 0.3951 - accuracy: 0.82 - ETA: 2s - loss: 0.3935 - accuracy: 0.82 - ETA: 2s - loss: 0.3961 - accuracy: 0.82 - ETA: 1s - loss: 0.3953 - accuracy: 0.82 - ETA: 1s - loss: 0.3928 - accuracy: 0.82 - ETA: 1s - loss: 0.3903 - accuracy: 0.82 - ETA: 1s - loss: 0.3878 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3867 - accuracy: 0.82 - ETA: 1s - loss: 0.3855 - accuracy: 0.82 - ETA: 1s - loss: 0.3853 - accuracy: 0.82 - ETA: 1s - loss: 0.3840 - accuracy: 0.82 - ETA: 1s - loss: 0.3824 - accuracy: 0.82 - ETA: 1s - loss: 0.3827 - accuracy: 0.82 - ETA: 1s - loss: 0.3851 - accuracy: 0.82 - ETA: 1s - loss: 0.3871 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3863 - accuracy: 0.82 - ETA: 0s - loss: 0.3866 - accuracy: 0.82 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.3856 - accuracy: 0.82 - ETA: 0s - loss: 0.3862 - accuracy: 0.82 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3872 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - 4s 378us/step - loss: 0.3866 - accuracy: 0.8232 - val_loss: 0.3566 - val_accuracy: 0.8509\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31344\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3897 - accuracy: 0.82 - ETA: 2s - loss: 0.3700 - accuracy: 0.82 - ETA: 2s - loss: 0.3834 - accuracy: 0.81 - ETA: 2s - loss: 0.3921 - accuracy: 0.80 - ETA: 2s - loss: 0.3969 - accuracy: 0.81 - ETA: 2s - loss: 0.4010 - accuracy: 0.80 - ETA: 2s - loss: 0.3849 - accuracy: 0.81 - ETA: 2s - loss: 0.3859 - accuracy: 0.81 - ETA: 2s - loss: 0.3855 - accuracy: 0.81 - ETA: 2s - loss: 0.3895 - accuracy: 0.81 - ETA: 2s - loss: 0.3924 - accuracy: 0.80 - ETA: 2s - loss: 0.3916 - accuracy: 0.81 - ETA: 2s - loss: 0.3895 - accuracy: 0.81 - ETA: 2s - loss: 0.3893 - accuracy: 0.81 - ETA: 2s - loss: 0.3903 - accuracy: 0.81 - ETA: 1s - loss: 0.3851 - accuracy: 0.81 - ETA: 1s - loss: 0.3852 - accuracy: 0.81 - ETA: 1s - loss: 0.3829 - accuracy: 0.81 - ETA: 1s - loss: 0.3861 - accuracy: 0.81 - ETA: 1s - loss: 0.3843 - accuracy: 0.81 - ETA: 1s - loss: 0.3833 - accuracy: 0.81 - ETA: 1s - loss: 0.3844 - accuracy: 0.81 - ETA: 1s - loss: 0.3839 - accuracy: 0.81 - ETA: 1s - loss: 0.3849 - accuracy: 0.81 - ETA: 1s - loss: 0.3838 - accuracy: 0.81 - ETA: 1s - loss: 0.3829 - accuracy: 0.81 - ETA: 1s - loss: 0.3821 - accuracy: 0.82 - ETA: 1s - loss: 0.3821 - accuracy: 0.82 - ETA: 1s - loss: 0.3810 - accuracy: 0.82 - ETA: 1s - loss: 0.3800 - accuracy: 0.82 - ETA: 1s - loss: 0.3804 - accuracy: 0.82 - ETA: 1s - loss: 0.3805 - accuracy: 0.82 - ETA: 0s - loss: 0.3810 - accuracy: 0.82 - ETA: 0s - loss: 0.3793 - accuracy: 0.82 - ETA: 0s - loss: 0.3793 - accuracy: 0.82 - ETA: 0s - loss: 0.3792 - accuracy: 0.82 - ETA: 0s - loss: 0.3800 - accuracy: 0.82 - ETA: 0s - loss: 0.3805 - accuracy: 0.82 - ETA: 0s - loss: 0.3796 - accuracy: 0.82 - ETA: 0s - loss: 0.3800 - accuracy: 0.82 - ETA: 0s - loss: 0.3787 - accuracy: 0.82 - ETA: 0s - loss: 0.3778 - accuracy: 0.82 - ETA: 0s - loss: 0.3769 - accuracy: 0.82 - ETA: 0s - loss: 0.3809 - accuracy: 0.82 - 3s 321us/step - loss: 0.3814 - accuracy: 0.8234 - val_loss: 0.3738 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31344\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4931 - accuracy: 0.77 - ETA: 2s - loss: 0.4085 - accuracy: 0.81 - ETA: 2s - loss: 0.4148 - accuracy: 0.81 - ETA: 2s - loss: 0.4035 - accuracy: 0.81 - ETA: 2s - loss: 0.4033 - accuracy: 0.81 - ETA: 2s - loss: 0.4082 - accuracy: 0.80 - ETA: 2s - loss: 0.4012 - accuracy: 0.81 - ETA: 2s - loss: 0.3960 - accuracy: 0.81 - ETA: 2s - loss: 0.3991 - accuracy: 0.81 - ETA: 2s - loss: 0.3949 - accuracy: 0.81 - ETA: 2s - loss: 0.3918 - accuracy: 0.81 - ETA: 2s - loss: 0.3916 - accuracy: 0.81 - ETA: 2s - loss: 0.3893 - accuracy: 0.82 - ETA: 2s - loss: 0.3902 - accuracy: 0.81 - ETA: 2s - loss: 0.3914 - accuracy: 0.81 - ETA: 2s - loss: 0.3872 - accuracy: 0.81 - ETA: 1s - loss: 0.3840 - accuracy: 0.82 - ETA: 1s - loss: 0.3863 - accuracy: 0.82 - ETA: 1s - loss: 0.3842 - accuracy: 0.82 - ETA: 1s - loss: 0.3847 - accuracy: 0.82 - ETA: 1s - loss: 0.3845 - accuracy: 0.82 - ETA: 1s - loss: 0.3843 - accuracy: 0.82 - ETA: 1s - loss: 0.3831 - accuracy: 0.82 - ETA: 1s - loss: 0.3843 - accuracy: 0.82 - ETA: 1s - loss: 0.3858 - accuracy: 0.82 - ETA: 1s - loss: 0.3852 - accuracy: 0.82 - ETA: 1s - loss: 0.3847 - accuracy: 0.82 - ETA: 1s - loss: 0.3828 - accuracy: 0.82 - ETA: 1s - loss: 0.3822 - accuracy: 0.82 - ETA: 1s - loss: 0.3815 - accuracy: 0.82 - ETA: 1s - loss: 0.3808 - accuracy: 0.82 - ETA: 1s - loss: 0.3792 - accuracy: 0.82 - ETA: 1s - loss: 0.3770 - accuracy: 0.82 - ETA: 1s - loss: 0.3786 - accuracy: 0.82 - ETA: 0s - loss: 0.3801 - accuracy: 0.82 - ETA: 0s - loss: 0.3806 - accuracy: 0.82 - ETA: 0s - loss: 0.3819 - accuracy: 0.82 - ETA: 0s - loss: 0.3819 - accuracy: 0.82 - ETA: 0s - loss: 0.3817 - accuracy: 0.82 - ETA: 0s - loss: 0.3805 - accuracy: 0.82 - ETA: 0s - loss: 0.3799 - accuracy: 0.82 - ETA: 0s - loss: 0.3780 - accuracy: 0.82 - ETA: 0s - loss: 0.3794 - accuracy: 0.82 - ETA: 0s - loss: 0.3792 - accuracy: 0.82 - ETA: 0s - loss: 0.3776 - accuracy: 0.82 - ETA: 0s - loss: 0.3766 - accuracy: 0.82 - 4s 330us/step - loss: 0.3763 - accuracy: 0.8270 - val_loss: 0.3489 - val_accuracy: 0.8585\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31344\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3558 - accuracy: 0.82 - ETA: 2s - loss: 0.3596 - accuracy: 0.82 - ETA: 2s - loss: 0.3916 - accuracy: 0.81 - ETA: 2s - loss: 0.3825 - accuracy: 0.81 - ETA: 2s - loss: 0.3861 - accuracy: 0.81 - ETA: 3s - loss: 0.3789 - accuracy: 0.82 - ETA: 2s - loss: 0.3888 - accuracy: 0.81 - ETA: 2s - loss: 0.3867 - accuracy: 0.81 - ETA: 2s - loss: 0.3719 - accuracy: 0.82 - ETA: 2s - loss: 0.3740 - accuracy: 0.82 - ETA: 2s - loss: 0.3673 - accuracy: 0.83 - ETA: 2s - loss: 0.3674 - accuracy: 0.83 - ETA: 2s - loss: 0.3748 - accuracy: 0.82 - ETA: 2s - loss: 0.3731 - accuracy: 0.82 - ETA: 2s - loss: 0.3727 - accuracy: 0.82 - ETA: 2s - loss: 0.3744 - accuracy: 0.82 - ETA: 2s - loss: 0.3731 - accuracy: 0.83 - ETA: 2s - loss: 0.3755 - accuracy: 0.83 - ETA: 2s - loss: 0.3757 - accuracy: 0.83 - ETA: 1s - loss: 0.3738 - accuracy: 0.83 - ETA: 1s - loss: 0.3752 - accuracy: 0.83 - ETA: 1s - loss: 0.3738 - accuracy: 0.83 - ETA: 1s - loss: 0.3761 - accuracy: 0.83 - ETA: 1s - loss: 0.3755 - accuracy: 0.83 - ETA: 1s - loss: 0.3754 - accuracy: 0.83 - ETA: 1s - loss: 0.3754 - accuracy: 0.83 - ETA: 1s - loss: 0.3759 - accuracy: 0.83 - ETA: 1s - loss: 0.3771 - accuracy: 0.83 - ETA: 1s - loss: 0.3784 - accuracy: 0.83 - ETA: 1s - loss: 0.3769 - accuracy: 0.83 - ETA: 1s - loss: 0.3769 - accuracy: 0.83 - ETA: 1s - loss: 0.3765 - accuracy: 0.83 - ETA: 1s - loss: 0.3789 - accuracy: 0.82 - ETA: 1s - loss: 0.3783 - accuracy: 0.82 - ETA: 1s - loss: 0.3789 - accuracy: 0.82 - ETA: 0s - loss: 0.3781 - accuracy: 0.83 - ETA: 0s - loss: 0.3787 - accuracy: 0.83 - ETA: 0s - loss: 0.3784 - accuracy: 0.82 - ETA: 0s - loss: 0.3777 - accuracy: 0.83 - ETA: 0s - loss: 0.3779 - accuracy: 0.83 - ETA: 0s - loss: 0.3792 - accuracy: 0.82 - ETA: 0s - loss: 0.3786 - accuracy: 0.82 - ETA: 0s - loss: 0.3763 - accuracy: 0.83 - ETA: 0s - loss: 0.3750 - accuracy: 0.83 - ETA: 0s - loss: 0.3759 - accuracy: 0.83 - ETA: 0s - loss: 0.3755 - accuracy: 0.83 - ETA: 0s - loss: 0.3751 - accuracy: 0.83 - ETA: 0s - loss: 0.3755 - accuracy: 0.83 - 4s 338us/step - loss: 0.3754 - accuracy: 0.8304 - val_loss: 0.4068 - val_accuracy: 0.7995\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31344\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3691 - accuracy: 0.85 - ETA: 3s - loss: 0.3770 - accuracy: 0.82 - ETA: 3s - loss: 0.3956 - accuracy: 0.81 - ETA: 3s - loss: 0.3856 - accuracy: 0.82 - ETA: 3s - loss: 0.3924 - accuracy: 0.82 - ETA: 3s - loss: 0.3973 - accuracy: 0.81 - ETA: 3s - loss: 0.3911 - accuracy: 0.82 - ETA: 2s - loss: 0.3901 - accuracy: 0.82 - ETA: 2s - loss: 0.3963 - accuracy: 0.82 - ETA: 2s - loss: 0.3921 - accuracy: 0.82 - ETA: 2s - loss: 0.3892 - accuracy: 0.82 - ETA: 2s - loss: 0.3849 - accuracy: 0.82 - ETA: 2s - loss: 0.3816 - accuracy: 0.82 - ETA: 2s - loss: 0.3770 - accuracy: 0.82 - ETA: 2s - loss: 0.3732 - accuracy: 0.83 - ETA: 2s - loss: 0.3711 - accuracy: 0.83 - ETA: 2s - loss: 0.3729 - accuracy: 0.83 - ETA: 2s - loss: 0.3745 - accuracy: 0.83 - ETA: 1s - loss: 0.3710 - accuracy: 0.83 - ETA: 1s - loss: 0.3680 - accuracy: 0.83 - ETA: 1s - loss: 0.3660 - accuracy: 0.83 - ETA: 1s - loss: 0.3652 - accuracy: 0.83 - ETA: 1s - loss: 0.3657 - accuracy: 0.83 - ETA: 1s - loss: 0.3647 - accuracy: 0.83 - ETA: 1s - loss: 0.3655 - accuracy: 0.83 - ETA: 1s - loss: 0.3649 - accuracy: 0.83 - ETA: 1s - loss: 0.3681 - accuracy: 0.83 - ETA: 1s - loss: 0.3667 - accuracy: 0.83 - ETA: 0s - loss: 0.3673 - accuracy: 0.83 - ETA: 0s - loss: 0.3680 - accuracy: 0.83 - ETA: 0s - loss: 0.3669 - accuracy: 0.83 - ETA: 0s - loss: 0.3665 - accuracy: 0.83 - ETA: 0s - loss: 0.3676 - accuracy: 0.83 - ETA: 0s - loss: 0.3669 - accuracy: 0.83 - ETA: 0s - loss: 0.3662 - accuracy: 0.83 - ETA: 0s - loss: 0.3666 - accuracy: 0.83 - ETA: 0s - loss: 0.3669 - accuracy: 0.83 - ETA: 0s - loss: 0.3665 - accuracy: 0.83 - ETA: 0s - loss: 0.3672 - accuracy: 0.83 - ETA: 0s - loss: 0.3666 - accuracy: 0.83 - ETA: 0s - loss: 0.3674 - accuracy: 0.83 - ETA: 0s - loss: 0.3697 - accuracy: 0.83 - ETA: 0s - loss: 0.3690 - accuracy: 0.83 - 4s 330us/step - loss: 0.3693 - accuracy: 0.8338 - val_loss: 0.3586 - val_accuracy: 0.8425\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31344\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3622 - accuracy: 0.85 - ETA: 2s - loss: 0.3339 - accuracy: 0.85 - ETA: 2s - loss: 0.3447 - accuracy: 0.84 - ETA: 2s - loss: 0.3712 - accuracy: 0.82 - ETA: 2s - loss: 0.3640 - accuracy: 0.83 - ETA: 2s - loss: 0.3704 - accuracy: 0.83 - ETA: 2s - loss: 0.3721 - accuracy: 0.83 - ETA: 2s - loss: 0.3673 - accuracy: 0.83 - ETA: 2s - loss: 0.3630 - accuracy: 0.83 - ETA: 2s - loss: 0.3594 - accuracy: 0.83 - ETA: 2s - loss: 0.3557 - accuracy: 0.83 - ETA: 2s - loss: 0.3558 - accuracy: 0.84 - ETA: 2s - loss: 0.3581 - accuracy: 0.83 - ETA: 2s - loss: 0.3593 - accuracy: 0.83 - ETA: 2s - loss: 0.3602 - accuracy: 0.83 - ETA: 2s - loss: 0.3573 - accuracy: 0.83 - ETA: 2s - loss: 0.3581 - accuracy: 0.83 - ETA: 1s - loss: 0.3583 - accuracy: 0.83 - ETA: 1s - loss: 0.3588 - accuracy: 0.84 - ETA: 1s - loss: 0.3610 - accuracy: 0.84 - ETA: 1s - loss: 0.3620 - accuracy: 0.83 - ETA: 1s - loss: 0.3618 - accuracy: 0.83 - ETA: 1s - loss: 0.3612 - accuracy: 0.83 - ETA: 1s - loss: 0.3622 - accuracy: 0.83 - ETA: 1s - loss: 0.3638 - accuracy: 0.83 - ETA: 1s - loss: 0.3651 - accuracy: 0.83 - ETA: 1s - loss: 0.3644 - accuracy: 0.83 - ETA: 1s - loss: 0.3645 - accuracy: 0.83 - ETA: 1s - loss: 0.3621 - accuracy: 0.83 - ETA: 1s - loss: 0.3630 - accuracy: 0.83 - ETA: 1s - loss: 0.3615 - accuracy: 0.83 - ETA: 0s - loss: 0.3597 - accuracy: 0.83 - ETA: 0s - loss: 0.3613 - accuracy: 0.83 - ETA: 0s - loss: 0.3617 - accuracy: 0.83 - ETA: 0s - loss: 0.3588 - accuracy: 0.83 - ETA: 0s - loss: 0.3600 - accuracy: 0.83 - ETA: 0s - loss: 0.3601 - accuracy: 0.83 - ETA: 0s - loss: 0.3608 - accuracy: 0.83 - ETA: 0s - loss: 0.3621 - accuracy: 0.83 - ETA: 0s - loss: 0.3614 - accuracy: 0.83 - ETA: 0s - loss: 0.3622 - accuracy: 0.83 - ETA: 0s - loss: 0.3636 - accuracy: 0.83 - 3s 321us/step - loss: 0.3627 - accuracy: 0.8372 - val_loss: 0.3757 - val_accuracy: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31344\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3757 - accuracy: 0.83 - ETA: 2s - loss: 0.3260 - accuracy: 0.85 - ETA: 2s - loss: 0.3483 - accuracy: 0.85 - ETA: 2s - loss: 0.3676 - accuracy: 0.84 - ETA: 2s - loss: 0.3584 - accuracy: 0.84 - ETA: 2s - loss: 0.3537 - accuracy: 0.84 - ETA: 2s - loss: 0.3564 - accuracy: 0.84 - ETA: 2s - loss: 0.3631 - accuracy: 0.83 - ETA: 2s - loss: 0.3719 - accuracy: 0.82 - ETA: 2s - loss: 0.3713 - accuracy: 0.83 - ETA: 2s - loss: 0.3721 - accuracy: 0.82 - ETA: 2s - loss: 0.3723 - accuracy: 0.83 - ETA: 2s - loss: 0.3704 - accuracy: 0.83 - ETA: 2s - loss: 0.3717 - accuracy: 0.83 - ETA: 1s - loss: 0.3785 - accuracy: 0.82 - ETA: 1s - loss: 0.3798 - accuracy: 0.82 - ETA: 1s - loss: 0.3803 - accuracy: 0.82 - ETA: 1s - loss: 0.3770 - accuracy: 0.82 - ETA: 1s - loss: 0.3749 - accuracy: 0.83 - ETA: 1s - loss: 0.3734 - accuracy: 0.83 - ETA: 1s - loss: 0.3700 - accuracy: 0.83 - ETA: 1s - loss: 0.3662 - accuracy: 0.83 - ETA: 1s - loss: 0.3664 - accuracy: 0.83 - ETA: 1s - loss: 0.3646 - accuracy: 0.83 - ETA: 1s - loss: 0.3650 - accuracy: 0.83 - ETA: 1s - loss: 0.3660 - accuracy: 0.83 - ETA: 1s - loss: 0.3674 - accuracy: 0.83 - ETA: 0s - loss: 0.3671 - accuracy: 0.83 - ETA: 0s - loss: 0.3681 - accuracy: 0.83 - ETA: 0s - loss: 0.3674 - accuracy: 0.83 - ETA: 0s - loss: 0.3693 - accuracy: 0.83 - ETA: 0s - loss: 0.3698 - accuracy: 0.83 - ETA: 0s - loss: 0.3684 - accuracy: 0.83 - ETA: 0s - loss: 0.3670 - accuracy: 0.83 - ETA: 0s - loss: 0.3660 - accuracy: 0.83 - ETA: 0s - loss: 0.3647 - accuracy: 0.83 - ETA: 0s - loss: 0.3644 - accuracy: 0.83 - ETA: 0s - loss: 0.3656 - accuracy: 0.83 - ETA: 0s - loss: 0.3656 - accuracy: 0.83 - 3s 312us/step - loss: 0.3661 - accuracy: 0.8348 - val_loss: 0.3506 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31344\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3976 - accuracy: 0.81 - ETA: 3s - loss: 0.3774 - accuracy: 0.82 - ETA: 2s - loss: 0.3759 - accuracy: 0.82 - ETA: 2s - loss: 0.3629 - accuracy: 0.82 - ETA: 2s - loss: 0.3736 - accuracy: 0.82 - ETA: 2s - loss: 0.3693 - accuracy: 0.83 - ETA: 2s - loss: 0.3711 - accuracy: 0.83 - ETA: 2s - loss: 0.3696 - accuracy: 0.83 - ETA: 2s - loss: 0.3633 - accuracy: 0.83 - ETA: 2s - loss: 0.3563 - accuracy: 0.84 - ETA: 2s - loss: 0.3603 - accuracy: 0.83 - ETA: 2s - loss: 0.3579 - accuracy: 0.84 - ETA: 2s - loss: 0.3592 - accuracy: 0.84 - ETA: 2s - loss: 0.3589 - accuracy: 0.84 - ETA: 2s - loss: 0.3592 - accuracy: 0.84 - ETA: 2s - loss: 0.3582 - accuracy: 0.84 - ETA: 2s - loss: 0.3599 - accuracy: 0.84 - ETA: 2s - loss: 0.3589 - accuracy: 0.84 - ETA: 2s - loss: 0.3574 - accuracy: 0.84 - ETA: 1s - loss: 0.3582 - accuracy: 0.84 - ETA: 1s - loss: 0.3570 - accuracy: 0.84 - ETA: 1s - loss: 0.3598 - accuracy: 0.84 - ETA: 1s - loss: 0.3606 - accuracy: 0.84 - ETA: 1s - loss: 0.3603 - accuracy: 0.84 - ETA: 1s - loss: 0.3637 - accuracy: 0.84 - ETA: 1s - loss: 0.3631 - accuracy: 0.84 - ETA: 1s - loss: 0.3621 - accuracy: 0.84 - ETA: 1s - loss: 0.3624 - accuracy: 0.83 - ETA: 1s - loss: 0.3600 - accuracy: 0.84 - ETA: 1s - loss: 0.3592 - accuracy: 0.84 - ETA: 1s - loss: 0.3560 - accuracy: 0.84 - ETA: 0s - loss: 0.3566 - accuracy: 0.84 - ETA: 0s - loss: 0.3558 - accuracy: 0.84 - ETA: 0s - loss: 0.3534 - accuracy: 0.84 - ETA: 0s - loss: 0.3537 - accuracy: 0.84 - ETA: 0s - loss: 0.3545 - accuracy: 0.84 - ETA: 0s - loss: 0.3541 - accuracy: 0.84 - ETA: 0s - loss: 0.3536 - accuracy: 0.84 - ETA: 0s - loss: 0.3526 - accuracy: 0.84 - ETA: 0s - loss: 0.3530 - accuracy: 0.84 - ETA: 0s - loss: 0.3526 - accuracy: 0.84 - ETA: 0s - loss: 0.3526 - accuracy: 0.84 - ETA: 0s - loss: 0.3526 - accuracy: 0.84 - ETA: 0s - loss: 0.3525 - accuracy: 0.84 - 4s 333us/step - loss: 0.3521 - accuracy: 0.8433 - val_loss: 0.3313 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.31344\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3209 - accuracy: 0.88 - ETA: 3s - loss: 0.3551 - accuracy: 0.82 - ETA: 3s - loss: 0.3394 - accuracy: 0.84 - ETA: 3s - loss: 0.3416 - accuracy: 0.84 - ETA: 2s - loss: 0.3486 - accuracy: 0.84 - ETA: 2s - loss: 0.3588 - accuracy: 0.83 - ETA: 2s - loss: 0.3556 - accuracy: 0.84 - ETA: 2s - loss: 0.3682 - accuracy: 0.82 - ETA: 2s - loss: 0.3613 - accuracy: 0.83 - ETA: 2s - loss: 0.3626 - accuracy: 0.83 - ETA: 2s - loss: 0.3619 - accuracy: 0.83 - ETA: 2s - loss: 0.3604 - accuracy: 0.83 - ETA: 2s - loss: 0.3639 - accuracy: 0.82 - ETA: 2s - loss: 0.3588 - accuracy: 0.83 - ETA: 2s - loss: 0.3593 - accuracy: 0.83 - ETA: 2s - loss: 0.3557 - accuracy: 0.83 - ETA: 2s - loss: 0.3525 - accuracy: 0.83 - ETA: 2s - loss: 0.3487 - accuracy: 0.83 - ETA: 2s - loss: 0.3483 - accuracy: 0.83 - ETA: 2s - loss: 0.3483 - accuracy: 0.84 - ETA: 2s - loss: 0.3472 - accuracy: 0.84 - ETA: 2s - loss: 0.3483 - accuracy: 0.84 - ETA: 1s - loss: 0.3499 - accuracy: 0.84 - ETA: 1s - loss: 0.3494 - accuracy: 0.84 - ETA: 1s - loss: 0.3503 - accuracy: 0.84 - ETA: 1s - loss: 0.3520 - accuracy: 0.84 - ETA: 1s - loss: 0.3525 - accuracy: 0.84 - ETA: 1s - loss: 0.3515 - accuracy: 0.84 - ETA: 1s - loss: 0.3520 - accuracy: 0.84 - ETA: 1s - loss: 0.3504 - accuracy: 0.84 - ETA: 1s - loss: 0.3538 - accuracy: 0.84 - ETA: 1s - loss: 0.3527 - accuracy: 0.84 - ETA: 1s - loss: 0.3539 - accuracy: 0.84 - ETA: 1s - loss: 0.3528 - accuracy: 0.84 - ETA: 0s - loss: 0.3563 - accuracy: 0.84 - ETA: 0s - loss: 0.3551 - accuracy: 0.84 - ETA: 0s - loss: 0.3561 - accuracy: 0.84 - ETA: 0s - loss: 0.3544 - accuracy: 0.84 - ETA: 0s - loss: 0.3541 - accuracy: 0.84 - ETA: 0s - loss: 0.3529 - accuracy: 0.84 - ETA: 0s - loss: 0.3517 - accuracy: 0.84 - ETA: 0s - loss: 0.3516 - accuracy: 0.84 - ETA: 0s - loss: 0.3532 - accuracy: 0.84 - ETA: 0s - loss: 0.3530 - accuracy: 0.84 - ETA: 0s - loss: 0.3551 - accuracy: 0.83 - 4s 330us/step - loss: 0.3550 - accuracy: 0.8396 - val_loss: 0.3488 - val_accuracy: 0.8425\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31344\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3163 - accuracy: 0.84 - ETA: 2s - loss: 0.3577 - accuracy: 0.82 - ETA: 2s - loss: 0.3663 - accuracy: 0.82 - ETA: 2s - loss: 0.3628 - accuracy: 0.82 - ETA: 2s - loss: 0.3676 - accuracy: 0.82 - ETA: 2s - loss: 0.3699 - accuracy: 0.82 - ETA: 2s - loss: 0.3748 - accuracy: 0.82 - ETA: 2s - loss: 0.3747 - accuracy: 0.82 - ETA: 2s - loss: 0.3752 - accuracy: 0.82 - ETA: 2s - loss: 0.3705 - accuracy: 0.82 - ETA: 2s - loss: 0.3697 - accuracy: 0.82 - ETA: 2s - loss: 0.3676 - accuracy: 0.82 - ETA: 2s - loss: 0.3683 - accuracy: 0.83 - ETA: 2s - loss: 0.3659 - accuracy: 0.83 - ETA: 2s - loss: 0.3656 - accuracy: 0.83 - ETA: 1s - loss: 0.3651 - accuracy: 0.83 - ETA: 1s - loss: 0.3634 - accuracy: 0.83 - ETA: 1s - loss: 0.3665 - accuracy: 0.83 - ETA: 1s - loss: 0.3624 - accuracy: 0.83 - ETA: 1s - loss: 0.3607 - accuracy: 0.83 - ETA: 1s - loss: 0.3604 - accuracy: 0.83 - ETA: 1s - loss: 0.3615 - accuracy: 0.83 - ETA: 1s - loss: 0.3635 - accuracy: 0.83 - ETA: 1s - loss: 0.3633 - accuracy: 0.83 - ETA: 1s - loss: 0.3635 - accuracy: 0.83 - ETA: 1s - loss: 0.3622 - accuracy: 0.83 - ETA: 1s - loss: 0.3622 - accuracy: 0.83 - ETA: 1s - loss: 0.3608 - accuracy: 0.83 - ETA: 0s - loss: 0.3599 - accuracy: 0.83 - ETA: 0s - loss: 0.3600 - accuracy: 0.83 - ETA: 0s - loss: 0.3599 - accuracy: 0.83 - ETA: 0s - loss: 0.3591 - accuracy: 0.83 - ETA: 0s - loss: 0.3581 - accuracy: 0.83 - ETA: 0s - loss: 0.3613 - accuracy: 0.83 - ETA: 0s - loss: 0.3620 - accuracy: 0.83 - ETA: 0s - loss: 0.3620 - accuracy: 0.83 - ETA: 0s - loss: 0.3588 - accuracy: 0.83 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - ETA: 0s - loss: 0.3578 - accuracy: 0.83 - 3s 308us/step - loss: 0.3575 - accuracy: 0.8368 - val_loss: 0.3394 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.31344\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3340 - accuracy: 0.85 - ETA: 3s - loss: 0.3255 - accuracy: 0.86 - ETA: 2s - loss: 0.3410 - accuracy: 0.84 - ETA: 2s - loss: 0.3446 - accuracy: 0.84 - ETA: 2s - loss: 0.3474 - accuracy: 0.84 - ETA: 2s - loss: 0.3512 - accuracy: 0.83 - ETA: 2s - loss: 0.3552 - accuracy: 0.83 - ETA: 2s - loss: 0.3468 - accuracy: 0.84 - ETA: 2s - loss: 0.3510 - accuracy: 0.84 - ETA: 2s - loss: 0.3557 - accuracy: 0.84 - ETA: 2s - loss: 0.3547 - accuracy: 0.84 - ETA: 2s - loss: 0.3557 - accuracy: 0.84 - ETA: 2s - loss: 0.3539 - accuracy: 0.84 - ETA: 2s - loss: 0.3516 - accuracy: 0.84 - ETA: 2s - loss: 0.3505 - accuracy: 0.84 - ETA: 1s - loss: 0.3482 - accuracy: 0.84 - ETA: 1s - loss: 0.3514 - accuracy: 0.84 - ETA: 1s - loss: 0.3518 - accuracy: 0.84 - ETA: 1s - loss: 0.3491 - accuracy: 0.84 - ETA: 1s - loss: 0.3492 - accuracy: 0.84 - ETA: 1s - loss: 0.3469 - accuracy: 0.84 - ETA: 1s - loss: 0.3447 - accuracy: 0.84 - ETA: 1s - loss: 0.3453 - accuracy: 0.84 - ETA: 1s - loss: 0.3458 - accuracy: 0.84 - ETA: 1s - loss: 0.3460 - accuracy: 0.84 - ETA: 1s - loss: 0.3484 - accuracy: 0.84 - ETA: 1s - loss: 0.3487 - accuracy: 0.84 - ETA: 1s - loss: 0.3492 - accuracy: 0.84 - ETA: 1s - loss: 0.3470 - accuracy: 0.84 - ETA: 1s - loss: 0.3464 - accuracy: 0.84 - ETA: 1s - loss: 0.3472 - accuracy: 0.84 - ETA: 1s - loss: 0.3492 - accuracy: 0.84 - ETA: 0s - loss: 0.3480 - accuracy: 0.84 - ETA: 0s - loss: 0.3485 - accuracy: 0.84 - ETA: 0s - loss: 0.3464 - accuracy: 0.84 - ETA: 0s - loss: 0.3465 - accuracy: 0.84 - ETA: 0s - loss: 0.3472 - accuracy: 0.84 - ETA: 0s - loss: 0.3493 - accuracy: 0.84 - ETA: 0s - loss: 0.3495 - accuracy: 0.84 - ETA: 0s - loss: 0.3501 - accuracy: 0.84 - ETA: 0s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.3497 - accuracy: 0.84 - ETA: 0s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3492 - accuracy: 0.84 - ETA: 0s - loss: 0.3498 - accuracy: 0.84 - ETA: 0s - loss: 0.3498 - accuracy: 0.84 - ETA: 0s - loss: 0.3499 - accuracy: 0.84 - ETA: 0s - loss: 0.3498 - accuracy: 0.84 - ETA: 0s - loss: 0.3494 - accuracy: 0.84 - 4s 339us/step - loss: 0.3491 - accuracy: 0.8431 - val_loss: 0.3627 - val_accuracy: 0.8239\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.31344\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3427 - accuracy: 0.86 - ETA: 2s - loss: 0.3567 - accuracy: 0.84 - ETA: 3s - loss: 0.3408 - accuracy: 0.85 - ETA: 3s - loss: 0.3340 - accuracy: 0.86 - ETA: 3s - loss: 0.3378 - accuracy: 0.85 - ETA: 3s - loss: 0.3450 - accuracy: 0.86 - ETA: 3s - loss: 0.3364 - accuracy: 0.85 - ETA: 3s - loss: 0.3366 - accuracy: 0.85 - ETA: 2s - loss: 0.3366 - accuracy: 0.85 - ETA: 2s - loss: 0.3336 - accuracy: 0.85 - ETA: 2s - loss: 0.3314 - accuracy: 0.85 - ETA: 2s - loss: 0.3337 - accuracy: 0.85 - ETA: 2s - loss: 0.3340 - accuracy: 0.85 - ETA: 2s - loss: 0.3336 - accuracy: 0.85 - ETA: 2s - loss: 0.3331 - accuracy: 0.85 - ETA: 2s - loss: 0.3356 - accuracy: 0.85 - ETA: 2s - loss: 0.3352 - accuracy: 0.85 - ETA: 2s - loss: 0.3381 - accuracy: 0.85 - ETA: 1s - loss: 0.3364 - accuracy: 0.85 - ETA: 1s - loss: 0.3380 - accuracy: 0.85 - ETA: 1s - loss: 0.3404 - accuracy: 0.85 - ETA: 1s - loss: 0.3441 - accuracy: 0.84 - ETA: 1s - loss: 0.3415 - accuracy: 0.84 - ETA: 1s - loss: 0.3406 - accuracy: 0.85 - ETA: 1s - loss: 0.3411 - accuracy: 0.85 - ETA: 1s - loss: 0.3438 - accuracy: 0.84 - ETA: 1s - loss: 0.3436 - accuracy: 0.84 - ETA: 1s - loss: 0.3451 - accuracy: 0.84 - ETA: 1s - loss: 0.3446 - accuracy: 0.85 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3489 - accuracy: 0.84 - ETA: 1s - loss: 0.3485 - accuracy: 0.84 - ETA: 0s - loss: 0.3497 - accuracy: 0.84 - ETA: 0s - loss: 0.3489 - accuracy: 0.84 - ETA: 0s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.3493 - accuracy: 0.84 - ETA: 0s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3503 - accuracy: 0.84 - ETA: 0s - loss: 0.3499 - accuracy: 0.84 - ETA: 0s - loss: 0.3482 - accuracy: 0.84 - ETA: 0s - loss: 0.3501 - accuracy: 0.84 - ETA: 0s - loss: 0.3522 - accuracy: 0.84 - ETA: 0s - loss: 0.3517 - accuracy: 0.84 - ETA: 0s - loss: 0.3505 - accuracy: 0.84 - ETA: 0s - loss: 0.3490 - accuracy: 0.84 - 3s 324us/step - loss: 0.3484 - accuracy: 0.8451 - val_loss: 0.3567 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.31344\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4050 - accuracy: 0.83 - ETA: 2s - loss: 0.3582 - accuracy: 0.84 - ETA: 2s - loss: 0.3417 - accuracy: 0.85 - ETA: 2s - loss: 0.3359 - accuracy: 0.85 - ETA: 2s - loss: 0.3379 - accuracy: 0.85 - ETA: 2s - loss: 0.3434 - accuracy: 0.85 - ETA: 2s - loss: 0.3389 - accuracy: 0.85 - ETA: 2s - loss: 0.3346 - accuracy: 0.85 - ETA: 2s - loss: 0.3349 - accuracy: 0.85 - ETA: 2s - loss: 0.3273 - accuracy: 0.85 - ETA: 2s - loss: 0.3278 - accuracy: 0.85 - ETA: 2s - loss: 0.3365 - accuracy: 0.85 - ETA: 2s - loss: 0.3360 - accuracy: 0.85 - ETA: 2s - loss: 0.3365 - accuracy: 0.85 - ETA: 1s - loss: 0.3392 - accuracy: 0.84 - ETA: 1s - loss: 0.3404 - accuracy: 0.84 - ETA: 1s - loss: 0.3421 - accuracy: 0.84 - ETA: 1s - loss: 0.3435 - accuracy: 0.84 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3486 - accuracy: 0.84 - ETA: 1s - loss: 0.3500 - accuracy: 0.84 - ETA: 1s - loss: 0.3482 - accuracy: 0.84 - ETA: 1s - loss: 0.3495 - accuracy: 0.84 - ETA: 1s - loss: 0.3483 - accuracy: 0.84 - ETA: 1s - loss: 0.3493 - accuracy: 0.84 - ETA: 1s - loss: 0.3490 - accuracy: 0.84 - ETA: 1s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3494 - accuracy: 0.84 - ETA: 0s - loss: 0.3502 - accuracy: 0.84 - ETA: 0s - loss: 0.3513 - accuracy: 0.84 - ETA: 0s - loss: 0.3492 - accuracy: 0.84 - ETA: 0s - loss: 0.3510 - accuracy: 0.84 - ETA: 0s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3525 - accuracy: 0.84 - ETA: 0s - loss: 0.3521 - accuracy: 0.84 - ETA: 0s - loss: 0.3513 - accuracy: 0.84 - ETA: 0s - loss: 0.3505 - accuracy: 0.84 - ETA: 0s - loss: 0.3501 - accuracy: 0.84 - ETA: 0s - loss: 0.3479 - accuracy: 0.84 - ETA: 0s - loss: 0.3492 - accuracy: 0.84 - 3s 303us/step - loss: 0.3471 - accuracy: 0.8445 - val_loss: 0.3517 - val_accuracy: 0.8382\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.31344\n",
      "Epoch 00026: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 167us/step\n",
      "[2020-05-18 16:04:52 RAM66.4% 0.69GB] Val Score : [0.33522232981727734, 0.8339651226997375]\n",
      "[2020-05-18 16:04:52 RAM66.4% 0.69GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:04:52 RAM66.4% 0.69GB] Training on Fold : 8\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 37s - loss: 5.9709 - accuracy: 0.413 - ETA: 14s - loss: 2.8391 - accuracy: 0.537 - ETA: 9s - loss: 2.0542 - accuracy: 0.566 - ETA: 7s - loss: 1.6671 - accuracy: 0.60 - ETA: 6s - loss: 1.4737 - accuracy: 0.59 - ETA: 5s - loss: 1.3408 - accuracy: 0.60 - ETA: 4s - loss: 1.2316 - accuracy: 0.61 - ETA: 4s - loss: 1.1528 - accuracy: 0.62 - ETA: 3s - loss: 1.0944 - accuracy: 0.62 - ETA: 3s - loss: 1.0484 - accuracy: 0.62 - ETA: 3s - loss: 1.0025 - accuracy: 0.63 - ETA: 3s - loss: 0.9888 - accuracy: 0.63 - ETA: 3s - loss: 0.9718 - accuracy: 0.63 - ETA: 3s - loss: 0.9410 - accuracy: 0.64 - ETA: 2s - loss: 0.9144 - accuracy: 0.64 - ETA: 2s - loss: 0.8959 - accuracy: 0.65 - ETA: 2s - loss: 0.8777 - accuracy: 0.65 - ETA: 2s - loss: 0.8570 - accuracy: 0.65 - ETA: 2s - loss: 0.8414 - accuracy: 0.66 - ETA: 2s - loss: 0.8266 - accuracy: 0.66 - ETA: 1s - loss: 0.8136 - accuracy: 0.66 - ETA: 1s - loss: 0.8016 - accuracy: 0.67 - ETA: 1s - loss: 0.7912 - accuracy: 0.67 - ETA: 1s - loss: 0.7801 - accuracy: 0.67 - ETA: 1s - loss: 0.7720 - accuracy: 0.68 - ETA: 1s - loss: 0.7621 - accuracy: 0.68 - ETA: 1s - loss: 0.7537 - accuracy: 0.68 - ETA: 1s - loss: 0.7475 - accuracy: 0.68 - ETA: 0s - loss: 0.7395 - accuracy: 0.68 - ETA: 0s - loss: 0.7329 - accuracy: 0.69 - ETA: 0s - loss: 0.7300 - accuracy: 0.69 - ETA: 0s - loss: 0.7238 - accuracy: 0.69 - ETA: 0s - loss: 0.7165 - accuracy: 0.69 - ETA: 0s - loss: 0.7113 - accuracy: 0.69 - ETA: 0s - loss: 0.7061 - accuracy: 0.70 - ETA: 0s - loss: 0.7016 - accuracy: 0.70 - ETA: 0s - loss: 0.6954 - accuracy: 0.70 - ETA: 0s - loss: 0.6905 - accuracy: 0.70 - 4s 361us/step - loss: 0.6879 - accuracy: 0.7056 - val_loss: 0.5007 - val_accuracy: 0.8020\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.31344\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5115 - accuracy: 0.75 - ETA: 2s - loss: 0.5188 - accuracy: 0.75 - ETA: 3s - loss: 0.5113 - accuracy: 0.75 - ETA: 2s - loss: 0.5166 - accuracy: 0.75 - ETA: 2s - loss: 0.5217 - accuracy: 0.75 - ETA: 2s - loss: 0.5172 - accuracy: 0.76 - ETA: 2s - loss: 0.5322 - accuracy: 0.75 - ETA: 2s - loss: 0.5302 - accuracy: 0.75 - ETA: 2s - loss: 0.5273 - accuracy: 0.75 - ETA: 2s - loss: 0.5266 - accuracy: 0.75 - ETA: 2s - loss: 0.5305 - accuracy: 0.75 - ETA: 2s - loss: 0.5317 - accuracy: 0.75 - ETA: 2s - loss: 0.5319 - accuracy: 0.75 - ETA: 2s - loss: 0.5281 - accuracy: 0.75 - ETA: 2s - loss: 0.5330 - accuracy: 0.75 - ETA: 2s - loss: 0.5349 - accuracy: 0.75 - ETA: 2s - loss: 0.5352 - accuracy: 0.75 - ETA: 1s - loss: 0.5342 - accuracy: 0.75 - ETA: 1s - loss: 0.5331 - accuracy: 0.75 - ETA: 1s - loss: 0.5318 - accuracy: 0.75 - ETA: 1s - loss: 0.5289 - accuracy: 0.76 - ETA: 1s - loss: 0.5260 - accuracy: 0.76 - ETA: 1s - loss: 0.5285 - accuracy: 0.76 - ETA: 1s - loss: 0.5263 - accuracy: 0.76 - ETA: 1s - loss: 0.5275 - accuracy: 0.76 - ETA: 1s - loss: 0.5275 - accuracy: 0.76 - ETA: 1s - loss: 0.5255 - accuracy: 0.76 - ETA: 1s - loss: 0.5243 - accuracy: 0.76 - ETA: 1s - loss: 0.5240 - accuracy: 0.76 - ETA: 0s - loss: 0.5248 - accuracy: 0.76 - ETA: 0s - loss: 0.5242 - accuracy: 0.76 - ETA: 0s - loss: 0.5234 - accuracy: 0.76 - ETA: 0s - loss: 0.5212 - accuracy: 0.76 - ETA: 0s - loss: 0.5245 - accuracy: 0.76 - ETA: 0s - loss: 0.5217 - accuracy: 0.76 - ETA: 0s - loss: 0.5197 - accuracy: 0.76 - ETA: 0s - loss: 0.5198 - accuracy: 0.76 - ETA: 0s - loss: 0.5184 - accuracy: 0.76 - ETA: 0s - loss: 0.5179 - accuracy: 0.76 - ETA: 0s - loss: 0.5163 - accuracy: 0.76 - ETA: 0s - loss: 0.5152 - accuracy: 0.76 - 3s 314us/step - loss: 0.5148 - accuracy: 0.7684 - val_loss: 0.4552 - val_accuracy: 0.8113\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31344\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5004 - accuracy: 0.76 - ETA: 3s - loss: 0.4650 - accuracy: 0.80 - ETA: 3s - loss: 0.4614 - accuracy: 0.79 - ETA: 3s - loss: 0.4481 - accuracy: 0.81 - ETA: 2s - loss: 0.4700 - accuracy: 0.78 - ETA: 2s - loss: 0.4639 - accuracy: 0.79 - ETA: 2s - loss: 0.4716 - accuracy: 0.79 - ETA: 2s - loss: 0.4864 - accuracy: 0.78 - ETA: 2s - loss: 0.4989 - accuracy: 0.77 - ETA: 2s - loss: 0.4946 - accuracy: 0.77 - ETA: 2s - loss: 0.4920 - accuracy: 0.77 - ETA: 2s - loss: 0.4932 - accuracy: 0.77 - ETA: 2s - loss: 0.4924 - accuracy: 0.77 - ETA: 2s - loss: 0.4937 - accuracy: 0.77 - ETA: 2s - loss: 0.4925 - accuracy: 0.77 - ETA: 1s - loss: 0.4940 - accuracy: 0.77 - ETA: 1s - loss: 0.4942 - accuracy: 0.77 - ETA: 1s - loss: 0.4915 - accuracy: 0.78 - ETA: 1s - loss: 0.4897 - accuracy: 0.78 - ETA: 1s - loss: 0.4910 - accuracy: 0.78 - ETA: 1s - loss: 0.4911 - accuracy: 0.78 - ETA: 1s - loss: 0.4925 - accuracy: 0.77 - ETA: 1s - loss: 0.4948 - accuracy: 0.77 - ETA: 1s - loss: 0.4955 - accuracy: 0.77 - ETA: 1s - loss: 0.4944 - accuracy: 0.77 - ETA: 1s - loss: 0.4957 - accuracy: 0.77 - ETA: 1s - loss: 0.4929 - accuracy: 0.77 - ETA: 1s - loss: 0.4934 - accuracy: 0.77 - ETA: 0s - loss: 0.4933 - accuracy: 0.77 - ETA: 0s - loss: 0.4935 - accuracy: 0.77 - ETA: 0s - loss: 0.4933 - accuracy: 0.77 - ETA: 0s - loss: 0.4926 - accuracy: 0.77 - ETA: 0s - loss: 0.4933 - accuracy: 0.77 - ETA: 0s - loss: 0.4946 - accuracy: 0.77 - ETA: 0s - loss: 0.4950 - accuracy: 0.77 - ETA: 0s - loss: 0.4927 - accuracy: 0.77 - ETA: 0s - loss: 0.4904 - accuracy: 0.77 - ETA: 0s - loss: 0.4912 - accuracy: 0.77 - ETA: 0s - loss: 0.4922 - accuracy: 0.77 - ETA: 0s - loss: 0.4919 - accuracy: 0.77 - ETA: 0s - loss: 0.4920 - accuracy: 0.77 - ETA: 0s - loss: 0.4898 - accuracy: 0.77 - 3s 311us/step - loss: 0.4895 - accuracy: 0.7787 - val_loss: 0.4573 - val_accuracy: 0.8088\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31344\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4319 - accuracy: 0.82 - ETA: 3s - loss: 0.4478 - accuracy: 0.80 - ETA: 3s - loss: 0.4537 - accuracy: 0.80 - ETA: 3s - loss: 0.4820 - accuracy: 0.79 - ETA: 3s - loss: 0.4861 - accuracy: 0.78 - ETA: 2s - loss: 0.4801 - accuracy: 0.78 - ETA: 2s - loss: 0.4737 - accuracy: 0.79 - ETA: 2s - loss: 0.4694 - accuracy: 0.79 - ETA: 2s - loss: 0.4655 - accuracy: 0.79 - ETA: 2s - loss: 0.4648 - accuracy: 0.79 - ETA: 2s - loss: 0.4657 - accuracy: 0.79 - ETA: 2s - loss: 0.4664 - accuracy: 0.79 - ETA: 2s - loss: 0.4721 - accuracy: 0.78 - ETA: 2s - loss: 0.4704 - accuracy: 0.78 - ETA: 2s - loss: 0.4740 - accuracy: 0.78 - ETA: 2s - loss: 0.4751 - accuracy: 0.78 - ETA: 2s - loss: 0.4744 - accuracy: 0.78 - ETA: 2s - loss: 0.4743 - accuracy: 0.78 - ETA: 1s - loss: 0.4750 - accuracy: 0.78 - ETA: 1s - loss: 0.4724 - accuracy: 0.78 - ETA: 1s - loss: 0.4710 - accuracy: 0.79 - ETA: 1s - loss: 0.4702 - accuracy: 0.79 - ETA: 1s - loss: 0.4689 - accuracy: 0.79 - ETA: 1s - loss: 0.4696 - accuracy: 0.79 - ETA: 1s - loss: 0.4697 - accuracy: 0.79 - ETA: 1s - loss: 0.4678 - accuracy: 0.79 - ETA: 1s - loss: 0.4690 - accuracy: 0.79 - ETA: 1s - loss: 0.4694 - accuracy: 0.78 - ETA: 1s - loss: 0.4686 - accuracy: 0.79 - ETA: 1s - loss: 0.4678 - accuracy: 0.79 - ETA: 0s - loss: 0.4669 - accuracy: 0.79 - ETA: 0s - loss: 0.4661 - accuracy: 0.79 - ETA: 0s - loss: 0.4667 - accuracy: 0.79 - ETA: 0s - loss: 0.4682 - accuracy: 0.79 - ETA: 0s - loss: 0.4672 - accuracy: 0.79 - ETA: 0s - loss: 0.4652 - accuracy: 0.79 - ETA: 0s - loss: 0.4651 - accuracy: 0.79 - ETA: 0s - loss: 0.4648 - accuracy: 0.79 - ETA: 0s - loss: 0.4655 - accuracy: 0.79 - ETA: 0s - loss: 0.4657 - accuracy: 0.79 - ETA: 0s - loss: 0.4666 - accuracy: 0.79 - ETA: 0s - loss: 0.4669 - accuracy: 0.79 - 3s 316us/step - loss: 0.4664 - accuracy: 0.7902 - val_loss: 0.4375 - val_accuracy: 0.8054\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.31344\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5170 - accuracy: 0.74 - ETA: 2s - loss: 0.4698 - accuracy: 0.78 - ETA: 2s - loss: 0.4545 - accuracy: 0.77 - ETA: 2s - loss: 0.4533 - accuracy: 0.77 - ETA: 2s - loss: 0.4479 - accuracy: 0.78 - ETA: 2s - loss: 0.4464 - accuracy: 0.78 - ETA: 2s - loss: 0.4597 - accuracy: 0.77 - ETA: 2s - loss: 0.4517 - accuracy: 0.78 - ETA: 2s - loss: 0.4493 - accuracy: 0.78 - ETA: 2s - loss: 0.4471 - accuracy: 0.78 - ETA: 2s - loss: 0.4464 - accuracy: 0.78 - ETA: 2s - loss: 0.4515 - accuracy: 0.78 - ETA: 2s - loss: 0.4501 - accuracy: 0.78 - ETA: 2s - loss: 0.4490 - accuracy: 0.78 - ETA: 1s - loss: 0.4486 - accuracy: 0.78 - ETA: 1s - loss: 0.4499 - accuracy: 0.78 - ETA: 1s - loss: 0.4506 - accuracy: 0.78 - ETA: 1s - loss: 0.4519 - accuracy: 0.78 - ETA: 1s - loss: 0.4479 - accuracy: 0.79 - ETA: 1s - loss: 0.4491 - accuracy: 0.79 - ETA: 1s - loss: 0.4512 - accuracy: 0.79 - ETA: 1s - loss: 0.4503 - accuracy: 0.79 - ETA: 1s - loss: 0.4533 - accuracy: 0.78 - ETA: 1s - loss: 0.4536 - accuracy: 0.78 - ETA: 1s - loss: 0.4534 - accuracy: 0.78 - ETA: 1s - loss: 0.4535 - accuracy: 0.78 - ETA: 1s - loss: 0.4550 - accuracy: 0.78 - ETA: 1s - loss: 0.4541 - accuracy: 0.78 - ETA: 0s - loss: 0.4557 - accuracy: 0.78 - ETA: 0s - loss: 0.4558 - accuracy: 0.78 - ETA: 0s - loss: 0.4552 - accuracy: 0.78 - ETA: 0s - loss: 0.4536 - accuracy: 0.78 - ETA: 0s - loss: 0.4539 - accuracy: 0.78 - ETA: 0s - loss: 0.4540 - accuracy: 0.78 - ETA: 0s - loss: 0.4531 - accuracy: 0.78 - ETA: 0s - loss: 0.4533 - accuracy: 0.78 - ETA: 0s - loss: 0.4521 - accuracy: 0.78 - ETA: 0s - loss: 0.4533 - accuracy: 0.78 - ETA: 0s - loss: 0.4523 - accuracy: 0.78 - ETA: 0s - loss: 0.4507 - accuracy: 0.78 - ETA: 0s - loss: 0.4529 - accuracy: 0.78 - 3s 309us/step - loss: 0.4524 - accuracy: 0.7887 - val_loss: 0.4067 - val_accuracy: 0.8222\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31344\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4192 - accuracy: 0.81 - ETA: 2s - loss: 0.4426 - accuracy: 0.78 - ETA: 2s - loss: 0.4587 - accuracy: 0.79 - ETA: 2s - loss: 0.4529 - accuracy: 0.79 - ETA: 2s - loss: 0.4467 - accuracy: 0.79 - ETA: 2s - loss: 0.4438 - accuracy: 0.79 - ETA: 2s - loss: 0.4417 - accuracy: 0.80 - ETA: 2s - loss: 0.4335 - accuracy: 0.80 - ETA: 2s - loss: 0.4310 - accuracy: 0.80 - ETA: 2s - loss: 0.4343 - accuracy: 0.80 - ETA: 2s - loss: 0.4334 - accuracy: 0.80 - ETA: 2s - loss: 0.4302 - accuracy: 0.80 - ETA: 2s - loss: 0.4259 - accuracy: 0.80 - ETA: 2s - loss: 0.4217 - accuracy: 0.81 - ETA: 2s - loss: 0.4216 - accuracy: 0.81 - ETA: 1s - loss: 0.4264 - accuracy: 0.80 - ETA: 1s - loss: 0.4264 - accuracy: 0.80 - ETA: 1s - loss: 0.4283 - accuracy: 0.80 - ETA: 1s - loss: 0.4283 - accuracy: 0.80 - ETA: 1s - loss: 0.4306 - accuracy: 0.80 - ETA: 1s - loss: 0.4326 - accuracy: 0.80 - ETA: 1s - loss: 0.4354 - accuracy: 0.79 - ETA: 1s - loss: 0.4347 - accuracy: 0.79 - ETA: 1s - loss: 0.4352 - accuracy: 0.79 - ETA: 1s - loss: 0.4372 - accuracy: 0.79 - ETA: 1s - loss: 0.4358 - accuracy: 0.79 - ETA: 1s - loss: 0.4354 - accuracy: 0.79 - ETA: 1s - loss: 0.4347 - accuracy: 0.79 - ETA: 0s - loss: 0.4344 - accuracy: 0.79 - ETA: 0s - loss: 0.4362 - accuracy: 0.79 - ETA: 0s - loss: 0.4380 - accuracy: 0.79 - ETA: 0s - loss: 0.4396 - accuracy: 0.79 - ETA: 0s - loss: 0.4395 - accuracy: 0.79 - ETA: 0s - loss: 0.4393 - accuracy: 0.79 - ETA: 0s - loss: 0.4389 - accuracy: 0.79 - ETA: 0s - loss: 0.4401 - accuracy: 0.79 - ETA: 0s - loss: 0.4403 - accuracy: 0.79 - ETA: 0s - loss: 0.4396 - accuracy: 0.79 - ETA: 0s - loss: 0.4393 - accuracy: 0.79 - ETA: 0s - loss: 0.4402 - accuracy: 0.79 - ETA: 0s - loss: 0.4400 - accuracy: 0.79 - 3s 313us/step - loss: 0.4393 - accuracy: 0.7954 - val_loss: 0.3863 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.31344\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4683 - accuracy: 0.75 - ETA: 2s - loss: 0.4500 - accuracy: 0.78 - ETA: 2s - loss: 0.4533 - accuracy: 0.78 - ETA: 2s - loss: 0.4404 - accuracy: 0.78 - ETA: 2s - loss: 0.4284 - accuracy: 0.79 - ETA: 2s - loss: 0.4291 - accuracy: 0.79 - ETA: 2s - loss: 0.4309 - accuracy: 0.80 - ETA: 2s - loss: 0.4264 - accuracy: 0.80 - ETA: 2s - loss: 0.4256 - accuracy: 0.80 - ETA: 2s - loss: 0.4279 - accuracy: 0.79 - ETA: 2s - loss: 0.4273 - accuracy: 0.79 - ETA: 2s - loss: 0.4275 - accuracy: 0.79 - ETA: 2s - loss: 0.4246 - accuracy: 0.80 - ETA: 2s - loss: 0.4274 - accuracy: 0.79 - ETA: 2s - loss: 0.4255 - accuracy: 0.80 - ETA: 2s - loss: 0.4266 - accuracy: 0.80 - ETA: 2s - loss: 0.4229 - accuracy: 0.80 - ETA: 2s - loss: 0.4265 - accuracy: 0.80 - ETA: 2s - loss: 0.4254 - accuracy: 0.80 - ETA: 2s - loss: 0.4250 - accuracy: 0.80 - ETA: 2s - loss: 0.4280 - accuracy: 0.80 - ETA: 1s - loss: 0.4289 - accuracy: 0.80 - ETA: 1s - loss: 0.4252 - accuracy: 0.80 - ETA: 1s - loss: 0.4257 - accuracy: 0.80 - ETA: 1s - loss: 0.4235 - accuracy: 0.80 - ETA: 1s - loss: 0.4246 - accuracy: 0.80 - ETA: 1s - loss: 0.4252 - accuracy: 0.80 - ETA: 1s - loss: 0.4254 - accuracy: 0.80 - ETA: 1s - loss: 0.4255 - accuracy: 0.80 - ETA: 1s - loss: 0.4260 - accuracy: 0.80 - ETA: 1s - loss: 0.4246 - accuracy: 0.80 - ETA: 1s - loss: 0.4237 - accuracy: 0.80 - ETA: 1s - loss: 0.4251 - accuracy: 0.80 - ETA: 1s - loss: 0.4257 - accuracy: 0.80 - ETA: 1s - loss: 0.4246 - accuracy: 0.80 - ETA: 1s - loss: 0.4240 - accuracy: 0.80 - ETA: 1s - loss: 0.4230 - accuracy: 0.80 - ETA: 0s - loss: 0.4262 - accuracy: 0.80 - ETA: 0s - loss: 0.4281 - accuracy: 0.79 - ETA: 0s - loss: 0.4275 - accuracy: 0.80 - ETA: 0s - loss: 0.4269 - accuracy: 0.80 - ETA: 0s - loss: 0.4256 - accuracy: 0.80 - ETA: 0s - loss: 0.4253 - accuracy: 0.80 - ETA: 0s - loss: 0.4266 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4269 - accuracy: 0.80 - ETA: 0s - loss: 0.4261 - accuracy: 0.80 - ETA: 0s - loss: 0.4263 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - 4s 337us/step - loss: 0.4262 - accuracy: 0.8010 - val_loss: 0.4126 - val_accuracy: 0.8104\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.31344\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4131 - accuracy: 0.81 - ETA: 2s - loss: 0.4105 - accuracy: 0.82 - ETA: 2s - loss: 0.4061 - accuracy: 0.81 - ETA: 2s - loss: 0.4080 - accuracy: 0.80 - ETA: 2s - loss: 0.4148 - accuracy: 0.80 - ETA: 2s - loss: 0.4175 - accuracy: 0.80 - ETA: 2s - loss: 0.4190 - accuracy: 0.80 - ETA: 2s - loss: 0.4192 - accuracy: 0.80 - ETA: 2s - loss: 0.4255 - accuracy: 0.80 - ETA: 2s - loss: 0.4202 - accuracy: 0.80 - ETA: 2s - loss: 0.4188 - accuracy: 0.80 - ETA: 2s - loss: 0.4156 - accuracy: 0.80 - ETA: 2s - loss: 0.4127 - accuracy: 0.80 - ETA: 2s - loss: 0.4144 - accuracy: 0.80 - ETA: 2s - loss: 0.4144 - accuracy: 0.80 - ETA: 2s - loss: 0.4177 - accuracy: 0.80 - ETA: 1s - loss: 0.4156 - accuracy: 0.80 - ETA: 1s - loss: 0.4178 - accuracy: 0.80 - ETA: 1s - loss: 0.4158 - accuracy: 0.80 - ETA: 1s - loss: 0.4179 - accuracy: 0.80 - ETA: 1s - loss: 0.4195 - accuracy: 0.80 - ETA: 1s - loss: 0.4207 - accuracy: 0.80 - ETA: 1s - loss: 0.4216 - accuracy: 0.80 - ETA: 1s - loss: 0.4216 - accuracy: 0.80 - ETA: 1s - loss: 0.4209 - accuracy: 0.80 - ETA: 1s - loss: 0.4189 - accuracy: 0.80 - ETA: 1s - loss: 0.4178 - accuracy: 0.80 - ETA: 1s - loss: 0.4190 - accuracy: 0.80 - ETA: 1s - loss: 0.4177 - accuracy: 0.80 - ETA: 0s - loss: 0.4185 - accuracy: 0.80 - ETA: 0s - loss: 0.4196 - accuracy: 0.80 - ETA: 0s - loss: 0.4196 - accuracy: 0.80 - ETA: 0s - loss: 0.4185 - accuracy: 0.80 - ETA: 0s - loss: 0.4174 - accuracy: 0.80 - ETA: 0s - loss: 0.4175 - accuracy: 0.80 - ETA: 0s - loss: 0.4176 - accuracy: 0.80 - ETA: 0s - loss: 0.4175 - accuracy: 0.80 - ETA: 0s - loss: 0.4170 - accuracy: 0.80 - ETA: 0s - loss: 0.4174 - accuracy: 0.80 - ETA: 0s - loss: 0.4167 - accuracy: 0.80 - ETA: 0s - loss: 0.4156 - accuracy: 0.80 - ETA: 0s - loss: 0.4156 - accuracy: 0.80 - ETA: 0s - loss: 0.4145 - accuracy: 0.80 - 3s 317us/step - loss: 0.4148 - accuracy: 0.8069 - val_loss: 0.3915 - val_accuracy: 0.8189\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.31344\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3430 - accuracy: 0.85 - ETA: 3s - loss: 0.3906 - accuracy: 0.81 - ETA: 3s - loss: 0.4020 - accuracy: 0.81 - ETA: 3s - loss: 0.4075 - accuracy: 0.81 - ETA: 2s - loss: 0.4145 - accuracy: 0.81 - ETA: 2s - loss: 0.4211 - accuracy: 0.80 - ETA: 2s - loss: 0.4266 - accuracy: 0.80 - ETA: 2s - loss: 0.4256 - accuracy: 0.80 - ETA: 2s - loss: 0.4340 - accuracy: 0.79 - ETA: 2s - loss: 0.4326 - accuracy: 0.79 - ETA: 2s - loss: 0.4285 - accuracy: 0.80 - ETA: 2s - loss: 0.4283 - accuracy: 0.80 - ETA: 2s - loss: 0.4306 - accuracy: 0.79 - ETA: 2s - loss: 0.4239 - accuracy: 0.80 - ETA: 2s - loss: 0.4190 - accuracy: 0.80 - ETA: 2s - loss: 0.4197 - accuracy: 0.80 - ETA: 2s - loss: 0.4226 - accuracy: 0.80 - ETA: 1s - loss: 0.4241 - accuracy: 0.80 - ETA: 1s - loss: 0.4248 - accuracy: 0.80 - ETA: 1s - loss: 0.4211 - accuracy: 0.80 - ETA: 1s - loss: 0.4232 - accuracy: 0.80 - ETA: 1s - loss: 0.4214 - accuracy: 0.80 - ETA: 1s - loss: 0.4193 - accuracy: 0.80 - ETA: 1s - loss: 0.4184 - accuracy: 0.80 - ETA: 1s - loss: 0.4191 - accuracy: 0.80 - ETA: 1s - loss: 0.4199 - accuracy: 0.80 - ETA: 1s - loss: 0.4198 - accuracy: 0.80 - ETA: 1s - loss: 0.4179 - accuracy: 0.80 - ETA: 1s - loss: 0.4182 - accuracy: 0.80 - ETA: 0s - loss: 0.4185 - accuracy: 0.80 - ETA: 0s - loss: 0.4209 - accuracy: 0.80 - ETA: 0s - loss: 0.4216 - accuracy: 0.80 - ETA: 0s - loss: 0.4216 - accuracy: 0.80 - ETA: 0s - loss: 0.4201 - accuracy: 0.80 - ETA: 0s - loss: 0.4199 - accuracy: 0.80 - ETA: 0s - loss: 0.4188 - accuracy: 0.80 - ETA: 0s - loss: 0.4209 - accuracy: 0.80 - ETA: 0s - loss: 0.4186 - accuracy: 0.80 - ETA: 0s - loss: 0.4194 - accuracy: 0.80 - ETA: 0s - loss: 0.4186 - accuracy: 0.80 - 3s 311us/step - loss: 0.4179 - accuracy: 0.8049 - val_loss: 0.3696 - val_accuracy: 0.8290\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.31344\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4376 - accuracy: 0.80 - ETA: 2s - loss: 0.4065 - accuracy: 0.82 - ETA: 2s - loss: 0.4050 - accuracy: 0.81 - ETA: 2s - loss: 0.4116 - accuracy: 0.80 - ETA: 2s - loss: 0.4133 - accuracy: 0.80 - ETA: 2s - loss: 0.4151 - accuracy: 0.80 - ETA: 2s - loss: 0.4204 - accuracy: 0.80 - ETA: 2s - loss: 0.4116 - accuracy: 0.80 - ETA: 2s - loss: 0.4103 - accuracy: 0.80 - ETA: 2s - loss: 0.4130 - accuracy: 0.80 - ETA: 2s - loss: 0.4204 - accuracy: 0.80 - ETA: 2s - loss: 0.4233 - accuracy: 0.79 - ETA: 2s - loss: 0.4244 - accuracy: 0.79 - ETA: 2s - loss: 0.4225 - accuracy: 0.79 - ETA: 2s - loss: 0.4205 - accuracy: 0.79 - ETA: 2s - loss: 0.4223 - accuracy: 0.79 - ETA: 1s - loss: 0.4213 - accuracy: 0.79 - ETA: 1s - loss: 0.4199 - accuracy: 0.79 - ETA: 1s - loss: 0.4165 - accuracy: 0.79 - ETA: 1s - loss: 0.4191 - accuracy: 0.79 - ETA: 1s - loss: 0.4174 - accuracy: 0.79 - ETA: 1s - loss: 0.4184 - accuracy: 0.79 - ETA: 1s - loss: 0.4172 - accuracy: 0.79 - ETA: 1s - loss: 0.4176 - accuracy: 0.79 - ETA: 1s - loss: 0.4166 - accuracy: 0.79 - ETA: 1s - loss: 0.4165 - accuracy: 0.79 - ETA: 1s - loss: 0.4134 - accuracy: 0.80 - ETA: 1s - loss: 0.4136 - accuracy: 0.80 - ETA: 1s - loss: 0.4125 - accuracy: 0.80 - ETA: 1s - loss: 0.4138 - accuracy: 0.80 - ETA: 1s - loss: 0.4142 - accuracy: 0.80 - ETA: 0s - loss: 0.4123 - accuracy: 0.80 - ETA: 0s - loss: 0.4111 - accuracy: 0.80 - ETA: 0s - loss: 0.4100 - accuracy: 0.80 - ETA: 0s - loss: 0.4085 - accuracy: 0.80 - ETA: 0s - loss: 0.4089 - accuracy: 0.80 - ETA: 0s - loss: 0.4106 - accuracy: 0.80 - ETA: 0s - loss: 0.4087 - accuracy: 0.80 - ETA: 0s - loss: 0.4078 - accuracy: 0.80 - ETA: 0s - loss: 0.4076 - accuracy: 0.80 - ETA: 0s - loss: 0.4079 - accuracy: 0.80 - ETA: 0s - loss: 0.4083 - accuracy: 0.80 - ETA: 0s - loss: 0.4070 - accuracy: 0.80 - 3s 316us/step - loss: 0.4075 - accuracy: 0.8074 - val_loss: 0.3765 - val_accuracy: 0.8307\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.31344\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4002 - accuracy: 0.82 - ETA: 2s - loss: 0.4133 - accuracy: 0.80 - ETA: 2s - loss: 0.4018 - accuracy: 0.80 - ETA: 2s - loss: 0.3963 - accuracy: 0.80 - ETA: 2s - loss: 0.3897 - accuracy: 0.81 - ETA: 2s - loss: 0.4000 - accuracy: 0.81 - ETA: 2s - loss: 0.4020 - accuracy: 0.80 - ETA: 2s - loss: 0.4038 - accuracy: 0.81 - ETA: 2s - loss: 0.4040 - accuracy: 0.80 - ETA: 2s - loss: 0.3988 - accuracy: 0.81 - ETA: 2s - loss: 0.3979 - accuracy: 0.81 - ETA: 2s - loss: 0.3927 - accuracy: 0.81 - ETA: 2s - loss: 0.3920 - accuracy: 0.81 - ETA: 2s - loss: 0.3865 - accuracy: 0.81 - ETA: 2s - loss: 0.3854 - accuracy: 0.81 - ETA: 2s - loss: 0.3864 - accuracy: 0.82 - ETA: 1s - loss: 0.3904 - accuracy: 0.81 - ETA: 1s - loss: 0.3911 - accuracy: 0.81 - ETA: 1s - loss: 0.3901 - accuracy: 0.81 - ETA: 1s - loss: 0.3927 - accuracy: 0.81 - ETA: 1s - loss: 0.3913 - accuracy: 0.81 - ETA: 1s - loss: 0.3887 - accuracy: 0.82 - ETA: 1s - loss: 0.3903 - accuracy: 0.82 - ETA: 1s - loss: 0.3898 - accuracy: 0.82 - ETA: 1s - loss: 0.3903 - accuracy: 0.82 - ETA: 1s - loss: 0.3923 - accuracy: 0.82 - ETA: 1s - loss: 0.3927 - accuracy: 0.82 - ETA: 1s - loss: 0.3928 - accuracy: 0.82 - ETA: 1s - loss: 0.3908 - accuracy: 0.82 - ETA: 1s - loss: 0.3904 - accuracy: 0.82 - ETA: 1s - loss: 0.3902 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3870 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3907 - accuracy: 0.81 - ETA: 0s - loss: 0.3913 - accuracy: 0.81 - ETA: 0s - loss: 0.3922 - accuracy: 0.81 - ETA: 0s - loss: 0.3930 - accuracy: 0.81 - ETA: 0s - loss: 0.3927 - accuracy: 0.81 - ETA: 0s - loss: 0.3931 - accuracy: 0.81 - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - ETA: 0s - loss: 0.3944 - accuracy: 0.81 - ETA: 0s - loss: 0.3942 - accuracy: 0.81 - 4s 349us/step - loss: 0.3951 - accuracy: 0.8157 - val_loss: 0.3945 - val_accuracy: 0.8054\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.31344\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3442 - accuracy: 0.86 - ETA: 4s - loss: 0.3791 - accuracy: 0.84 - ETA: 4s - loss: 0.3902 - accuracy: 0.83 - ETA: 4s - loss: 0.3870 - accuracy: 0.83 - ETA: 4s - loss: 0.3873 - accuracy: 0.83 - ETA: 3s - loss: 0.3929 - accuracy: 0.82 - ETA: 3s - loss: 0.3797 - accuracy: 0.83 - ETA: 3s - loss: 0.3755 - accuracy: 0.83 - ETA: 3s - loss: 0.3825 - accuracy: 0.83 - ETA: 3s - loss: 0.3855 - accuracy: 0.83 - ETA: 3s - loss: 0.3909 - accuracy: 0.83 - ETA: 3s - loss: 0.3925 - accuracy: 0.83 - ETA: 3s - loss: 0.3915 - accuracy: 0.83 - ETA: 2s - loss: 0.3890 - accuracy: 0.83 - ETA: 2s - loss: 0.3925 - accuracy: 0.82 - ETA: 2s - loss: 0.3959 - accuracy: 0.82 - ETA: 2s - loss: 0.3973 - accuracy: 0.82 - ETA: 2s - loss: 0.4020 - accuracy: 0.82 - ETA: 2s - loss: 0.4025 - accuracy: 0.81 - ETA: 2s - loss: 0.4023 - accuracy: 0.82 - ETA: 2s - loss: 0.4007 - accuracy: 0.82 - ETA: 2s - loss: 0.4065 - accuracy: 0.81 - ETA: 2s - loss: 0.4061 - accuracy: 0.81 - ETA: 2s - loss: 0.4029 - accuracy: 0.81 - ETA: 2s - loss: 0.4015 - accuracy: 0.81 - ETA: 2s - loss: 0.4004 - accuracy: 0.81 - ETA: 1s - loss: 0.4003 - accuracy: 0.81 - ETA: 1s - loss: 0.3993 - accuracy: 0.81 - ETA: 1s - loss: 0.3999 - accuracy: 0.81 - ETA: 1s - loss: 0.3977 - accuracy: 0.82 - ETA: 1s - loss: 0.3990 - accuracy: 0.81 - ETA: 1s - loss: 0.3986 - accuracy: 0.81 - ETA: 1s - loss: 0.3974 - accuracy: 0.81 - ETA: 1s - loss: 0.3949 - accuracy: 0.81 - ETA: 1s - loss: 0.3937 - accuracy: 0.82 - ETA: 1s - loss: 0.3925 - accuracy: 0.82 - ETA: 1s - loss: 0.3931 - accuracy: 0.82 - ETA: 1s - loss: 0.3934 - accuracy: 0.82 - ETA: 1s - loss: 0.3923 - accuracy: 0.82 - ETA: 1s - loss: 0.3909 - accuracy: 0.82 - ETA: 1s - loss: 0.3905 - accuracy: 0.82 - ETA: 1s - loss: 0.3896 - accuracy: 0.82 - ETA: 1s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3887 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.81 - ETA: 0s - loss: 0.3926 - accuracy: 0.81 - ETA: 0s - loss: 0.3933 - accuracy: 0.81 - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - ETA: 0s - loss: 0.3938 - accuracy: 0.81 - 4s 369us/step - loss: 0.3926 - accuracy: 0.8184 - val_loss: 0.3442 - val_accuracy: 0.8475\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.31344\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.4003 - accuracy: 0.75 - ETA: 3s - loss: 0.4289 - accuracy: 0.77 - ETA: 3s - loss: 0.4437 - accuracy: 0.76 - ETA: 2s - loss: 0.4247 - accuracy: 0.78 - ETA: 2s - loss: 0.4217 - accuracy: 0.80 - ETA: 2s - loss: 0.4348 - accuracy: 0.79 - ETA: 2s - loss: 0.4271 - accuracy: 0.80 - ETA: 2s - loss: 0.4252 - accuracy: 0.80 - ETA: 2s - loss: 0.4165 - accuracy: 0.80 - ETA: 2s - loss: 0.4126 - accuracy: 0.80 - ETA: 2s - loss: 0.4084 - accuracy: 0.81 - ETA: 2s - loss: 0.4058 - accuracy: 0.81 - ETA: 2s - loss: 0.4067 - accuracy: 0.81 - ETA: 2s - loss: 0.4070 - accuracy: 0.81 - ETA: 2s - loss: 0.4034 - accuracy: 0.81 - ETA: 2s - loss: 0.4010 - accuracy: 0.81 - ETA: 2s - loss: 0.4004 - accuracy: 0.81 - ETA: 2s - loss: 0.4034 - accuracy: 0.81 - ETA: 1s - loss: 0.4022 - accuracy: 0.81 - ETA: 1s - loss: 0.3998 - accuracy: 0.81 - ETA: 1s - loss: 0.3965 - accuracy: 0.81 - ETA: 1s - loss: 0.3913 - accuracy: 0.82 - ETA: 1s - loss: 0.3922 - accuracy: 0.82 - ETA: 1s - loss: 0.3914 - accuracy: 0.82 - ETA: 1s - loss: 0.3928 - accuracy: 0.82 - ETA: 1s - loss: 0.3940 - accuracy: 0.81 - ETA: 1s - loss: 0.3950 - accuracy: 0.81 - ETA: 1s - loss: 0.3934 - accuracy: 0.82 - ETA: 1s - loss: 0.3926 - accuracy: 0.82 - ETA: 1s - loss: 0.3932 - accuracy: 0.82 - ETA: 1s - loss: 0.3958 - accuracy: 0.82 - ETA: 1s - loss: 0.3954 - accuracy: 0.82 - ETA: 1s - loss: 0.3942 - accuracy: 0.82 - ETA: 1s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3943 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.3945 - accuracy: 0.82 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.81 - ETA: 0s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - 4s 329us/step - loss: 0.3937 - accuracy: 0.8205 - val_loss: 0.3988 - val_accuracy: 0.8155\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.31344\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3482 - accuracy: 0.85 - ETA: 3s - loss: 0.3497 - accuracy: 0.85 - ETA: 3s - loss: 0.3707 - accuracy: 0.84 - ETA: 3s - loss: 0.3760 - accuracy: 0.83 - ETA: 2s - loss: 0.3802 - accuracy: 0.83 - ETA: 2s - loss: 0.3873 - accuracy: 0.83 - ETA: 2s - loss: 0.3955 - accuracy: 0.82 - ETA: 2s - loss: 0.3991 - accuracy: 0.82 - ETA: 2s - loss: 0.3969 - accuracy: 0.82 - ETA: 2s - loss: 0.3897 - accuracy: 0.82 - ETA: 2s - loss: 0.3854 - accuracy: 0.82 - ETA: 2s - loss: 0.3871 - accuracy: 0.82 - ETA: 2s - loss: 0.3870 - accuracy: 0.82 - ETA: 2s - loss: 0.3885 - accuracy: 0.82 - ETA: 2s - loss: 0.3866 - accuracy: 0.82 - ETA: 2s - loss: 0.3861 - accuracy: 0.82 - ETA: 2s - loss: 0.3875 - accuracy: 0.82 - ETA: 1s - loss: 0.3880 - accuracy: 0.82 - ETA: 1s - loss: 0.3901 - accuracy: 0.82 - ETA: 1s - loss: 0.3898 - accuracy: 0.82 - ETA: 1s - loss: 0.3891 - accuracy: 0.82 - ETA: 1s - loss: 0.3874 - accuracy: 0.82 - ETA: 1s - loss: 0.3878 - accuracy: 0.82 - ETA: 1s - loss: 0.3909 - accuracy: 0.82 - ETA: 1s - loss: 0.3893 - accuracy: 0.82 - ETA: 1s - loss: 0.3892 - accuracy: 0.82 - ETA: 1s - loss: 0.3877 - accuracy: 0.82 - ETA: 1s - loss: 0.3877 - accuracy: 0.82 - ETA: 1s - loss: 0.3878 - accuracy: 0.82 - ETA: 1s - loss: 0.3874 - accuracy: 0.82 - ETA: 1s - loss: 0.3860 - accuracy: 0.82 - ETA: 1s - loss: 0.3851 - accuracy: 0.82 - ETA: 0s - loss: 0.3834 - accuracy: 0.82 - ETA: 0s - loss: 0.3851 - accuracy: 0.82 - ETA: 0s - loss: 0.3846 - accuracy: 0.82 - ETA: 0s - loss: 0.3836 - accuracy: 0.82 - ETA: 0s - loss: 0.3833 - accuracy: 0.82 - ETA: 0s - loss: 0.3837 - accuracy: 0.82 - ETA: 0s - loss: 0.3828 - accuracy: 0.82 - ETA: 0s - loss: 0.3819 - accuracy: 0.82 - ETA: 0s - loss: 0.3814 - accuracy: 0.82 - ETA: 0s - loss: 0.3833 - accuracy: 0.82 - ETA: 0s - loss: 0.3831 - accuracy: 0.82 - ETA: 0s - loss: 0.3845 - accuracy: 0.82 - ETA: 0s - loss: 0.3831 - accuracy: 0.82 - 3s 320us/step - loss: 0.3849 - accuracy: 0.8264 - val_loss: 0.3651 - val_accuracy: 0.8509\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.31344\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3994 - accuracy: 0.83 - ETA: 2s - loss: 0.3643 - accuracy: 0.83 - ETA: 2s - loss: 0.3862 - accuracy: 0.81 - ETA: 2s - loss: 0.3893 - accuracy: 0.82 - ETA: 2s - loss: 0.3875 - accuracy: 0.82 - ETA: 2s - loss: 0.3889 - accuracy: 0.82 - ETA: 2s - loss: 0.4070 - accuracy: 0.82 - ETA: 2s - loss: 0.4031 - accuracy: 0.82 - ETA: 2s - loss: 0.4084 - accuracy: 0.82 - ETA: 2s - loss: 0.4037 - accuracy: 0.81 - ETA: 2s - loss: 0.4029 - accuracy: 0.81 - ETA: 2s - loss: 0.4011 - accuracy: 0.81 - ETA: 2s - loss: 0.4025 - accuracy: 0.81 - ETA: 2s - loss: 0.3970 - accuracy: 0.81 - ETA: 2s - loss: 0.3943 - accuracy: 0.81 - ETA: 2s - loss: 0.3970 - accuracy: 0.81 - ETA: 2s - loss: 0.3964 - accuracy: 0.81 - ETA: 2s - loss: 0.3946 - accuracy: 0.81 - ETA: 1s - loss: 0.3924 - accuracy: 0.81 - ETA: 1s - loss: 0.3897 - accuracy: 0.81 - ETA: 1s - loss: 0.3913 - accuracy: 0.81 - ETA: 1s - loss: 0.3896 - accuracy: 0.81 - ETA: 1s - loss: 0.3908 - accuracy: 0.81 - ETA: 1s - loss: 0.3914 - accuracy: 0.81 - ETA: 1s - loss: 0.3904 - accuracy: 0.81 - ETA: 1s - loss: 0.3894 - accuracy: 0.82 - ETA: 1s - loss: 0.3884 - accuracy: 0.82 - ETA: 1s - loss: 0.3888 - accuracy: 0.82 - ETA: 1s - loss: 0.3892 - accuracy: 0.82 - ETA: 1s - loss: 0.3889 - accuracy: 0.81 - ETA: 1s - loss: 0.3887 - accuracy: 0.81 - ETA: 1s - loss: 0.3895 - accuracy: 0.81 - ETA: 1s - loss: 0.3886 - accuracy: 0.82 - ETA: 1s - loss: 0.3902 - accuracy: 0.81 - ETA: 1s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.3856 - accuracy: 0.82 - ETA: 0s - loss: 0.3857 - accuracy: 0.82 - ETA: 0s - loss: 0.3848 - accuracy: 0.82 - ETA: 0s - loss: 0.3849 - accuracy: 0.82 - ETA: 0s - loss: 0.3842 - accuracy: 0.82 - ETA: 0s - loss: 0.3853 - accuracy: 0.82 - ETA: 0s - loss: 0.3849 - accuracy: 0.82 - ETA: 0s - loss: 0.3844 - accuracy: 0.82 - ETA: 0s - loss: 0.3829 - accuracy: 0.82 - ETA: 0s - loss: 0.3826 - accuracy: 0.82 - ETA: 0s - loss: 0.3820 - accuracy: 0.82 - ETA: 0s - loss: 0.3825 - accuracy: 0.82 - ETA: 0s - loss: 0.3830 - accuracy: 0.82 - ETA: 0s - loss: 0.3819 - accuracy: 0.82 - ETA: 0s - loss: 0.3832 - accuracy: 0.82 - ETA: 0s - loss: 0.3817 - accuracy: 0.82 - 4s 349us/step - loss: 0.3809 - accuracy: 0.8245 - val_loss: 0.3460 - val_accuracy: 0.8492\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31344\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3070 - accuracy: 0.82 - ETA: 4s - loss: 0.3089 - accuracy: 0.83 - ETA: 4s - loss: 0.3634 - accuracy: 0.82 - ETA: 4s - loss: 0.3827 - accuracy: 0.82 - ETA: 4s - loss: 0.3767 - accuracy: 0.82 - ETA: 4s - loss: 0.3806 - accuracy: 0.81 - ETA: 3s - loss: 0.3891 - accuracy: 0.81 - ETA: 3s - loss: 0.3882 - accuracy: 0.81 - ETA: 3s - loss: 0.3884 - accuracy: 0.82 - ETA: 3s - loss: 0.3916 - accuracy: 0.81 - ETA: 3s - loss: 0.3891 - accuracy: 0.81 - ETA: 3s - loss: 0.3898 - accuracy: 0.81 - ETA: 3s - loss: 0.3925 - accuracy: 0.81 - ETA: 3s - loss: 0.3900 - accuracy: 0.81 - ETA: 3s - loss: 0.3799 - accuracy: 0.82 - ETA: 3s - loss: 0.3798 - accuracy: 0.82 - ETA: 3s - loss: 0.3760 - accuracy: 0.82 - ETA: 2s - loss: 0.3771 - accuracy: 0.82 - ETA: 2s - loss: 0.3799 - accuracy: 0.82 - ETA: 2s - loss: 0.3808 - accuracy: 0.82 - ETA: 2s - loss: 0.3799 - accuracy: 0.82 - ETA: 2s - loss: 0.3823 - accuracy: 0.82 - ETA: 2s - loss: 0.3805 - accuracy: 0.82 - ETA: 2s - loss: 0.3792 - accuracy: 0.82 - ETA: 2s - loss: 0.3803 - accuracy: 0.82 - ETA: 2s - loss: 0.3813 - accuracy: 0.82 - ETA: 2s - loss: 0.3785 - accuracy: 0.82 - ETA: 2s - loss: 0.3770 - accuracy: 0.83 - ETA: 2s - loss: 0.3763 - accuracy: 0.82 - ETA: 2s - loss: 0.3757 - accuracy: 0.82 - ETA: 2s - loss: 0.3763 - accuracy: 0.82 - ETA: 2s - loss: 0.3761 - accuracy: 0.82 - ETA: 2s - loss: 0.3760 - accuracy: 0.82 - ETA: 1s - loss: 0.3771 - accuracy: 0.82 - ETA: 1s - loss: 0.3749 - accuracy: 0.83 - ETA: 1s - loss: 0.3757 - accuracy: 0.82 - ETA: 1s - loss: 0.3761 - accuracy: 0.82 - ETA: 1s - loss: 0.3767 - accuracy: 0.83 - ETA: 1s - loss: 0.3762 - accuracy: 0.82 - ETA: 1s - loss: 0.3757 - accuracy: 0.82 - ETA: 1s - loss: 0.3746 - accuracy: 0.82 - ETA: 1s - loss: 0.3746 - accuracy: 0.82 - ETA: 1s - loss: 0.3724 - accuracy: 0.83 - ETA: 1s - loss: 0.3729 - accuracy: 0.82 - ETA: 1s - loss: 0.3719 - accuracy: 0.82 - ETA: 0s - loss: 0.3695 - accuracy: 0.83 - ETA: 0s - loss: 0.3705 - accuracy: 0.82 - ETA: 0s - loss: 0.3688 - accuracy: 0.83 - ETA: 0s - loss: 0.3703 - accuracy: 0.82 - ETA: 0s - loss: 0.3702 - accuracy: 0.82 - ETA: 0s - loss: 0.3708 - accuracy: 0.83 - ETA: 0s - loss: 0.3706 - accuracy: 0.83 - ETA: 0s - loss: 0.3715 - accuracy: 0.82 - ETA: 0s - loss: 0.3723 - accuracy: 0.82 - ETA: 0s - loss: 0.3723 - accuracy: 0.82 - ETA: 0s - loss: 0.3734 - accuracy: 0.82 - ETA: 0s - loss: 0.3729 - accuracy: 0.82 - ETA: 0s - loss: 0.3723 - accuracy: 0.82 - ETA: 0s - loss: 0.3719 - accuracy: 0.83 - 4s 362us/step - loss: 0.3722 - accuracy: 0.8301 - val_loss: 0.3837 - val_accuracy: 0.8290\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31344\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.5093 - accuracy: 0.79 - ETA: 3s - loss: 0.4616 - accuracy: 0.80 - ETA: 3s - loss: 0.4142 - accuracy: 0.82 - ETA: 3s - loss: 0.4197 - accuracy: 0.81 - ETA: 3s - loss: 0.4130 - accuracy: 0.81 - ETA: 3s - loss: 0.4027 - accuracy: 0.81 - ETA: 3s - loss: 0.4025 - accuracy: 0.82 - ETA: 3s - loss: 0.3968 - accuracy: 0.82 - ETA: 3s - loss: 0.3928 - accuracy: 0.82 - ETA: 3s - loss: 0.3859 - accuracy: 0.83 - ETA: 3s - loss: 0.3865 - accuracy: 0.83 - ETA: 3s - loss: 0.3827 - accuracy: 0.83 - ETA: 2s - loss: 0.3813 - accuracy: 0.83 - ETA: 2s - loss: 0.3860 - accuracy: 0.83 - ETA: 2s - loss: 0.3849 - accuracy: 0.83 - ETA: 2s - loss: 0.3815 - accuracy: 0.83 - ETA: 2s - loss: 0.3817 - accuracy: 0.83 - ETA: 2s - loss: 0.3815 - accuracy: 0.83 - ETA: 2s - loss: 0.3814 - accuracy: 0.83 - ETA: 2s - loss: 0.3736 - accuracy: 0.83 - ETA: 2s - loss: 0.3746 - accuracy: 0.83 - ETA: 1s - loss: 0.3763 - accuracy: 0.83 - ETA: 1s - loss: 0.3714 - accuracy: 0.83 - ETA: 1s - loss: 0.3701 - accuracy: 0.83 - ETA: 1s - loss: 0.3697 - accuracy: 0.83 - ETA: 1s - loss: 0.3695 - accuracy: 0.83 - ETA: 1s - loss: 0.3681 - accuracy: 0.83 - ETA: 1s - loss: 0.3681 - accuracy: 0.83 - ETA: 1s - loss: 0.3680 - accuracy: 0.83 - ETA: 1s - loss: 0.3663 - accuracy: 0.83 - ETA: 1s - loss: 0.3649 - accuracy: 0.83 - ETA: 1s - loss: 0.3646 - accuracy: 0.83 - ETA: 1s - loss: 0.3634 - accuracy: 0.83 - ETA: 0s - loss: 0.3637 - accuracy: 0.83 - ETA: 0s - loss: 0.3663 - accuracy: 0.83 - ETA: 0s - loss: 0.3657 - accuracy: 0.83 - ETA: 0s - loss: 0.3672 - accuracy: 0.83 - ETA: 0s - loss: 0.3695 - accuracy: 0.83 - ETA: 0s - loss: 0.3699 - accuracy: 0.83 - ETA: 0s - loss: 0.3694 - accuracy: 0.83 - ETA: 0s - loss: 0.3686 - accuracy: 0.83 - ETA: 0s - loss: 0.3717 - accuracy: 0.83 - ETA: 0s - loss: 0.3714 - accuracy: 0.83 - ETA: 0s - loss: 0.3713 - accuracy: 0.83 - ETA: 0s - loss: 0.3706 - accuracy: 0.83 - ETA: 0s - loss: 0.3696 - accuracy: 0.83 - 3s 322us/step - loss: 0.3699 - accuracy: 0.8333 - val_loss: 0.3360 - val_accuracy: 0.8602\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31344\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3370 - accuracy: 0.82 - ETA: 2s - loss: 0.3617 - accuracy: 0.81 - ETA: 2s - loss: 0.3572 - accuracy: 0.82 - ETA: 2s - loss: 0.3394 - accuracy: 0.83 - ETA: 2s - loss: 0.3306 - accuracy: 0.84 - ETA: 2s - loss: 0.3332 - accuracy: 0.84 - ETA: 2s - loss: 0.3287 - accuracy: 0.84 - ETA: 2s - loss: 0.3275 - accuracy: 0.84 - ETA: 2s - loss: 0.3284 - accuracy: 0.84 - ETA: 2s - loss: 0.3275 - accuracy: 0.84 - ETA: 2s - loss: 0.3291 - accuracy: 0.84 - ETA: 2s - loss: 0.3322 - accuracy: 0.84 - ETA: 2s - loss: 0.3308 - accuracy: 0.84 - ETA: 2s - loss: 0.3336 - accuracy: 0.84 - ETA: 2s - loss: 0.3416 - accuracy: 0.84 - ETA: 2s - loss: 0.3475 - accuracy: 0.83 - ETA: 1s - loss: 0.3487 - accuracy: 0.83 - ETA: 1s - loss: 0.3496 - accuracy: 0.83 - ETA: 1s - loss: 0.3506 - accuracy: 0.83 - ETA: 1s - loss: 0.3478 - accuracy: 0.83 - ETA: 1s - loss: 0.3496 - accuracy: 0.83 - ETA: 1s - loss: 0.3499 - accuracy: 0.83 - ETA: 1s - loss: 0.3549 - accuracy: 0.83 - ETA: 1s - loss: 0.3581 - accuracy: 0.83 - ETA: 1s - loss: 0.3575 - accuracy: 0.83 - ETA: 1s - loss: 0.3556 - accuracy: 0.83 - ETA: 1s - loss: 0.3543 - accuracy: 0.83 - ETA: 1s - loss: 0.3559 - accuracy: 0.83 - ETA: 0s - loss: 0.3590 - accuracy: 0.83 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - ETA: 0s - loss: 0.3596 - accuracy: 0.83 - ETA: 0s - loss: 0.3607 - accuracy: 0.83 - ETA: 0s - loss: 0.3606 - accuracy: 0.83 - ETA: 0s - loss: 0.3609 - accuracy: 0.83 - ETA: 0s - loss: 0.3606 - accuracy: 0.83 - ETA: 0s - loss: 0.3601 - accuracy: 0.83 - ETA: 0s - loss: 0.3602 - accuracy: 0.83 - ETA: 0s - loss: 0.3619 - accuracy: 0.83 - ETA: 0s - loss: 0.3616 - accuracy: 0.83 - ETA: 0s - loss: 0.3643 - accuracy: 0.83 - ETA: 0s - loss: 0.3636 - accuracy: 0.83 - ETA: 0s - loss: 0.3646 - accuracy: 0.83 - 3s 312us/step - loss: 0.3648 - accuracy: 0.8319 - val_loss: 0.3592 - val_accuracy: 0.8551\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31344\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3473 - accuracy: 0.84 - ETA: 2s - loss: 0.3810 - accuracy: 0.81 - ETA: 2s - loss: 0.3732 - accuracy: 0.82 - ETA: 2s - loss: 0.3634 - accuracy: 0.84 - ETA: 2s - loss: 0.3467 - accuracy: 0.84 - ETA: 2s - loss: 0.3459 - accuracy: 0.84 - ETA: 2s - loss: 0.3481 - accuracy: 0.84 - ETA: 2s - loss: 0.3557 - accuracy: 0.83 - ETA: 2s - loss: 0.3499 - accuracy: 0.84 - ETA: 2s - loss: 0.3540 - accuracy: 0.83 - ETA: 2s - loss: 0.3581 - accuracy: 0.83 - ETA: 2s - loss: 0.3544 - accuracy: 0.84 - ETA: 2s - loss: 0.3500 - accuracy: 0.84 - ETA: 2s - loss: 0.3491 - accuracy: 0.84 - ETA: 2s - loss: 0.3498 - accuracy: 0.84 - ETA: 1s - loss: 0.3525 - accuracy: 0.84 - ETA: 1s - loss: 0.3543 - accuracy: 0.84 - ETA: 1s - loss: 0.3545 - accuracy: 0.83 - ETA: 1s - loss: 0.3540 - accuracy: 0.83 - ETA: 1s - loss: 0.3550 - accuracy: 0.83 - ETA: 1s - loss: 0.3556 - accuracy: 0.83 - ETA: 1s - loss: 0.3569 - accuracy: 0.83 - ETA: 1s - loss: 0.3571 - accuracy: 0.83 - ETA: 1s - loss: 0.3544 - accuracy: 0.83 - ETA: 1s - loss: 0.3549 - accuracy: 0.83 - ETA: 1s - loss: 0.3524 - accuracy: 0.84 - ETA: 1s - loss: 0.3554 - accuracy: 0.84 - ETA: 1s - loss: 0.3526 - accuracy: 0.84 - ETA: 0s - loss: 0.3533 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3550 - accuracy: 0.84 - ETA: 0s - loss: 0.3570 - accuracy: 0.83 - ETA: 0s - loss: 0.3604 - accuracy: 0.83 - ETA: 0s - loss: 0.3604 - accuracy: 0.83 - ETA: 0s - loss: 0.3606 - accuracy: 0.83 - ETA: 0s - loss: 0.3617 - accuracy: 0.83 - ETA: 0s - loss: 0.3608 - accuracy: 0.83 - ETA: 0s - loss: 0.3617 - accuracy: 0.83 - ETA: 0s - loss: 0.3619 - accuracy: 0.83 - ETA: 0s - loss: 0.3624 - accuracy: 0.83 - 3s 303us/step - loss: 0.3630 - accuracy: 0.8344 - val_loss: 0.3536 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31344\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3351 - accuracy: 0.85 - ETA: 2s - loss: 0.3695 - accuracy: 0.83 - ETA: 2s - loss: 0.3562 - accuracy: 0.84 - ETA: 2s - loss: 0.3596 - accuracy: 0.83 - ETA: 2s - loss: 0.3621 - accuracy: 0.83 - ETA: 2s - loss: 0.3586 - accuracy: 0.83 - ETA: 2s - loss: 0.3506 - accuracy: 0.83 - ETA: 2s - loss: 0.3479 - accuracy: 0.84 - ETA: 2s - loss: 0.3489 - accuracy: 0.84 - ETA: 2s - loss: 0.3520 - accuracy: 0.83 - ETA: 2s - loss: 0.3554 - accuracy: 0.83 - ETA: 2s - loss: 0.3565 - accuracy: 0.83 - ETA: 2s - loss: 0.3549 - accuracy: 0.83 - ETA: 2s - loss: 0.3561 - accuracy: 0.83 - ETA: 1s - loss: 0.3559 - accuracy: 0.83 - ETA: 1s - loss: 0.3599 - accuracy: 0.83 - ETA: 1s - loss: 0.3632 - accuracy: 0.83 - ETA: 1s - loss: 0.3638 - accuracy: 0.83 - ETA: 1s - loss: 0.3663 - accuracy: 0.83 - ETA: 1s - loss: 0.3656 - accuracy: 0.83 - ETA: 1s - loss: 0.3645 - accuracy: 0.83 - ETA: 1s - loss: 0.3651 - accuracy: 0.83 - ETA: 1s - loss: 0.3647 - accuracy: 0.83 - ETA: 1s - loss: 0.3632 - accuracy: 0.83 - ETA: 1s - loss: 0.3606 - accuracy: 0.83 - ETA: 1s - loss: 0.3590 - accuracy: 0.83 - ETA: 1s - loss: 0.3600 - accuracy: 0.83 - ETA: 1s - loss: 0.3606 - accuracy: 0.83 - ETA: 1s - loss: 0.3618 - accuracy: 0.83 - ETA: 0s - loss: 0.3623 - accuracy: 0.83 - ETA: 0s - loss: 0.3622 - accuracy: 0.83 - ETA: 0s - loss: 0.3601 - accuracy: 0.83 - ETA: 0s - loss: 0.3592 - accuracy: 0.83 - ETA: 0s - loss: 0.3606 - accuracy: 0.83 - ETA: 0s - loss: 0.3607 - accuracy: 0.83 - ETA: 0s - loss: 0.3615 - accuracy: 0.83 - ETA: 0s - loss: 0.3610 - accuracy: 0.83 - ETA: 0s - loss: 0.3611 - accuracy: 0.83 - ETA: 0s - loss: 0.3625 - accuracy: 0.83 - ETA: 0s - loss: 0.3621 - accuracy: 0.83 - ETA: 0s - loss: 0.3627 - accuracy: 0.83 - ETA: 0s - loss: 0.3617 - accuracy: 0.83 - ETA: 0s - loss: 0.3621 - accuracy: 0.83 - ETA: 0s - loss: 0.3625 - accuracy: 0.83 - 3s 319us/step - loss: 0.3618 - accuracy: 0.8356 - val_loss: 0.3483 - val_accuracy: 0.8450\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31344\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3113 - accuracy: 0.81 - ETA: 3s - loss: 0.3268 - accuracy: 0.83 - ETA: 2s - loss: 0.3280 - accuracy: 0.84 - ETA: 2s - loss: 0.3262 - accuracy: 0.84 - ETA: 2s - loss: 0.3361 - accuracy: 0.84 - ETA: 2s - loss: 0.3348 - accuracy: 0.83 - ETA: 2s - loss: 0.3390 - accuracy: 0.83 - ETA: 2s - loss: 0.3518 - accuracy: 0.82 - ETA: 2s - loss: 0.3553 - accuracy: 0.82 - ETA: 2s - loss: 0.3520 - accuracy: 0.83 - ETA: 2s - loss: 0.3465 - accuracy: 0.83 - ETA: 2s - loss: 0.3532 - accuracy: 0.83 - ETA: 2s - loss: 0.3566 - accuracy: 0.82 - ETA: 2s - loss: 0.3549 - accuracy: 0.82 - ETA: 2s - loss: 0.3583 - accuracy: 0.82 - ETA: 2s - loss: 0.3566 - accuracy: 0.82 - ETA: 2s - loss: 0.3555 - accuracy: 0.82 - ETA: 2s - loss: 0.3605 - accuracy: 0.82 - ETA: 2s - loss: 0.3563 - accuracy: 0.82 - ETA: 2s - loss: 0.3561 - accuracy: 0.82 - ETA: 1s - loss: 0.3560 - accuracy: 0.82 - ETA: 1s - loss: 0.3542 - accuracy: 0.83 - ETA: 1s - loss: 0.3566 - accuracy: 0.82 - ETA: 1s - loss: 0.3570 - accuracy: 0.82 - ETA: 1s - loss: 0.3564 - accuracy: 0.83 - ETA: 1s - loss: 0.3577 - accuracy: 0.83 - ETA: 1s - loss: 0.3581 - accuracy: 0.82 - ETA: 1s - loss: 0.3575 - accuracy: 0.83 - ETA: 1s - loss: 0.3586 - accuracy: 0.82 - ETA: 1s - loss: 0.3573 - accuracy: 0.83 - ETA: 1s - loss: 0.3567 - accuracy: 0.83 - ETA: 1s - loss: 0.3575 - accuracy: 0.83 - ETA: 1s - loss: 0.3576 - accuracy: 0.83 - ETA: 1s - loss: 0.3567 - accuracy: 0.83 - ETA: 1s - loss: 0.3583 - accuracy: 0.82 - ETA: 1s - loss: 0.3585 - accuracy: 0.82 - ETA: 1s - loss: 0.3576 - accuracy: 0.82 - ETA: 1s - loss: 0.3579 - accuracy: 0.83 - ETA: 1s - loss: 0.3574 - accuracy: 0.83 - ETA: 1s - loss: 0.3573 - accuracy: 0.83 - ETA: 1s - loss: 0.3573 - accuracy: 0.83 - ETA: 0s - loss: 0.3568 - accuracy: 0.83 - ETA: 0s - loss: 0.3566 - accuracy: 0.83 - ETA: 0s - loss: 0.3573 - accuracy: 0.83 - ETA: 0s - loss: 0.3567 - accuracy: 0.83 - ETA: 0s - loss: 0.3573 - accuracy: 0.83 - ETA: 0s - loss: 0.3576 - accuracy: 0.83 - ETA: 0s - loss: 0.3572 - accuracy: 0.83 - ETA: 0s - loss: 0.3580 - accuracy: 0.83 - ETA: 0s - loss: 0.3564 - accuracy: 0.83 - ETA: 0s - loss: 0.3569 - accuracy: 0.83 - ETA: 0s - loss: 0.3557 - accuracy: 0.83 - ETA: 0s - loss: 0.3553 - accuracy: 0.83 - ETA: 0s - loss: 0.3556 - accuracy: 0.83 - ETA: 0s - loss: 0.3559 - accuracy: 0.83 - ETA: 0s - loss: 0.3552 - accuracy: 0.83 - ETA: 0s - loss: 0.3561 - accuracy: 0.83 - ETA: 0s - loss: 0.3551 - accuracy: 0.83 - ETA: 0s - loss: 0.3554 - accuracy: 0.83 - ETA: 0s - loss: 0.3549 - accuracy: 0.83 - ETA: 0s - loss: 0.3552 - accuracy: 0.83 - 4s 373us/step - loss: 0.3552 - accuracy: 0.8361 - val_loss: 0.3513 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.31344\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.2488 - accuracy: 0.87 - ETA: 3s - loss: 0.3485 - accuracy: 0.81 - ETA: 3s - loss: 0.3578 - accuracy: 0.81 - ETA: 3s - loss: 0.3546 - accuracy: 0.82 - ETA: 3s - loss: 0.3491 - accuracy: 0.83 - ETA: 3s - loss: 0.3664 - accuracy: 0.82 - ETA: 3s - loss: 0.3656 - accuracy: 0.82 - ETA: 3s - loss: 0.3678 - accuracy: 0.82 - ETA: 3s - loss: 0.3612 - accuracy: 0.82 - ETA: 3s - loss: 0.3572 - accuracy: 0.83 - ETA: 3s - loss: 0.3502 - accuracy: 0.83 - ETA: 3s - loss: 0.3505 - accuracy: 0.83 - ETA: 3s - loss: 0.3507 - accuracy: 0.83 - ETA: 3s - loss: 0.3488 - accuracy: 0.83 - ETA: 3s - loss: 0.3493 - accuracy: 0.83 - ETA: 3s - loss: 0.3477 - accuracy: 0.83 - ETA: 3s - loss: 0.3526 - accuracy: 0.83 - ETA: 2s - loss: 0.3527 - accuracy: 0.83 - ETA: 2s - loss: 0.3536 - accuracy: 0.84 - ETA: 2s - loss: 0.3532 - accuracy: 0.84 - ETA: 2s - loss: 0.3532 - accuracy: 0.83 - ETA: 2s - loss: 0.3541 - accuracy: 0.83 - ETA: 2s - loss: 0.3533 - accuracy: 0.83 - ETA: 2s - loss: 0.3542 - accuracy: 0.83 - ETA: 2s - loss: 0.3560 - accuracy: 0.83 - ETA: 2s - loss: 0.3569 - accuracy: 0.83 - ETA: 2s - loss: 0.3554 - accuracy: 0.83 - ETA: 2s - loss: 0.3560 - accuracy: 0.83 - ETA: 1s - loss: 0.3576 - accuracy: 0.83 - ETA: 1s - loss: 0.3572 - accuracy: 0.83 - ETA: 1s - loss: 0.3579 - accuracy: 0.83 - ETA: 1s - loss: 0.3596 - accuracy: 0.83 - ETA: 1s - loss: 0.3601 - accuracy: 0.83 - ETA: 1s - loss: 0.3595 - accuracy: 0.83 - ETA: 1s - loss: 0.3598 - accuracy: 0.83 - ETA: 1s - loss: 0.3616 - accuracy: 0.83 - ETA: 1s - loss: 0.3619 - accuracy: 0.83 - ETA: 1s - loss: 0.3606 - accuracy: 0.83 - ETA: 1s - loss: 0.3609 - accuracy: 0.83 - ETA: 1s - loss: 0.3597 - accuracy: 0.83 - ETA: 1s - loss: 0.3597 - accuracy: 0.83 - ETA: 0s - loss: 0.3586 - accuracy: 0.83 - ETA: 0s - loss: 0.3596 - accuracy: 0.83 - ETA: 0s - loss: 0.3618 - accuracy: 0.83 - ETA: 0s - loss: 0.3608 - accuracy: 0.83 - ETA: 0s - loss: 0.3614 - accuracy: 0.83 - ETA: 0s - loss: 0.3612 - accuracy: 0.83 - ETA: 0s - loss: 0.3619 - accuracy: 0.83 - ETA: 0s - loss: 0.3608 - accuracy: 0.83 - ETA: 0s - loss: 0.3600 - accuracy: 0.83 - ETA: 0s - loss: 0.3588 - accuracy: 0.83 - ETA: 0s - loss: 0.3583 - accuracy: 0.83 - ETA: 0s - loss: 0.3584 - accuracy: 0.84 - ETA: 0s - loss: 0.3592 - accuracy: 0.83 - ETA: 0s - loss: 0.3587 - accuracy: 0.83 - 4s 361us/step - loss: 0.3588 - accuracy: 0.8397 - val_loss: 0.3325 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31344\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3141 - accuracy: 0.86 - ETA: 3s - loss: 0.3268 - accuracy: 0.86 - ETA: 3s - loss: 0.3151 - accuracy: 0.87 - ETA: 3s - loss: 0.3454 - accuracy: 0.85 - ETA: 3s - loss: 0.3421 - accuracy: 0.84 - ETA: 3s - loss: 0.3432 - accuracy: 0.84 - ETA: 3s - loss: 0.3580 - accuracy: 0.84 - ETA: 3s - loss: 0.3555 - accuracy: 0.84 - ETA: 3s - loss: 0.3529 - accuracy: 0.85 - ETA: 3s - loss: 0.3491 - accuracy: 0.84 - ETA: 3s - loss: 0.3526 - accuracy: 0.84 - ETA: 2s - loss: 0.3558 - accuracy: 0.84 - ETA: 2s - loss: 0.3529 - accuracy: 0.84 - ETA: 2s - loss: 0.3464 - accuracy: 0.85 - ETA: 2s - loss: 0.3510 - accuracy: 0.84 - ETA: 2s - loss: 0.3574 - accuracy: 0.84 - ETA: 2s - loss: 0.3570 - accuracy: 0.84 - ETA: 2s - loss: 0.3542 - accuracy: 0.84 - ETA: 2s - loss: 0.3537 - accuracy: 0.84 - ETA: 2s - loss: 0.3519 - accuracy: 0.84 - ETA: 2s - loss: 0.3532 - accuracy: 0.84 - ETA: 2s - loss: 0.3541 - accuracy: 0.84 - ETA: 2s - loss: 0.3564 - accuracy: 0.84 - ETA: 2s - loss: 0.3567 - accuracy: 0.84 - ETA: 1s - loss: 0.3579 - accuracy: 0.84 - ETA: 1s - loss: 0.3586 - accuracy: 0.84 - ETA: 1s - loss: 0.3578 - accuracy: 0.83 - ETA: 1s - loss: 0.3575 - accuracy: 0.83 - ETA: 1s - loss: 0.3569 - accuracy: 0.83 - ETA: 1s - loss: 0.3553 - accuracy: 0.84 - ETA: 1s - loss: 0.3562 - accuracy: 0.83 - ETA: 1s - loss: 0.3553 - accuracy: 0.83 - ETA: 1s - loss: 0.3550 - accuracy: 0.83 - ETA: 1s - loss: 0.3542 - accuracy: 0.83 - ETA: 1s - loss: 0.3534 - accuracy: 0.83 - ETA: 1s - loss: 0.3520 - accuracy: 0.83 - ETA: 1s - loss: 0.3515 - accuracy: 0.83 - ETA: 1s - loss: 0.3517 - accuracy: 0.83 - ETA: 1s - loss: 0.3516 - accuracy: 0.83 - ETA: 0s - loss: 0.3514 - accuracy: 0.83 - ETA: 0s - loss: 0.3528 - accuracy: 0.83 - ETA: 0s - loss: 0.3530 - accuracy: 0.83 - ETA: 0s - loss: 0.3536 - accuracy: 0.83 - ETA: 0s - loss: 0.3540 - accuracy: 0.83 - ETA: 0s - loss: 0.3526 - accuracy: 0.84 - ETA: 0s - loss: 0.3534 - accuracy: 0.83 - ETA: 0s - loss: 0.3529 - accuracy: 0.84 - ETA: 0s - loss: 0.3520 - accuracy: 0.84 - ETA: 0s - loss: 0.3524 - accuracy: 0.84 - ETA: 0s - loss: 0.3518 - accuracy: 0.84 - ETA: 0s - loss: 0.3537 - accuracy: 0.84 - ETA: 0s - loss: 0.3532 - accuracy: 0.84 - 4s 348us/step - loss: 0.3523 - accuracy: 0.8408 - val_loss: 0.3223 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.31344\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3347 - accuracy: 0.82 - ETA: 3s - loss: 0.3331 - accuracy: 0.84 - ETA: 3s - loss: 0.3359 - accuracy: 0.84 - ETA: 3s - loss: 0.3502 - accuracy: 0.83 - ETA: 3s - loss: 0.3539 - accuracy: 0.83 - ETA: 2s - loss: 0.3528 - accuracy: 0.83 - ETA: 2s - loss: 0.3572 - accuracy: 0.83 - ETA: 2s - loss: 0.3610 - accuracy: 0.83 - ETA: 2s - loss: 0.3667 - accuracy: 0.82 - ETA: 2s - loss: 0.3637 - accuracy: 0.83 - ETA: 2s - loss: 0.3588 - accuracy: 0.83 - ETA: 2s - loss: 0.3590 - accuracy: 0.83 - ETA: 2s - loss: 0.3526 - accuracy: 0.83 - ETA: 2s - loss: 0.3534 - accuracy: 0.83 - ETA: 2s - loss: 0.3530 - accuracy: 0.83 - ETA: 2s - loss: 0.3531 - accuracy: 0.83 - ETA: 2s - loss: 0.3530 - accuracy: 0.84 - ETA: 2s - loss: 0.3535 - accuracy: 0.83 - ETA: 2s - loss: 0.3549 - accuracy: 0.83 - ETA: 2s - loss: 0.3557 - accuracy: 0.83 - ETA: 2s - loss: 0.3589 - accuracy: 0.83 - ETA: 1s - loss: 0.3558 - accuracy: 0.84 - ETA: 1s - loss: 0.3536 - accuracy: 0.84 - ETA: 1s - loss: 0.3542 - accuracy: 0.84 - ETA: 1s - loss: 0.3538 - accuracy: 0.84 - ETA: 1s - loss: 0.3510 - accuracy: 0.84 - ETA: 1s - loss: 0.3493 - accuracy: 0.84 - ETA: 1s - loss: 0.3499 - accuracy: 0.84 - ETA: 1s - loss: 0.3525 - accuracy: 0.84 - ETA: 1s - loss: 0.3522 - accuracy: 0.84 - ETA: 1s - loss: 0.3513 - accuracy: 0.84 - ETA: 1s - loss: 0.3500 - accuracy: 0.84 - ETA: 1s - loss: 0.3489 - accuracy: 0.84 - ETA: 1s - loss: 0.3474 - accuracy: 0.84 - ETA: 1s - loss: 0.3475 - accuracy: 0.84 - ETA: 1s - loss: 0.3479 - accuracy: 0.84 - ETA: 1s - loss: 0.3468 - accuracy: 0.84 - ETA: 1s - loss: 0.3472 - accuracy: 0.84 - ETA: 1s - loss: 0.3455 - accuracy: 0.84 - ETA: 0s - loss: 0.3452 - accuracy: 0.84 - ETA: 0s - loss: 0.3453 - accuracy: 0.84 - ETA: 0s - loss: 0.3440 - accuracy: 0.84 - ETA: 0s - loss: 0.3425 - accuracy: 0.84 - ETA: 0s - loss: 0.3420 - accuracy: 0.84 - ETA: 0s - loss: 0.3413 - accuracy: 0.84 - ETA: 0s - loss: 0.3436 - accuracy: 0.84 - ETA: 0s - loss: 0.3441 - accuracy: 0.84 - ETA: 0s - loss: 0.3447 - accuracy: 0.84 - ETA: 0s - loss: 0.3452 - accuracy: 0.84 - ETA: 0s - loss: 0.3456 - accuracy: 0.84 - ETA: 0s - loss: 0.3471 - accuracy: 0.84 - ETA: 0s - loss: 0.3480 - accuracy: 0.84 - ETA: 0s - loss: 0.3475 - accuracy: 0.84 - ETA: 0s - loss: 0.3474 - accuracy: 0.84 - ETA: 0s - loss: 0.3486 - accuracy: 0.84 - ETA: 0s - loss: 0.3482 - accuracy: 0.84 - 4s 361us/step - loss: 0.3481 - accuracy: 0.8437 - val_loss: 0.3263 - val_accuracy: 0.8644\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.31344\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3098 - accuracy: 0.84 - ETA: 3s - loss: 0.3411 - accuracy: 0.83 - ETA: 3s - loss: 0.3434 - accuracy: 0.83 - ETA: 3s - loss: 0.3524 - accuracy: 0.82 - ETA: 3s - loss: 0.3472 - accuracy: 0.83 - ETA: 2s - loss: 0.3428 - accuracy: 0.84 - ETA: 2s - loss: 0.3326 - accuracy: 0.84 - ETA: 2s - loss: 0.3447 - accuracy: 0.84 - ETA: 2s - loss: 0.3519 - accuracy: 0.84 - ETA: 2s - loss: 0.3522 - accuracy: 0.84 - ETA: 2s - loss: 0.3527 - accuracy: 0.84 - ETA: 2s - loss: 0.3522 - accuracy: 0.84 - ETA: 2s - loss: 0.3509 - accuracy: 0.84 - ETA: 2s - loss: 0.3510 - accuracy: 0.84 - ETA: 2s - loss: 0.3472 - accuracy: 0.84 - ETA: 2s - loss: 0.3431 - accuracy: 0.84 - ETA: 2s - loss: 0.3444 - accuracy: 0.84 - ETA: 2s - loss: 0.3432 - accuracy: 0.84 - ETA: 2s - loss: 0.3448 - accuracy: 0.84 - ETA: 1s - loss: 0.3467 - accuracy: 0.84 - ETA: 1s - loss: 0.3447 - accuracy: 0.84 - ETA: 1s - loss: 0.3443 - accuracy: 0.84 - ETA: 1s - loss: 0.3448 - accuracy: 0.84 - ETA: 1s - loss: 0.3439 - accuracy: 0.84 - ETA: 1s - loss: 0.3450 - accuracy: 0.84 - ETA: 1s - loss: 0.3443 - accuracy: 0.84 - ETA: 1s - loss: 0.3422 - accuracy: 0.84 - ETA: 1s - loss: 0.3423 - accuracy: 0.84 - ETA: 1s - loss: 0.3412 - accuracy: 0.84 - ETA: 1s - loss: 0.3410 - accuracy: 0.84 - ETA: 1s - loss: 0.3396 - accuracy: 0.84 - ETA: 1s - loss: 0.3393 - accuracy: 0.84 - ETA: 1s - loss: 0.3388 - accuracy: 0.84 - ETA: 0s - loss: 0.3399 - accuracy: 0.84 - ETA: 0s - loss: 0.3393 - accuracy: 0.84 - ETA: 0s - loss: 0.3383 - accuracy: 0.84 - ETA: 0s - loss: 0.3389 - accuracy: 0.84 - ETA: 0s - loss: 0.3392 - accuracy: 0.84 - ETA: 0s - loss: 0.3397 - accuracy: 0.84 - ETA: 0s - loss: 0.3399 - accuracy: 0.84 - ETA: 0s - loss: 0.3389 - accuracy: 0.84 - ETA: 0s - loss: 0.3408 - accuracy: 0.84 - ETA: 0s - loss: 0.3409 - accuracy: 0.84 - ETA: 0s - loss: 0.3404 - accuracy: 0.84 - ETA: 0s - loss: 0.3394 - accuracy: 0.84 - ETA: 0s - loss: 0.3396 - accuracy: 0.84 - ETA: 0s - loss: 0.3400 - accuracy: 0.84 - ETA: 0s - loss: 0.3405 - accuracy: 0.84 - 4s 346us/step - loss: 0.3404 - accuracy: 0.8465 - val_loss: 0.3338 - val_accuracy: 0.8366\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.31344\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3493 - accuracy: 0.84 - ETA: 3s - loss: 0.3594 - accuracy: 0.82 - ETA: 2s - loss: 0.3663 - accuracy: 0.82 - ETA: 2s - loss: 0.3547 - accuracy: 0.83 - ETA: 2s - loss: 0.3473 - accuracy: 0.83 - ETA: 2s - loss: 0.3488 - accuracy: 0.83 - ETA: 2s - loss: 0.3475 - accuracy: 0.84 - ETA: 2s - loss: 0.3515 - accuracy: 0.84 - ETA: 2s - loss: 0.3474 - accuracy: 0.84 - ETA: 2s - loss: 0.3493 - accuracy: 0.83 - ETA: 2s - loss: 0.3484 - accuracy: 0.84 - ETA: 2s - loss: 0.3482 - accuracy: 0.84 - ETA: 2s - loss: 0.3422 - accuracy: 0.84 - ETA: 2s - loss: 0.3384 - accuracy: 0.84 - ETA: 2s - loss: 0.3396 - accuracy: 0.84 - ETA: 2s - loss: 0.3382 - accuracy: 0.84 - ETA: 2s - loss: 0.3405 - accuracy: 0.84 - ETA: 2s - loss: 0.3421 - accuracy: 0.84 - ETA: 1s - loss: 0.3403 - accuracy: 0.84 - ETA: 1s - loss: 0.3435 - accuracy: 0.84 - ETA: 1s - loss: 0.3413 - accuracy: 0.84 - ETA: 1s - loss: 0.3405 - accuracy: 0.84 - ETA: 1s - loss: 0.3396 - accuracy: 0.85 - ETA: 1s - loss: 0.3387 - accuracy: 0.85 - ETA: 1s - loss: 0.3397 - accuracy: 0.85 - ETA: 1s - loss: 0.3425 - accuracy: 0.84 - ETA: 1s - loss: 0.3415 - accuracy: 0.84 - ETA: 1s - loss: 0.3403 - accuracy: 0.84 - ETA: 1s - loss: 0.3397 - accuracy: 0.84 - ETA: 1s - loss: 0.3415 - accuracy: 0.84 - ETA: 1s - loss: 0.3395 - accuracy: 0.84 - ETA: 1s - loss: 0.3389 - accuracy: 0.84 - ETA: 1s - loss: 0.3388 - accuracy: 0.84 - ETA: 0s - loss: 0.3371 - accuracy: 0.85 - ETA: 0s - loss: 0.3381 - accuracy: 0.84 - ETA: 0s - loss: 0.3388 - accuracy: 0.84 - ETA: 0s - loss: 0.3399 - accuracy: 0.84 - ETA: 0s - loss: 0.3409 - accuracy: 0.84 - ETA: 0s - loss: 0.3391 - accuracy: 0.84 - ETA: 0s - loss: 0.3398 - accuracy: 0.84 - ETA: 0s - loss: 0.3399 - accuracy: 0.84 - ETA: 0s - loss: 0.3396 - accuracy: 0.84 - ETA: 0s - loss: 0.3417 - accuracy: 0.84 - ETA: 0s - loss: 0.3417 - accuracy: 0.84 - ETA: 0s - loss: 0.3412 - accuracy: 0.84 - ETA: 0s - loss: 0.3416 - accuracy: 0.84 - 4s 337us/step - loss: 0.3422 - accuracy: 0.8483 - val_loss: 0.3493 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.31344\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 3s - loss: 0.3584 - accuracy: 0.80 - ETA: 3s - loss: 0.3167 - accuracy: 0.85 - ETA: 3s - loss: 0.3164 - accuracy: 0.85 - ETA: 3s - loss: 0.3138 - accuracy: 0.85 - ETA: 3s - loss: 0.3274 - accuracy: 0.85 - ETA: 3s - loss: 0.3220 - accuracy: 0.85 - ETA: 3s - loss: 0.3169 - accuracy: 0.85 - ETA: 3s - loss: 0.3180 - accuracy: 0.85 - ETA: 3s - loss: 0.3193 - accuracy: 0.85 - ETA: 3s - loss: 0.3255 - accuracy: 0.85 - ETA: 3s - loss: 0.3373 - accuracy: 0.85 - ETA: 2s - loss: 0.3487 - accuracy: 0.84 - ETA: 2s - loss: 0.3495 - accuracy: 0.84 - ETA: 2s - loss: 0.3462 - accuracy: 0.84 - ETA: 2s - loss: 0.3450 - accuracy: 0.84 - ETA: 2s - loss: 0.3444 - accuracy: 0.84 - ETA: 2s - loss: 0.3433 - accuracy: 0.84 - ETA: 2s - loss: 0.3469 - accuracy: 0.84 - ETA: 2s - loss: 0.3467 - accuracy: 0.84 - ETA: 2s - loss: 0.3512 - accuracy: 0.84 - ETA: 1s - loss: 0.3471 - accuracy: 0.84 - ETA: 1s - loss: 0.3479 - accuracy: 0.84 - ETA: 1s - loss: 0.3450 - accuracy: 0.84 - ETA: 1s - loss: 0.3465 - accuracy: 0.84 - ETA: 1s - loss: 0.3477 - accuracy: 0.84 - ETA: 1s - loss: 0.3465 - accuracy: 0.84 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3470 - accuracy: 0.84 - ETA: 1s - loss: 0.3493 - accuracy: 0.84 - ETA: 1s - loss: 0.3473 - accuracy: 0.84 - ETA: 1s - loss: 0.3457 - accuracy: 0.84 - ETA: 1s - loss: 0.3447 - accuracy: 0.84 - ETA: 1s - loss: 0.3454 - accuracy: 0.84 - ETA: 1s - loss: 0.3465 - accuracy: 0.84 - ETA: 1s - loss: 0.3460 - accuracy: 0.84 - ETA: 0s - loss: 0.3464 - accuracy: 0.84 - ETA: 0s - loss: 0.3450 - accuracy: 0.84 - ETA: 0s - loss: 0.3453 - accuracy: 0.84 - ETA: 0s - loss: 0.3452 - accuracy: 0.84 - ETA: 0s - loss: 0.3454 - accuracy: 0.84 - ETA: 0s - loss: 0.3474 - accuracy: 0.84 - ETA: 0s - loss: 0.3483 - accuracy: 0.84 - ETA: 0s - loss: 0.3475 - accuracy: 0.84 - ETA: 0s - loss: 0.3463 - accuracy: 0.84 - ETA: 0s - loss: 0.3452 - accuracy: 0.84 - ETA: 0s - loss: 0.3449 - accuracy: 0.84 - ETA: 0s - loss: 0.3438 - accuracy: 0.84 - 4s 337us/step - loss: 0.3437 - accuracy: 0.8452 - val_loss: 0.3290 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.31344\n",
      "Epoch 28/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2650 - accuracy: 0.87 - ETA: 3s - loss: 0.2768 - accuracy: 0.86 - ETA: 3s - loss: 0.3017 - accuracy: 0.86 - ETA: 3s - loss: 0.2980 - accuracy: 0.87 - ETA: 3s - loss: 0.3114 - accuracy: 0.86 - ETA: 3s - loss: 0.3031 - accuracy: 0.87 - ETA: 3s - loss: 0.3158 - accuracy: 0.85 - ETA: 3s - loss: 0.3133 - accuracy: 0.86 - ETA: 3s - loss: 0.3129 - accuracy: 0.85 - ETA: 2s - loss: 0.3142 - accuracy: 0.85 - ETA: 2s - loss: 0.3224 - accuracy: 0.85 - ETA: 2s - loss: 0.3188 - accuracy: 0.85 - ETA: 2s - loss: 0.3209 - accuracy: 0.85 - ETA: 2s - loss: 0.3240 - accuracy: 0.85 - ETA: 2s - loss: 0.3255 - accuracy: 0.85 - ETA: 2s - loss: 0.3294 - accuracy: 0.84 - ETA: 2s - loss: 0.3293 - accuracy: 0.84 - ETA: 2s - loss: 0.3251 - accuracy: 0.84 - ETA: 2s - loss: 0.3277 - accuracy: 0.84 - ETA: 2s - loss: 0.3299 - accuracy: 0.84 - ETA: 2s - loss: 0.3291 - accuracy: 0.84 - ETA: 2s - loss: 0.3278 - accuracy: 0.84 - ETA: 2s - loss: 0.3278 - accuracy: 0.84 - ETA: 2s - loss: 0.3292 - accuracy: 0.84 - ETA: 3s - loss: 0.3308 - accuracy: 0.84 - ETA: 3s - loss: 0.3297 - accuracy: 0.84 - ETA: 3s - loss: 0.3316 - accuracy: 0.84 - ETA: 3s - loss: 0.3301 - accuracy: 0.84 - ETA: 3s - loss: 0.3316 - accuracy: 0.84 - ETA: 3s - loss: 0.3324 - accuracy: 0.84 - ETA: 3s - loss: 0.3332 - accuracy: 0.84 - ETA: 2s - loss: 0.3333 - accuracy: 0.84 - ETA: 2s - loss: 0.3321 - accuracy: 0.84 - ETA: 2s - loss: 0.3298 - accuracy: 0.84 - ETA: 2s - loss: 0.3304 - accuracy: 0.84 - ETA: 2s - loss: 0.3316 - accuracy: 0.84 - ETA: 2s - loss: 0.3312 - accuracy: 0.84 - ETA: 2s - loss: 0.3313 - accuracy: 0.84 - ETA: 2s - loss: 0.3331 - accuracy: 0.84 - ETA: 2s - loss: 0.3336 - accuracy: 0.84 - ETA: 2s - loss: 0.3348 - accuracy: 0.84 - ETA: 2s - loss: 0.3348 - accuracy: 0.84 - ETA: 2s - loss: 0.3345 - accuracy: 0.84 - ETA: 1s - loss: 0.3361 - accuracy: 0.84 - ETA: 1s - loss: 0.3359 - accuracy: 0.84 - ETA: 1s - loss: 0.3371 - accuracy: 0.84 - ETA: 1s - loss: 0.3363 - accuracy: 0.84 - ETA: 1s - loss: 0.3357 - accuracy: 0.84 - ETA: 1s - loss: 0.3354 - accuracy: 0.84 - ETA: 1s - loss: 0.3344 - accuracy: 0.84 - ETA: 1s - loss: 0.3350 - accuracy: 0.84 - ETA: 1s - loss: 0.3339 - accuracy: 0.84 - ETA: 1s - loss: 0.3343 - accuracy: 0.84 - ETA: 0s - loss: 0.3339 - accuracy: 0.84 - ETA: 0s - loss: 0.3333 - accuracy: 0.84 - ETA: 0s - loss: 0.3350 - accuracy: 0.84 - ETA: 0s - loss: 0.3348 - accuracy: 0.84 - ETA: 0s - loss: 0.3345 - accuracy: 0.84 - ETA: 0s - loss: 0.3337 - accuracy: 0.84 - ETA: 0s - loss: 0.3332 - accuracy: 0.84 - ETA: 0s - loss: 0.3343 - accuracy: 0.84 - 7s 649us/step - loss: 0.3342 - accuracy: 0.8495 - val_loss: 0.3093 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.31344 to 0.30931, saving model to ttbox_deep_model.h5\n",
      "Epoch 29/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3192 - accuracy: 0.84 - ETA: 3s - loss: 0.2926 - accuracy: 0.87 - ETA: 3s - loss: 0.2913 - accuracy: 0.86 - ETA: 3s - loss: 0.3219 - accuracy: 0.85 - ETA: 3s - loss: 0.3215 - accuracy: 0.85 - ETA: 3s - loss: 0.3206 - accuracy: 0.85 - ETA: 3s - loss: 0.3237 - accuracy: 0.85 - ETA: 2s - loss: 0.3168 - accuracy: 0.86 - ETA: 2s - loss: 0.3153 - accuracy: 0.85 - ETA: 2s - loss: 0.3104 - accuracy: 0.85 - ETA: 2s - loss: 0.3139 - accuracy: 0.85 - ETA: 2s - loss: 0.3131 - accuracy: 0.85 - ETA: 2s - loss: 0.3109 - accuracy: 0.85 - ETA: 2s - loss: 0.3174 - accuracy: 0.85 - ETA: 2s - loss: 0.3182 - accuracy: 0.85 - ETA: 2s - loss: 0.3192 - accuracy: 0.85 - ETA: 2s - loss: 0.3194 - accuracy: 0.85 - ETA: 2s - loss: 0.3207 - accuracy: 0.85 - ETA: 2s - loss: 0.3248 - accuracy: 0.85 - ETA: 2s - loss: 0.3220 - accuracy: 0.85 - ETA: 1s - loss: 0.3190 - accuracy: 0.85 - ETA: 1s - loss: 0.3238 - accuracy: 0.85 - ETA: 1s - loss: 0.3230 - accuracy: 0.85 - ETA: 1s - loss: 0.3232 - accuracy: 0.85 - ETA: 1s - loss: 0.3243 - accuracy: 0.85 - ETA: 1s - loss: 0.3231 - accuracy: 0.85 - ETA: 1s - loss: 0.3252 - accuracy: 0.85 - ETA: 1s - loss: 0.3246 - accuracy: 0.85 - ETA: 1s - loss: 0.3227 - accuracy: 0.85 - ETA: 1s - loss: 0.3256 - accuracy: 0.85 - ETA: 0s - loss: 0.3258 - accuracy: 0.85 - ETA: 0s - loss: 0.3273 - accuracy: 0.85 - ETA: 0s - loss: 0.3283 - accuracy: 0.85 - ETA: 0s - loss: 0.3291 - accuracy: 0.85 - ETA: 0s - loss: 0.3273 - accuracy: 0.85 - ETA: 0s - loss: 0.3288 - accuracy: 0.85 - ETA: 0s - loss: 0.3299 - accuracy: 0.84 - ETA: 0s - loss: 0.3305 - accuracy: 0.84 - ETA: 0s - loss: 0.3300 - accuracy: 0.84 - ETA: 0s - loss: 0.3309 - accuracy: 0.84 - ETA: 0s - loss: 0.3306 - accuracy: 0.84 - ETA: 0s - loss: 0.3313 - accuracy: 0.84 - 3s 291us/step - loss: 0.3312 - accuracy: 0.8490 - val_loss: 0.3684 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.30931\n",
      "Epoch 30/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4105 - accuracy: 0.78 - ETA: 2s - loss: 0.3402 - accuracy: 0.83 - ETA: 2s - loss: 0.3355 - accuracy: 0.84 - ETA: 2s - loss: 0.3356 - accuracy: 0.84 - ETA: 2s - loss: 0.3368 - accuracy: 0.84 - ETA: 2s - loss: 0.3329 - accuracy: 0.85 - ETA: 2s - loss: 0.3365 - accuracy: 0.84 - ETA: 1s - loss: 0.3364 - accuracy: 0.84 - ETA: 1s - loss: 0.3341 - accuracy: 0.84 - ETA: 1s - loss: 0.3315 - accuracy: 0.84 - ETA: 1s - loss: 0.3316 - accuracy: 0.84 - ETA: 1s - loss: 0.3309 - accuracy: 0.84 - ETA: 1s - loss: 0.3317 - accuracy: 0.84 - ETA: 1s - loss: 0.3283 - accuracy: 0.85 - ETA: 1s - loss: 0.3325 - accuracy: 0.84 - ETA: 1s - loss: 0.3333 - accuracy: 0.84 - ETA: 1s - loss: 0.3348 - accuracy: 0.84 - ETA: 1s - loss: 0.3324 - accuracy: 0.84 - ETA: 1s - loss: 0.3330 - accuracy: 0.85 - ETA: 1s - loss: 0.3360 - accuracy: 0.85 - ETA: 1s - loss: 0.3332 - accuracy: 0.85 - ETA: 1s - loss: 0.3324 - accuracy: 0.85 - ETA: 1s - loss: 0.3328 - accuracy: 0.85 - ETA: 0s - loss: 0.3308 - accuracy: 0.85 - ETA: 0s - loss: 0.3326 - accuracy: 0.85 - ETA: 0s - loss: 0.3332 - accuracy: 0.85 - ETA: 0s - loss: 0.3346 - accuracy: 0.85 - ETA: 0s - loss: 0.3345 - accuracy: 0.85 - ETA: 0s - loss: 0.3350 - accuracy: 0.85 - ETA: 0s - loss: 0.3345 - accuracy: 0.85 - ETA: 0s - loss: 0.3348 - accuracy: 0.85 - ETA: 0s - loss: 0.3340 - accuracy: 0.85 - ETA: 0s - loss: 0.3344 - accuracy: 0.85 - ETA: 0s - loss: 0.3373 - accuracy: 0.84 - ETA: 0s - loss: 0.3354 - accuracy: 0.84 - ETA: 0s - loss: 0.3350 - accuracy: 0.85 - ETA: 0s - loss: 0.3340 - accuracy: 0.85 - 3s 246us/step - loss: 0.3342 - accuracy: 0.8507 - val_loss: 0.2981 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.30931 to 0.29811, saving model to ttbox_deep_model.h5\n",
      "Epoch 31/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2392 - accuracy: 0.90 - ETA: 2s - loss: 0.2794 - accuracy: 0.88 - ETA: 2s - loss: 0.3281 - accuracy: 0.86 - ETA: 2s - loss: 0.3217 - accuracy: 0.86 - ETA: 2s - loss: 0.3287 - accuracy: 0.85 - ETA: 2s - loss: 0.3321 - accuracy: 0.85 - ETA: 1s - loss: 0.3285 - accuracy: 0.85 - ETA: 1s - loss: 0.3252 - accuracy: 0.85 - ETA: 1s - loss: 0.3262 - accuracy: 0.85 - ETA: 1s - loss: 0.3276 - accuracy: 0.85 - ETA: 1s - loss: 0.3276 - accuracy: 0.85 - ETA: 1s - loss: 0.3290 - accuracy: 0.85 - ETA: 1s - loss: 0.3260 - accuracy: 0.85 - ETA: 1s - loss: 0.3245 - accuracy: 0.85 - ETA: 1s - loss: 0.3238 - accuracy: 0.85 - ETA: 1s - loss: 0.3224 - accuracy: 0.85 - ETA: 1s - loss: 0.3218 - accuracy: 0.85 - ETA: 1s - loss: 0.3257 - accuracy: 0.85 - ETA: 1s - loss: 0.3261 - accuracy: 0.85 - ETA: 1s - loss: 0.3286 - accuracy: 0.84 - ETA: 1s - loss: 0.3277 - accuracy: 0.84 - ETA: 1s - loss: 0.3263 - accuracy: 0.84 - ETA: 0s - loss: 0.3264 - accuracy: 0.85 - ETA: 0s - loss: 0.3251 - accuracy: 0.85 - ETA: 0s - loss: 0.3243 - accuracy: 0.85 - ETA: 0s - loss: 0.3246 - accuracy: 0.85 - ETA: 0s - loss: 0.3254 - accuracy: 0.85 - ETA: 0s - loss: 0.3255 - accuracy: 0.85 - ETA: 0s - loss: 0.3260 - accuracy: 0.85 - ETA: 0s - loss: 0.3256 - accuracy: 0.85 - ETA: 0s - loss: 0.3246 - accuracy: 0.85 - ETA: 0s - loss: 0.3241 - accuracy: 0.85 - ETA: 0s - loss: 0.3256 - accuracy: 0.85 - ETA: 0s - loss: 0.3258 - accuracy: 0.85 - ETA: 0s - loss: 0.3268 - accuracy: 0.85 - ETA: 0s - loss: 0.3262 - accuracy: 0.85 - ETA: 0s - loss: 0.3264 - accuracy: 0.85 - 2s 234us/step - loss: 0.3259 - accuracy: 0.8532 - val_loss: 0.3264 - val_accuracy: 0.8399\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.29811\n",
      "Epoch 32/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3273 - accuracy: 0.84 - ETA: 2s - loss: 0.3034 - accuracy: 0.87 - ETA: 2s - loss: 0.2919 - accuracy: 0.87 - ETA: 2s - loss: 0.2893 - accuracy: 0.87 - ETA: 2s - loss: 0.2880 - accuracy: 0.87 - ETA: 2s - loss: 0.2928 - accuracy: 0.87 - ETA: 2s - loss: 0.2961 - accuracy: 0.86 - ETA: 1s - loss: 0.3054 - accuracy: 0.86 - ETA: 1s - loss: 0.3064 - accuracy: 0.86 - ETA: 1s - loss: 0.3163 - accuracy: 0.85 - ETA: 1s - loss: 0.3144 - accuracy: 0.85 - ETA: 1s - loss: 0.3139 - accuracy: 0.86 - ETA: 1s - loss: 0.3128 - accuracy: 0.86 - ETA: 1s - loss: 0.3115 - accuracy: 0.86 - ETA: 1s - loss: 0.3114 - accuracy: 0.86 - ETA: 1s - loss: 0.3149 - accuracy: 0.86 - ETA: 1s - loss: 0.3145 - accuracy: 0.85 - ETA: 1s - loss: 0.3155 - accuracy: 0.85 - ETA: 1s - loss: 0.3164 - accuracy: 0.85 - ETA: 1s - loss: 0.3182 - accuracy: 0.85 - ETA: 1s - loss: 0.3198 - accuracy: 0.85 - ETA: 1s - loss: 0.3215 - accuracy: 0.85 - ETA: 1s - loss: 0.3200 - accuracy: 0.85 - ETA: 0s - loss: 0.3224 - accuracy: 0.85 - ETA: 0s - loss: 0.3209 - accuracy: 0.85 - ETA: 0s - loss: 0.3203 - accuracy: 0.85 - ETA: 0s - loss: 0.3207 - accuracy: 0.85 - ETA: 0s - loss: 0.3202 - accuracy: 0.85 - ETA: 0s - loss: 0.3198 - accuracy: 0.85 - ETA: 0s - loss: 0.3192 - accuracy: 0.85 - ETA: 0s - loss: 0.3192 - accuracy: 0.85 - ETA: 0s - loss: 0.3199 - accuracy: 0.85 - ETA: 0s - loss: 0.3213 - accuracy: 0.85 - ETA: 0s - loss: 0.3212 - accuracy: 0.85 - ETA: 0s - loss: 0.3210 - accuracy: 0.85 - ETA: 0s - loss: 0.3214 - accuracy: 0.85 - ETA: 0s - loss: 0.3211 - accuracy: 0.85 - 3s 238us/step - loss: 0.3214 - accuracy: 0.8573 - val_loss: 0.3311 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.29811\n",
      "Epoch 33/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3024 - accuracy: 0.87 - ETA: 2s - loss: 0.2920 - accuracy: 0.87 - ETA: 2s - loss: 0.2818 - accuracy: 0.87 - ETA: 2s - loss: 0.2880 - accuracy: 0.87 - ETA: 2s - loss: 0.3070 - accuracy: 0.86 - ETA: 2s - loss: 0.3126 - accuracy: 0.86 - ETA: 1s - loss: 0.3138 - accuracy: 0.85 - ETA: 1s - loss: 0.3123 - accuracy: 0.86 - ETA: 1s - loss: 0.3143 - accuracy: 0.86 - ETA: 1s - loss: 0.3157 - accuracy: 0.86 - ETA: 1s - loss: 0.3156 - accuracy: 0.86 - ETA: 1s - loss: 0.3185 - accuracy: 0.86 - ETA: 1s - loss: 0.3183 - accuracy: 0.86 - ETA: 1s - loss: 0.3167 - accuracy: 0.86 - ETA: 1s - loss: 0.3193 - accuracy: 0.86 - ETA: 1s - loss: 0.3216 - accuracy: 0.85 - ETA: 1s - loss: 0.3227 - accuracy: 0.85 - ETA: 1s - loss: 0.3212 - accuracy: 0.85 - ETA: 1s - loss: 0.3204 - accuracy: 0.85 - ETA: 1s - loss: 0.3191 - accuracy: 0.85 - ETA: 1s - loss: 0.3186 - accuracy: 0.86 - ETA: 0s - loss: 0.3211 - accuracy: 0.85 - ETA: 0s - loss: 0.3191 - accuracy: 0.85 - ETA: 0s - loss: 0.3195 - accuracy: 0.85 - ETA: 0s - loss: 0.3190 - accuracy: 0.86 - ETA: 0s - loss: 0.3187 - accuracy: 0.86 - ETA: 0s - loss: 0.3187 - accuracy: 0.86 - ETA: 0s - loss: 0.3183 - accuracy: 0.86 - ETA: 0s - loss: 0.3219 - accuracy: 0.86 - ETA: 0s - loss: 0.3205 - accuracy: 0.86 - ETA: 0s - loss: 0.3203 - accuracy: 0.86 - ETA: 0s - loss: 0.3211 - accuracy: 0.86 - ETA: 0s - loss: 0.3223 - accuracy: 0.86 - ETA: 0s - loss: 0.3224 - accuracy: 0.86 - ETA: 0s - loss: 0.3215 - accuracy: 0.86 - ETA: 0s - loss: 0.3210 - accuracy: 0.86 - ETA: 0s - loss: 0.3202 - accuracy: 0.86 - 2s 229us/step - loss: 0.3197 - accuracy: 0.8610 - val_loss: 0.3176 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.29811\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3125 - accuracy: 0.83 - ETA: 2s - loss: 0.3052 - accuracy: 0.85 - ETA: 2s - loss: 0.3086 - accuracy: 0.85 - ETA: 2s - loss: 0.3349 - accuracy: 0.84 - ETA: 1s - loss: 0.3334 - accuracy: 0.85 - ETA: 1s - loss: 0.3268 - accuracy: 0.85 - ETA: 1s - loss: 0.3217 - accuracy: 0.85 - ETA: 1s - loss: 0.3187 - accuracy: 0.85 - ETA: 1s - loss: 0.3203 - accuracy: 0.85 - ETA: 1s - loss: 0.3250 - accuracy: 0.85 - ETA: 1s - loss: 0.3261 - accuracy: 0.85 - ETA: 1s - loss: 0.3270 - accuracy: 0.85 - ETA: 1s - loss: 0.3257 - accuracy: 0.85 - ETA: 1s - loss: 0.3242 - accuracy: 0.85 - ETA: 1s - loss: 0.3235 - accuracy: 0.85 - ETA: 1s - loss: 0.3245 - accuracy: 0.85 - ETA: 1s - loss: 0.3202 - accuracy: 0.85 - ETA: 1s - loss: 0.3230 - accuracy: 0.85 - ETA: 1s - loss: 0.3183 - accuracy: 0.86 - ETA: 1s - loss: 0.3195 - accuracy: 0.85 - ETA: 1s - loss: 0.3202 - accuracy: 0.85 - ETA: 0s - loss: 0.3195 - accuracy: 0.85 - ETA: 0s - loss: 0.3229 - accuracy: 0.85 - ETA: 0s - loss: 0.3229 - accuracy: 0.85 - ETA: 0s - loss: 0.3212 - accuracy: 0.85 - ETA: 0s - loss: 0.3239 - accuracy: 0.85 - ETA: 0s - loss: 0.3245 - accuracy: 0.85 - ETA: 0s - loss: 0.3248 - accuracy: 0.85 - ETA: 0s - loss: 0.3223 - accuracy: 0.85 - ETA: 0s - loss: 0.3218 - accuracy: 0.85 - ETA: 0s - loss: 0.3207 - accuracy: 0.85 - ETA: 0s - loss: 0.3218 - accuracy: 0.85 - ETA: 0s - loss: 0.3219 - accuracy: 0.85 - ETA: 0s - loss: 0.3201 - accuracy: 0.85 - ETA: 0s - loss: 0.3187 - accuracy: 0.85 - ETA: 0s - loss: 0.3190 - accuracy: 0.85 - ETA: 0s - loss: 0.3176 - accuracy: 0.85 - 2s 225us/step - loss: 0.3170 - accuracy: 0.8589 - val_loss: 0.3146 - val_accuracy: 0.8534\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.29811\n",
      "Epoch 35/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3819 - accuracy: 0.85 - ETA: 2s - loss: 0.2905 - accuracy: 0.88 - ETA: 2s - loss: 0.3231 - accuracy: 0.85 - ETA: 2s - loss: 0.3301 - accuracy: 0.85 - ETA: 2s - loss: 0.3108 - accuracy: 0.86 - ETA: 2s - loss: 0.3131 - accuracy: 0.86 - ETA: 2s - loss: 0.3101 - accuracy: 0.86 - ETA: 1s - loss: 0.3108 - accuracy: 0.86 - ETA: 1s - loss: 0.3075 - accuracy: 0.86 - ETA: 1s - loss: 0.3025 - accuracy: 0.86 - ETA: 1s - loss: 0.2965 - accuracy: 0.87 - ETA: 1s - loss: 0.2957 - accuracy: 0.87 - ETA: 1s - loss: 0.2961 - accuracy: 0.87 - ETA: 1s - loss: 0.2952 - accuracy: 0.87 - ETA: 1s - loss: 0.3026 - accuracy: 0.86 - ETA: 1s - loss: 0.3034 - accuracy: 0.86 - ETA: 1s - loss: 0.3051 - accuracy: 0.86 - ETA: 1s - loss: 0.3070 - accuracy: 0.86 - ETA: 1s - loss: 0.3074 - accuracy: 0.86 - ETA: 1s - loss: 0.3070 - accuracy: 0.86 - ETA: 1s - loss: 0.3025 - accuracy: 0.86 - ETA: 0s - loss: 0.3045 - accuracy: 0.86 - ETA: 0s - loss: 0.3044 - accuracy: 0.86 - ETA: 0s - loss: 0.3056 - accuracy: 0.86 - ETA: 0s - loss: 0.3063 - accuracy: 0.86 - ETA: 0s - loss: 0.3074 - accuracy: 0.86 - ETA: 0s - loss: 0.3079 - accuracy: 0.86 - ETA: 0s - loss: 0.3095 - accuracy: 0.86 - ETA: 0s - loss: 0.3082 - accuracy: 0.86 - ETA: 0s - loss: 0.3089 - accuracy: 0.86 - ETA: 0s - loss: 0.3105 - accuracy: 0.86 - ETA: 0s - loss: 0.3108 - accuracy: 0.86 - ETA: 0s - loss: 0.3111 - accuracy: 0.86 - ETA: 0s - loss: 0.3089 - accuracy: 0.86 - ETA: 0s - loss: 0.3095 - accuracy: 0.86 - ETA: 0s - loss: 0.3108 - accuracy: 0.86 - ETA: 0s - loss: 0.3115 - accuracy: 0.86 - 2s 227us/step - loss: 0.3110 - accuracy: 0.8604 - val_loss: 0.3174 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.29811\n",
      "Epoch 00035: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 133us/step\n",
      "[2020-05-18 16:06:56 RAM70.2% 0.61GB] Val Score : [0.29731400019522414, 0.857467770576477]\n",
      "[2020-05-18 16:06:56 RAM70.2% 0.61GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:06:56 RAM70.2% 0.61GB] Training on Fold : 9\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 32s - loss: 4.9709 - accuracy: 0.413 - ETA: 11s - loss: 2.5244 - accuracy: 0.549 - ETA: 7s - loss: 1.8618 - accuracy: 0.569 - ETA: 5s - loss: 1.5492 - accuracy: 0.58 - ETA: 4s - loss: 1.3733 - accuracy: 0.59 - ETA: 4s - loss: 1.2457 - accuracy: 0.59 - ETA: 3s - loss: 1.1538 - accuracy: 0.60 - ETA: 3s - loss: 1.0889 - accuracy: 0.60 - ETA: 3s - loss: 1.0381 - accuracy: 0.61 - ETA: 2s - loss: 0.9994 - accuracy: 0.61 - ETA: 2s - loss: 0.9639 - accuracy: 0.62 - ETA: 2s - loss: 0.9344 - accuracy: 0.62 - ETA: 2s - loss: 0.9125 - accuracy: 0.62 - ETA: 2s - loss: 0.8890 - accuracy: 0.63 - ETA: 1s - loss: 0.8743 - accuracy: 0.62 - ETA: 1s - loss: 0.8580 - accuracy: 0.63 - ETA: 1s - loss: 0.8448 - accuracy: 0.63 - ETA: 1s - loss: 0.8316 - accuracy: 0.63 - ETA: 1s - loss: 0.8201 - accuracy: 0.64 - ETA: 1s - loss: 0.8076 - accuracy: 0.64 - ETA: 1s - loss: 0.7959 - accuracy: 0.64 - ETA: 1s - loss: 0.7832 - accuracy: 0.65 - ETA: 1s - loss: 0.7759 - accuracy: 0.65 - ETA: 1s - loss: 0.7666 - accuracy: 0.66 - ETA: 0s - loss: 0.7577 - accuracy: 0.66 - ETA: 0s - loss: 0.7519 - accuracy: 0.66 - ETA: 0s - loss: 0.7472 - accuracy: 0.66 - ETA: 0s - loss: 0.7409 - accuracy: 0.66 - ETA: 0s - loss: 0.7349 - accuracy: 0.66 - ETA: 0s - loss: 0.7289 - accuracy: 0.67 - ETA: 0s - loss: 0.7228 - accuracy: 0.67 - ETA: 0s - loss: 0.7174 - accuracy: 0.67 - ETA: 0s - loss: 0.7134 - accuracy: 0.67 - ETA: 0s - loss: 0.7088 - accuracy: 0.68 - ETA: 0s - loss: 0.7042 - accuracy: 0.68 - ETA: 0s - loss: 0.6995 - accuracy: 0.68 - ETA: 0s - loss: 0.6948 - accuracy: 0.68 - 3s 266us/step - loss: 0.6933 - accuracy: 0.6864 - val_loss: 0.4938 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.29811\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5555 - accuracy: 0.78 - ETA: 1s - loss: 0.5575 - accuracy: 0.76 - ETA: 1s - loss: 0.5416 - accuracy: 0.76 - ETA: 1s - loss: 0.5297 - accuracy: 0.76 - ETA: 1s - loss: 0.5132 - accuracy: 0.76 - ETA: 1s - loss: 0.5232 - accuracy: 0.75 - ETA: 1s - loss: 0.5231 - accuracy: 0.76 - ETA: 1s - loss: 0.5210 - accuracy: 0.76 - ETA: 1s - loss: 0.5259 - accuracy: 0.75 - ETA: 1s - loss: 0.5283 - accuracy: 0.75 - ETA: 1s - loss: 0.5240 - accuracy: 0.75 - ETA: 1s - loss: 0.5311 - accuracy: 0.75 - ETA: 1s - loss: 0.5315 - accuracy: 0.75 - ETA: 1s - loss: 0.5263 - accuracy: 0.75 - ETA: 1s - loss: 0.5235 - accuracy: 0.75 - ETA: 1s - loss: 0.5189 - accuracy: 0.76 - ETA: 1s - loss: 0.5235 - accuracy: 0.76 - ETA: 1s - loss: 0.5189 - accuracy: 0.76 - ETA: 1s - loss: 0.5218 - accuracy: 0.76 - ETA: 1s - loss: 0.5227 - accuracy: 0.76 - ETA: 0s - loss: 0.5252 - accuracy: 0.75 - ETA: 0s - loss: 0.5247 - accuracy: 0.75 - ETA: 0s - loss: 0.5263 - accuracy: 0.75 - ETA: 0s - loss: 0.5246 - accuracy: 0.75 - ETA: 0s - loss: 0.5219 - accuracy: 0.75 - ETA: 0s - loss: 0.5231 - accuracy: 0.75 - ETA: 0s - loss: 0.5238 - accuracy: 0.75 - ETA: 0s - loss: 0.5225 - accuracy: 0.75 - ETA: 0s - loss: 0.5224 - accuracy: 0.75 - ETA: 0s - loss: 0.5236 - accuracy: 0.75 - ETA: 0s - loss: 0.5218 - accuracy: 0.76 - ETA: 0s - loss: 0.5204 - accuracy: 0.76 - ETA: 0s - loss: 0.5189 - accuracy: 0.76 - ETA: 0s - loss: 0.5200 - accuracy: 0.76 - ETA: 0s - loss: 0.5197 - accuracy: 0.76 - ETA: 0s - loss: 0.5194 - accuracy: 0.76 - ETA: 0s - loss: 0.5181 - accuracy: 0.76 - 2s 209us/step - loss: 0.5182 - accuracy: 0.7618 - val_loss: 0.4742 - val_accuracy: 0.8054\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.29811\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5728 - accuracy: 0.75 - ETA: 2s - loss: 0.5055 - accuracy: 0.79 - ETA: 2s - loss: 0.4939 - accuracy: 0.78 - ETA: 2s - loss: 0.5050 - accuracy: 0.77 - ETA: 2s - loss: 0.4994 - accuracy: 0.77 - ETA: 1s - loss: 0.4954 - accuracy: 0.78 - ETA: 1s - loss: 0.4976 - accuracy: 0.78 - ETA: 1s - loss: 0.4975 - accuracy: 0.78 - ETA: 1s - loss: 0.4936 - accuracy: 0.78 - ETA: 1s - loss: 0.4858 - accuracy: 0.78 - ETA: 1s - loss: 0.4832 - accuracy: 0.78 - ETA: 1s - loss: 0.4844 - accuracy: 0.78 - ETA: 1s - loss: 0.4864 - accuracy: 0.78 - ETA: 1s - loss: 0.4884 - accuracy: 0.78 - ETA: 1s - loss: 0.4862 - accuracy: 0.78 - ETA: 1s - loss: 0.4909 - accuracy: 0.78 - ETA: 1s - loss: 0.4873 - accuracy: 0.78 - ETA: 1s - loss: 0.4872 - accuracy: 0.78 - ETA: 1s - loss: 0.4900 - accuracy: 0.78 - ETA: 1s - loss: 0.4939 - accuracy: 0.78 - ETA: 1s - loss: 0.4907 - accuracy: 0.78 - ETA: 0s - loss: 0.4921 - accuracy: 0.78 - ETA: 0s - loss: 0.4923 - accuracy: 0.78 - ETA: 0s - loss: 0.4905 - accuracy: 0.78 - ETA: 0s - loss: 0.4895 - accuracy: 0.78 - ETA: 0s - loss: 0.4895 - accuracy: 0.78 - ETA: 0s - loss: 0.4885 - accuracy: 0.78 - ETA: 0s - loss: 0.4896 - accuracy: 0.78 - ETA: 0s - loss: 0.4887 - accuracy: 0.78 - ETA: 0s - loss: 0.4883 - accuracy: 0.78 - ETA: 0s - loss: 0.4893 - accuracy: 0.78 - ETA: 0s - loss: 0.4904 - accuracy: 0.78 - ETA: 0s - loss: 0.4905 - accuracy: 0.78 - ETA: 0s - loss: 0.4904 - accuracy: 0.78 - ETA: 0s - loss: 0.4893 - accuracy: 0.78 - ETA: 0s - loss: 0.4889 - accuracy: 0.78 - ETA: 0s - loss: 0.4873 - accuracy: 0.78 - 2s 213us/step - loss: 0.4859 - accuracy: 0.7824 - val_loss: 0.4195 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29811\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4930 - accuracy: 0.77 - ETA: 1s - loss: 0.4843 - accuracy: 0.77 - ETA: 1s - loss: 0.4799 - accuracy: 0.78 - ETA: 1s - loss: 0.4955 - accuracy: 0.78 - ETA: 1s - loss: 0.4903 - accuracy: 0.78 - ETA: 1s - loss: 0.4785 - accuracy: 0.79 - ETA: 1s - loss: 0.4687 - accuracy: 0.79 - ETA: 1s - loss: 0.4747 - accuracy: 0.78 - ETA: 1s - loss: 0.4722 - accuracy: 0.78 - ETA: 1s - loss: 0.4677 - accuracy: 0.79 - ETA: 1s - loss: 0.4704 - accuracy: 0.78 - ETA: 1s - loss: 0.4698 - accuracy: 0.78 - ETA: 1s - loss: 0.4702 - accuracy: 0.78 - ETA: 1s - loss: 0.4690 - accuracy: 0.79 - ETA: 1s - loss: 0.4745 - accuracy: 0.78 - ETA: 1s - loss: 0.4710 - accuracy: 0.79 - ETA: 1s - loss: 0.4720 - accuracy: 0.78 - ETA: 1s - loss: 0.4733 - accuracy: 0.78 - ETA: 1s - loss: 0.4727 - accuracy: 0.78 - ETA: 1s - loss: 0.4718 - accuracy: 0.78 - ETA: 0s - loss: 0.4737 - accuracy: 0.78 - ETA: 0s - loss: 0.4736 - accuracy: 0.78 - ETA: 0s - loss: 0.4708 - accuracy: 0.78 - ETA: 0s - loss: 0.4690 - accuracy: 0.78 - ETA: 0s - loss: 0.4688 - accuracy: 0.78 - ETA: 0s - loss: 0.4698 - accuracy: 0.78 - ETA: 0s - loss: 0.4712 - accuracy: 0.78 - ETA: 0s - loss: 0.4693 - accuracy: 0.78 - ETA: 0s - loss: 0.4680 - accuracy: 0.79 - ETA: 0s - loss: 0.4696 - accuracy: 0.78 - ETA: 0s - loss: 0.4679 - accuracy: 0.78 - ETA: 0s - loss: 0.4671 - accuracy: 0.79 - ETA: 0s - loss: 0.4672 - accuracy: 0.79 - ETA: 0s - loss: 0.4683 - accuracy: 0.78 - ETA: 0s - loss: 0.4681 - accuracy: 0.78 - ETA: 0s - loss: 0.4690 - accuracy: 0.78 - ETA: 0s - loss: 0.4689 - accuracy: 0.78 - 2s 209us/step - loss: 0.4697 - accuracy: 0.7890 - val_loss: 0.4198 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29811\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4755 - accuracy: 0.79 - ETA: 1s - loss: 0.4845 - accuracy: 0.77 - ETA: 1s - loss: 0.4858 - accuracy: 0.78 - ETA: 1s - loss: 0.4762 - accuracy: 0.78 - ETA: 1s - loss: 0.4751 - accuracy: 0.78 - ETA: 1s - loss: 0.4692 - accuracy: 0.78 - ETA: 1s - loss: 0.4698 - accuracy: 0.78 - ETA: 1s - loss: 0.4619 - accuracy: 0.78 - ETA: 1s - loss: 0.4601 - accuracy: 0.78 - ETA: 1s - loss: 0.4633 - accuracy: 0.78 - ETA: 1s - loss: 0.4618 - accuracy: 0.79 - ETA: 1s - loss: 0.4645 - accuracy: 0.78 - ETA: 1s - loss: 0.4653 - accuracy: 0.78 - ETA: 1s - loss: 0.4614 - accuracy: 0.78 - ETA: 1s - loss: 0.4575 - accuracy: 0.79 - ETA: 1s - loss: 0.4541 - accuracy: 0.79 - ETA: 1s - loss: 0.4568 - accuracy: 0.79 - ETA: 1s - loss: 0.4585 - accuracy: 0.79 - ETA: 1s - loss: 0.4581 - accuracy: 0.79 - ETA: 1s - loss: 0.4565 - accuracy: 0.79 - ETA: 0s - loss: 0.4565 - accuracy: 0.79 - ETA: 0s - loss: 0.4574 - accuracy: 0.79 - ETA: 0s - loss: 0.4565 - accuracy: 0.79 - ETA: 0s - loss: 0.4561 - accuracy: 0.79 - ETA: 0s - loss: 0.4561 - accuracy: 0.79 - ETA: 0s - loss: 0.4547 - accuracy: 0.79 - ETA: 0s - loss: 0.4556 - accuracy: 0.79 - ETA: 0s - loss: 0.4589 - accuracy: 0.79 - ETA: 0s - loss: 0.4606 - accuracy: 0.79 - ETA: 0s - loss: 0.4595 - accuracy: 0.79 - ETA: 0s - loss: 0.4613 - accuracy: 0.79 - ETA: 0s - loss: 0.4592 - accuracy: 0.79 - ETA: 0s - loss: 0.4579 - accuracy: 0.79 - ETA: 0s - loss: 0.4582 - accuracy: 0.79 - ETA: 0s - loss: 0.4562 - accuracy: 0.79 - ETA: 0s - loss: 0.4563 - accuracy: 0.79 - ETA: 0s - loss: 0.4556 - accuracy: 0.79 - 2s 210us/step - loss: 0.4559 - accuracy: 0.7961 - val_loss: 0.4246 - val_accuracy: 0.8206\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29811\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4636 - accuracy: 0.82 - ETA: 1s - loss: 0.4570 - accuracy: 0.80 - ETA: 1s - loss: 0.4464 - accuracy: 0.80 - ETA: 1s - loss: 0.4386 - accuracy: 0.80 - ETA: 1s - loss: 0.4390 - accuracy: 0.80 - ETA: 1s - loss: 0.4460 - accuracy: 0.80 - ETA: 1s - loss: 0.4467 - accuracy: 0.80 - ETA: 1s - loss: 0.4443 - accuracy: 0.80 - ETA: 1s - loss: 0.4428 - accuracy: 0.80 - ETA: 1s - loss: 0.4413 - accuracy: 0.80 - ETA: 1s - loss: 0.4439 - accuracy: 0.80 - ETA: 1s - loss: 0.4457 - accuracy: 0.79 - ETA: 1s - loss: 0.4447 - accuracy: 0.79 - ETA: 1s - loss: 0.4412 - accuracy: 0.79 - ETA: 1s - loss: 0.4408 - accuracy: 0.80 - ETA: 1s - loss: 0.4409 - accuracy: 0.80 - ETA: 1s - loss: 0.4433 - accuracy: 0.79 - ETA: 1s - loss: 0.4459 - accuracy: 0.79 - ETA: 1s - loss: 0.4471 - accuracy: 0.79 - ETA: 1s - loss: 0.4493 - accuracy: 0.79 - ETA: 0s - loss: 0.4467 - accuracy: 0.79 - ETA: 0s - loss: 0.4453 - accuracy: 0.79 - ETA: 0s - loss: 0.4447 - accuracy: 0.79 - ETA: 0s - loss: 0.4473 - accuracy: 0.79 - ETA: 0s - loss: 0.4473 - accuracy: 0.79 - ETA: 0s - loss: 0.4451 - accuracy: 0.79 - ETA: 0s - loss: 0.4457 - accuracy: 0.79 - ETA: 0s - loss: 0.4468 - accuracy: 0.79 - ETA: 0s - loss: 0.4463 - accuracy: 0.79 - ETA: 0s - loss: 0.4455 - accuracy: 0.79 - ETA: 0s - loss: 0.4448 - accuracy: 0.79 - ETA: 0s - loss: 0.4469 - accuracy: 0.79 - ETA: 0s - loss: 0.4460 - accuracy: 0.79 - ETA: 0s - loss: 0.4464 - accuracy: 0.79 - ETA: 0s - loss: 0.4450 - accuracy: 0.79 - ETA: 0s - loss: 0.4463 - accuracy: 0.79 - ETA: 0s - loss: 0.4443 - accuracy: 0.79 - 2s 211us/step - loss: 0.4439 - accuracy: 0.7984 - val_loss: 0.3894 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.29811\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4330 - accuracy: 0.82 - ETA: 1s - loss: 0.4481 - accuracy: 0.81 - ETA: 2s - loss: 0.4640 - accuracy: 0.80 - ETA: 2s - loss: 0.4580 - accuracy: 0.80 - ETA: 1s - loss: 0.4518 - accuracy: 0.80 - ETA: 1s - loss: 0.4511 - accuracy: 0.79 - ETA: 1s - loss: 0.4490 - accuracy: 0.79 - ETA: 1s - loss: 0.4425 - accuracy: 0.79 - ETA: 1s - loss: 0.4448 - accuracy: 0.80 - ETA: 1s - loss: 0.4460 - accuracy: 0.79 - ETA: 1s - loss: 0.4464 - accuracy: 0.79 - ETA: 1s - loss: 0.4464 - accuracy: 0.79 - ETA: 1s - loss: 0.4440 - accuracy: 0.79 - ETA: 1s - loss: 0.4448 - accuracy: 0.79 - ETA: 1s - loss: 0.4496 - accuracy: 0.79 - ETA: 1s - loss: 0.4508 - accuracy: 0.79 - ETA: 1s - loss: 0.4530 - accuracy: 0.79 - ETA: 1s - loss: 0.4550 - accuracy: 0.79 - ETA: 1s - loss: 0.4510 - accuracy: 0.79 - ETA: 1s - loss: 0.4501 - accuracy: 0.79 - ETA: 0s - loss: 0.4463 - accuracy: 0.79 - ETA: 0s - loss: 0.4439 - accuracy: 0.79 - ETA: 0s - loss: 0.4445 - accuracy: 0.79 - ETA: 0s - loss: 0.4469 - accuracy: 0.79 - ETA: 0s - loss: 0.4471 - accuracy: 0.79 - ETA: 0s - loss: 0.4434 - accuracy: 0.79 - ETA: 0s - loss: 0.4431 - accuracy: 0.80 - ETA: 0s - loss: 0.4434 - accuracy: 0.79 - ETA: 0s - loss: 0.4437 - accuracy: 0.79 - ETA: 0s - loss: 0.4428 - accuracy: 0.79 - ETA: 0s - loss: 0.4420 - accuracy: 0.80 - ETA: 0s - loss: 0.4418 - accuracy: 0.80 - ETA: 0s - loss: 0.4422 - accuracy: 0.80 - ETA: 0s - loss: 0.4411 - accuracy: 0.80 - ETA: 0s - loss: 0.4411 - accuracy: 0.80 - ETA: 0s - loss: 0.4409 - accuracy: 0.80 - ETA: 0s - loss: 0.4392 - accuracy: 0.80 - 2s 210us/step - loss: 0.4394 - accuracy: 0.8023 - val_loss: 0.4169 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29811\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4349 - accuracy: 0.82 - ETA: 2s - loss: 0.4035 - accuracy: 0.82 - ETA: 2s - loss: 0.4119 - accuracy: 0.81 - ETA: 1s - loss: 0.4186 - accuracy: 0.81 - ETA: 1s - loss: 0.4140 - accuracy: 0.81 - ETA: 1s - loss: 0.4270 - accuracy: 0.79 - ETA: 1s - loss: 0.4350 - accuracy: 0.79 - ETA: 1s - loss: 0.4366 - accuracy: 0.79 - ETA: 1s - loss: 0.4347 - accuracy: 0.79 - ETA: 1s - loss: 0.4279 - accuracy: 0.80 - ETA: 1s - loss: 0.4291 - accuracy: 0.79 - ETA: 1s - loss: 0.4297 - accuracy: 0.79 - ETA: 1s - loss: 0.4263 - accuracy: 0.80 - ETA: 1s - loss: 0.4286 - accuracy: 0.80 - ETA: 1s - loss: 0.4261 - accuracy: 0.79 - ETA: 1s - loss: 0.4225 - accuracy: 0.80 - ETA: 1s - loss: 0.4237 - accuracy: 0.80 - ETA: 1s - loss: 0.4248 - accuracy: 0.80 - ETA: 1s - loss: 0.4241 - accuracy: 0.80 - ETA: 1s - loss: 0.4237 - accuracy: 0.80 - ETA: 0s - loss: 0.4224 - accuracy: 0.80 - ETA: 0s - loss: 0.4257 - accuracy: 0.80 - ETA: 0s - loss: 0.4259 - accuracy: 0.80 - ETA: 0s - loss: 0.4270 - accuracy: 0.80 - ETA: 0s - loss: 0.4273 - accuracy: 0.80 - ETA: 0s - loss: 0.4266 - accuracy: 0.80 - ETA: 0s - loss: 0.4250 - accuracy: 0.80 - ETA: 0s - loss: 0.4258 - accuracy: 0.80 - ETA: 0s - loss: 0.4250 - accuracy: 0.80 - ETA: 0s - loss: 0.4250 - accuracy: 0.80 - ETA: 0s - loss: 0.4265 - accuracy: 0.80 - ETA: 0s - loss: 0.4269 - accuracy: 0.80 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4258 - accuracy: 0.80 - ETA: 0s - loss: 0.4271 - accuracy: 0.80 - ETA: 0s - loss: 0.4270 - accuracy: 0.80 - 2s 214us/step - loss: 0.4261 - accuracy: 0.8044 - val_loss: 0.4268 - val_accuracy: 0.8029\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29811\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5355 - accuracy: 0.75 - ETA: 1s - loss: 0.4749 - accuracy: 0.77 - ETA: 1s - loss: 0.4407 - accuracy: 0.79 - ETA: 1s - loss: 0.4277 - accuracy: 0.79 - ETA: 1s - loss: 0.4191 - accuracy: 0.80 - ETA: 1s - loss: 0.4199 - accuracy: 0.80 - ETA: 1s - loss: 0.4141 - accuracy: 0.80 - ETA: 1s - loss: 0.4126 - accuracy: 0.80 - ETA: 1s - loss: 0.4127 - accuracy: 0.81 - ETA: 1s - loss: 0.4121 - accuracy: 0.81 - ETA: 1s - loss: 0.4108 - accuracy: 0.81 - ETA: 1s - loss: 0.4119 - accuracy: 0.81 - ETA: 1s - loss: 0.4176 - accuracy: 0.80 - ETA: 1s - loss: 0.4184 - accuracy: 0.80 - ETA: 1s - loss: 0.4240 - accuracy: 0.80 - ETA: 1s - loss: 0.4238 - accuracy: 0.80 - ETA: 1s - loss: 0.4257 - accuracy: 0.80 - ETA: 1s - loss: 0.4268 - accuracy: 0.80 - ETA: 1s - loss: 0.4276 - accuracy: 0.80 - ETA: 1s - loss: 0.4280 - accuracy: 0.80 - ETA: 0s - loss: 0.4272 - accuracy: 0.80 - ETA: 0s - loss: 0.4339 - accuracy: 0.79 - ETA: 0s - loss: 0.4364 - accuracy: 0.79 - ETA: 0s - loss: 0.4354 - accuracy: 0.79 - ETA: 0s - loss: 0.4347 - accuracy: 0.79 - ETA: 0s - loss: 0.4346 - accuracy: 0.79 - ETA: 0s - loss: 0.4327 - accuracy: 0.80 - ETA: 0s - loss: 0.4317 - accuracy: 0.80 - ETA: 0s - loss: 0.4325 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.4288 - accuracy: 0.80 - ETA: 0s - loss: 0.4277 - accuracy: 0.80 - ETA: 0s - loss: 0.4267 - accuracy: 0.80 - ETA: 0s - loss: 0.4263 - accuracy: 0.80 - ETA: 0s - loss: 0.4259 - accuracy: 0.80 - ETA: 0s - loss: 0.4258 - accuracy: 0.80 - ETA: 0s - loss: 0.4264 - accuracy: 0.80 - 2s 210us/step - loss: 0.4268 - accuracy: 0.8041 - val_loss: 0.3854 - val_accuracy: 0.8315\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.29811\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3864 - accuracy: 0.78 - ETA: 2s - loss: 0.4415 - accuracy: 0.78 - ETA: 2s - loss: 0.4280 - accuracy: 0.79 - ETA: 2s - loss: 0.4195 - accuracy: 0.80 - ETA: 1s - loss: 0.4198 - accuracy: 0.80 - ETA: 1s - loss: 0.4128 - accuracy: 0.81 - ETA: 1s - loss: 0.4147 - accuracy: 0.80 - ETA: 1s - loss: 0.4201 - accuracy: 0.80 - ETA: 1s - loss: 0.4239 - accuracy: 0.80 - ETA: 1s - loss: 0.4174 - accuracy: 0.80 - ETA: 1s - loss: 0.4193 - accuracy: 0.80 - ETA: 1s - loss: 0.4161 - accuracy: 0.80 - ETA: 1s - loss: 0.4130 - accuracy: 0.80 - ETA: 1s - loss: 0.4146 - accuracy: 0.80 - ETA: 1s - loss: 0.4161 - accuracy: 0.80 - ETA: 1s - loss: 0.4147 - accuracy: 0.80 - ETA: 1s - loss: 0.4149 - accuracy: 0.80 - ETA: 1s - loss: 0.4145 - accuracy: 0.80 - ETA: 1s - loss: 0.4162 - accuracy: 0.80 - ETA: 1s - loss: 0.4147 - accuracy: 0.80 - ETA: 1s - loss: 0.4116 - accuracy: 0.80 - ETA: 0s - loss: 0.4092 - accuracy: 0.80 - ETA: 0s - loss: 0.4127 - accuracy: 0.80 - ETA: 0s - loss: 0.4133 - accuracy: 0.80 - ETA: 0s - loss: 0.4136 - accuracy: 0.80 - ETA: 0s - loss: 0.4154 - accuracy: 0.80 - ETA: 0s - loss: 0.4142 - accuracy: 0.80 - ETA: 0s - loss: 0.4148 - accuracy: 0.80 - ETA: 0s - loss: 0.4156 - accuracy: 0.80 - ETA: 0s - loss: 0.4159 - accuracy: 0.80 - ETA: 0s - loss: 0.4141 - accuracy: 0.80 - ETA: 0s - loss: 0.4136 - accuracy: 0.80 - ETA: 0s - loss: 0.4136 - accuracy: 0.80 - ETA: 0s - loss: 0.4137 - accuracy: 0.80 - ETA: 0s - loss: 0.4146 - accuracy: 0.80 - ETA: 0s - loss: 0.4144 - accuracy: 0.80 - ETA: 0s - loss: 0.4156 - accuracy: 0.80 - 2s 228us/step - loss: 0.4151 - accuracy: 0.8077 - val_loss: 0.4086 - val_accuracy: 0.8062\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29811\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.4541 - accuracy: 0.80 - ETA: 2s - loss: 0.4023 - accuracy: 0.81 - ETA: 2s - loss: 0.4192 - accuracy: 0.80 - ETA: 2s - loss: 0.4128 - accuracy: 0.81 - ETA: 2s - loss: 0.4101 - accuracy: 0.81 - ETA: 2s - loss: 0.4115 - accuracy: 0.81 - ETA: 1s - loss: 0.4108 - accuracy: 0.80 - ETA: 1s - loss: 0.4199 - accuracy: 0.80 - ETA: 1s - loss: 0.4154 - accuracy: 0.80 - ETA: 1s - loss: 0.4184 - accuracy: 0.80 - ETA: 1s - loss: 0.4147 - accuracy: 0.80 - ETA: 1s - loss: 0.4090 - accuracy: 0.80 - ETA: 1s - loss: 0.4157 - accuracy: 0.80 - ETA: 1s - loss: 0.4161 - accuracy: 0.80 - ETA: 1s - loss: 0.4148 - accuracy: 0.80 - ETA: 1s - loss: 0.4180 - accuracy: 0.80 - ETA: 1s - loss: 0.4176 - accuracy: 0.80 - ETA: 1s - loss: 0.4169 - accuracy: 0.80 - ETA: 1s - loss: 0.4126 - accuracy: 0.80 - ETA: 1s - loss: 0.4115 - accuracy: 0.80 - ETA: 1s - loss: 0.4119 - accuracy: 0.80 - ETA: 0s - loss: 0.4118 - accuracy: 0.80 - ETA: 0s - loss: 0.4097 - accuracy: 0.80 - ETA: 0s - loss: 0.4084 - accuracy: 0.80 - ETA: 0s - loss: 0.4075 - accuracy: 0.80 - ETA: 0s - loss: 0.4086 - accuracy: 0.80 - ETA: 0s - loss: 0.4064 - accuracy: 0.80 - ETA: 0s - loss: 0.4069 - accuracy: 0.80 - ETA: 0s - loss: 0.4103 - accuracy: 0.80 - ETA: 0s - loss: 0.4101 - accuracy: 0.80 - ETA: 0s - loss: 0.4098 - accuracy: 0.80 - ETA: 0s - loss: 0.4085 - accuracy: 0.80 - ETA: 0s - loss: 0.4077 - accuracy: 0.80 - ETA: 0s - loss: 0.4081 - accuracy: 0.80 - ETA: 0s - loss: 0.4073 - accuracy: 0.81 - ETA: 0s - loss: 0.4073 - accuracy: 0.81 - ETA: 0s - loss: 0.4057 - accuracy: 0.81 - 2s 229us/step - loss: 0.4051 - accuracy: 0.8106 - val_loss: 0.3823 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.29811\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5248 - accuracy: 0.73 - ETA: 2s - loss: 0.4489 - accuracy: 0.78 - ETA: 2s - loss: 0.4293 - accuracy: 0.80 - ETA: 2s - loss: 0.4137 - accuracy: 0.80 - ETA: 2s - loss: 0.4162 - accuracy: 0.81 - ETA: 1s - loss: 0.4231 - accuracy: 0.80 - ETA: 1s - loss: 0.4172 - accuracy: 0.80 - ETA: 1s - loss: 0.4113 - accuracy: 0.81 - ETA: 1s - loss: 0.4065 - accuracy: 0.81 - ETA: 1s - loss: 0.4071 - accuracy: 0.81 - ETA: 1s - loss: 0.4094 - accuracy: 0.81 - ETA: 1s - loss: 0.4099 - accuracy: 0.81 - ETA: 1s - loss: 0.4103 - accuracy: 0.81 - ETA: 1s - loss: 0.4084 - accuracy: 0.81 - ETA: 1s - loss: 0.4075 - accuracy: 0.81 - ETA: 1s - loss: 0.4104 - accuracy: 0.81 - ETA: 1s - loss: 0.4089 - accuracy: 0.81 - ETA: 1s - loss: 0.4068 - accuracy: 0.81 - ETA: 1s - loss: 0.4084 - accuracy: 0.81 - ETA: 1s - loss: 0.4052 - accuracy: 0.81 - ETA: 1s - loss: 0.4041 - accuracy: 0.81 - ETA: 1s - loss: 0.4054 - accuracy: 0.81 - ETA: 0s - loss: 0.4059 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4060 - accuracy: 0.81 - ETA: 0s - loss: 0.4070 - accuracy: 0.81 - ETA: 0s - loss: 0.4046 - accuracy: 0.81 - ETA: 0s - loss: 0.4027 - accuracy: 0.81 - ETA: 0s - loss: 0.4019 - accuracy: 0.81 - ETA: 0s - loss: 0.3999 - accuracy: 0.81 - ETA: 0s - loss: 0.4014 - accuracy: 0.81 - ETA: 0s - loss: 0.4034 - accuracy: 0.81 - ETA: 0s - loss: 0.4019 - accuracy: 0.81 - ETA: 0s - loss: 0.3993 - accuracy: 0.81 - ETA: 0s - loss: 0.3997 - accuracy: 0.81 - ETA: 0s - loss: 0.3989 - accuracy: 0.81 - ETA: 0s - loss: 0.4005 - accuracy: 0.81 - 3s 234us/step - loss: 0.4009 - accuracy: 0.8147 - val_loss: 0.4210 - val_accuracy: 0.7885\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.29811\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4301 - accuracy: 0.78 - ETA: 2s - loss: 0.3847 - accuracy: 0.82 - ETA: 2s - loss: 0.3729 - accuracy: 0.82 - ETA: 2s - loss: 0.3732 - accuracy: 0.83 - ETA: 2s - loss: 0.3847 - accuracy: 0.82 - ETA: 1s - loss: 0.3964 - accuracy: 0.81 - ETA: 1s - loss: 0.3932 - accuracy: 0.81 - ETA: 1s - loss: 0.3941 - accuracy: 0.81 - ETA: 1s - loss: 0.3922 - accuracy: 0.81 - ETA: 1s - loss: 0.3966 - accuracy: 0.81 - ETA: 1s - loss: 0.3901 - accuracy: 0.81 - ETA: 1s - loss: 0.3934 - accuracy: 0.81 - ETA: 1s - loss: 0.3946 - accuracy: 0.81 - ETA: 1s - loss: 0.3900 - accuracy: 0.81 - ETA: 1s - loss: 0.3878 - accuracy: 0.81 - ETA: 1s - loss: 0.3878 - accuracy: 0.82 - ETA: 1s - loss: 0.3892 - accuracy: 0.82 - ETA: 1s - loss: 0.3873 - accuracy: 0.82 - ETA: 1s - loss: 0.3880 - accuracy: 0.82 - ETA: 1s - loss: 0.3907 - accuracy: 0.81 - ETA: 1s - loss: 0.3923 - accuracy: 0.81 - ETA: 0s - loss: 0.3931 - accuracy: 0.81 - ETA: 0s - loss: 0.3941 - accuracy: 0.81 - ETA: 0s - loss: 0.3944 - accuracy: 0.81 - ETA: 0s - loss: 0.3980 - accuracy: 0.81 - ETA: 0s - loss: 0.3994 - accuracy: 0.81 - ETA: 0s - loss: 0.4007 - accuracy: 0.81 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.4032 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.81 - ETA: 0s - loss: 0.4012 - accuracy: 0.81 - ETA: 0s - loss: 0.4021 - accuracy: 0.81 - ETA: 0s - loss: 0.4004 - accuracy: 0.81 - ETA: 0s - loss: 0.3994 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.81 - ETA: 0s - loss: 0.3994 - accuracy: 0.81 - 2s 222us/step - loss: 0.3992 - accuracy: 0.8155 - val_loss: 0.3537 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.29811\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3564 - accuracy: 0.82 - ETA: 2s - loss: 0.3967 - accuracy: 0.80 - ETA: 2s - loss: 0.3777 - accuracy: 0.83 - ETA: 1s - loss: 0.3724 - accuracy: 0.83 - ETA: 1s - loss: 0.3754 - accuracy: 0.83 - ETA: 1s - loss: 0.3733 - accuracy: 0.83 - ETA: 1s - loss: 0.3810 - accuracy: 0.82 - ETA: 1s - loss: 0.3784 - accuracy: 0.82 - ETA: 1s - loss: 0.3778 - accuracy: 0.82 - ETA: 1s - loss: 0.3853 - accuracy: 0.82 - ETA: 1s - loss: 0.3838 - accuracy: 0.82 - ETA: 1s - loss: 0.3852 - accuracy: 0.82 - ETA: 1s - loss: 0.3856 - accuracy: 0.82 - ETA: 1s - loss: 0.3908 - accuracy: 0.81 - ETA: 1s - loss: 0.3901 - accuracy: 0.81 - ETA: 1s - loss: 0.3911 - accuracy: 0.81 - ETA: 1s - loss: 0.3894 - accuracy: 0.81 - ETA: 1s - loss: 0.3905 - accuracy: 0.81 - ETA: 1s - loss: 0.3904 - accuracy: 0.81 - ETA: 1s - loss: 0.3908 - accuracy: 0.81 - ETA: 1s - loss: 0.3912 - accuracy: 0.81 - ETA: 0s - loss: 0.3912 - accuracy: 0.81 - ETA: 0s - loss: 0.3919 - accuracy: 0.81 - ETA: 0s - loss: 0.3925 - accuracy: 0.81 - ETA: 0s - loss: 0.3918 - accuracy: 0.81 - ETA: 0s - loss: 0.3926 - accuracy: 0.81 - ETA: 0s - loss: 0.3921 - accuracy: 0.81 - ETA: 0s - loss: 0.3932 - accuracy: 0.81 - ETA: 0s - loss: 0.3924 - accuracy: 0.81 - ETA: 0s - loss: 0.3905 - accuracy: 0.81 - ETA: 0s - loss: 0.3897 - accuracy: 0.81 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.81 - ETA: 0s - loss: 0.3892 - accuracy: 0.81 - ETA: 0s - loss: 0.3886 - accuracy: 0.81 - ETA: 0s - loss: 0.3874 - accuracy: 0.81 - 2s 219us/step - loss: 0.3875 - accuracy: 0.8200 - val_loss: 0.3503 - val_accuracy: 0.8382\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.29811\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3743 - accuracy: 0.83 - ETA: 1s - loss: 0.3482 - accuracy: 0.84 - ETA: 1s - loss: 0.3550 - accuracy: 0.84 - ETA: 1s - loss: 0.3692 - accuracy: 0.83 - ETA: 1s - loss: 0.3752 - accuracy: 0.82 - ETA: 1s - loss: 0.3873 - accuracy: 0.82 - ETA: 1s - loss: 0.3930 - accuracy: 0.82 - ETA: 1s - loss: 0.3895 - accuracy: 0.82 - ETA: 1s - loss: 0.3902 - accuracy: 0.82 - ETA: 1s - loss: 0.3830 - accuracy: 0.82 - ETA: 1s - loss: 0.3866 - accuracy: 0.82 - ETA: 1s - loss: 0.3859 - accuracy: 0.82 - ETA: 1s - loss: 0.3834 - accuracy: 0.82 - ETA: 1s - loss: 0.3880 - accuracy: 0.82 - ETA: 1s - loss: 0.3911 - accuracy: 0.82 - ETA: 1s - loss: 0.3906 - accuracy: 0.82 - ETA: 1s - loss: 0.3886 - accuracy: 0.82 - ETA: 1s - loss: 0.3868 - accuracy: 0.82 - ETA: 1s - loss: 0.3835 - accuracy: 0.82 - ETA: 0s - loss: 0.3810 - accuracy: 0.82 - ETA: 0s - loss: 0.3831 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3869 - accuracy: 0.82 - ETA: 0s - loss: 0.3866 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3918 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - 2s 209us/step - loss: 0.3902 - accuracy: 0.8221 - val_loss: 0.3717 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.29811\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3234 - accuracy: 0.82 - ETA: 1s - loss: 0.3745 - accuracy: 0.82 - ETA: 1s - loss: 0.3784 - accuracy: 0.82 - ETA: 1s - loss: 0.3678 - accuracy: 0.83 - ETA: 1s - loss: 0.3682 - accuracy: 0.83 - ETA: 1s - loss: 0.3674 - accuracy: 0.83 - ETA: 1s - loss: 0.3606 - accuracy: 0.83 - ETA: 1s - loss: 0.3726 - accuracy: 0.83 - ETA: 1s - loss: 0.3692 - accuracy: 0.83 - ETA: 1s - loss: 0.3715 - accuracy: 0.83 - ETA: 1s - loss: 0.3707 - accuracy: 0.83 - ETA: 1s - loss: 0.3758 - accuracy: 0.82 - ETA: 1s - loss: 0.3776 - accuracy: 0.82 - ETA: 1s - loss: 0.3769 - accuracy: 0.82 - ETA: 1s - loss: 0.3748 - accuracy: 0.82 - ETA: 1s - loss: 0.3784 - accuracy: 0.82 - ETA: 1s - loss: 0.3790 - accuracy: 0.82 - ETA: 1s - loss: 0.3801 - accuracy: 0.82 - ETA: 1s - loss: 0.3791 - accuracy: 0.82 - ETA: 0s - loss: 0.3788 - accuracy: 0.82 - ETA: 0s - loss: 0.3783 - accuracy: 0.82 - ETA: 0s - loss: 0.3805 - accuracy: 0.82 - ETA: 0s - loss: 0.3808 - accuracy: 0.82 - ETA: 0s - loss: 0.3828 - accuracy: 0.82 - ETA: 0s - loss: 0.3832 - accuracy: 0.82 - ETA: 0s - loss: 0.3813 - accuracy: 0.82 - ETA: 0s - loss: 0.3821 - accuracy: 0.82 - ETA: 0s - loss: 0.3842 - accuracy: 0.82 - ETA: 0s - loss: 0.3836 - accuracy: 0.82 - ETA: 0s - loss: 0.3821 - accuracy: 0.82 - ETA: 0s - loss: 0.3809 - accuracy: 0.82 - ETA: 0s - loss: 0.3821 - accuracy: 0.82 - ETA: 0s - loss: 0.3819 - accuracy: 0.82 - ETA: 0s - loss: 0.3809 - accuracy: 0.82 - ETA: 0s - loss: 0.3816 - accuracy: 0.82 - ETA: 0s - loss: 0.3814 - accuracy: 0.82 - 2s 209us/step - loss: 0.3809 - accuracy: 0.8242 - val_loss: 0.3709 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.29811\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2828 - accuracy: 0.88 - ETA: 1s - loss: 0.3288 - accuracy: 0.85 - ETA: 1s - loss: 0.3777 - accuracy: 0.83 - ETA: 1s - loss: 0.3842 - accuracy: 0.82 - ETA: 1s - loss: 0.3895 - accuracy: 0.82 - ETA: 1s - loss: 0.3934 - accuracy: 0.81 - ETA: 1s - loss: 0.3947 - accuracy: 0.81 - ETA: 1s - loss: 0.3945 - accuracy: 0.81 - ETA: 1s - loss: 0.3988 - accuracy: 0.81 - ETA: 1s - loss: 0.3943 - accuracy: 0.81 - ETA: 1s - loss: 0.3937 - accuracy: 0.81 - ETA: 1s - loss: 0.3950 - accuracy: 0.81 - ETA: 1s - loss: 0.3972 - accuracy: 0.81 - ETA: 1s - loss: 0.4006 - accuracy: 0.81 - ETA: 1s - loss: 0.3988 - accuracy: 0.81 - ETA: 1s - loss: 0.3960 - accuracy: 0.82 - ETA: 1s - loss: 0.3975 - accuracy: 0.81 - ETA: 1s - loss: 0.3958 - accuracy: 0.81 - ETA: 1s - loss: 0.3948 - accuracy: 0.81 - ETA: 0s - loss: 0.3934 - accuracy: 0.81 - ETA: 0s - loss: 0.3948 - accuracy: 0.81 - ETA: 0s - loss: 0.3937 - accuracy: 0.81 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3870 - accuracy: 0.82 - ETA: 0s - loss: 0.3869 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3869 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3872 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3856 - accuracy: 0.82 - ETA: 0s - loss: 0.3842 - accuracy: 0.82 - 2s 213us/step - loss: 0.3843 - accuracy: 0.8219 - val_loss: 0.3505 - val_accuracy: 0.8416\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.29811\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3269 - accuracy: 0.85 - ETA: 1s - loss: 0.3728 - accuracy: 0.83 - ETA: 1s - loss: 0.3737 - accuracy: 0.83 - ETA: 1s - loss: 0.3714 - accuracy: 0.83 - ETA: 1s - loss: 0.3787 - accuracy: 0.82 - ETA: 1s - loss: 0.3931 - accuracy: 0.81 - ETA: 1s - loss: 0.3872 - accuracy: 0.81 - ETA: 1s - loss: 0.3931 - accuracy: 0.81 - ETA: 1s - loss: 0.3802 - accuracy: 0.82 - ETA: 1s - loss: 0.3758 - accuracy: 0.82 - ETA: 1s - loss: 0.3727 - accuracy: 0.82 - ETA: 1s - loss: 0.3691 - accuracy: 0.82 - ETA: 1s - loss: 0.3682 - accuracy: 0.82 - ETA: 1s - loss: 0.3713 - accuracy: 0.82 - ETA: 1s - loss: 0.3715 - accuracy: 0.82 - ETA: 1s - loss: 0.3741 - accuracy: 0.82 - ETA: 1s - loss: 0.3735 - accuracy: 0.82 - ETA: 1s - loss: 0.3746 - accuracy: 0.82 - ETA: 0s - loss: 0.3755 - accuracy: 0.82 - ETA: 0s - loss: 0.3763 - accuracy: 0.82 - ETA: 0s - loss: 0.3777 - accuracy: 0.82 - ETA: 0s - loss: 0.3781 - accuracy: 0.82 - ETA: 0s - loss: 0.3772 - accuracy: 0.82 - ETA: 0s - loss: 0.3775 - accuracy: 0.82 - ETA: 0s - loss: 0.3789 - accuracy: 0.82 - ETA: 0s - loss: 0.3787 - accuracy: 0.82 - ETA: 0s - loss: 0.3770 - accuracy: 0.82 - ETA: 0s - loss: 0.3774 - accuracy: 0.82 - ETA: 0s - loss: 0.3767 - accuracy: 0.82 - ETA: 0s - loss: 0.3777 - accuracy: 0.82 - ETA: 0s - loss: 0.3779 - accuracy: 0.82 - ETA: 0s - loss: 0.3768 - accuracy: 0.82 - ETA: 0s - loss: 0.3766 - accuracy: 0.82 - ETA: 0s - loss: 0.3774 - accuracy: 0.82 - ETA: 0s - loss: 0.3789 - accuracy: 0.82 - 2s 211us/step - loss: 0.3775 - accuracy: 0.8265 - val_loss: 0.3403 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.29811\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3418 - accuracy: 0.84 - ETA: 2s - loss: 0.3743 - accuracy: 0.83 - ETA: 2s - loss: 0.3924 - accuracy: 0.83 - ETA: 2s - loss: 0.3780 - accuracy: 0.83 - ETA: 1s - loss: 0.3761 - accuracy: 0.83 - ETA: 1s - loss: 0.3738 - accuracy: 0.83 - ETA: 1s - loss: 0.3714 - accuracy: 0.83 - ETA: 1s - loss: 0.3728 - accuracy: 0.83 - ETA: 1s - loss: 0.3806 - accuracy: 0.83 - ETA: 1s - loss: 0.3814 - accuracy: 0.83 - ETA: 1s - loss: 0.3762 - accuracy: 0.83 - ETA: 1s - loss: 0.3737 - accuracy: 0.83 - ETA: 1s - loss: 0.3720 - accuracy: 0.83 - ETA: 1s - loss: 0.3658 - accuracy: 0.83 - ETA: 1s - loss: 0.3621 - accuracy: 0.84 - ETA: 1s - loss: 0.3624 - accuracy: 0.84 - ETA: 1s - loss: 0.3597 - accuracy: 0.84 - ETA: 1s - loss: 0.3591 - accuracy: 0.84 - ETA: 1s - loss: 0.3600 - accuracy: 0.84 - ETA: 0s - loss: 0.3623 - accuracy: 0.84 - ETA: 0s - loss: 0.3612 - accuracy: 0.84 - ETA: 0s - loss: 0.3606 - accuracy: 0.84 - ETA: 0s - loss: 0.3591 - accuracy: 0.84 - ETA: 0s - loss: 0.3587 - accuracy: 0.84 - ETA: 0s - loss: 0.3597 - accuracy: 0.84 - ETA: 0s - loss: 0.3595 - accuracy: 0.84 - ETA: 0s - loss: 0.3597 - accuracy: 0.84 - ETA: 0s - loss: 0.3595 - accuracy: 0.84 - ETA: 0s - loss: 0.3595 - accuracy: 0.84 - ETA: 0s - loss: 0.3604 - accuracy: 0.84 - ETA: 0s - loss: 0.3634 - accuracy: 0.84 - ETA: 0s - loss: 0.3642 - accuracy: 0.83 - ETA: 0s - loss: 0.3654 - accuracy: 0.83 - ETA: 0s - loss: 0.3658 - accuracy: 0.83 - ETA: 0s - loss: 0.3654 - accuracy: 0.83 - 2s 210us/step - loss: 0.3641 - accuracy: 0.8397 - val_loss: 0.3411 - val_accuracy: 0.8458\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.29811\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3369 - accuracy: 0.85 - ETA: 2s - loss: 0.3315 - accuracy: 0.85 - ETA: 2s - loss: 0.3405 - accuracy: 0.84 - ETA: 2s - loss: 0.3590 - accuracy: 0.82 - ETA: 1s - loss: 0.3641 - accuracy: 0.82 - ETA: 1s - loss: 0.3746 - accuracy: 0.82 - ETA: 1s - loss: 0.3732 - accuracy: 0.82 - ETA: 1s - loss: 0.3813 - accuracy: 0.82 - ETA: 1s - loss: 0.3787 - accuracy: 0.82 - ETA: 1s - loss: 0.3707 - accuracy: 0.83 - ETA: 1s - loss: 0.3680 - accuracy: 0.83 - ETA: 1s - loss: 0.3698 - accuracy: 0.83 - ETA: 1s - loss: 0.3677 - accuracy: 0.83 - ETA: 1s - loss: 0.3698 - accuracy: 0.83 - ETA: 1s - loss: 0.3719 - accuracy: 0.83 - ETA: 1s - loss: 0.3688 - accuracy: 0.83 - ETA: 1s - loss: 0.3690 - accuracy: 0.83 - ETA: 1s - loss: 0.3677 - accuracy: 0.83 - ETA: 1s - loss: 0.3693 - accuracy: 0.83 - ETA: 1s - loss: 0.3696 - accuracy: 0.83 - ETA: 0s - loss: 0.3703 - accuracy: 0.82 - ETA: 0s - loss: 0.3692 - accuracy: 0.83 - ETA: 0s - loss: 0.3688 - accuracy: 0.82 - ETA: 0s - loss: 0.3700 - accuracy: 0.82 - ETA: 0s - loss: 0.3703 - accuracy: 0.83 - ETA: 0s - loss: 0.3704 - accuracy: 0.82 - ETA: 0s - loss: 0.3698 - accuracy: 0.82 - ETA: 0s - loss: 0.3691 - accuracy: 0.83 - ETA: 0s - loss: 0.3687 - accuracy: 0.83 - ETA: 0s - loss: 0.3696 - accuracy: 0.82 - ETA: 0s - loss: 0.3691 - accuracy: 0.82 - ETA: 0s - loss: 0.3677 - accuracy: 0.82 - ETA: 0s - loss: 0.3678 - accuracy: 0.83 - ETA: 0s - loss: 0.3679 - accuracy: 0.83 - ETA: 0s - loss: 0.3679 - accuracy: 0.83 - ETA: 0s - loss: 0.3684 - accuracy: 0.83 - 2s 212us/step - loss: 0.3681 - accuracy: 0.8308 - val_loss: 0.3503 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.29811\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4210 - accuracy: 0.79 - ETA: 1s - loss: 0.4065 - accuracy: 0.81 - ETA: 1s - loss: 0.3936 - accuracy: 0.81 - ETA: 2s - loss: 0.3873 - accuracy: 0.82 - ETA: 1s - loss: 0.3884 - accuracy: 0.82 - ETA: 1s - loss: 0.3805 - accuracy: 0.83 - ETA: 1s - loss: 0.3852 - accuracy: 0.82 - ETA: 1s - loss: 0.3882 - accuracy: 0.82 - ETA: 1s - loss: 0.3877 - accuracy: 0.82 - ETA: 1s - loss: 0.3856 - accuracy: 0.82 - ETA: 1s - loss: 0.3799 - accuracy: 0.82 - ETA: 1s - loss: 0.3747 - accuracy: 0.82 - ETA: 1s - loss: 0.3757 - accuracy: 0.82 - ETA: 1s - loss: 0.3784 - accuracy: 0.82 - ETA: 1s - loss: 0.3771 - accuracy: 0.82 - ETA: 1s - loss: 0.3744 - accuracy: 0.82 - ETA: 1s - loss: 0.3717 - accuracy: 0.82 - ETA: 1s - loss: 0.3699 - accuracy: 0.82 - ETA: 1s - loss: 0.3716 - accuracy: 0.82 - ETA: 0s - loss: 0.3707 - accuracy: 0.82 - ETA: 0s - loss: 0.3702 - accuracy: 0.82 - ETA: 0s - loss: 0.3728 - accuracy: 0.82 - ETA: 0s - loss: 0.3723 - accuracy: 0.82 - ETA: 0s - loss: 0.3714 - accuracy: 0.82 - ETA: 0s - loss: 0.3721 - accuracy: 0.82 - ETA: 0s - loss: 0.3701 - accuracy: 0.82 - ETA: 0s - loss: 0.3687 - accuracy: 0.82 - ETA: 0s - loss: 0.3666 - accuracy: 0.82 - ETA: 0s - loss: 0.3663 - accuracy: 0.82 - ETA: 0s - loss: 0.3648 - accuracy: 0.82 - ETA: 0s - loss: 0.3664 - accuracy: 0.82 - ETA: 0s - loss: 0.3673 - accuracy: 0.82 - ETA: 0s - loss: 0.3663 - accuracy: 0.82 - ETA: 0s - loss: 0.3679 - accuracy: 0.82 - ETA: 0s - loss: 0.3677 - accuracy: 0.82 - ETA: 0s - loss: 0.3671 - accuracy: 0.82 - 2s 214us/step - loss: 0.3669 - accuracy: 0.8287 - val_loss: 0.3472 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.29811\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3725 - accuracy: 0.80 - ETA: 2s - loss: 0.3693 - accuracy: 0.82 - ETA: 2s - loss: 0.3560 - accuracy: 0.82 - ETA: 2s - loss: 0.3679 - accuracy: 0.82 - ETA: 1s - loss: 0.3664 - accuracy: 0.83 - ETA: 1s - loss: 0.3535 - accuracy: 0.83 - ETA: 1s - loss: 0.3541 - accuracy: 0.84 - ETA: 1s - loss: 0.3576 - accuracy: 0.84 - ETA: 1s - loss: 0.3559 - accuracy: 0.84 - ETA: 1s - loss: 0.3607 - accuracy: 0.84 - ETA: 1s - loss: 0.3601 - accuracy: 0.84 - ETA: 1s - loss: 0.3557 - accuracy: 0.84 - ETA: 1s - loss: 0.3559 - accuracy: 0.84 - ETA: 1s - loss: 0.3547 - accuracy: 0.84 - ETA: 1s - loss: 0.3554 - accuracy: 0.84 - ETA: 1s - loss: 0.3562 - accuracy: 0.84 - ETA: 1s - loss: 0.3573 - accuracy: 0.83 - ETA: 1s - loss: 0.3601 - accuracy: 0.83 - ETA: 1s - loss: 0.3618 - accuracy: 0.83 - ETA: 0s - loss: 0.3615 - accuracy: 0.83 - ETA: 0s - loss: 0.3605 - accuracy: 0.83 - ETA: 0s - loss: 0.3618 - accuracy: 0.83 - ETA: 0s - loss: 0.3599 - accuracy: 0.83 - ETA: 0s - loss: 0.3584 - accuracy: 0.83 - ETA: 0s - loss: 0.3589 - accuracy: 0.83 - ETA: 0s - loss: 0.3585 - accuracy: 0.83 - ETA: 0s - loss: 0.3577 - accuracy: 0.83 - ETA: 0s - loss: 0.3566 - accuracy: 0.84 - ETA: 0s - loss: 0.3591 - accuracy: 0.83 - ETA: 0s - loss: 0.3591 - accuracy: 0.83 - ETA: 0s - loss: 0.3577 - accuracy: 0.83 - ETA: 0s - loss: 0.3587 - accuracy: 0.83 - ETA: 0s - loss: 0.3583 - accuracy: 0.83 - ETA: 0s - loss: 0.3584 - accuracy: 0.83 - ETA: 0s - loss: 0.3585 - accuracy: 0.83 - ETA: 0s - loss: 0.3589 - accuracy: 0.83 - 2s 210us/step - loss: 0.3583 - accuracy: 0.8384 - val_loss: 0.3230 - val_accuracy: 0.8576\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.29811\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3597 - accuracy: 0.84 - ETA: 2s - loss: 0.3350 - accuracy: 0.85 - ETA: 2s - loss: 0.3326 - accuracy: 0.84 - ETA: 1s - loss: 0.3430 - accuracy: 0.84 - ETA: 1s - loss: 0.3447 - accuracy: 0.84 - ETA: 1s - loss: 0.3443 - accuracy: 0.84 - ETA: 1s - loss: 0.3423 - accuracy: 0.83 - ETA: 1s - loss: 0.3387 - accuracy: 0.84 - ETA: 1s - loss: 0.3444 - accuracy: 0.83 - ETA: 1s - loss: 0.3470 - accuracy: 0.83 - ETA: 1s - loss: 0.3493 - accuracy: 0.83 - ETA: 1s - loss: 0.3507 - accuracy: 0.83 - ETA: 1s - loss: 0.3499 - accuracy: 0.83 - ETA: 1s - loss: 0.3497 - accuracy: 0.83 - ETA: 1s - loss: 0.3524 - accuracy: 0.83 - ETA: 1s - loss: 0.3503 - accuracy: 0.83 - ETA: 1s - loss: 0.3528 - accuracy: 0.83 - ETA: 1s - loss: 0.3530 - accuracy: 0.83 - ETA: 1s - loss: 0.3538 - accuracy: 0.83 - ETA: 0s - loss: 0.3555 - accuracy: 0.83 - ETA: 0s - loss: 0.3548 - accuracy: 0.83 - ETA: 0s - loss: 0.3543 - accuracy: 0.83 - ETA: 0s - loss: 0.3531 - accuracy: 0.83 - ETA: 0s - loss: 0.3540 - accuracy: 0.83 - ETA: 0s - loss: 0.3531 - accuracy: 0.83 - ETA: 0s - loss: 0.3518 - accuracy: 0.83 - ETA: 0s - loss: 0.3547 - accuracy: 0.83 - ETA: 0s - loss: 0.3543 - accuracy: 0.83 - ETA: 0s - loss: 0.3560 - accuracy: 0.83 - ETA: 0s - loss: 0.3545 - accuracy: 0.83 - ETA: 0s - loss: 0.3536 - accuracy: 0.83 - ETA: 0s - loss: 0.3532 - accuracy: 0.83 - ETA: 0s - loss: 0.3533 - accuracy: 0.83 - ETA: 0s - loss: 0.3531 - accuracy: 0.83 - ETA: 0s - loss: 0.3535 - accuracy: 0.83 - ETA: 0s - loss: 0.3528 - accuracy: 0.83 - 2s 210us/step - loss: 0.3527 - accuracy: 0.8342 - val_loss: 0.3374 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.29811\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3600 - accuracy: 0.85 - ETA: 1s - loss: 0.4194 - accuracy: 0.81 - ETA: 1s - loss: 0.3773 - accuracy: 0.82 - ETA: 1s - loss: 0.3684 - accuracy: 0.83 - ETA: 1s - loss: 0.3631 - accuracy: 0.83 - ETA: 1s - loss: 0.3575 - accuracy: 0.83 - ETA: 1s - loss: 0.3580 - accuracy: 0.84 - ETA: 1s - loss: 0.3545 - accuracy: 0.83 - ETA: 1s - loss: 0.3461 - accuracy: 0.84 - ETA: 1s - loss: 0.3456 - accuracy: 0.83 - ETA: 1s - loss: 0.3533 - accuracy: 0.83 - ETA: 1s - loss: 0.3583 - accuracy: 0.83 - ETA: 1s - loss: 0.3546 - accuracy: 0.83 - ETA: 1s - loss: 0.3559 - accuracy: 0.83 - ETA: 1s - loss: 0.3526 - accuracy: 0.83 - ETA: 1s - loss: 0.3517 - accuracy: 0.83 - ETA: 1s - loss: 0.3536 - accuracy: 0.83 - ETA: 1s - loss: 0.3547 - accuracy: 0.83 - ETA: 1s - loss: 0.3537 - accuracy: 0.83 - ETA: 0s - loss: 0.3547 - accuracy: 0.83 - ETA: 0s - loss: 0.3555 - accuracy: 0.83 - ETA: 0s - loss: 0.3551 - accuracy: 0.83 - ETA: 0s - loss: 0.3545 - accuracy: 0.83 - ETA: 0s - loss: 0.3550 - accuracy: 0.83 - ETA: 0s - loss: 0.3579 - accuracy: 0.83 - ETA: 0s - loss: 0.3597 - accuracy: 0.83 - ETA: 0s - loss: 0.3588 - accuracy: 0.83 - ETA: 0s - loss: 0.3584 - accuracy: 0.83 - ETA: 0s - loss: 0.3591 - accuracy: 0.83 - ETA: 0s - loss: 0.3587 - accuracy: 0.83 - ETA: 0s - loss: 0.3579 - accuracy: 0.83 - ETA: 0s - loss: 0.3599 - accuracy: 0.83 - ETA: 0s - loss: 0.3583 - accuracy: 0.83 - ETA: 0s - loss: 0.3568 - accuracy: 0.83 - ETA: 0s - loss: 0.3575 - accuracy: 0.83 - ETA: 0s - loss: 0.3581 - accuracy: 0.83 - 2s 209us/step - loss: 0.3577 - accuracy: 0.8380 - val_loss: 0.3263 - val_accuracy: 0.8441\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.29811\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3767 - accuracy: 0.80 - ETA: 1s - loss: 0.3644 - accuracy: 0.83 - ETA: 1s - loss: 0.3397 - accuracy: 0.84 - ETA: 1s - loss: 0.3323 - accuracy: 0.84 - ETA: 1s - loss: 0.3471 - accuracy: 0.84 - ETA: 1s - loss: 0.3542 - accuracy: 0.84 - ETA: 1s - loss: 0.3542 - accuracy: 0.84 - ETA: 1s - loss: 0.3539 - accuracy: 0.84 - ETA: 1s - loss: 0.3520 - accuracy: 0.84 - ETA: 1s - loss: 0.3483 - accuracy: 0.84 - ETA: 1s - loss: 0.3464 - accuracy: 0.84 - ETA: 1s - loss: 0.3507 - accuracy: 0.84 - ETA: 1s - loss: 0.3483 - accuracy: 0.84 - ETA: 1s - loss: 0.3467 - accuracy: 0.84 - ETA: 1s - loss: 0.3447 - accuracy: 0.84 - ETA: 1s - loss: 0.3433 - accuracy: 0.84 - ETA: 1s - loss: 0.3423 - accuracy: 0.84 - ETA: 1s - loss: 0.3443 - accuracy: 0.84 - ETA: 1s - loss: 0.3462 - accuracy: 0.84 - ETA: 0s - loss: 0.3440 - accuracy: 0.84 - ETA: 0s - loss: 0.3445 - accuracy: 0.84 - ETA: 0s - loss: 0.3415 - accuracy: 0.84 - ETA: 0s - loss: 0.3448 - accuracy: 0.84 - ETA: 0s - loss: 0.3435 - accuracy: 0.84 - ETA: 0s - loss: 0.3441 - accuracy: 0.84 - ETA: 0s - loss: 0.3453 - accuracy: 0.84 - ETA: 0s - loss: 0.3453 - accuracy: 0.84 - ETA: 0s - loss: 0.3451 - accuracy: 0.84 - ETA: 0s - loss: 0.3443 - accuracy: 0.84 - ETA: 0s - loss: 0.3452 - accuracy: 0.84 - ETA: 0s - loss: 0.3445 - accuracy: 0.84 - ETA: 0s - loss: 0.3477 - accuracy: 0.84 - ETA: 0s - loss: 0.3475 - accuracy: 0.84 - ETA: 0s - loss: 0.3483 - accuracy: 0.84 - ETA: 0s - loss: 0.3468 - accuracy: 0.84 - ETA: 0s - loss: 0.3480 - accuracy: 0.84 - 2s 210us/step - loss: 0.3479 - accuracy: 0.8458 - val_loss: 0.3259 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.29811\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2969 - accuracy: 0.87 - ETA: 2s - loss: 0.3590 - accuracy: 0.84 - ETA: 2s - loss: 0.3490 - accuracy: 0.84 - ETA: 1s - loss: 0.3475 - accuracy: 0.84 - ETA: 1s - loss: 0.3529 - accuracy: 0.83 - ETA: 1s - loss: 0.3591 - accuracy: 0.83 - ETA: 1s - loss: 0.3626 - accuracy: 0.82 - ETA: 1s - loss: 0.3649 - accuracy: 0.82 - ETA: 1s - loss: 0.3628 - accuracy: 0.83 - ETA: 1s - loss: 0.3658 - accuracy: 0.82 - ETA: 1s - loss: 0.3643 - accuracy: 0.82 - ETA: 1s - loss: 0.3642 - accuracy: 0.83 - ETA: 1s - loss: 0.3597 - accuracy: 0.83 - ETA: 1s - loss: 0.3558 - accuracy: 0.83 - ETA: 1s - loss: 0.3589 - accuracy: 0.83 - ETA: 1s - loss: 0.3551 - accuracy: 0.83 - ETA: 1s - loss: 0.3569 - accuracy: 0.83 - ETA: 1s - loss: 0.3538 - accuracy: 0.83 - ETA: 1s - loss: 0.3534 - accuracy: 0.83 - ETA: 0s - loss: 0.3524 - accuracy: 0.83 - ETA: 0s - loss: 0.3520 - accuracy: 0.83 - ETA: 0s - loss: 0.3502 - accuracy: 0.83 - ETA: 0s - loss: 0.3504 - accuracy: 0.83 - ETA: 0s - loss: 0.3486 - accuracy: 0.83 - ETA: 0s - loss: 0.3470 - accuracy: 0.83 - ETA: 0s - loss: 0.3455 - accuracy: 0.83 - ETA: 0s - loss: 0.3437 - accuracy: 0.84 - ETA: 0s - loss: 0.3440 - accuracy: 0.84 - ETA: 0s - loss: 0.3430 - accuracy: 0.84 - ETA: 0s - loss: 0.3400 - accuracy: 0.84 - ETA: 0s - loss: 0.3404 - accuracy: 0.84 - ETA: 0s - loss: 0.3394 - accuracy: 0.84 - ETA: 0s - loss: 0.3394 - accuracy: 0.84 - ETA: 0s - loss: 0.3402 - accuracy: 0.84 - ETA: 0s - loss: 0.3407 - accuracy: 0.84 - ETA: 0s - loss: 0.3405 - accuracy: 0.84 - 2s 209us/step - loss: 0.3403 - accuracy: 0.8434 - val_loss: 0.3105 - val_accuracy: 0.8450\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.29811\n",
      "Epoch 27/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3649 - accuracy: 0.82 - ETA: 1s - loss: 0.3896 - accuracy: 0.80 - ETA: 1s - loss: 0.3788 - accuracy: 0.81 - ETA: 1s - loss: 0.3685 - accuracy: 0.82 - ETA: 1s - loss: 0.3557 - accuracy: 0.83 - ETA: 1s - loss: 0.3548 - accuracy: 0.83 - ETA: 1s - loss: 0.3576 - accuracy: 0.82 - ETA: 1s - loss: 0.3467 - accuracy: 0.82 - ETA: 1s - loss: 0.3544 - accuracy: 0.82 - ETA: 1s - loss: 0.3507 - accuracy: 0.82 - ETA: 1s - loss: 0.3495 - accuracy: 0.82 - ETA: 1s - loss: 0.3487 - accuracy: 0.83 - ETA: 1s - loss: 0.3501 - accuracy: 0.83 - ETA: 1s - loss: 0.3500 - accuracy: 0.83 - ETA: 1s - loss: 0.3504 - accuracy: 0.83 - ETA: 1s - loss: 0.3462 - accuracy: 0.83 - ETA: 1s - loss: 0.3429 - accuracy: 0.83 - ETA: 1s - loss: 0.3456 - accuracy: 0.83 - ETA: 1s - loss: 0.3454 - accuracy: 0.83 - ETA: 0s - loss: 0.3440 - accuracy: 0.83 - ETA: 0s - loss: 0.3423 - accuracy: 0.83 - ETA: 0s - loss: 0.3422 - accuracy: 0.83 - ETA: 0s - loss: 0.3460 - accuracy: 0.83 - ETA: 0s - loss: 0.3485 - accuracy: 0.83 - ETA: 0s - loss: 0.3490 - accuracy: 0.83 - ETA: 0s - loss: 0.3479 - accuracy: 0.83 - ETA: 0s - loss: 0.3486 - accuracy: 0.83 - ETA: 0s - loss: 0.3486 - accuracy: 0.83 - ETA: 0s - loss: 0.3477 - accuracy: 0.83 - ETA: 0s - loss: 0.3491 - accuracy: 0.83 - ETA: 0s - loss: 0.3492 - accuracy: 0.83 - ETA: 0s - loss: 0.3484 - accuracy: 0.83 - ETA: 0s - loss: 0.3452 - accuracy: 0.83 - ETA: 0s - loss: 0.3459 - accuracy: 0.83 - ETA: 0s - loss: 0.3452 - accuracy: 0.83 - ETA: 0s - loss: 0.3471 - accuracy: 0.83 - 2s 211us/step - loss: 0.3475 - accuracy: 0.8366 - val_loss: 0.3059 - val_accuracy: 0.8736\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.29811\n",
      "Epoch 28/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3808 - accuracy: 0.82 - ETA: 2s - loss: 0.3677 - accuracy: 0.83 - ETA: 1s - loss: 0.3483 - accuracy: 0.85 - ETA: 2s - loss: 0.3548 - accuracy: 0.84 - ETA: 1s - loss: 0.3522 - accuracy: 0.84 - ETA: 1s - loss: 0.3414 - accuracy: 0.85 - ETA: 1s - loss: 0.3429 - accuracy: 0.84 - ETA: 1s - loss: 0.3446 - accuracy: 0.84 - ETA: 1s - loss: 0.3480 - accuracy: 0.84 - ETA: 1s - loss: 0.3458 - accuracy: 0.84 - ETA: 1s - loss: 0.3412 - accuracy: 0.84 - ETA: 1s - loss: 0.3375 - accuracy: 0.84 - ETA: 1s - loss: 0.3331 - accuracy: 0.85 - ETA: 1s - loss: 0.3323 - accuracy: 0.85 - ETA: 1s - loss: 0.3349 - accuracy: 0.85 - ETA: 1s - loss: 0.3404 - accuracy: 0.84 - ETA: 1s - loss: 0.3403 - accuracy: 0.84 - ETA: 1s - loss: 0.3438 - accuracy: 0.84 - ETA: 1s - loss: 0.3436 - accuracy: 0.84 - ETA: 1s - loss: 0.3403 - accuracy: 0.84 - ETA: 0s - loss: 0.3391 - accuracy: 0.84 - ETA: 0s - loss: 0.3381 - accuracy: 0.84 - ETA: 0s - loss: 0.3360 - accuracy: 0.84 - ETA: 0s - loss: 0.3360 - accuracy: 0.84 - ETA: 0s - loss: 0.3348 - accuracy: 0.84 - ETA: 0s - loss: 0.3335 - accuracy: 0.84 - ETA: 0s - loss: 0.3379 - accuracy: 0.84 - ETA: 0s - loss: 0.3373 - accuracy: 0.84 - ETA: 0s - loss: 0.3365 - accuracy: 0.84 - ETA: 0s - loss: 0.3368 - accuracy: 0.84 - ETA: 0s - loss: 0.3364 - accuracy: 0.84 - ETA: 0s - loss: 0.3358 - accuracy: 0.84 - ETA: 0s - loss: 0.3368 - accuracy: 0.84 - ETA: 0s - loss: 0.3350 - accuracy: 0.84 - ETA: 0s - loss: 0.3354 - accuracy: 0.84 - ETA: 0s - loss: 0.3372 - accuracy: 0.84 - ETA: 0s - loss: 0.3378 - accuracy: 0.84 - 2s 210us/step - loss: 0.3381 - accuracy: 0.8461 - val_loss: 0.3222 - val_accuracy: 0.8416\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.29811\n",
      "Epoch 29/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2901 - accuracy: 0.86 - ETA: 2s - loss: 0.3259 - accuracy: 0.84 - ETA: 2s - loss: 0.3268 - accuracy: 0.84 - ETA: 2s - loss: 0.3097 - accuracy: 0.85 - ETA: 1s - loss: 0.3025 - accuracy: 0.85 - ETA: 1s - loss: 0.3068 - accuracy: 0.85 - ETA: 1s - loss: 0.3095 - accuracy: 0.85 - ETA: 1s - loss: 0.3189 - accuracy: 0.85 - ETA: 1s - loss: 0.3156 - accuracy: 0.85 - ETA: 1s - loss: 0.3180 - accuracy: 0.85 - ETA: 1s - loss: 0.3216 - accuracy: 0.85 - ETA: 1s - loss: 0.3207 - accuracy: 0.85 - ETA: 1s - loss: 0.3189 - accuracy: 0.85 - ETA: 1s - loss: 0.3230 - accuracy: 0.85 - ETA: 1s - loss: 0.3233 - accuracy: 0.85 - ETA: 1s - loss: 0.3256 - accuracy: 0.85 - ETA: 1s - loss: 0.3254 - accuracy: 0.84 - ETA: 1s - loss: 0.3267 - accuracy: 0.85 - ETA: 1s - loss: 0.3282 - accuracy: 0.84 - ETA: 0s - loss: 0.3307 - accuracy: 0.85 - ETA: 0s - loss: 0.3331 - accuracy: 0.84 - ETA: 0s - loss: 0.3325 - accuracy: 0.84 - ETA: 0s - loss: 0.3314 - accuracy: 0.84 - ETA: 0s - loss: 0.3302 - accuracy: 0.85 - ETA: 0s - loss: 0.3309 - accuracy: 0.84 - ETA: 0s - loss: 0.3312 - accuracy: 0.84 - ETA: 0s - loss: 0.3302 - accuracy: 0.85 - ETA: 0s - loss: 0.3312 - accuracy: 0.84 - ETA: 0s - loss: 0.3316 - accuracy: 0.84 - ETA: 0s - loss: 0.3330 - accuracy: 0.84 - ETA: 0s - loss: 0.3332 - accuracy: 0.84 - ETA: 0s - loss: 0.3343 - accuracy: 0.84 - ETA: 0s - loss: 0.3351 - accuracy: 0.84 - ETA: 0s - loss: 0.3350 - accuracy: 0.84 - ETA: 0s - loss: 0.3343 - accuracy: 0.84 - ETA: 0s - loss: 0.3340 - accuracy: 0.84 - 2s 212us/step - loss: 0.3356 - accuracy: 0.8486 - val_loss: 0.3125 - val_accuracy: 0.8576\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.29811\n",
      "Epoch 30/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2692 - accuracy: 0.87 - ETA: 2s - loss: 0.2912 - accuracy: 0.88 - ETA: 2s - loss: 0.3127 - accuracy: 0.85 - ETA: 1s - loss: 0.2965 - accuracy: 0.86 - ETA: 1s - loss: 0.2939 - accuracy: 0.87 - ETA: 1s - loss: 0.2895 - accuracy: 0.87 - ETA: 1s - loss: 0.3052 - accuracy: 0.86 - ETA: 1s - loss: 0.3005 - accuracy: 0.86 - ETA: 1s - loss: 0.3099 - accuracy: 0.86 - ETA: 1s - loss: 0.3076 - accuracy: 0.86 - ETA: 1s - loss: 0.3114 - accuracy: 0.86 - ETA: 1s - loss: 0.3186 - accuracy: 0.85 - ETA: 1s - loss: 0.3198 - accuracy: 0.85 - ETA: 1s - loss: 0.3222 - accuracy: 0.85 - ETA: 1s - loss: 0.3263 - accuracy: 0.85 - ETA: 1s - loss: 0.3283 - accuracy: 0.85 - ETA: 1s - loss: 0.3268 - accuracy: 0.85 - ETA: 1s - loss: 0.3287 - accuracy: 0.85 - ETA: 1s - loss: 0.3319 - accuracy: 0.85 - ETA: 1s - loss: 0.3320 - accuracy: 0.85 - ETA: 0s - loss: 0.3307 - accuracy: 0.85 - ETA: 0s - loss: 0.3292 - accuracy: 0.85 - ETA: 0s - loss: 0.3299 - accuracy: 0.84 - ETA: 0s - loss: 0.3296 - accuracy: 0.84 - ETA: 0s - loss: 0.3299 - accuracy: 0.84 - ETA: 0s - loss: 0.3308 - accuracy: 0.84 - ETA: 0s - loss: 0.3312 - accuracy: 0.84 - ETA: 0s - loss: 0.3299 - accuracy: 0.84 - ETA: 0s - loss: 0.3297 - accuracy: 0.84 - ETA: 0s - loss: 0.3306 - accuracy: 0.84 - ETA: 0s - loss: 0.3292 - accuracy: 0.85 - ETA: 0s - loss: 0.3287 - accuracy: 0.85 - ETA: 0s - loss: 0.3289 - accuracy: 0.85 - ETA: 0s - loss: 0.3325 - accuracy: 0.84 - ETA: 0s - loss: 0.3332 - accuracy: 0.84 - ETA: 0s - loss: 0.3331 - accuracy: 0.84 - ETA: 0s - loss: 0.3306 - accuracy: 0.85 - ETA: 0s - loss: 0.3314 - accuracy: 0.85 - 2s 216us/step - loss: 0.3310 - accuracy: 0.8504 - val_loss: 0.3477 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.29811\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3775 - accuracy: 0.84 - ETA: 2s - loss: 0.3360 - accuracy: 0.85 - ETA: 2s - loss: 0.3266 - accuracy: 0.85 - ETA: 2s - loss: 0.3149 - accuracy: 0.85 - ETA: 1s - loss: 0.3237 - accuracy: 0.84 - ETA: 1s - loss: 0.3158 - accuracy: 0.85 - ETA: 1s - loss: 0.3127 - accuracy: 0.85 - ETA: 1s - loss: 0.3116 - accuracy: 0.85 - ETA: 1s - loss: 0.3152 - accuracy: 0.85 - ETA: 1s - loss: 0.3149 - accuracy: 0.85 - ETA: 1s - loss: 0.3139 - accuracy: 0.85 - ETA: 1s - loss: 0.3181 - accuracy: 0.85 - ETA: 1s - loss: 0.3216 - accuracy: 0.85 - ETA: 1s - loss: 0.3245 - accuracy: 0.85 - ETA: 1s - loss: 0.3232 - accuracy: 0.85 - ETA: 1s - loss: 0.3250 - accuracy: 0.84 - ETA: 1s - loss: 0.3243 - accuracy: 0.84 - ETA: 1s - loss: 0.3247 - accuracy: 0.85 - ETA: 1s - loss: 0.3250 - accuracy: 0.85 - ETA: 1s - loss: 0.3232 - accuracy: 0.85 - ETA: 0s - loss: 0.3203 - accuracy: 0.85 - ETA: 0s - loss: 0.3212 - accuracy: 0.85 - ETA: 0s - loss: 0.3201 - accuracy: 0.85 - ETA: 0s - loss: 0.3227 - accuracy: 0.85 - ETA: 0s - loss: 0.3241 - accuracy: 0.85 - ETA: 0s - loss: 0.3249 - accuracy: 0.85 - ETA: 0s - loss: 0.3268 - accuracy: 0.85 - ETA: 0s - loss: 0.3286 - accuracy: 0.85 - ETA: 0s - loss: 0.3274 - accuracy: 0.85 - ETA: 0s - loss: 0.3267 - accuracy: 0.85 - ETA: 0s - loss: 0.3250 - accuracy: 0.85 - ETA: 0s - loss: 0.3259 - accuracy: 0.85 - ETA: 0s - loss: 0.3260 - accuracy: 0.85 - ETA: 0s - loss: 0.3257 - accuracy: 0.85 - ETA: 0s - loss: 0.3245 - accuracy: 0.85 - ETA: 0s - loss: 0.3237 - accuracy: 0.85 - ETA: 0s - loss: 0.3235 - accuracy: 0.85 - 2s 211us/step - loss: 0.3238 - accuracy: 0.8527 - val_loss: 0.3119 - val_accuracy: 0.8517\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.29811\n",
      "Epoch 32/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3339 - accuracy: 0.84 - ETA: 2s - loss: 0.3135 - accuracy: 0.85 - ETA: 1s - loss: 0.2959 - accuracy: 0.86 - ETA: 1s - loss: 0.2911 - accuracy: 0.87 - ETA: 1s - loss: 0.2949 - accuracy: 0.86 - ETA: 1s - loss: 0.3029 - accuracy: 0.85 - ETA: 1s - loss: 0.3074 - accuracy: 0.85 - ETA: 1s - loss: 0.3038 - accuracy: 0.85 - ETA: 1s - loss: 0.3006 - accuracy: 0.85 - ETA: 1s - loss: 0.3054 - accuracy: 0.85 - ETA: 1s - loss: 0.3069 - accuracy: 0.85 - ETA: 1s - loss: 0.3100 - accuracy: 0.85 - ETA: 1s - loss: 0.3118 - accuracy: 0.85 - ETA: 1s - loss: 0.3176 - accuracy: 0.85 - ETA: 1s - loss: 0.3169 - accuracy: 0.85 - ETA: 1s - loss: 0.3151 - accuracy: 0.85 - ETA: 1s - loss: 0.3165 - accuracy: 0.85 - ETA: 1s - loss: 0.3184 - accuracy: 0.85 - ETA: 1s - loss: 0.3175 - accuracy: 0.85 - ETA: 1s - loss: 0.3161 - accuracy: 0.85 - ETA: 0s - loss: 0.3177 - accuracy: 0.85 - ETA: 0s - loss: 0.3196 - accuracy: 0.85 - ETA: 0s - loss: 0.3195 - accuracy: 0.85 - ETA: 0s - loss: 0.3186 - accuracy: 0.85 - ETA: 0s - loss: 0.3166 - accuracy: 0.85 - ETA: 0s - loss: 0.3174 - accuracy: 0.85 - ETA: 0s - loss: 0.3166 - accuracy: 0.86 - ETA: 0s - loss: 0.3149 - accuracy: 0.86 - ETA: 0s - loss: 0.3157 - accuracy: 0.86 - ETA: 0s - loss: 0.3173 - accuracy: 0.86 - ETA: 0s - loss: 0.3190 - accuracy: 0.85 - ETA: 0s - loss: 0.3199 - accuracy: 0.85 - ETA: 0s - loss: 0.3198 - accuracy: 0.85 - ETA: 0s - loss: 0.3190 - accuracy: 0.85 - ETA: 0s - loss: 0.3190 - accuracy: 0.85 - ETA: 0s - loss: 0.3204 - accuracy: 0.85 - ETA: 0s - loss: 0.3209 - accuracy: 0.85 - 2s 210us/step - loss: 0.3207 - accuracy: 0.8575 - val_loss: 0.3512 - val_accuracy: 0.8357\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.29811\n",
      "Epoch 00032: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "[2020-05-18 16:08:12 RAM69.1% 0.65GB] Val Score : [0.34150469694018276, 0.8438210487365723]\n",
      "[2020-05-18 16:08:12 RAM69.1% 0.65GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 16:08:12 RAM69.1% 0.65GB] Training on Fold : 10\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 18s - loss: 12.0650 - accuracy: 0.32 - ETA: 7s - loss: 4.8242 - accuracy: 0.4598 - ETA: 5s - loss: 3.2717 - accuracy: 0.48 - ETA: 4s - loss: 2.6099 - accuracy: 0.51 - ETA: 3s - loss: 2.2064 - accuracy: 0.53 - ETA: 3s - loss: 1.9429 - accuracy: 0.55 - ETA: 2s - loss: 1.7548 - accuracy: 0.56 - ETA: 2s - loss: 1.6084 - accuracy: 0.58 - ETA: 2s - loss: 1.4909 - accuracy: 0.59 - ETA: 2s - loss: 1.4048 - accuracy: 0.60 - ETA: 2s - loss: 1.3308 - accuracy: 0.61 - ETA: 2s - loss: 1.2689 - accuracy: 0.61 - ETA: 1s - loss: 1.2169 - accuracy: 0.62 - ETA: 1s - loss: 1.1697 - accuracy: 0.63 - ETA: 1s - loss: 1.1290 - accuracy: 0.63 - ETA: 1s - loss: 1.0976 - accuracy: 0.63 - ETA: 1s - loss: 1.0675 - accuracy: 0.64 - ETA: 1s - loss: 1.0397 - accuracy: 0.64 - ETA: 1s - loss: 1.0133 - accuracy: 0.65 - ETA: 1s - loss: 0.9902 - accuracy: 0.65 - ETA: 1s - loss: 0.9663 - accuracy: 0.66 - ETA: 1s - loss: 0.9497 - accuracy: 0.66 - ETA: 1s - loss: 0.9347 - accuracy: 0.66 - ETA: 0s - loss: 0.9166 - accuracy: 0.67 - ETA: 0s - loss: 0.9041 - accuracy: 0.67 - ETA: 0s - loss: 0.8903 - accuracy: 0.67 - ETA: 0s - loss: 0.8791 - accuracy: 0.67 - ETA: 0s - loss: 0.8668 - accuracy: 0.68 - ETA: 0s - loss: 0.8556 - accuracy: 0.68 - ETA: 0s - loss: 0.8448 - accuracy: 0.68 - ETA: 0s - loss: 0.8341 - accuracy: 0.68 - ETA: 0s - loss: 0.8268 - accuracy: 0.68 - ETA: 0s - loss: 0.8143 - accuracy: 0.68 - ETA: 0s - loss: 0.8055 - accuracy: 0.69 - ETA: 0s - loss: 0.7963 - accuracy: 0.69 - ETA: 0s - loss: 0.7895 - accuracy: 0.69 - 3s 242us/step - loss: 0.7837 - accuracy: 0.6969 - val_loss: 0.4913 - val_accuracy: 0.7843\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.29811\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5275 - accuracy: 0.75 - ETA: 2s - loss: 0.5625 - accuracy: 0.74 - ETA: 2s - loss: 0.5308 - accuracy: 0.75 - ETA: 1s - loss: 0.5264 - accuracy: 0.76 - ETA: 1s - loss: 0.5201 - accuracy: 0.77 - ETA: 1s - loss: 0.5235 - accuracy: 0.76 - ETA: 1s - loss: 0.5189 - accuracy: 0.77 - ETA: 1s - loss: 0.5165 - accuracy: 0.77 - ETA: 1s - loss: 0.5166 - accuracy: 0.77 - ETA: 1s - loss: 0.5205 - accuracy: 0.76 - ETA: 1s - loss: 0.5283 - accuracy: 0.76 - ETA: 1s - loss: 0.5329 - accuracy: 0.75 - ETA: 1s - loss: 0.5315 - accuracy: 0.75 - ETA: 1s - loss: 0.5288 - accuracy: 0.75 - ETA: 1s - loss: 0.5293 - accuracy: 0.75 - ETA: 1s - loss: 0.5263 - accuracy: 0.76 - ETA: 1s - loss: 0.5305 - accuracy: 0.75 - ETA: 1s - loss: 0.5323 - accuracy: 0.75 - ETA: 1s - loss: 0.5300 - accuracy: 0.75 - ETA: 1s - loss: 0.5280 - accuracy: 0.75 - ETA: 0s - loss: 0.5266 - accuracy: 0.75 - ETA: 0s - loss: 0.5237 - accuracy: 0.75 - ETA: 0s - loss: 0.5253 - accuracy: 0.75 - ETA: 0s - loss: 0.5248 - accuracy: 0.75 - ETA: 0s - loss: 0.5260 - accuracy: 0.75 - ETA: 0s - loss: 0.5249 - accuracy: 0.75 - ETA: 0s - loss: 0.5236 - accuracy: 0.76 - ETA: 0s - loss: 0.5209 - accuracy: 0.76 - ETA: 0s - loss: 0.5197 - accuracy: 0.76 - ETA: 0s - loss: 0.5177 - accuracy: 0.76 - ETA: 0s - loss: 0.5170 - accuracy: 0.76 - ETA: 0s - loss: 0.5150 - accuracy: 0.76 - ETA: 0s - loss: 0.5140 - accuracy: 0.76 - ETA: 0s - loss: 0.5149 - accuracy: 0.76 - ETA: 0s - loss: 0.5141 - accuracy: 0.76 - ETA: 0s - loss: 0.5126 - accuracy: 0.76 - ETA: 0s - loss: 0.5122 - accuracy: 0.76 - 2s 211us/step - loss: 0.5121 - accuracy: 0.7688 - val_loss: 0.4537 - val_accuracy: 0.8104\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.29811\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5266 - accuracy: 0.80 - ETA: 2s - loss: 0.5113 - accuracy: 0.79 - ETA: 2s - loss: 0.4966 - accuracy: 0.78 - ETA: 2s - loss: 0.5017 - accuracy: 0.78 - ETA: 1s - loss: 0.4925 - accuracy: 0.78 - ETA: 1s - loss: 0.4948 - accuracy: 0.78 - ETA: 1s - loss: 0.4975 - accuracy: 0.78 - ETA: 1s - loss: 0.5009 - accuracy: 0.78 - ETA: 1s - loss: 0.5098 - accuracy: 0.77 - ETA: 1s - loss: 0.5048 - accuracy: 0.77 - ETA: 1s - loss: 0.5091 - accuracy: 0.77 - ETA: 1s - loss: 0.5058 - accuracy: 0.77 - ETA: 1s - loss: 0.5005 - accuracy: 0.77 - ETA: 1s - loss: 0.4987 - accuracy: 0.77 - ETA: 1s - loss: 0.4989 - accuracy: 0.77 - ETA: 1s - loss: 0.5016 - accuracy: 0.77 - ETA: 1s - loss: 0.5001 - accuracy: 0.77 - ETA: 1s - loss: 0.4972 - accuracy: 0.78 - ETA: 1s - loss: 0.4940 - accuracy: 0.78 - ETA: 1s - loss: 0.4918 - accuracy: 0.78 - ETA: 1s - loss: 0.4903 - accuracy: 0.78 - ETA: 0s - loss: 0.4897 - accuracy: 0.78 - ETA: 0s - loss: 0.4899 - accuracy: 0.78 - ETA: 0s - loss: 0.4879 - accuracy: 0.78 - ETA: 0s - loss: 0.4871 - accuracy: 0.78 - ETA: 0s - loss: 0.4848 - accuracy: 0.78 - ETA: 0s - loss: 0.4867 - accuracy: 0.78 - ETA: 0s - loss: 0.4861 - accuracy: 0.78 - ETA: 0s - loss: 0.4854 - accuracy: 0.78 - ETA: 0s - loss: 0.4840 - accuracy: 0.78 - ETA: 0s - loss: 0.4831 - accuracy: 0.78 - ETA: 0s - loss: 0.4815 - accuracy: 0.78 - ETA: 0s - loss: 0.4818 - accuracy: 0.78 - ETA: 0s - loss: 0.4810 - accuracy: 0.78 - ETA: 0s - loss: 0.4806 - accuracy: 0.78 - ETA: 0s - loss: 0.4793 - accuracy: 0.78 - ETA: 0s - loss: 0.4785 - accuracy: 0.78 - 2s 214us/step - loss: 0.4801 - accuracy: 0.7859 - val_loss: 0.4591 - val_accuracy: 0.8138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29811\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5010 - accuracy: 0.75 - ETA: 2s - loss: 0.4804 - accuracy: 0.76 - ETA: 2s - loss: 0.4763 - accuracy: 0.76 - ETA: 2s - loss: 0.4595 - accuracy: 0.78 - ETA: 1s - loss: 0.4593 - accuracy: 0.78 - ETA: 1s - loss: 0.4634 - accuracy: 0.78 - ETA: 1s - loss: 0.4650 - accuracy: 0.78 - ETA: 1s - loss: 0.4681 - accuracy: 0.78 - ETA: 1s - loss: 0.4632 - accuracy: 0.78 - ETA: 1s - loss: 0.4662 - accuracy: 0.78 - ETA: 1s - loss: 0.4658 - accuracy: 0.78 - ETA: 1s - loss: 0.4674 - accuracy: 0.78 - ETA: 1s - loss: 0.4631 - accuracy: 0.78 - ETA: 1s - loss: 0.4660 - accuracy: 0.78 - ETA: 1s - loss: 0.4695 - accuracy: 0.78 - ETA: 1s - loss: 0.4714 - accuracy: 0.78 - ETA: 1s - loss: 0.4721 - accuracy: 0.78 - ETA: 1s - loss: 0.4723 - accuracy: 0.78 - ETA: 1s - loss: 0.4707 - accuracy: 0.78 - ETA: 1s - loss: 0.4661 - accuracy: 0.78 - ETA: 0s - loss: 0.4639 - accuracy: 0.78 - ETA: 0s - loss: 0.4640 - accuracy: 0.78 - ETA: 0s - loss: 0.4622 - accuracy: 0.78 - ETA: 0s - loss: 0.4613 - accuracy: 0.78 - ETA: 0s - loss: 0.4611 - accuracy: 0.78 - ETA: 0s - loss: 0.4630 - accuracy: 0.78 - ETA: 0s - loss: 0.4636 - accuracy: 0.78 - ETA: 0s - loss: 0.4637 - accuracy: 0.78 - ETA: 0s - loss: 0.4615 - accuracy: 0.78 - ETA: 0s - loss: 0.4631 - accuracy: 0.78 - ETA: 0s - loss: 0.4619 - accuracy: 0.78 - ETA: 0s - loss: 0.4598 - accuracy: 0.79 - ETA: 0s - loss: 0.4600 - accuracy: 0.79 - ETA: 0s - loss: 0.4597 - accuracy: 0.79 - ETA: 0s - loss: 0.4608 - accuracy: 0.79 - ETA: 0s - loss: 0.4594 - accuracy: 0.79 - 2s 212us/step - loss: 0.4595 - accuracy: 0.7907 - val_loss: 0.4138 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29811\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4006 - accuracy: 0.88 - ETA: 2s - loss: 0.3986 - accuracy: 0.85 - ETA: 1s - loss: 0.4230 - accuracy: 0.83 - ETA: 1s - loss: 0.4462 - accuracy: 0.81 - ETA: 1s - loss: 0.4499 - accuracy: 0.81 - ETA: 1s - loss: 0.4499 - accuracy: 0.80 - ETA: 1s - loss: 0.4528 - accuracy: 0.80 - ETA: 1s - loss: 0.4432 - accuracy: 0.80 - ETA: 1s - loss: 0.4421 - accuracy: 0.80 - ETA: 1s - loss: 0.4382 - accuracy: 0.80 - ETA: 1s - loss: 0.4447 - accuracy: 0.80 - ETA: 1s - loss: 0.4456 - accuracy: 0.80 - ETA: 1s - loss: 0.4429 - accuracy: 0.80 - ETA: 1s - loss: 0.4478 - accuracy: 0.80 - ETA: 1s - loss: 0.4507 - accuracy: 0.79 - ETA: 1s - loss: 0.4534 - accuracy: 0.79 - ETA: 1s - loss: 0.4530 - accuracy: 0.79 - ETA: 1s - loss: 0.4547 - accuracy: 0.79 - ETA: 1s - loss: 0.4557 - accuracy: 0.79 - ETA: 0s - loss: 0.4527 - accuracy: 0.79 - ETA: 0s - loss: 0.4530 - accuracy: 0.79 - ETA: 0s - loss: 0.4528 - accuracy: 0.79 - ETA: 0s - loss: 0.4546 - accuracy: 0.79 - ETA: 0s - loss: 0.4563 - accuracy: 0.79 - ETA: 0s - loss: 0.4561 - accuracy: 0.79 - ETA: 0s - loss: 0.4562 - accuracy: 0.79 - ETA: 0s - loss: 0.4568 - accuracy: 0.79 - ETA: 0s - loss: 0.4539 - accuracy: 0.79 - ETA: 0s - loss: 0.4532 - accuracy: 0.79 - ETA: 0s - loss: 0.4527 - accuracy: 0.79 - ETA: 0s - loss: 0.4529 - accuracy: 0.79 - ETA: 0s - loss: 0.4522 - accuracy: 0.79 - ETA: 0s - loss: 0.4514 - accuracy: 0.79 - ETA: 0s - loss: 0.4495 - accuracy: 0.79 - ETA: 0s - loss: 0.4492 - accuracy: 0.79 - 2s 210us/step - loss: 0.4486 - accuracy: 0.7944 - val_loss: 0.3964 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29811\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4147 - accuracy: 0.85 - ETA: 2s - loss: 0.4449 - accuracy: 0.80 - ETA: 2s - loss: 0.4543 - accuracy: 0.79 - ETA: 2s - loss: 0.4434 - accuracy: 0.79 - ETA: 1s - loss: 0.4398 - accuracy: 0.80 - ETA: 1s - loss: 0.4415 - accuracy: 0.79 - ETA: 1s - loss: 0.4376 - accuracy: 0.79 - ETA: 1s - loss: 0.4356 - accuracy: 0.79 - ETA: 1s - loss: 0.4347 - accuracy: 0.80 - ETA: 1s - loss: 0.4297 - accuracy: 0.80 - ETA: 1s - loss: 0.4371 - accuracy: 0.79 - ETA: 1s - loss: 0.4377 - accuracy: 0.80 - ETA: 1s - loss: 0.4345 - accuracy: 0.80 - ETA: 1s - loss: 0.4310 - accuracy: 0.80 - ETA: 1s - loss: 0.4324 - accuracy: 0.80 - ETA: 1s - loss: 0.4328 - accuracy: 0.80 - ETA: 1s - loss: 0.4329 - accuracy: 0.80 - ETA: 1s - loss: 0.4308 - accuracy: 0.80 - ETA: 1s - loss: 0.4333 - accuracy: 0.80 - ETA: 1s - loss: 0.4346 - accuracy: 0.79 - ETA: 0s - loss: 0.4346 - accuracy: 0.79 - ETA: 0s - loss: 0.4348 - accuracy: 0.79 - ETA: 0s - loss: 0.4357 - accuracy: 0.79 - ETA: 0s - loss: 0.4344 - accuracy: 0.79 - ETA: 0s - loss: 0.4332 - accuracy: 0.80 - ETA: 0s - loss: 0.4319 - accuracy: 0.80 - ETA: 0s - loss: 0.4313 - accuracy: 0.80 - ETA: 0s - loss: 0.4290 - accuracy: 0.80 - ETA: 0s - loss: 0.4293 - accuracy: 0.80 - ETA: 0s - loss: 0.4306 - accuracy: 0.80 - ETA: 0s - loss: 0.4305 - accuracy: 0.80 - ETA: 0s - loss: 0.4319 - accuracy: 0.80 - ETA: 0s - loss: 0.4339 - accuracy: 0.80 - ETA: 0s - loss: 0.4333 - accuracy: 0.80 - ETA: 0s - loss: 0.4325 - accuracy: 0.79 - ETA: 0s - loss: 0.4348 - accuracy: 0.79 - 2s 218us/step - loss: 0.4354 - accuracy: 0.7976 - val_loss: 0.4089 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.29811\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4016 - accuracy: 0.83 - ETA: 1s - loss: 0.4469 - accuracy: 0.80 - ETA: 1s - loss: 0.4584 - accuracy: 0.79 - ETA: 1s - loss: 0.4612 - accuracy: 0.78 - ETA: 1s - loss: 0.4516 - accuracy: 0.79 - ETA: 1s - loss: 0.4522 - accuracy: 0.79 - ETA: 1s - loss: 0.4444 - accuracy: 0.79 - ETA: 1s - loss: 0.4425 - accuracy: 0.79 - ETA: 1s - loss: 0.4471 - accuracy: 0.79 - ETA: 1s - loss: 0.4435 - accuracy: 0.79 - ETA: 1s - loss: 0.4456 - accuracy: 0.79 - ETA: 1s - loss: 0.4434 - accuracy: 0.79 - ETA: 1s - loss: 0.4445 - accuracy: 0.79 - ETA: 1s - loss: 0.4443 - accuracy: 0.79 - ETA: 1s - loss: 0.4432 - accuracy: 0.80 - ETA: 1s - loss: 0.4398 - accuracy: 0.80 - ETA: 1s - loss: 0.4367 - accuracy: 0.80 - ETA: 1s - loss: 0.4377 - accuracy: 0.80 - ETA: 0s - loss: 0.4326 - accuracy: 0.80 - ETA: 0s - loss: 0.4319 - accuracy: 0.80 - ETA: 0s - loss: 0.4335 - accuracy: 0.80 - ETA: 0s - loss: 0.4324 - accuracy: 0.80 - ETA: 0s - loss: 0.4318 - accuracy: 0.80 - ETA: 0s - loss: 0.4311 - accuracy: 0.80 - ETA: 0s - loss: 0.4340 - accuracy: 0.80 - ETA: 0s - loss: 0.4340 - accuracy: 0.80 - ETA: 0s - loss: 0.4326 - accuracy: 0.80 - ETA: 0s - loss: 0.4319 - accuracy: 0.80 - ETA: 0s - loss: 0.4303 - accuracy: 0.80 - ETA: 0s - loss: 0.4316 - accuracy: 0.80 - ETA: 0s - loss: 0.4306 - accuracy: 0.80 - ETA: 0s - loss: 0.4303 - accuracy: 0.80 - ETA: 0s - loss: 0.4288 - accuracy: 0.80 - ETA: 0s - loss: 0.4285 - accuracy: 0.80 - ETA: 0s - loss: 0.4273 - accuracy: 0.80 - 2s 211us/step - loss: 0.4271 - accuracy: 0.8078 - val_loss: 0.4080 - val_accuracy: 0.8189\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29811\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4014 - accuracy: 0.80 - ETA: 2s - loss: 0.4000 - accuracy: 0.82 - ETA: 1s - loss: 0.4022 - accuracy: 0.80 - ETA: 1s - loss: 0.4117 - accuracy: 0.80 - ETA: 1s - loss: 0.4162 - accuracy: 0.80 - ETA: 1s - loss: 0.4223 - accuracy: 0.80 - ETA: 1s - loss: 0.4222 - accuracy: 0.80 - ETA: 1s - loss: 0.4255 - accuracy: 0.80 - ETA: 1s - loss: 0.4239 - accuracy: 0.79 - ETA: 1s - loss: 0.4200 - accuracy: 0.80 - ETA: 1s - loss: 0.4221 - accuracy: 0.79 - ETA: 1s - loss: 0.4224 - accuracy: 0.79 - ETA: 1s - loss: 0.4260 - accuracy: 0.79 - ETA: 1s - loss: 0.4272 - accuracy: 0.79 - ETA: 1s - loss: 0.4262 - accuracy: 0.79 - ETA: 1s - loss: 0.4240 - accuracy: 0.79 - ETA: 1s - loss: 0.4250 - accuracy: 0.79 - ETA: 1s - loss: 0.4292 - accuracy: 0.79 - ETA: 1s - loss: 0.4281 - accuracy: 0.79 - ETA: 1s - loss: 0.4252 - accuracy: 0.80 - ETA: 0s - loss: 0.4241 - accuracy: 0.80 - ETA: 0s - loss: 0.4232 - accuracy: 0.80 - ETA: 0s - loss: 0.4217 - accuracy: 0.80 - ETA: 0s - loss: 0.4212 - accuracy: 0.80 - ETA: 0s - loss: 0.4219 - accuracy: 0.80 - ETA: 0s - loss: 0.4215 - accuracy: 0.80 - ETA: 0s - loss: 0.4216 - accuracy: 0.80 - ETA: 0s - loss: 0.4209 - accuracy: 0.80 - ETA: 0s - loss: 0.4188 - accuracy: 0.80 - ETA: 0s - loss: 0.4198 - accuracy: 0.80 - ETA: 0s - loss: 0.4198 - accuracy: 0.80 - ETA: 0s - loss: 0.4191 - accuracy: 0.80 - ETA: 0s - loss: 0.4193 - accuracy: 0.80 - ETA: 0s - loss: 0.4175 - accuracy: 0.80 - 2s 212us/step - loss: 0.4176 - accuracy: 0.8077 - val_loss: 0.3720 - val_accuracy: 0.8399\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29811\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4038 - accuracy: 0.82 - ETA: 2s - loss: 0.4262 - accuracy: 0.81 - ETA: 2s - loss: 0.4088 - accuracy: 0.81 - ETA: 1s - loss: 0.4134 - accuracy: 0.81 - ETA: 1s - loss: 0.4226 - accuracy: 0.81 - ETA: 1s - loss: 0.4236 - accuracy: 0.80 - ETA: 1s - loss: 0.4272 - accuracy: 0.81 - ETA: 1s - loss: 0.4301 - accuracy: 0.80 - ETA: 1s - loss: 0.4271 - accuracy: 0.80 - ETA: 1s - loss: 0.4255 - accuracy: 0.80 - ETA: 1s - loss: 0.4198 - accuracy: 0.81 - ETA: 1s - loss: 0.4207 - accuracy: 0.81 - ETA: 1s - loss: 0.4198 - accuracy: 0.81 - ETA: 1s - loss: 0.4162 - accuracy: 0.81 - ETA: 1s - loss: 0.4152 - accuracy: 0.81 - ETA: 1s - loss: 0.4159 - accuracy: 0.81 - ETA: 1s - loss: 0.4125 - accuracy: 0.81 - ETA: 1s - loss: 0.4160 - accuracy: 0.80 - ETA: 1s - loss: 0.4179 - accuracy: 0.80 - ETA: 0s - loss: 0.4197 - accuracy: 0.80 - ETA: 0s - loss: 0.4191 - accuracy: 0.80 - ETA: 0s - loss: 0.4204 - accuracy: 0.80 - ETA: 0s - loss: 0.4198 - accuracy: 0.80 - ETA: 0s - loss: 0.4233 - accuracy: 0.80 - ETA: 0s - loss: 0.4230 - accuracy: 0.80 - ETA: 0s - loss: 0.4197 - accuracy: 0.80 - ETA: 0s - loss: 0.4207 - accuracy: 0.80 - ETA: 0s - loss: 0.4179 - accuracy: 0.80 - ETA: 0s - loss: 0.4171 - accuracy: 0.80 - ETA: 0s - loss: 0.4175 - accuracy: 0.80 - ETA: 0s - loss: 0.4181 - accuracy: 0.80 - ETA: 0s - loss: 0.4161 - accuracy: 0.80 - ETA: 0s - loss: 0.4154 - accuracy: 0.80 - ETA: 0s - loss: 0.4158 - accuracy: 0.80 - ETA: 0s - loss: 0.4154 - accuracy: 0.80 - ETA: 0s - loss: 0.4145 - accuracy: 0.81 - 2s 210us/step - loss: 0.4139 - accuracy: 0.8108 - val_loss: 0.3833 - val_accuracy: 0.8399\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.29811\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.5683 - accuracy: 0.75 - ETA: 2s - loss: 0.4710 - accuracy: 0.77 - ETA: 2s - loss: 0.4565 - accuracy: 0.79 - ETA: 2s - loss: 0.4318 - accuracy: 0.79 - ETA: 1s - loss: 0.4301 - accuracy: 0.79 - ETA: 1s - loss: 0.4384 - accuracy: 0.79 - ETA: 1s - loss: 0.4426 - accuracy: 0.79 - ETA: 1s - loss: 0.4423 - accuracy: 0.79 - ETA: 1s - loss: 0.4348 - accuracy: 0.79 - ETA: 1s - loss: 0.4297 - accuracy: 0.79 - ETA: 1s - loss: 0.4281 - accuracy: 0.79 - ETA: 1s - loss: 0.4217 - accuracy: 0.80 - ETA: 1s - loss: 0.4164 - accuracy: 0.80 - ETA: 1s - loss: 0.4171 - accuracy: 0.80 - ETA: 1s - loss: 0.4153 - accuracy: 0.80 - ETA: 1s - loss: 0.4127 - accuracy: 0.80 - ETA: 1s - loss: 0.4135 - accuracy: 0.80 - ETA: 1s - loss: 0.4116 - accuracy: 0.80 - ETA: 1s - loss: 0.4173 - accuracy: 0.80 - ETA: 0s - loss: 0.4210 - accuracy: 0.80 - ETA: 0s - loss: 0.4208 - accuracy: 0.80 - ETA: 0s - loss: 0.4175 - accuracy: 0.80 - ETA: 0s - loss: 0.4170 - accuracy: 0.80 - ETA: 0s - loss: 0.4160 - accuracy: 0.80 - ETA: 0s - loss: 0.4159 - accuracy: 0.80 - ETA: 0s - loss: 0.4187 - accuracy: 0.80 - ETA: 0s - loss: 0.4160 - accuracy: 0.80 - ETA: 0s - loss: 0.4159 - accuracy: 0.80 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - ETA: 0s - loss: 0.4117 - accuracy: 0.81 - ETA: 0s - loss: 0.4123 - accuracy: 0.80 - ETA: 0s - loss: 0.4127 - accuracy: 0.80 - ETA: 0s - loss: 0.4119 - accuracy: 0.80 - ETA: 0s - loss: 0.4117 - accuracy: 0.81 - ETA: 0s - loss: 0.4116 - accuracy: 0.81 - ETA: 0s - loss: 0.4108 - accuracy: 0.81 - 2s 210us/step - loss: 0.4107 - accuracy: 0.8107 - val_loss: 0.3804 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29811\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4282 - accuracy: 0.78 - ETA: 1s - loss: 0.3582 - accuracy: 0.83 - ETA: 1s - loss: 0.3776 - accuracy: 0.82 - ETA: 2s - loss: 0.3879 - accuracy: 0.82 - ETA: 1s - loss: 0.3898 - accuracy: 0.82 - ETA: 1s - loss: 0.3893 - accuracy: 0.82 - ETA: 1s - loss: 0.3963 - accuracy: 0.81 - ETA: 1s - loss: 0.3954 - accuracy: 0.81 - ETA: 1s - loss: 0.3908 - accuracy: 0.82 - ETA: 1s - loss: 0.3934 - accuracy: 0.81 - ETA: 1s - loss: 0.3924 - accuracy: 0.82 - ETA: 1s - loss: 0.3931 - accuracy: 0.82 - ETA: 1s - loss: 0.3909 - accuracy: 0.82 - ETA: 1s - loss: 0.3939 - accuracy: 0.82 - ETA: 1s - loss: 0.3927 - accuracy: 0.82 - ETA: 1s - loss: 0.3974 - accuracy: 0.82 - ETA: 1s - loss: 0.3949 - accuracy: 0.82 - ETA: 1s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3967 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3977 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.81 - ETA: 0s - loss: 0.4024 - accuracy: 0.81 - ETA: 0s - loss: 0.4005 - accuracy: 0.81 - ETA: 0s - loss: 0.4005 - accuracy: 0.81 - ETA: 0s - loss: 0.4016 - accuracy: 0.81 - ETA: 0s - loss: 0.4030 - accuracy: 0.81 - ETA: 0s - loss: 0.4016 - accuracy: 0.81 - ETA: 0s - loss: 0.4016 - accuracy: 0.81 - ETA: 0s - loss: 0.4015 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4032 - accuracy: 0.81 - ETA: 0s - loss: 0.4047 - accuracy: 0.81 - ETA: 0s - loss: 0.4041 - accuracy: 0.81 - 2s 209us/step - loss: 0.4040 - accuracy: 0.8169 - val_loss: 0.4313 - val_accuracy: 0.7978\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.29811\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4665 - accuracy: 0.77 - ETA: 2s - loss: 0.4079 - accuracy: 0.81 - ETA: 2s - loss: 0.4229 - accuracy: 0.81 - ETA: 2s - loss: 0.4016 - accuracy: 0.82 - ETA: 1s - loss: 0.3936 - accuracy: 0.82 - ETA: 1s - loss: 0.3955 - accuracy: 0.82 - ETA: 1s - loss: 0.3956 - accuracy: 0.81 - ETA: 1s - loss: 0.3992 - accuracy: 0.81 - ETA: 1s - loss: 0.3991 - accuracy: 0.81 - ETA: 1s - loss: 0.3939 - accuracy: 0.82 - ETA: 1s - loss: 0.3927 - accuracy: 0.82 - ETA: 1s - loss: 0.3907 - accuracy: 0.82 - ETA: 1s - loss: 0.3862 - accuracy: 0.82 - ETA: 1s - loss: 0.3852 - accuracy: 0.82 - ETA: 1s - loss: 0.3871 - accuracy: 0.81 - ETA: 1s - loss: 0.3850 - accuracy: 0.82 - ETA: 1s - loss: 0.3866 - accuracy: 0.82 - ETA: 1s - loss: 0.3858 - accuracy: 0.81 - ETA: 1s - loss: 0.3876 - accuracy: 0.81 - ETA: 0s - loss: 0.3905 - accuracy: 0.81 - ETA: 0s - loss: 0.3903 - accuracy: 0.81 - ETA: 0s - loss: 0.3900 - accuracy: 0.81 - ETA: 0s - loss: 0.3911 - accuracy: 0.81 - ETA: 0s - loss: 0.3915 - accuracy: 0.81 - ETA: 0s - loss: 0.3918 - accuracy: 0.81 - ETA: 0s - loss: 0.3919 - accuracy: 0.81 - ETA: 0s - loss: 0.3904 - accuracy: 0.81 - ETA: 0s - loss: 0.3897 - accuracy: 0.81 - ETA: 0s - loss: 0.3916 - accuracy: 0.81 - ETA: 0s - loss: 0.3917 - accuracy: 0.81 - ETA: 0s - loss: 0.3928 - accuracy: 0.81 - ETA: 0s - loss: 0.3934 - accuracy: 0.81 - ETA: 0s - loss: 0.3930 - accuracy: 0.81 - ETA: 0s - loss: 0.3934 - accuracy: 0.81 - ETA: 0s - loss: 0.3921 - accuracy: 0.81 - 2s 209us/step - loss: 0.3917 - accuracy: 0.8177 - val_loss: 0.3530 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.29811\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4748 - accuracy: 0.79 - ETA: 2s - loss: 0.4042 - accuracy: 0.82 - ETA: 1s - loss: 0.3966 - accuracy: 0.82 - ETA: 1s - loss: 0.4057 - accuracy: 0.82 - ETA: 1s - loss: 0.4127 - accuracy: 0.81 - ETA: 1s - loss: 0.4068 - accuracy: 0.82 - ETA: 1s - loss: 0.4022 - accuracy: 0.82 - ETA: 1s - loss: 0.4031 - accuracy: 0.82 - ETA: 1s - loss: 0.3939 - accuracy: 0.82 - ETA: 1s - loss: 0.3906 - accuracy: 0.82 - ETA: 1s - loss: 0.3902 - accuracy: 0.82 - ETA: 1s - loss: 0.3966 - accuracy: 0.82 - ETA: 1s - loss: 0.3921 - accuracy: 0.82 - ETA: 1s - loss: 0.3948 - accuracy: 0.82 - ETA: 1s - loss: 0.3891 - accuracy: 0.82 - ETA: 1s - loss: 0.3895 - accuracy: 0.82 - ETA: 1s - loss: 0.3909 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3918 - accuracy: 0.82 - ETA: 0s - loss: 0.3909 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3908 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3908 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3910 - accuracy: 0.81 - ETA: 0s - loss: 0.3901 - accuracy: 0.81 - ETA: 0s - loss: 0.3916 - accuracy: 0.81 - ETA: 0s - loss: 0.3929 - accuracy: 0.81 - 2s 210us/step - loss: 0.3929 - accuracy: 0.8185 - val_loss: 0.3686 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.29811\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3685 - accuracy: 0.83 - ETA: 3s - loss: 0.3487 - accuracy: 0.83 - ETA: 2s - loss: 0.3887 - accuracy: 0.81 - ETA: 2s - loss: 0.3937 - accuracy: 0.80 - ETA: 2s - loss: 0.3961 - accuracy: 0.80 - ETA: 2s - loss: 0.3912 - accuracy: 0.81 - ETA: 1s - loss: 0.3970 - accuracy: 0.80 - ETA: 1s - loss: 0.3908 - accuracy: 0.81 - ETA: 1s - loss: 0.3856 - accuracy: 0.81 - ETA: 1s - loss: 0.3860 - accuracy: 0.81 - ETA: 1s - loss: 0.3832 - accuracy: 0.81 - ETA: 1s - loss: 0.3844 - accuracy: 0.81 - ETA: 1s - loss: 0.3827 - accuracy: 0.81 - ETA: 1s - loss: 0.3853 - accuracy: 0.81 - ETA: 1s - loss: 0.3887 - accuracy: 0.81 - ETA: 1s - loss: 0.3869 - accuracy: 0.81 - ETA: 1s - loss: 0.3868 - accuracy: 0.81 - ETA: 1s - loss: 0.3881 - accuracy: 0.81 - ETA: 1s - loss: 0.3875 - accuracy: 0.81 - ETA: 1s - loss: 0.3864 - accuracy: 0.81 - ETA: 0s - loss: 0.3860 - accuracy: 0.81 - ETA: 0s - loss: 0.3875 - accuracy: 0.81 - ETA: 0s - loss: 0.3884 - accuracy: 0.81 - ETA: 0s - loss: 0.3860 - accuracy: 0.81 - ETA: 0s - loss: 0.3847 - accuracy: 0.82 - ETA: 0s - loss: 0.3847 - accuracy: 0.82 - ETA: 0s - loss: 0.3863 - accuracy: 0.81 - ETA: 0s - loss: 0.3864 - accuracy: 0.81 - ETA: 0s - loss: 0.3877 - accuracy: 0.81 - ETA: 0s - loss: 0.3886 - accuracy: 0.81 - ETA: 0s - loss: 0.3870 - accuracy: 0.81 - ETA: 0s - loss: 0.3877 - accuracy: 0.81 - ETA: 0s - loss: 0.3904 - accuracy: 0.81 - ETA: 0s - loss: 0.3892 - accuracy: 0.81 - ETA: 0s - loss: 0.3883 - accuracy: 0.81 - ETA: 0s - loss: 0.3899 - accuracy: 0.81 - 2s 213us/step - loss: 0.3885 - accuracy: 0.8196 - val_loss: 0.3567 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.29811\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3487 - accuracy: 0.85 - ETA: 2s - loss: 0.3796 - accuracy: 0.82 - ETA: 2s - loss: 0.3949 - accuracy: 0.82 - ETA: 2s - loss: 0.3913 - accuracy: 0.82 - ETA: 2s - loss: 0.3990 - accuracy: 0.82 - ETA: 2s - loss: 0.3963 - accuracy: 0.82 - ETA: 1s - loss: 0.4004 - accuracy: 0.81 - ETA: 1s - loss: 0.3993 - accuracy: 0.81 - ETA: 1s - loss: 0.4052 - accuracy: 0.81 - ETA: 1s - loss: 0.3989 - accuracy: 0.81 - ETA: 1s - loss: 0.3971 - accuracy: 0.81 - ETA: 1s - loss: 0.3970 - accuracy: 0.82 - ETA: 1s - loss: 0.3976 - accuracy: 0.82 - ETA: 1s - loss: 0.3956 - accuracy: 0.82 - ETA: 1s - loss: 0.3962 - accuracy: 0.81 - ETA: 1s - loss: 0.3942 - accuracy: 0.82 - ETA: 1s - loss: 0.3973 - accuracy: 0.81 - ETA: 1s - loss: 0.3969 - accuracy: 0.81 - ETA: 1s - loss: 0.3982 - accuracy: 0.81 - ETA: 0s - loss: 0.3962 - accuracy: 0.81 - ETA: 0s - loss: 0.3950 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3918 - accuracy: 0.82 - ETA: 0s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3925 - accuracy: 0.82 - ETA: 0s - loss: 0.3932 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3881 - accuracy: 0.82 - ETA: 0s - loss: 0.3867 - accuracy: 0.82 - ETA: 0s - loss: 0.3860 - accuracy: 0.82 - ETA: 0s - loss: 0.3851 - accuracy: 0.82 - ETA: 0s - loss: 0.3839 - accuracy: 0.82 - ETA: 0s - loss: 0.3839 - accuracy: 0.82 - 2s 212us/step - loss: 0.3832 - accuracy: 0.8250 - val_loss: 0.3661 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.29811\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.2897 - accuracy: 0.88 - ETA: 2s - loss: 0.3512 - accuracy: 0.84 - ETA: 1s - loss: 0.3611 - accuracy: 0.82 - ETA: 1s - loss: 0.3599 - accuracy: 0.82 - ETA: 1s - loss: 0.3610 - accuracy: 0.83 - ETA: 1s - loss: 0.3654 - accuracy: 0.82 - ETA: 1s - loss: 0.3678 - accuracy: 0.82 - ETA: 1s - loss: 0.3611 - accuracy: 0.82 - ETA: 1s - loss: 0.3630 - accuracy: 0.82 - ETA: 1s - loss: 0.3632 - accuracy: 0.82 - ETA: 1s - loss: 0.3700 - accuracy: 0.82 - ETA: 1s - loss: 0.3700 - accuracy: 0.82 - ETA: 1s - loss: 0.3758 - accuracy: 0.82 - ETA: 1s - loss: 0.3753 - accuracy: 0.82 - ETA: 1s - loss: 0.3745 - accuracy: 0.82 - ETA: 1s - loss: 0.3761 - accuracy: 0.82 - ETA: 1s - loss: 0.3794 - accuracy: 0.82 - ETA: 1s - loss: 0.3793 - accuracy: 0.82 - ETA: 1s - loss: 0.3811 - accuracy: 0.82 - ETA: 1s - loss: 0.3820 - accuracy: 0.82 - ETA: 1s - loss: 0.3796 - accuracy: 0.82 - ETA: 1s - loss: 0.3834 - accuracy: 0.81 - ETA: 1s - loss: 0.3815 - accuracy: 0.82 - ETA: 1s - loss: 0.3837 - accuracy: 0.81 - ETA: 0s - loss: 0.3820 - accuracy: 0.82 - ETA: 0s - loss: 0.3813 - accuracy: 0.82 - ETA: 0s - loss: 0.3796 - accuracy: 0.82 - ETA: 0s - loss: 0.3791 - accuracy: 0.82 - ETA: 0s - loss: 0.3806 - accuracy: 0.82 - ETA: 0s - loss: 0.3826 - accuracy: 0.82 - ETA: 0s - loss: 0.3825 - accuracy: 0.82 - ETA: 0s - loss: 0.3824 - accuracy: 0.82 - ETA: 0s - loss: 0.3824 - accuracy: 0.82 - ETA: 0s - loss: 0.3827 - accuracy: 0.82 - ETA: 0s - loss: 0.3807 - accuracy: 0.82 - ETA: 0s - loss: 0.3798 - accuracy: 0.82 - ETA: 0s - loss: 0.3796 - accuracy: 0.82 - ETA: 0s - loss: 0.3808 - accuracy: 0.82 - ETA: 0s - loss: 0.3811 - accuracy: 0.82 - 2s 227us/step - loss: 0.3811 - accuracy: 0.8217 - val_loss: 0.3566 - val_accuracy: 0.8399\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.29811\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4037 - accuracy: 0.83 - ETA: 2s - loss: 0.3663 - accuracy: 0.84 - ETA: 1s - loss: 0.3864 - accuracy: 0.82 - ETA: 1s - loss: 0.3633 - accuracy: 0.83 - ETA: 1s - loss: 0.3586 - accuracy: 0.83 - ETA: 1s - loss: 0.3708 - accuracy: 0.83 - ETA: 1s - loss: 0.3703 - accuracy: 0.83 - ETA: 1s - loss: 0.3643 - accuracy: 0.83 - ETA: 1s - loss: 0.3716 - accuracy: 0.82 - ETA: 1s - loss: 0.3664 - accuracy: 0.83 - ETA: 1s - loss: 0.3658 - accuracy: 0.83 - ETA: 1s - loss: 0.3689 - accuracy: 0.83 - ETA: 1s - loss: 0.3708 - accuracy: 0.82 - ETA: 1s - loss: 0.3767 - accuracy: 0.82 - ETA: 1s - loss: 0.3780 - accuracy: 0.82 - ETA: 1s - loss: 0.3764 - accuracy: 0.82 - ETA: 1s - loss: 0.3746 - accuracy: 0.83 - ETA: 1s - loss: 0.3758 - accuracy: 0.82 - ETA: 1s - loss: 0.3798 - accuracy: 0.82 - ETA: 0s - loss: 0.3824 - accuracy: 0.82 - ETA: 0s - loss: 0.3837 - accuracy: 0.82 - ETA: 0s - loss: 0.3823 - accuracy: 0.82 - ETA: 0s - loss: 0.3831 - accuracy: 0.82 - ETA: 0s - loss: 0.3828 - accuracy: 0.82 - ETA: 0s - loss: 0.3815 - accuracy: 0.82 - ETA: 0s - loss: 0.3821 - accuracy: 0.82 - ETA: 0s - loss: 0.3803 - accuracy: 0.82 - ETA: 0s - loss: 0.3805 - accuracy: 0.82 - ETA: 0s - loss: 0.3798 - accuracy: 0.82 - ETA: 0s - loss: 0.3785 - accuracy: 0.82 - ETA: 0s - loss: 0.3797 - accuracy: 0.82 - ETA: 0s - loss: 0.3784 - accuracy: 0.82 - ETA: 0s - loss: 0.3790 - accuracy: 0.82 - ETA: 0s - loss: 0.3780 - accuracy: 0.82 - ETA: 0s - loss: 0.3796 - accuracy: 0.82 - ETA: 0s - loss: 0.3787 - accuracy: 0.82 - 2s 210us/step - loss: 0.3796 - accuracy: 0.8273 - val_loss: 0.3507 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.29811\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3322 - accuracy: 0.87 - ETA: 2s - loss: 0.3791 - accuracy: 0.84 - ETA: 1s - loss: 0.3813 - accuracy: 0.82 - ETA: 1s - loss: 0.3645 - accuracy: 0.84 - ETA: 1s - loss: 0.3709 - accuracy: 0.84 - ETA: 1s - loss: 0.3692 - accuracy: 0.83 - ETA: 1s - loss: 0.3650 - accuracy: 0.84 - ETA: 1s - loss: 0.3597 - accuracy: 0.84 - ETA: 1s - loss: 0.3620 - accuracy: 0.84 - ETA: 1s - loss: 0.3596 - accuracy: 0.84 - ETA: 1s - loss: 0.3631 - accuracy: 0.84 - ETA: 1s - loss: 0.3667 - accuracy: 0.83 - ETA: 1s - loss: 0.3669 - accuracy: 0.83 - ETA: 1s - loss: 0.3676 - accuracy: 0.83 - ETA: 1s - loss: 0.3697 - accuracy: 0.83 - ETA: 1s - loss: 0.3695 - accuracy: 0.83 - ETA: 1s - loss: 0.3698 - accuracy: 0.83 - ETA: 1s - loss: 0.3721 - accuracy: 0.83 - ETA: 0s - loss: 0.3724 - accuracy: 0.83 - ETA: 0s - loss: 0.3740 - accuracy: 0.83 - ETA: 0s - loss: 0.3721 - accuracy: 0.83 - ETA: 0s - loss: 0.3733 - accuracy: 0.83 - ETA: 0s - loss: 0.3737 - accuracy: 0.83 - ETA: 0s - loss: 0.3744 - accuracy: 0.83 - ETA: 0s - loss: 0.3747 - accuracy: 0.83 - ETA: 0s - loss: 0.3763 - accuracy: 0.82 - ETA: 0s - loss: 0.3765 - accuracy: 0.83 - ETA: 0s - loss: 0.3766 - accuracy: 0.83 - ETA: 0s - loss: 0.3742 - accuracy: 0.83 - ETA: 0s - loss: 0.3746 - accuracy: 0.83 - ETA: 0s - loss: 0.3747 - accuracy: 0.83 - ETA: 0s - loss: 0.3739 - accuracy: 0.83 - ETA: 0s - loss: 0.3752 - accuracy: 0.83 - ETA: 0s - loss: 0.3742 - accuracy: 0.83 - ETA: 0s - loss: 0.3742 - accuracy: 0.83 - ETA: 0s - loss: 0.3759 - accuracy: 0.83 - 2s 215us/step - loss: 0.3763 - accuracy: 0.8304 - val_loss: 0.3672 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.29811\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4001 - accuracy: 0.80 - ETA: 2s - loss: 0.3857 - accuracy: 0.82 - ETA: 2s - loss: 0.3899 - accuracy: 0.81 - ETA: 2s - loss: 0.3862 - accuracy: 0.81 - ETA: 1s - loss: 0.3802 - accuracy: 0.81 - ETA: 1s - loss: 0.3886 - accuracy: 0.81 - ETA: 1s - loss: 0.3800 - accuracy: 0.81 - ETA: 1s - loss: 0.3763 - accuracy: 0.82 - ETA: 1s - loss: 0.3726 - accuracy: 0.82 - ETA: 1s - loss: 0.3731 - accuracy: 0.82 - ETA: 1s - loss: 0.3757 - accuracy: 0.82 - ETA: 1s - loss: 0.3776 - accuracy: 0.82 - ETA: 1s - loss: 0.3738 - accuracy: 0.82 - ETA: 1s - loss: 0.3715 - accuracy: 0.82 - ETA: 1s - loss: 0.3700 - accuracy: 0.82 - ETA: 1s - loss: 0.3698 - accuracy: 0.82 - ETA: 1s - loss: 0.3708 - accuracy: 0.82 - ETA: 1s - loss: 0.3727 - accuracy: 0.82 - ETA: 1s - loss: 0.3731 - accuracy: 0.82 - ETA: 1s - loss: 0.3738 - accuracy: 0.82 - ETA: 0s - loss: 0.3721 - accuracy: 0.82 - ETA: 0s - loss: 0.3700 - accuracy: 0.82 - ETA: 0s - loss: 0.3730 - accuracy: 0.82 - ETA: 0s - loss: 0.3729 - accuracy: 0.82 - ETA: 0s - loss: 0.3713 - accuracy: 0.82 - ETA: 0s - loss: 0.3717 - accuracy: 0.82 - ETA: 0s - loss: 0.3721 - accuracy: 0.82 - ETA: 0s - loss: 0.3705 - accuracy: 0.82 - ETA: 0s - loss: 0.3711 - accuracy: 0.82 - ETA: 0s - loss: 0.3730 - accuracy: 0.82 - ETA: 0s - loss: 0.3745 - accuracy: 0.82 - ETA: 0s - loss: 0.3747 - accuracy: 0.82 - ETA: 0s - loss: 0.3762 - accuracy: 0.82 - ETA: 0s - loss: 0.3760 - accuracy: 0.82 - ETA: 0s - loss: 0.3743 - accuracy: 0.82 - ETA: 0s - loss: 0.3733 - accuracy: 0.82 - 2s 211us/step - loss: 0.3735 - accuracy: 0.8277 - val_loss: 0.4069 - val_accuracy: 0.8012\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.29811\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.4417 - accuracy: 0.77 - ETA: 2s - loss: 0.4220 - accuracy: 0.78 - ETA: 2s - loss: 0.3893 - accuracy: 0.81 - ETA: 2s - loss: 0.3862 - accuracy: 0.82 - ETA: 1s - loss: 0.3817 - accuracy: 0.82 - ETA: 1s - loss: 0.3799 - accuracy: 0.81 - ETA: 1s - loss: 0.3712 - accuracy: 0.82 - ETA: 1s - loss: 0.3746 - accuracy: 0.82 - ETA: 1s - loss: 0.3762 - accuracy: 0.82 - ETA: 1s - loss: 0.3745 - accuracy: 0.82 - ETA: 1s - loss: 0.3759 - accuracy: 0.82 - ETA: 1s - loss: 0.3735 - accuracy: 0.82 - ETA: 1s - loss: 0.3727 - accuracy: 0.82 - ETA: 1s - loss: 0.3717 - accuracy: 0.83 - ETA: 1s - loss: 0.3720 - accuracy: 0.83 - ETA: 1s - loss: 0.3729 - accuracy: 0.83 - ETA: 1s - loss: 0.3745 - accuracy: 0.83 - ETA: 1s - loss: 0.3760 - accuracy: 0.83 - ETA: 1s - loss: 0.3738 - accuracy: 0.83 - ETA: 0s - loss: 0.3725 - accuracy: 0.83 - ETA: 0s - loss: 0.3742 - accuracy: 0.83 - ETA: 0s - loss: 0.3745 - accuracy: 0.83 - ETA: 0s - loss: 0.3746 - accuracy: 0.83 - ETA: 0s - loss: 0.3741 - accuracy: 0.83 - ETA: 0s - loss: 0.3730 - accuracy: 0.82 - ETA: 0s - loss: 0.3711 - accuracy: 0.83 - ETA: 0s - loss: 0.3704 - accuracy: 0.83 - ETA: 0s - loss: 0.3712 - accuracy: 0.83 - ETA: 0s - loss: 0.3714 - accuracy: 0.83 - ETA: 0s - loss: 0.3695 - accuracy: 0.83 - ETA: 0s - loss: 0.3690 - accuracy: 0.83 - ETA: 0s - loss: 0.3690 - accuracy: 0.83 - ETA: 0s - loss: 0.3685 - accuracy: 0.83 - ETA: 0s - loss: 0.3699 - accuracy: 0.83 - ETA: 0s - loss: 0.3712 - accuracy: 0.83 - ETA: 0s - loss: 0.3723 - accuracy: 0.83 - 2s 210us/step - loss: 0.3718 - accuracy: 0.8302 - val_loss: 0.3717 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.29811\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3304 - accuracy: 0.86 - ETA: 2s - loss: 0.3195 - accuracy: 0.87 - ETA: 1s - loss: 0.3367 - accuracy: 0.86 - ETA: 1s - loss: 0.3364 - accuracy: 0.86 - ETA: 1s - loss: 0.3451 - accuracy: 0.85 - ETA: 1s - loss: 0.3479 - accuracy: 0.84 - ETA: 1s - loss: 0.3509 - accuracy: 0.84 - ETA: 1s - loss: 0.3628 - accuracy: 0.83 - ETA: 1s - loss: 0.3594 - accuracy: 0.83 - ETA: 1s - loss: 0.3651 - accuracy: 0.83 - ETA: 1s - loss: 0.3599 - accuracy: 0.84 - ETA: 1s - loss: 0.3603 - accuracy: 0.83 - ETA: 1s - loss: 0.3581 - accuracy: 0.84 - ETA: 1s - loss: 0.3634 - accuracy: 0.83 - ETA: 1s - loss: 0.3652 - accuracy: 0.83 - ETA: 1s - loss: 0.3652 - accuracy: 0.83 - ETA: 1s - loss: 0.3678 - accuracy: 0.83 - ETA: 1s - loss: 0.3660 - accuracy: 0.83 - ETA: 0s - loss: 0.3666 - accuracy: 0.83 - ETA: 0s - loss: 0.3675 - accuracy: 0.83 - ETA: 0s - loss: 0.3704 - accuracy: 0.83 - ETA: 0s - loss: 0.3723 - accuracy: 0.83 - ETA: 0s - loss: 0.3717 - accuracy: 0.83 - ETA: 0s - loss: 0.3699 - accuracy: 0.83 - ETA: 0s - loss: 0.3688 - accuracy: 0.83 - ETA: 0s - loss: 0.3678 - accuracy: 0.83 - ETA: 0s - loss: 0.3669 - accuracy: 0.83 - ETA: 0s - loss: 0.3647 - accuracy: 0.83 - ETA: 0s - loss: 0.3701 - accuracy: 0.83 - ETA: 0s - loss: 0.3682 - accuracy: 0.83 - ETA: 0s - loss: 0.3690 - accuracy: 0.83 - ETA: 0s - loss: 0.3681 - accuracy: 0.83 - ETA: 0s - loss: 0.3698 - accuracy: 0.83 - ETA: 0s - loss: 0.3709 - accuracy: 0.83 - ETA: 0s - loss: 0.3699 - accuracy: 0.83 - 2s 210us/step - loss: 0.3697 - accuracy: 0.8317 - val_loss: 0.3839 - val_accuracy: 0.8104\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.29811\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3640 - accuracy: 0.84 - ETA: 1s - loss: 0.3324 - accuracy: 0.85 - ETA: 2s - loss: 0.3361 - accuracy: 0.84 - ETA: 1s - loss: 0.3544 - accuracy: 0.83 - ETA: 1s - loss: 0.3556 - accuracy: 0.83 - ETA: 1s - loss: 0.3597 - accuracy: 0.83 - ETA: 1s - loss: 0.3631 - accuracy: 0.83 - ETA: 1s - loss: 0.3694 - accuracy: 0.82 - ETA: 1s - loss: 0.3622 - accuracy: 0.83 - ETA: 1s - loss: 0.3609 - accuracy: 0.83 - ETA: 1s - loss: 0.3629 - accuracy: 0.83 - ETA: 1s - loss: 0.3630 - accuracy: 0.83 - ETA: 1s - loss: 0.3600 - accuracy: 0.83 - ETA: 1s - loss: 0.3595 - accuracy: 0.83 - ETA: 1s - loss: 0.3642 - accuracy: 0.83 - ETA: 1s - loss: 0.3654 - accuracy: 0.83 - ETA: 1s - loss: 0.3652 - accuracy: 0.83 - ETA: 1s - loss: 0.3657 - accuracy: 0.83 - ETA: 0s - loss: 0.3668 - accuracy: 0.83 - ETA: 0s - loss: 0.3634 - accuracy: 0.83 - ETA: 0s - loss: 0.3655 - accuracy: 0.83 - ETA: 0s - loss: 0.3656 - accuracy: 0.82 - ETA: 0s - loss: 0.3637 - accuracy: 0.83 - ETA: 0s - loss: 0.3649 - accuracy: 0.82 - ETA: 0s - loss: 0.3647 - accuracy: 0.82 - ETA: 0s - loss: 0.3658 - accuracy: 0.82 - ETA: 0s - loss: 0.3674 - accuracy: 0.82 - ETA: 0s - loss: 0.3678 - accuracy: 0.82 - ETA: 0s - loss: 0.3668 - accuracy: 0.82 - ETA: 0s - loss: 0.3677 - accuracy: 0.82 - ETA: 0s - loss: 0.3660 - accuracy: 0.82 - ETA: 0s - loss: 0.3670 - accuracy: 0.82 - ETA: 0s - loss: 0.3680 - accuracy: 0.82 - ETA: 0s - loss: 0.3667 - accuracy: 0.82 - ETA: 0s - loss: 0.3660 - accuracy: 0.83 - 2s 209us/step - loss: 0.3650 - accuracy: 0.8321 - val_loss: 0.3297 - val_accuracy: 0.8551\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.29811\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3155 - accuracy: 0.86 - ETA: 2s - loss: 0.3176 - accuracy: 0.85 - ETA: 2s - loss: 0.3393 - accuracy: 0.84 - ETA: 2s - loss: 0.3609 - accuracy: 0.83 - ETA: 1s - loss: 0.3576 - accuracy: 0.83 - ETA: 1s - loss: 0.3494 - accuracy: 0.84 - ETA: 1s - loss: 0.3489 - accuracy: 0.84 - ETA: 1s - loss: 0.3455 - accuracy: 0.84 - ETA: 1s - loss: 0.3442 - accuracy: 0.84 - ETA: 1s - loss: 0.3447 - accuracy: 0.84 - ETA: 1s - loss: 0.3432 - accuracy: 0.85 - ETA: 1s - loss: 0.3445 - accuracy: 0.85 - ETA: 1s - loss: 0.3457 - accuracy: 0.85 - ETA: 1s - loss: 0.3462 - accuracy: 0.85 - ETA: 1s - loss: 0.3486 - accuracy: 0.84 - ETA: 1s - loss: 0.3490 - accuracy: 0.84 - ETA: 1s - loss: 0.3551 - accuracy: 0.84 - ETA: 1s - loss: 0.3582 - accuracy: 0.84 - ETA: 1s - loss: 0.3601 - accuracy: 0.84 - ETA: 1s - loss: 0.3605 - accuracy: 0.84 - ETA: 0s - loss: 0.3602 - accuracy: 0.84 - ETA: 0s - loss: 0.3590 - accuracy: 0.84 - ETA: 0s - loss: 0.3614 - accuracy: 0.84 - ETA: 0s - loss: 0.3617 - accuracy: 0.83 - ETA: 0s - loss: 0.3621 - accuracy: 0.84 - ETA: 0s - loss: 0.3598 - accuracy: 0.84 - ETA: 0s - loss: 0.3595 - accuracy: 0.84 - ETA: 0s - loss: 0.3612 - accuracy: 0.84 - ETA: 0s - loss: 0.3610 - accuracy: 0.84 - ETA: 0s - loss: 0.3620 - accuracy: 0.84 - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 0s - loss: 0.3606 - accuracy: 0.84 - ETA: 0s - loss: 0.3612 - accuracy: 0.84 - ETA: 0s - loss: 0.3613 - accuracy: 0.84 - ETA: 0s - loss: 0.3608 - accuracy: 0.84 - ETA: 0s - loss: 0.3608 - accuracy: 0.84 - ETA: 0s - loss: 0.3634 - accuracy: 0.83 - 2s 215us/step - loss: 0.3639 - accuracy: 0.8398 - val_loss: 0.3490 - val_accuracy: 0.8517\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.29811\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3894 - accuracy: 0.82 - ETA: 2s - loss: 0.3352 - accuracy: 0.85 - ETA: 2s - loss: 0.3854 - accuracy: 0.82 - ETA: 1s - loss: 0.3729 - accuracy: 0.82 - ETA: 1s - loss: 0.3590 - accuracy: 0.83 - ETA: 1s - loss: 0.3610 - accuracy: 0.83 - ETA: 1s - loss: 0.3521 - accuracy: 0.83 - ETA: 1s - loss: 0.3534 - accuracy: 0.83 - ETA: 1s - loss: 0.3513 - accuracy: 0.83 - ETA: 1s - loss: 0.3526 - accuracy: 0.83 - ETA: 1s - loss: 0.3517 - accuracy: 0.83 - ETA: 1s - loss: 0.3518 - accuracy: 0.83 - ETA: 1s - loss: 0.3485 - accuracy: 0.84 - ETA: 1s - loss: 0.3499 - accuracy: 0.84 - ETA: 1s - loss: 0.3487 - accuracy: 0.84 - ETA: 1s - loss: 0.3523 - accuracy: 0.84 - ETA: 1s - loss: 0.3533 - accuracy: 0.84 - ETA: 1s - loss: 0.3557 - accuracy: 0.84 - ETA: 1s - loss: 0.3539 - accuracy: 0.84 - ETA: 1s - loss: 0.3545 - accuracy: 0.84 - ETA: 0s - loss: 0.3535 - accuracy: 0.84 - ETA: 0s - loss: 0.3545 - accuracy: 0.84 - ETA: 0s - loss: 0.3534 - accuracy: 0.84 - ETA: 0s - loss: 0.3547 - accuracy: 0.84 - ETA: 0s - loss: 0.3564 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3544 - accuracy: 0.84 - ETA: 0s - loss: 0.3550 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3572 - accuracy: 0.83 - ETA: 0s - loss: 0.3584 - accuracy: 0.83 - ETA: 0s - loss: 0.3587 - accuracy: 0.83 - ETA: 0s - loss: 0.3574 - accuracy: 0.83 - ETA: 0s - loss: 0.3574 - accuracy: 0.83 - ETA: 0s - loss: 0.3583 - accuracy: 0.83 - 2s 210us/step - loss: 0.3581 - accuracy: 0.8380 - val_loss: 0.3554 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.29811\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3079 - accuracy: 0.81 - ETA: 2s - loss: 0.2833 - accuracy: 0.86 - ETA: 1s - loss: 0.3113 - accuracy: 0.85 - ETA: 1s - loss: 0.3460 - accuracy: 0.84 - ETA: 1s - loss: 0.3501 - accuracy: 0.84 - ETA: 1s - loss: 0.3589 - accuracy: 0.84 - ETA: 1s - loss: 0.3541 - accuracy: 0.84 - ETA: 1s - loss: 0.3577 - accuracy: 0.83 - ETA: 1s - loss: 0.3600 - accuracy: 0.83 - ETA: 1s - loss: 0.3618 - accuracy: 0.83 - ETA: 1s - loss: 0.3640 - accuracy: 0.83 - ETA: 1s - loss: 0.3602 - accuracy: 0.83 - ETA: 1s - loss: 0.3566 - accuracy: 0.84 - ETA: 1s - loss: 0.3551 - accuracy: 0.84 - ETA: 1s - loss: 0.3539 - accuracy: 0.84 - ETA: 1s - loss: 0.3541 - accuracy: 0.84 - ETA: 1s - loss: 0.3540 - accuracy: 0.84 - ETA: 1s - loss: 0.3526 - accuracy: 0.84 - ETA: 1s - loss: 0.3547 - accuracy: 0.84 - ETA: 0s - loss: 0.3578 - accuracy: 0.84 - ETA: 0s - loss: 0.3567 - accuracy: 0.84 - ETA: 0s - loss: 0.3555 - accuracy: 0.84 - ETA: 0s - loss: 0.3530 - accuracy: 0.84 - ETA: 0s - loss: 0.3543 - accuracy: 0.84 - ETA: 0s - loss: 0.3548 - accuracy: 0.84 - ETA: 0s - loss: 0.3557 - accuracy: 0.84 - ETA: 0s - loss: 0.3578 - accuracy: 0.84 - ETA: 0s - loss: 0.3596 - accuracy: 0.84 - ETA: 0s - loss: 0.3597 - accuracy: 0.84 - ETA: 0s - loss: 0.3577 - accuracy: 0.84 - ETA: 0s - loss: 0.3574 - accuracy: 0.84 - ETA: 0s - loss: 0.3582 - accuracy: 0.84 - ETA: 0s - loss: 0.3592 - accuracy: 0.84 - ETA: 0s - loss: 0.3594 - accuracy: 0.84 - ETA: 0s - loss: 0.3621 - accuracy: 0.84 - ETA: 0s - loss: 0.3607 - accuracy: 0.84 - 2s 210us/step - loss: 0.3608 - accuracy: 0.8403 - val_loss: 0.3361 - val_accuracy: 0.8526\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.29811\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3627 - accuracy: 0.82 - ETA: 1s - loss: 0.3363 - accuracy: 0.85 - ETA: 1s - loss: 0.3439 - accuracy: 0.84 - ETA: 1s - loss: 0.3307 - accuracy: 0.85 - ETA: 1s - loss: 0.3344 - accuracy: 0.85 - ETA: 1s - loss: 0.3489 - accuracy: 0.84 - ETA: 1s - loss: 0.3495 - accuracy: 0.84 - ETA: 1s - loss: 0.3448 - accuracy: 0.85 - ETA: 1s - loss: 0.3461 - accuracy: 0.84 - ETA: 1s - loss: 0.3421 - accuracy: 0.85 - ETA: 1s - loss: 0.3439 - accuracy: 0.84 - ETA: 1s - loss: 0.3442 - accuracy: 0.84 - ETA: 1s - loss: 0.3460 - accuracy: 0.84 - ETA: 1s - loss: 0.3449 - accuracy: 0.84 - ETA: 1s - loss: 0.3421 - accuracy: 0.84 - ETA: 1s - loss: 0.3439 - accuracy: 0.84 - ETA: 1s - loss: 0.3489 - accuracy: 0.84 - ETA: 1s - loss: 0.3500 - accuracy: 0.84 - ETA: 1s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3490 - accuracy: 0.84 - ETA: 0s - loss: 0.3486 - accuracy: 0.84 - ETA: 0s - loss: 0.3474 - accuracy: 0.84 - ETA: 0s - loss: 0.3478 - accuracy: 0.84 - ETA: 0s - loss: 0.3490 - accuracy: 0.84 - ETA: 0s - loss: 0.3486 - accuracy: 0.84 - ETA: 0s - loss: 0.3493 - accuracy: 0.84 - ETA: 0s - loss: 0.3516 - accuracy: 0.84 - ETA: 0s - loss: 0.3514 - accuracy: 0.84 - ETA: 0s - loss: 0.3521 - accuracy: 0.84 - ETA: 0s - loss: 0.3521 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3556 - accuracy: 0.83 - ETA: 0s - loss: 0.3551 - accuracy: 0.84 - ETA: 0s - loss: 0.3550 - accuracy: 0.83 - ETA: 0s - loss: 0.3558 - accuracy: 0.83 - 2s 210us/step - loss: 0.3556 - accuracy: 0.8396 - val_loss: 0.3623 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.29811\n",
      "Epoch 27/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.3623 - accuracy: 0.82 - ETA: 2s - loss: 0.3754 - accuracy: 0.83 - ETA: 2s - loss: 0.3649 - accuracy: 0.84 - ETA: 1s - loss: 0.3701 - accuracy: 0.84 - ETA: 1s - loss: 0.3701 - accuracy: 0.83 - ETA: 1s - loss: 0.3631 - accuracy: 0.84 - ETA: 1s - loss: 0.3615 - accuracy: 0.84 - ETA: 1s - loss: 0.3557 - accuracy: 0.84 - ETA: 1s - loss: 0.3592 - accuracy: 0.84 - ETA: 1s - loss: 0.3579 - accuracy: 0.84 - ETA: 1s - loss: 0.3592 - accuracy: 0.83 - ETA: 1s - loss: 0.3638 - accuracy: 0.83 - ETA: 1s - loss: 0.3618 - accuracy: 0.83 - ETA: 1s - loss: 0.3596 - accuracy: 0.83 - ETA: 1s - loss: 0.3558 - accuracy: 0.83 - ETA: 1s - loss: 0.3525 - accuracy: 0.84 - ETA: 1s - loss: 0.3541 - accuracy: 0.83 - ETA: 1s - loss: 0.3551 - accuracy: 0.83 - ETA: 1s - loss: 0.3538 - accuracy: 0.84 - ETA: 1s - loss: 0.3533 - accuracy: 0.84 - ETA: 0s - loss: 0.3539 - accuracy: 0.84 - ETA: 0s - loss: 0.3525 - accuracy: 0.84 - ETA: 0s - loss: 0.3532 - accuracy: 0.84 - ETA: 0s - loss: 0.3527 - accuracy: 0.84 - ETA: 0s - loss: 0.3503 - accuracy: 0.84 - ETA: 0s - loss: 0.3507 - accuracy: 0.84 - ETA: 0s - loss: 0.3502 - accuracy: 0.84 - ETA: 0s - loss: 0.3502 - accuracy: 0.84 - ETA: 0s - loss: 0.3503 - accuracy: 0.84 - ETA: 0s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.3510 - accuracy: 0.84 - ETA: 0s - loss: 0.3527 - accuracy: 0.84 - ETA: 0s - loss: 0.3537 - accuracy: 0.84 - ETA: 0s - loss: 0.3538 - accuracy: 0.84 - ETA: 0s - loss: 0.3535 - accuracy: 0.84 - ETA: 0s - loss: 0.3537 - accuracy: 0.84 - ETA: 0s - loss: 0.3536 - accuracy: 0.84 - 2s 215us/step - loss: 0.3540 - accuracy: 0.8397 - val_loss: 0.3529 - val_accuracy: 0.8500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.29811\n",
      "Epoch 00027: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - ETA:  - 0s 117us/step\n",
      "[2020-05-18 16:09:14 RAM69.6% 0.68GB] Val Score : [0.35184762670381037, 0.8483699560165405]\n",
      "[2020-05-18 16:09:14 RAM69.6% 0.68GB] ============================================================================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save the model history in a list after fitting so we can plot later \n",
    "model_history=[]\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold :\", i+1)\n",
    "    x_t, x_v, y_t, y_v = train_test_split(train_x, train_y, test_size=0.10, random_state = 2020)\n",
    "    model_history.append(fit_and_evaluate(x_t, x_v, y_t, y_v, epochs, batch_size))\n",
    "    print(\"=============\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "#Load the model that was saved by ModelCheckpoint\n",
    "model1 = load_model('ttbox_deep_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4518/4518 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 122us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24190440077883787, 0.9178839921951294]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(test_x, test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels: \n",
    "predicted_classes= model1.predict(test_x)\n",
    "\n",
    "predicted_classes=np.argmax(np.round(predicted_classes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_predictions = model1.predict_classes(test_x, batch_size=128, verbose=0)\n",
    "rounded_predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rounded_labels=np.argmax(test_y, axis=1)\n",
    "rounded_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:13:31 RAM67.3% 0.69GB] Confusion matrix, without normalization\n",
      "[2020-05-18 16:13:31 RAM67.3% 0.69GB] [[ 783   35]\n",
      " [ 336 3364]]\n",
      "[2020-05-18 16:13:31 RAM67.3% 0.69GB] Normalized confusion matrix\n",
      "[2020-05-18 16:13:31 RAM67.3% 0.69GB] [[0.96 0.04]\n",
      " [0.09 0.91]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 16:13:32 RAM67.3% 0.69GB] \n",
      "Clasification report TTBOX+ CNN:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "missing_queen       0.70      0.96      0.81       818\n",
      "       active       0.99      0.91      0.95      3700\n",
      "\n",
      "     accuracy                           0.92      4518\n",
      "    macro avg       0.84      0.93      0.88      4518\n",
      " weighted avg       0.94      0.92      0.92      4518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(rounded_labels, rounded_predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='TTBOX + CNN Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "target_names=['missing_queen', 'active']\n",
    "print ('\\nClasification report TTBOX+ CNN:\\n', classification_report(rounded_labels, rounded_predictions, target_names=target_names ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: TTBOX+ DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17702, 164, 1), (17702, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ttbox= x_ttbox.reshape(-1, 164, 1)\n",
    "Y_ttbox=y_ttbox.reshape(-1, 1)\n",
    "X_ttbox.shape, Y_ttbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13183, 164, 1), (4518, 164, 1), (13183, 2), (4518, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X_ttbox = np.array(X_ttbox.tolist())\n",
    "Y_ttbox = np.array(Y_ttbox.tolist())\n",
    "# Encode the classification labels\n",
    "te = LabelEncoder()\n",
    "y_ttbox = to_categorical(te.fit_transform(Y_ttbox)) \n",
    "# split the dataset \n",
    "# split the dataset \n",
    "train_x= X_ttbox[4519:]\n",
    "test_x=X_ttbox[: 4518]\n",
    "test_y=y_ttbox[:4518]\n",
    "train_y= y_ttbox[4519:]\n",
    "#train_x, test_x, train_y, test_y = train_test_split(X_ttbox, y_ttbox, test_size=0.3, random_state = 2020, shuffle=False)\n",
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dense_Net(size): \n",
    "    # size=(164,1 )\n",
    "    # Create the model\n",
    "    model=Sequential()\n",
    "    # the first Dense layer\n",
    "    model.add(Flatten(input_shape=size))\n",
    "    model.add(Dense(328))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # the second dense layer \n",
    "    model.add(Dense(328))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # the third dense layer \n",
    "    model.add(Dense(328))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    # the output layer \n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 164)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 328)               54120     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 328)               1312      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 328)               107912    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 328)               1312      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 328)               107912    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 328)               1312      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 658       \n",
      "=================================================================\n",
      "Total params: 274,538\n",
      "Trainable params: 272,570\n",
      "Non-trainable params: 1,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "size=(164, 1)\n",
    "model2= Dense_Net(size)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping critiria\n",
    "pat=5 # this is the number of epocks with no improvment after which the training will stop \n",
    "early_stopping= EarlyStopping(monitor='val_loss', patience=pat, verbose=1 )\n",
    "\n",
    "# define the model check point callback -> this will keep saving the model as a physical file \n",
    "model_checkpoint= ModelCheckpoint('DenseNet_bee_1.h5', verbose=1, save_best_only=True )\n",
    "\n",
    "# define a function to fit the model\n",
    "\n",
    "def fit_and_evaluate(train_x, val_x, train_y, val_y, EPOCHS=50, BATCH_SIZE=145 ):\n",
    "    model=None\n",
    "    model=Dense_Net(( 164,1 ))\n",
    "    results= model.fit(train_x, train_y, epochs=EPOCHS, batch_size= BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], verbose=1, validation_split=0.1)\n",
    "    print(\"Val Score :\", model.evaluate(val_x, val_y))\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 17:21:08 RAM61.9% 0.35GB] Training on Fold : 1\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 48s - loss: 1.0249 - accuracy: 0.462 - ETA: 10s - loss: 1.2034 - accuracy: 0.627 - ETA: 4s - loss: 0.8733 - accuracy: 0.697 - ETA: 3s - loss: 0.7619 - accuracy: 0.72 - ETA: 2s - loss: 0.7099 - accuracy: 0.72 - ETA: 1s - loss: 0.6710 - accuracy: 0.73 - ETA: 1s - loss: 0.6272 - accuracy: 0.75 - ETA: 1s - loss: 0.6063 - accuracy: 0.75 - ETA: 0s - loss: 0.5922 - accuracy: 0.75 - ETA: 0s - loss: 0.5730 - accuracy: 0.76 - ETA: 0s - loss: 0.5608 - accuracy: 0.76 - ETA: 0s - loss: 0.5553 - accuracy: 0.76 - ETA: 0s - loss: 0.5431 - accuracy: 0.77 - ETA: 0s - loss: 0.5313 - accuracy: 0.77 - ETA: 0s - loss: 0.5183 - accuracy: 0.78 - ETA: 0s - loss: 0.5121 - accuracy: 0.78 - 2s 153us/step - loss: 0.5100 - accuracy: 0.7853 - val_loss: 0.9370 - val_accuracy: 0.5358\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93696, saving model to DenseNet_bee_1.h5\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.80 - ETA: 0s - loss: 0.3949 - accuracy: 0.81 - ETA: 0s - loss: 0.4128 - accuracy: 0.80 - ETA: 0s - loss: 0.4006 - accuracy: 0.81 - ETA: 0s - loss: 0.3977 - accuracy: 0.81 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3998 - accuracy: 0.82 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3947 - accuracy: 0.82 - ETA: 0s - loss: 0.3988 - accuracy: 0.82 - ETA: 0s - loss: 0.3947 - accuracy: 0.82 - ETA: 0s - loss: 0.3928 - accuracy: 0.82 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3840 - accuracy: 0.83 - 1s 78us/step - loss: 0.3847 - accuracy: 0.8325 - val_loss: 0.8222 - val_accuracy: 0.6529\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93696 to 0.82220, saving model to DenseNet_bee_1.h5\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.88 - ETA: 0s - loss: 0.3636 - accuracy: 0.84 - ETA: 0s - loss: 0.3403 - accuracy: 0.85 - ETA: 0s - loss: 0.3450 - accuracy: 0.85 - ETA: 0s - loss: 0.3375 - accuracy: 0.85 - ETA: 0s - loss: 0.3509 - accuracy: 0.85 - ETA: 0s - loss: 0.3533 - accuracy: 0.84 - ETA: 0s - loss: 0.3560 - accuracy: 0.84 - ETA: 0s - loss: 0.3511 - accuracy: 0.84 - ETA: 0s - loss: 0.3525 - accuracy: 0.84 - ETA: 0s - loss: 0.3537 - accuracy: 0.84 - ETA: 0s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3489 - accuracy: 0.84 - ETA: 0s - loss: 0.3433 - accuracy: 0.85 - ETA: 0s - loss: 0.3436 - accuracy: 0.85 - 1s 78us/step - loss: 0.3425 - accuracy: 0.8515 - val_loss: 0.4481 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.82220 to 0.44810, saving model to DenseNet_bee_1.h5\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3367 - accuracy: 0.84 - ETA: 0s - loss: 0.3360 - accuracy: 0.85 - ETA: 0s - loss: 0.3262 - accuracy: 0.86 - ETA: 0s - loss: 0.3327 - accuracy: 0.85 - ETA: 0s - loss: 0.3274 - accuracy: 0.86 - ETA: 0s - loss: 0.3305 - accuracy: 0.86 - ETA: 0s - loss: 0.3310 - accuracy: 0.86 - ETA: 0s - loss: 0.3282 - accuracy: 0.85 - ETA: 0s - loss: 0.3231 - accuracy: 0.86 - ETA: 0s - loss: 0.3237 - accuracy: 0.86 - ETA: 0s - loss: 0.3210 - accuracy: 0.86 - ETA: 0s - loss: 0.3211 - accuracy: 0.86 - ETA: 0s - loss: 0.3205 - accuracy: 0.86 - ETA: 0s - loss: 0.3208 - accuracy: 0.86 - 1s 75us/step - loss: 0.3223 - accuracy: 0.8630 - val_loss: 0.3030 - val_accuracy: 0.8728\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44810 to 0.30299, saving model to DenseNet_bee_1.h5\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.86 - ETA: 0s - loss: 0.3182 - accuracy: 0.87 - ETA: 0s - loss: 0.3224 - accuracy: 0.86 - ETA: 0s - loss: 0.3212 - accuracy: 0.86 - ETA: 0s - loss: 0.3101 - accuracy: 0.86 - ETA: 0s - loss: 0.3172 - accuracy: 0.86 - ETA: 0s - loss: 0.3201 - accuracy: 0.86 - ETA: 0s - loss: 0.3197 - accuracy: 0.86 - ETA: 0s - loss: 0.3166 - accuracy: 0.86 - ETA: 0s - loss: 0.3096 - accuracy: 0.86 - ETA: 0s - loss: 0.3026 - accuracy: 0.87 - ETA: 0s - loss: 0.3034 - accuracy: 0.86 - ETA: 0s - loss: 0.3046 - accuracy: 0.86 - ETA: 0s - loss: 0.3059 - accuracy: 0.86 - 1s 76us/step - loss: 0.3037 - accuracy: 0.8681 - val_loss: 0.3341 - val_accuracy: 0.8551\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30299\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.90 - ETA: 0s - loss: 0.3155 - accuracy: 0.86 - ETA: 0s - loss: 0.3104 - accuracy: 0.86 - ETA: 0s - loss: 0.3217 - accuracy: 0.85 - ETA: 0s - loss: 0.3152 - accuracy: 0.86 - ETA: 0s - loss: 0.3082 - accuracy: 0.86 - ETA: 0s - loss: 0.3122 - accuracy: 0.86 - ETA: 0s - loss: 0.3170 - accuracy: 0.86 - ETA: 0s - loss: 0.3111 - accuracy: 0.86 - ETA: 0s - loss: 0.3079 - accuracy: 0.86 - ETA: 0s - loss: 0.3064 - accuracy: 0.86 - ETA: 0s - loss: 0.3065 - accuracy: 0.86 - ETA: 0s - loss: 0.3041 - accuracy: 0.86 - ETA: 0s - loss: 0.3037 - accuracy: 0.86 - 1s 75us/step - loss: 0.3044 - accuracy: 0.8666 - val_loss: 0.2864 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30299 to 0.28643, saving model to DenseNet_bee_1.h5\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.87 - ETA: 0s - loss: 0.2583 - accuracy: 0.89 - ETA: 0s - loss: 0.2784 - accuracy: 0.88 - ETA: 0s - loss: 0.2838 - accuracy: 0.87 - ETA: 0s - loss: 0.2827 - accuracy: 0.88 - ETA: 0s - loss: 0.2912 - accuracy: 0.87 - ETA: 0s - loss: 0.2850 - accuracy: 0.87 - ETA: 0s - loss: 0.2840 - accuracy: 0.87 - ETA: 0s - loss: 0.2852 - accuracy: 0.87 - ETA: 0s - loss: 0.2850 - accuracy: 0.87 - ETA: 0s - loss: 0.2859 - accuracy: 0.87 - ETA: 0s - loss: 0.2897 - accuracy: 0.87 - ETA: 0s - loss: 0.2902 - accuracy: 0.87 - ETA: 0s - loss: 0.2882 - accuracy: 0.87 - 1s 75us/step - loss: 0.2877 - accuracy: 0.8771 - val_loss: 0.3185 - val_accuracy: 0.8475\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.28643\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.90 - ETA: 0s - loss: 0.2440 - accuracy: 0.90 - ETA: 0s - loss: 0.2662 - accuracy: 0.89 - ETA: 0s - loss: 0.2619 - accuracy: 0.89 - ETA: 0s - loss: 0.2701 - accuracy: 0.88 - ETA: 0s - loss: 0.2704 - accuracy: 0.88 - ETA: 0s - loss: 0.2817 - accuracy: 0.88 - ETA: 0s - loss: 0.2792 - accuracy: 0.88 - ETA: 0s - loss: 0.2769 - accuracy: 0.88 - ETA: 0s - loss: 0.2782 - accuracy: 0.88 - ETA: 0s - loss: 0.2800 - accuracy: 0.88 - ETA: 0s - loss: 0.2790 - accuracy: 0.88 - ETA: 0s - loss: 0.2788 - accuracy: 0.88 - ETA: 0s - loss: 0.2801 - accuracy: 0.87 - ETA: 0s - loss: 0.2778 - accuracy: 0.87 - 1s 75us/step - loss: 0.2774 - accuracy: 0.8802 - val_loss: 0.4049 - val_accuracy: 0.7995\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.28643\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.91 - ETA: 0s - loss: 0.2324 - accuracy: 0.90 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.89 - ETA: 0s - loss: 0.2588 - accuracy: 0.88 - ETA: 0s - loss: 0.2590 - accuracy: 0.88 - ETA: 0s - loss: 0.2632 - accuracy: 0.88 - ETA: 0s - loss: 0.2599 - accuracy: 0.88 - ETA: 0s - loss: 0.2666 - accuracy: 0.88 - ETA: 0s - loss: 0.2687 - accuracy: 0.88 - ETA: 0s - loss: 0.2671 - accuracy: 0.88 - ETA: 0s - loss: 0.2702 - accuracy: 0.88 - ETA: 0s - loss: 0.2707 - accuracy: 0.88 - ETA: 0s - loss: 0.2704 - accuracy: 0.88 - ETA: 0s - loss: 0.2693 - accuracy: 0.88 - 1s 78us/step - loss: 0.2681 - accuracy: 0.8859 - val_loss: 0.2516 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.28643 to 0.25163, saving model to DenseNet_bee_1.h5\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.88 - ETA: 0s - loss: 0.2515 - accuracy: 0.90 - ETA: 0s - loss: 0.2583 - accuracy: 0.89 - ETA: 0s - loss: 0.2620 - accuracy: 0.88 - ETA: 0s - loss: 0.2660 - accuracy: 0.88 - ETA: 0s - loss: 0.2629 - accuracy: 0.88 - ETA: 0s - loss: 0.2610 - accuracy: 0.88 - ETA: 0s - loss: 0.2645 - accuracy: 0.88 - ETA: 0s - loss: 0.2670 - accuracy: 0.88 - ETA: 0s - loss: 0.2656 - accuracy: 0.88 - ETA: 0s - loss: 0.2604 - accuracy: 0.88 - ETA: 0s - loss: 0.2622 - accuracy: 0.88 - ETA: 0s - loss: 0.2608 - accuracy: 0.88 - ETA: 0s - loss: 0.2645 - accuracy: 0.88 - 1s 75us/step - loss: 0.2640 - accuracy: 0.8863 - val_loss: 0.2777 - val_accuracy: 0.8812\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25163\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.91 - ETA: 0s - loss: 0.2526 - accuracy: 0.88 - ETA: 0s - loss: 0.2618 - accuracy: 0.88 - ETA: 0s - loss: 0.2504 - accuracy: 0.89 - ETA: 0s - loss: 0.2532 - accuracy: 0.89 - ETA: 0s - loss: 0.2546 - accuracy: 0.89 - ETA: 0s - loss: 0.2555 - accuracy: 0.89 - ETA: 0s - loss: 0.2557 - accuracy: 0.88 - ETA: 0s - loss: 0.2594 - accuracy: 0.88 - ETA: 0s - loss: 0.2558 - accuracy: 0.88 - ETA: 0s - loss: 0.2557 - accuracy: 0.88 - ETA: 0s - loss: 0.2541 - accuracy: 0.88 - ETA: 0s - loss: 0.2567 - accuracy: 0.88 - ETA: 0s - loss: 0.2564 - accuracy: 0.88 - 1s 74us/step - loss: 0.2548 - accuracy: 0.8900 - val_loss: 0.2735 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25163\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.91 - ETA: 0s - loss: 0.2795 - accuracy: 0.87 - ETA: 0s - loss: 0.2574 - accuracy: 0.89 - ETA: 0s - loss: 0.2586 - accuracy: 0.89 - ETA: 0s - loss: 0.2677 - accuracy: 0.89 - ETA: 0s - loss: 0.2588 - accuracy: 0.89 - ETA: 0s - loss: 0.2566 - accuracy: 0.89 - ETA: 0s - loss: 0.2597 - accuracy: 0.88 - ETA: 0s - loss: 0.2579 - accuracy: 0.88 - ETA: 0s - loss: 0.2570 - accuracy: 0.88 - ETA: 0s - loss: 0.2579 - accuracy: 0.88 - ETA: 0s - loss: 0.2579 - accuracy: 0.88 - ETA: 0s - loss: 0.2582 - accuracy: 0.88 - 1s 72us/step - loss: 0.2572 - accuracy: 0.8896 - val_loss: 0.2675 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25163\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.88 - ETA: 0s - loss: 0.2726 - accuracy: 0.88 - ETA: 0s - loss: 0.2598 - accuracy: 0.88 - ETA: 0s - loss: 0.2563 - accuracy: 0.88 - ETA: 0s - loss: 0.2586 - accuracy: 0.88 - ETA: 0s - loss: 0.2548 - accuracy: 0.89 - ETA: 0s - loss: 0.2587 - accuracy: 0.88 - ETA: 0s - loss: 0.2621 - accuracy: 0.88 - ETA: 0s - loss: 0.2595 - accuracy: 0.88 - ETA: 0s - loss: 0.2595 - accuracy: 0.88 - ETA: 0s - loss: 0.2541 - accuracy: 0.89 - ETA: 0s - loss: 0.2524 - accuracy: 0.89 - ETA: 0s - loss: 0.2517 - accuracy: 0.89 - ETA: 0s - loss: 0.2528 - accuracy: 0.89 - 1s 74us/step - loss: 0.2520 - accuracy: 0.8919 - val_loss: 0.4087 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25163\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.85 - ETA: 0s - loss: 0.2528 - accuracy: 0.88 - ETA: 0s - loss: 0.2642 - accuracy: 0.88 - ETA: 0s - loss: 0.2538 - accuracy: 0.89 - ETA: 0s - loss: 0.2546 - accuracy: 0.89 - ETA: 0s - loss: 0.2514 - accuracy: 0.89 - ETA: 0s - loss: 0.2534 - accuracy: 0.89 - ETA: 0s - loss: 0.2493 - accuracy: 0.89 - ETA: 0s - loss: 0.2490 - accuracy: 0.89 - ETA: 0s - loss: 0.2495 - accuracy: 0.89 - ETA: 0s - loss: 0.2446 - accuracy: 0.89 - ETA: 0s - loss: 0.2466 - accuracy: 0.89 - ETA: 0s - loss: 0.2470 - accuracy: 0.89 - ETA: 0s - loss: 0.2472 - accuracy: 0.89 - ETA: 0s - loss: 0.2497 - accuracy: 0.89 - 1s 79us/step - loss: 0.2474 - accuracy: 0.8973 - val_loss: 0.3497 - val_accuracy: 0.8441\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.25163\n",
      "Epoch 00014: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 55us/step\n",
      "[2020-05-18 17:21:22 RAM62.8% 0.42GB] Val Score : [0.31884692775380713, 0.8483699560165405]\n",
      "[2020-05-18 17:21:22 RAM62.8% 0.42GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:21:22 RAM62.8% 0.42GB] Training on Fold : 2\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 42s - loss: 0.8618 - accuracy: 0.517 - ETA: 7s - loss: 1.1069 - accuracy: 0.663 - ETA: 4s - loss: 0.8861 - accuracy: 0.70 - ETA: 2s - loss: 0.7552 - accuracy: 0.73 - ETA: 2s - loss: 0.6813 - accuracy: 0.75 - ETA: 1s - loss: 0.6380 - accuracy: 0.76 - ETA: 1s - loss: 0.6030 - accuracy: 0.77 - ETA: 1s - loss: 0.5819 - accuracy: 0.77 - ETA: 0s - loss: 0.5760 - accuracy: 0.77 - ETA: 0s - loss: 0.5603 - accuracy: 0.77 - ETA: 0s - loss: 0.5504 - accuracy: 0.77 - ETA: 0s - loss: 0.5416 - accuracy: 0.77 - ETA: 0s - loss: 0.5359 - accuracy: 0.77 - ETA: 0s - loss: 0.5268 - accuracy: 0.78 - ETA: 0s - loss: 0.5185 - accuracy: 0.78 - 2s 144us/step - loss: 0.5147 - accuracy: 0.7872 - val_loss: 1.6268 - val_accuracy: 0.5670\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25163\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3860 - accuracy: 0.85 - ETA: 0s - loss: 0.3735 - accuracy: 0.84 - ETA: 0s - loss: 0.4011 - accuracy: 0.82 - ETA: 0s - loss: 0.3826 - accuracy: 0.82 - ETA: 0s - loss: 0.3772 - accuracy: 0.83 - ETA: 0s - loss: 0.3811 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.81 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.81 - ETA: 0s - loss: 0.3983 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - 1s 78us/step - loss: 0.3910 - accuracy: 0.8261 - val_loss: 0.7389 - val_accuracy: 0.6664\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25163\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.80 - ETA: 0s - loss: 0.3718 - accuracy: 0.82 - ETA: 0s - loss: 0.3562 - accuracy: 0.84 - ETA: 0s - loss: 0.3535 - accuracy: 0.84 - ETA: 0s - loss: 0.3578 - accuracy: 0.84 - ETA: 0s - loss: 0.3615 - accuracy: 0.83 - ETA: 0s - loss: 0.3697 - accuracy: 0.83 - ETA: 0s - loss: 0.3634 - accuracy: 0.83 - ETA: 0s - loss: 0.3578 - accuracy: 0.84 - ETA: 0s - loss: 0.3578 - accuracy: 0.84 - ETA: 0s - loss: 0.3522 - accuracy: 0.84 - ETA: 0s - loss: 0.3514 - accuracy: 0.84 - ETA: 0s - loss: 0.3505 - accuracy: 0.84 - ETA: 0s - loss: 0.3520 - accuracy: 0.84 - ETA: 0s - loss: 0.3497 - accuracy: 0.84 - 1s 77us/step - loss: 0.3493 - accuracy: 0.8452 - val_loss: 0.6216 - val_accuracy: 0.7397\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25163\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.86 - ETA: 0s - loss: 0.3112 - accuracy: 0.86 - ETA: 0s - loss: 0.3384 - accuracy: 0.84 - ETA: 0s - loss: 0.3308 - accuracy: 0.85 - ETA: 0s - loss: 0.3407 - accuracy: 0.84 - ETA: 0s - loss: 0.3406 - accuracy: 0.85 - ETA: 0s - loss: 0.3368 - accuracy: 0.85 - ETA: 0s - loss: 0.3288 - accuracy: 0.85 - ETA: 0s - loss: 0.3299 - accuracy: 0.85 - ETA: 0s - loss: 0.3291 - accuracy: 0.85 - ETA: 0s - loss: 0.3263 - accuracy: 0.85 - ETA: 0s - loss: 0.3274 - accuracy: 0.85 - ETA: 0s - loss: 0.3266 - accuracy: 0.85 - ETA: 0s - loss: 0.3251 - accuracy: 0.85 - ETA: 0s - loss: 0.3245 - accuracy: 0.85 - ETA: 0s - loss: 0.3265 - accuracy: 0.85 - 1s 85us/step - loss: 0.3259 - accuracy: 0.8579 - val_loss: 0.3766 - val_accuracy: 0.8391\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25163\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.88 - ETA: 0s - loss: 0.2842 - accuracy: 0.87 - ETA: 0s - loss: 0.3093 - accuracy: 0.86 - ETA: 0s - loss: 0.3144 - accuracy: 0.85 - ETA: 0s - loss: 0.3060 - accuracy: 0.86 - ETA: 0s - loss: 0.3071 - accuracy: 0.86 - ETA: 0s - loss: 0.3072 - accuracy: 0.86 - ETA: 0s - loss: 0.3085 - accuracy: 0.86 - ETA: 0s - loss: 0.3095 - accuracy: 0.86 - ETA: 0s - loss: 0.3118 - accuracy: 0.86 - ETA: 0s - loss: 0.3138 - accuracy: 0.86 - ETA: 0s - loss: 0.3097 - accuracy: 0.86 - ETA: 0s - loss: 0.3103 - accuracy: 0.86 - ETA: 0s - loss: 0.3131 - accuracy: 0.86 - ETA: 0s - loss: 0.3107 - accuracy: 0.86 - ETA: 0s - loss: 0.3141 - accuracy: 0.86 - ETA: 0s - loss: 0.3128 - accuracy: 0.86 - 1s 91us/step - loss: 0.3124 - accuracy: 0.8639 - val_loss: 0.3205 - val_accuracy: 0.8703\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25163\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.1779 - accuracy: 0.93 - ETA: 0s - loss: 0.2310 - accuracy: 0.89 - ETA: 0s - loss: 0.2777 - accuracy: 0.87 - ETA: 0s - loss: 0.2965 - accuracy: 0.86 - ETA: 0s - loss: 0.2919 - accuracy: 0.86 - ETA: 0s - loss: 0.2893 - accuracy: 0.87 - ETA: 0s - loss: 0.2915 - accuracy: 0.87 - ETA: 0s - loss: 0.2951 - accuracy: 0.87 - ETA: 0s - loss: 0.2956 - accuracy: 0.87 - ETA: 0s - loss: 0.2953 - accuracy: 0.87 - ETA: 0s - loss: 0.2922 - accuracy: 0.87 - ETA: 0s - loss: 0.2927 - accuracy: 0.87 - ETA: 0s - loss: 0.2952 - accuracy: 0.87 - ETA: 0s - loss: 0.2951 - accuracy: 0.87 - ETA: 0s - loss: 0.2971 - accuracy: 0.87 - ETA: 0s - loss: 0.2966 - accuracy: 0.87 - 1s 82us/step - loss: 0.2962 - accuracy: 0.8752 - val_loss: 0.4022 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25163\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.86 - ETA: 0s - loss: 0.2799 - accuracy: 0.87 - ETA: 0s - loss: 0.2772 - accuracy: 0.87 - ETA: 0s - loss: 0.2677 - accuracy: 0.88 - ETA: 0s - loss: 0.2761 - accuracy: 0.87 - ETA: 0s - loss: 0.2850 - accuracy: 0.87 - ETA: 0s - loss: 0.2874 - accuracy: 0.87 - ETA: 0s - loss: 0.2910 - accuracy: 0.87 - ETA: 0s - loss: 0.2930 - accuracy: 0.87 - ETA: 0s - loss: 0.2939 - accuracy: 0.87 - ETA: 0s - loss: 0.2926 - accuracy: 0.87 - ETA: 0s - loss: 0.2939 - accuracy: 0.87 - ETA: 0s - loss: 0.2923 - accuracy: 0.87 - ETA: 0s - loss: 0.2879 - accuracy: 0.87 - 1s 75us/step - loss: 0.2879 - accuracy: 0.8755 - val_loss: 0.3209 - val_accuracy: 0.8703\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25163\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2503 - accuracy: 0.89 - ETA: 0s - loss: 0.2843 - accuracy: 0.87 - ETA: 0s - loss: 0.2758 - accuracy: 0.88 - ETA: 0s - loss: 0.2785 - accuracy: 0.88 - ETA: 0s - loss: 0.2821 - accuracy: 0.87 - ETA: 0s - loss: 0.2784 - accuracy: 0.87 - ETA: 0s - loss: 0.2791 - accuracy: 0.87 - ETA: 0s - loss: 0.2743 - accuracy: 0.87 - ETA: 0s - loss: 0.2760 - accuracy: 0.87 - ETA: 0s - loss: 0.2729 - accuracy: 0.87 - ETA: 0s - loss: 0.2762 - accuracy: 0.87 - ETA: 0s - loss: 0.2759 - accuracy: 0.87 - 1s 73us/step - loss: 0.2763 - accuracy: 0.8790 - val_loss: 0.3294 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25163\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3538 - accuracy: 0.82 - ETA: 0s - loss: 0.3084 - accuracy: 0.87 - ETA: 0s - loss: 0.2894 - accuracy: 0.87 - ETA: 0s - loss: 0.2851 - accuracy: 0.87 - ETA: 0s - loss: 0.2825 - accuracy: 0.88 - ETA: 0s - loss: 0.2820 - accuracy: 0.88 - ETA: 0s - loss: 0.2811 - accuracy: 0.88 - ETA: 0s - loss: 0.2777 - accuracy: 0.88 - ETA: 0s - loss: 0.2742 - accuracy: 0.88 - ETA: 0s - loss: 0.2740 - accuracy: 0.88 - ETA: 0s - loss: 0.2703 - accuracy: 0.88 - ETA: 0s - loss: 0.2730 - accuracy: 0.88 - 1s 72us/step - loss: 0.2736 - accuracy: 0.8843 - val_loss: 0.2970 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25163\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2409 - accuracy: 0.91 - ETA: 0s - loss: 0.2715 - accuracy: 0.88 - ETA: 0s - loss: 0.2672 - accuracy: 0.88 - ETA: 0s - loss: 0.2685 - accuracy: 0.88 - ETA: 0s - loss: 0.2713 - accuracy: 0.88 - ETA: 0s - loss: 0.2668 - accuracy: 0.88 - ETA: 0s - loss: 0.2656 - accuracy: 0.88 - ETA: 0s - loss: 0.2626 - accuracy: 0.88 - ETA: 0s - loss: 0.2588 - accuracy: 0.88 - ETA: 0s - loss: 0.2572 - accuracy: 0.88 - ETA: 0s - loss: 0.2589 - accuracy: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.88 - 1s 72us/step - loss: 0.2602 - accuracy: 0.8881 - val_loss: 0.3213 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25163\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2515 - accuracy: 0.89 - ETA: 0s - loss: 0.2534 - accuracy: 0.88 - ETA: 0s - loss: 0.2527 - accuracy: 0.88 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2569 - accuracy: 0.88 - ETA: 0s - loss: 0.2513 - accuracy: 0.89 - ETA: 0s - loss: 0.2564 - accuracy: 0.88 - ETA: 0s - loss: 0.2509 - accuracy: 0.89 - ETA: 0s - loss: 0.2538 - accuracy: 0.89 - ETA: 0s - loss: 0.2519 - accuracy: 0.89 - ETA: 0s - loss: 0.2527 - accuracy: 0.89 - ETA: 0s - loss: 0.2530 - accuracy: 0.88 - ETA: 0s - loss: 0.2537 - accuracy: 0.88 - ETA: 0s - loss: 0.2539 - accuracy: 0.88 - ETA: 0s - loss: 0.2519 - accuracy: 0.89 - 1s 80us/step - loss: 0.2523 - accuracy: 0.8897 - val_loss: 0.4605 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25163\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2129 - accuracy: 0.93 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2655 - accuracy: 0.89 - ETA: 0s - loss: 0.2569 - accuracy: 0.89 - ETA: 0s - loss: 0.2671 - accuracy: 0.89 - ETA: 0s - loss: 0.2655 - accuracy: 0.89 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2541 - accuracy: 0.89 - ETA: 0s - loss: 0.2589 - accuracy: 0.89 - ETA: 0s - loss: 0.2538 - accuracy: 0.89 - ETA: 0s - loss: 0.2548 - accuracy: 0.89 - ETA: 0s - loss: 0.2571 - accuracy: 0.88 - ETA: 0s - loss: 0.2566 - accuracy: 0.88 - ETA: 0s - loss: 0.2569 - accuracy: 0.88 - 1s 79us/step - loss: 0.2566 - accuracy: 0.8887 - val_loss: 0.3747 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25163\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.90 - ETA: 0s - loss: 0.2442 - accuracy: 0.90 - ETA: 0s - loss: 0.2419 - accuracy: 0.89 - ETA: 0s - loss: 0.2512 - accuracy: 0.89 - ETA: 0s - loss: 0.2520 - accuracy: 0.89 - ETA: 0s - loss: 0.2507 - accuracy: 0.89 - ETA: 0s - loss: 0.2527 - accuracy: 0.89 - ETA: 0s - loss: 0.2534 - accuracy: 0.89 - ETA: 0s - loss: 0.2535 - accuracy: 0.89 - ETA: 0s - loss: 0.2544 - accuracy: 0.89 - ETA: 0s - loss: 0.2531 - accuracy: 0.89 - ETA: 0s - loss: 0.2519 - accuracy: 0.89 - 1s 72us/step - loss: 0.2520 - accuracy: 0.8934 - val_loss: 0.3431 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25163\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2456 - accuracy: 0.88 - ETA: 0s - loss: 0.2535 - accuracy: 0.89 - ETA: 0s - loss: 0.2420 - accuracy: 0.90 - ETA: 0s - loss: 0.2409 - accuracy: 0.89 - ETA: 0s - loss: 0.2413 - accuracy: 0.89 - ETA: 0s - loss: 0.2445 - accuracy: 0.89 - ETA: 0s - loss: 0.2456 - accuracy: 0.89 - ETA: 0s - loss: 0.2446 - accuracy: 0.89 - ETA: 0s - loss: 0.2431 - accuracy: 0.89 - ETA: 0s - loss: 0.2442 - accuracy: 0.89 - ETA: 0s - loss: 0.2450 - accuracy: 0.89 - ETA: 0s - loss: 0.2480 - accuracy: 0.89 - 1s 69us/step - loss: 0.2490 - accuracy: 0.8929 - val_loss: 0.3132 - val_accuracy: 0.8812\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.25163\n",
      "Epoch 00014: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 59us/step\n",
      "[2020-05-18 17:21:37 RAM63.7% 0.47GB] Val Score : [0.2648600951752218, 0.887035608291626]\n",
      "[2020-05-18 17:21:37 RAM63.7% 0.47GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:21:37 RAM63.7% 0.47GB] Training on Fold : 3\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 35s - loss: 0.9176 - accuracy: 0.489 - ETA: 6s - loss: 0.9720 - accuracy: 0.659 - ETA: 2s - loss: 0.7544 - accuracy: 0.72 - ETA: 1s - loss: 0.6802 - accuracy: 0.73 - ETA: 1s - loss: 0.6415 - accuracy: 0.74 - ETA: 1s - loss: 0.5959 - accuracy: 0.75 - ETA: 0s - loss: 0.5896 - accuracy: 0.75 - ETA: 0s - loss: 0.5696 - accuracy: 0.75 - ETA: 0s - loss: 0.5485 - accuracy: 0.76 - ETA: 0s - loss: 0.5401 - accuracy: 0.76 - ETA: 0s - loss: 0.5273 - accuracy: 0.77 - ETA: 0s - loss: 0.5218 - accuracy: 0.77 - 1s 124us/step - loss: 0.5107 - accuracy: 0.7807 - val_loss: 1.3548 - val_accuracy: 0.6040\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.25163\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.4516 - accuracy: 0.78 - ETA: 0s - loss: 0.4216 - accuracy: 0.80 - ETA: 0s - loss: 0.4159 - accuracy: 0.81 - ETA: 0s - loss: 0.3980 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3906 - accuracy: 0.82 - ETA: 0s - loss: 0.3890 - accuracy: 0.82 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3856 - accuracy: 0.83 - 1s 72us/step - loss: 0.3833 - accuracy: 0.8315 - val_loss: 0.8133 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25163\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.4764 - accuracy: 0.80 - ETA: 0s - loss: 0.3837 - accuracy: 0.82 - ETA: 0s - loss: 0.3705 - accuracy: 0.83 - ETA: 0s - loss: 0.3769 - accuracy: 0.83 - ETA: 0s - loss: 0.3696 - accuracy: 0.83 - ETA: 0s - loss: 0.3671 - accuracy: 0.83 - ETA: 0s - loss: 0.3579 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3514 - accuracy: 0.84 - ETA: 0s - loss: 0.3529 - accuracy: 0.84 - ETA: 0s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3512 - accuracy: 0.84 - 1s 69us/step - loss: 0.3504 - accuracy: 0.8450 - val_loss: 0.3951 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25163\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3090 - accuracy: 0.88 - ETA: 0s - loss: 0.3218 - accuracy: 0.87 - ETA: 0s - loss: 0.3293 - accuracy: 0.86 - ETA: 0s - loss: 0.3403 - accuracy: 0.85 - ETA: 0s - loss: 0.3453 - accuracy: 0.84 - ETA: 0s - loss: 0.3396 - accuracy: 0.85 - ETA: 0s - loss: 0.3386 - accuracy: 0.85 - ETA: 0s - loss: 0.3299 - accuracy: 0.85 - ETA: 0s - loss: 0.3317 - accuracy: 0.85 - ETA: 0s - loss: 0.3313 - accuracy: 0.85 - ETA: 0s - loss: 0.3315 - accuracy: 0.85 - ETA: 0s - loss: 0.3307 - accuracy: 0.85 - ETA: 0s - loss: 0.3267 - accuracy: 0.85 - ETA: 0s - loss: 0.3268 - accuracy: 0.85 - 1s 81us/step - loss: 0.3250 - accuracy: 0.8600 - val_loss: 0.3728 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25163\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3564 - accuracy: 0.83 - ETA: 0s - loss: 0.3056 - accuracy: 0.86 - ETA: 0s - loss: 0.3132 - accuracy: 0.86 - ETA: 0s - loss: 0.3159 - accuracy: 0.86 - ETA: 0s - loss: 0.3175 - accuracy: 0.86 - ETA: 0s - loss: 0.3170 - accuracy: 0.85 - ETA: 0s - loss: 0.3176 - accuracy: 0.86 - ETA: 0s - loss: 0.3147 - accuracy: 0.86 - ETA: 0s - loss: 0.3160 - accuracy: 0.86 - ETA: 0s - loss: 0.3145 - accuracy: 0.86 - ETA: 0s - loss: 0.3079 - accuracy: 0.86 - ETA: 0s - loss: 0.3089 - accuracy: 0.86 - ETA: 0s - loss: 0.3106 - accuracy: 0.86 - ETA: 0s - loss: 0.3133 - accuracy: 0.86 - 1s 77us/step - loss: 0.3124 - accuracy: 0.8663 - val_loss: 0.2976 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25163\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2878 - accuracy: 0.86 - ETA: 0s - loss: 0.2923 - accuracy: 0.87 - ETA: 0s - loss: 0.3038 - accuracy: 0.87 - ETA: 0s - loss: 0.3025 - accuracy: 0.87 - ETA: 0s - loss: 0.2963 - accuracy: 0.87 - ETA: 0s - loss: 0.3048 - accuracy: 0.87 - ETA: 0s - loss: 0.3046 - accuracy: 0.87 - ETA: 0s - loss: 0.3028 - accuracy: 0.87 - ETA: 0s - loss: 0.3019 - accuracy: 0.87 - ETA: 0s - loss: 0.3000 - accuracy: 0.87 - ETA: 0s - loss: 0.2988 - accuracy: 0.87 - ETA: 0s - loss: 0.2999 - accuracy: 0.87 - ETA: 0s - loss: 0.2992 - accuracy: 0.87 - ETA: 0s - loss: 0.2983 - accuracy: 0.87 - ETA: 0s - loss: 0.2948 - accuracy: 0.87 - 1s 82us/step - loss: 0.2958 - accuracy: 0.8746 - val_loss: 0.3067 - val_accuracy: 0.8644\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25163\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2533 - accuracy: 0.87 - ETA: 0s - loss: 0.3017 - accuracy: 0.85 - ETA: 0s - loss: 0.2952 - accuracy: 0.86 - ETA: 0s - loss: 0.2995 - accuracy: 0.86 - ETA: 0s - loss: 0.2965 - accuracy: 0.86 - ETA: 0s - loss: 0.2815 - accuracy: 0.87 - ETA: 0s - loss: 0.2811 - accuracy: 0.87 - ETA: 0s - loss: 0.2821 - accuracy: 0.87 - ETA: 0s - loss: 0.2885 - accuracy: 0.86 - ETA: 0s - loss: 0.2856 - accuracy: 0.87 - ETA: 0s - loss: 0.2886 - accuracy: 0.87 - ETA: 0s - loss: 0.2896 - accuracy: 0.87 - ETA: 0s - loss: 0.2878 - accuracy: 0.87 - ETA: 0s - loss: 0.2888 - accuracy: 0.87 - 1s 80us/step - loss: 0.2876 - accuracy: 0.8725 - val_loss: 0.2492 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25163 to 0.24919, saving model to DenseNet_bee_1.h5\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2840 - accuracy: 0.86 - ETA: 0s - loss: 0.2843 - accuracy: 0.89 - ETA: 0s - loss: 0.2838 - accuracy: 0.88 - ETA: 0s - loss: 0.2902 - accuracy: 0.87 - ETA: 0s - loss: 0.2849 - accuracy: 0.88 - ETA: 0s - loss: 0.2853 - accuracy: 0.88 - ETA: 0s - loss: 0.2842 - accuracy: 0.88 - ETA: 0s - loss: 0.2863 - accuracy: 0.88 - ETA: 0s - loss: 0.2821 - accuracy: 0.88 - ETA: 0s - loss: 0.2785 - accuracy: 0.88 - ETA: 0s - loss: 0.2807 - accuracy: 0.88 - ETA: 0s - loss: 0.2792 - accuracy: 0.87 - ETA: 0s - loss: 0.2774 - accuracy: 0.88 - 1s 78us/step - loss: 0.2776 - accuracy: 0.8812 - val_loss: 0.2716 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24919\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2501 - accuracy: 0.91 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2677 - accuracy: 0.88 - ETA: 0s - loss: 0.2664 - accuracy: 0.88 - ETA: 0s - loss: 0.2642 - accuracy: 0.88 - ETA: 0s - loss: 0.2586 - accuracy: 0.88 - ETA: 0s - loss: 0.2609 - accuracy: 0.88 - ETA: 0s - loss: 0.2610 - accuracy: 0.88 - ETA: 0s - loss: 0.2578 - accuracy: 0.88 - ETA: 0s - loss: 0.2594 - accuracy: 0.88 - ETA: 0s - loss: 0.2636 - accuracy: 0.88 - ETA: 0s - loss: 0.2669 - accuracy: 0.88 - ETA: 0s - loss: 0.2676 - accuracy: 0.88 - ETA: 0s - loss: 0.2670 - accuracy: 0.88 - 1s 81us/step - loss: 0.2675 - accuracy: 0.8847 - val_loss: 0.2917 - val_accuracy: 0.8745\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24919\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2291 - accuracy: 0.91 - ETA: 0s - loss: 0.2511 - accuracy: 0.90 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2575 - accuracy: 0.90 - ETA: 0s - loss: 0.2651 - accuracy: 0.89 - ETA: 0s - loss: 0.2607 - accuracy: 0.89 - ETA: 0s - loss: 0.2616 - accuracy: 0.89 - ETA: 0s - loss: 0.2630 - accuracy: 0.89 - ETA: 0s - loss: 0.2644 - accuracy: 0.89 - ETA: 0s - loss: 0.2646 - accuracy: 0.89 - ETA: 0s - loss: 0.2598 - accuracy: 0.89 - ETA: 0s - loss: 0.2595 - accuracy: 0.89 - ETA: 0s - loss: 0.2629 - accuracy: 0.89 - ETA: 0s - loss: 0.2632 - accuracy: 0.89 - ETA: 0s - loss: 0.2647 - accuracy: 0.89 - 1s 82us/step - loss: 0.2644 - accuracy: 0.8905 - val_loss: 0.3659 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24919\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2280 - accuracy: 0.92 - ETA: 0s - loss: 0.2595 - accuracy: 0.88 - ETA: 0s - loss: 0.2611 - accuracy: 0.88 - ETA: 0s - loss: 0.2652 - accuracy: 0.88 - ETA: 0s - loss: 0.2608 - accuracy: 0.88 - ETA: 0s - loss: 0.2530 - accuracy: 0.88 - ETA: 0s - loss: 0.2546 - accuracy: 0.88 - ETA: 0s - loss: 0.2538 - accuracy: 0.89 - ETA: 0s - loss: 0.2529 - accuracy: 0.89 - ETA: 0s - loss: 0.2546 - accuracy: 0.89 - ETA: 0s - loss: 0.2564 - accuracy: 0.89 - ETA: 0s - loss: 0.2570 - accuracy: 0.88 - ETA: 0s - loss: 0.2568 - accuracy: 0.88 - ETA: 0s - loss: 0.2540 - accuracy: 0.89 - ETA: 0s - loss: 0.2524 - accuracy: 0.89 - ETA: 0s - loss: 0.2528 - accuracy: 0.89 - 1s 86us/step - loss: 0.2523 - accuracy: 0.8914 - val_loss: 0.3521 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24919\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.88 - ETA: 0s - loss: 0.2658 - accuracy: 0.88 - ETA: 0s - loss: 0.2772 - accuracy: 0.88 - ETA: 0s - loss: 0.2656 - accuracy: 0.88 - ETA: 0s - loss: 0.2660 - accuracy: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.88 - ETA: 0s - loss: 0.2589 - accuracy: 0.89 - ETA: 0s - loss: 0.2539 - accuracy: 0.89 - ETA: 0s - loss: 0.2503 - accuracy: 0.89 - ETA: 0s - loss: 0.2510 - accuracy: 0.89 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2500 - accuracy: 0.89 - ETA: 0s - loss: 0.2486 - accuracy: 0.89 - ETA: 0s - loss: 0.2495 - accuracy: 0.89 - ETA: 0s - loss: 0.2514 - accuracy: 0.89 - ETA: 0s - loss: 0.2522 - accuracy: 0.89 - 1s 83us/step - loss: 0.2513 - accuracy: 0.8927 - val_loss: 0.3039 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.24919\n",
      "Epoch 00012: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 61us/step\n",
      "[2020-05-18 17:21:49 RAM64.5% 0.52GB] Val Score : [0.27563616173331984, 0.8786959648132324]\n",
      "[2020-05-18 17:21:49 RAM64.5% 0.52GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:21:49 RAM64.5% 0.52GB] Training on Fold : 4\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 39s - loss: 0.7839 - accuracy: 0.662 - ETA: 5s - loss: 1.0717 - accuracy: 0.715 - ETA: 3s - loss: 0.8409 - accuracy: 0.74 - ETA: 2s - loss: 0.7404 - accuracy: 0.75 - ETA: 1s - loss: 0.7068 - accuracy: 0.75 - ETA: 1s - loss: 0.6671 - accuracy: 0.75 - ETA: 1s - loss: 0.6436 - accuracy: 0.76 - ETA: 0s - loss: 0.6191 - accuracy: 0.76 - ETA: 0s - loss: 0.6023 - accuracy: 0.77 - ETA: 0s - loss: 0.5790 - accuracy: 0.77 - ETA: 0s - loss: 0.5617 - accuracy: 0.78 - ETA: 0s - loss: 0.5548 - accuracy: 0.78 - ETA: 0s - loss: 0.5393 - accuracy: 0.78 - 1s 132us/step - loss: 0.5273 - accuracy: 0.7925 - val_loss: 1.9751 - val_accuracy: 0.3614\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.24919\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3587 - accuracy: 0.85 - ETA: 0s - loss: 0.4123 - accuracy: 0.82 - ETA: 0s - loss: 0.4154 - accuracy: 0.82 - ETA: 0s - loss: 0.4356 - accuracy: 0.81 - ETA: 0s - loss: 0.4189 - accuracy: 0.82 - ETA: 0s - loss: 0.4207 - accuracy: 0.81 - ETA: 0s - loss: 0.4087 - accuracy: 0.82 - ETA: 0s - loss: 0.4143 - accuracy: 0.82 - ETA: 0s - loss: 0.4049 - accuracy: 0.82 - ETA: 0s - loss: 0.4024 - accuracy: 0.82 - ETA: 0s - loss: 0.3991 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - 1s 73us/step - loss: 0.3951 - accuracy: 0.8269 - val_loss: 1.1393 - val_accuracy: 0.4701\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.24919\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.5379 - accuracy: 0.71 - ETA: 0s - loss: 0.3707 - accuracy: 0.82 - ETA: 0s - loss: 0.3665 - accuracy: 0.83 - ETA: 0s - loss: 0.3559 - accuracy: 0.84 - ETA: 0s - loss: 0.3491 - accuracy: 0.84 - ETA: 0s - loss: 0.3517 - accuracy: 0.84 - ETA: 0s - loss: 0.3553 - accuracy: 0.84 - ETA: 0s - loss: 0.3517 - accuracy: 0.84 - ETA: 0s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.3523 - accuracy: 0.84 - ETA: 0s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3511 - accuracy: 0.84 - ETA: 0s - loss: 0.3505 - accuracy: 0.84 - 1s 77us/step - loss: 0.3479 - accuracy: 0.8490 - val_loss: 0.4288 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.24919\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 0.84 - ETA: 0s - loss: 0.3266 - accuracy: 0.87 - ETA: 0s - loss: 0.3324 - accuracy: 0.86 - ETA: 0s - loss: 0.3218 - accuracy: 0.86 - ETA: 0s - loss: 0.3170 - accuracy: 0.86 - ETA: 0s - loss: 0.3127 - accuracy: 0.86 - ETA: 0s - loss: 0.3187 - accuracy: 0.86 - ETA: 0s - loss: 0.3227 - accuracy: 0.86 - ETA: 0s - loss: 0.3226 - accuracy: 0.86 - ETA: 0s - loss: 0.3291 - accuracy: 0.85 - ETA: 0s - loss: 0.3270 - accuracy: 0.86 - ETA: 0s - loss: 0.3261 - accuracy: 0.86 - ETA: 0s - loss: 0.3271 - accuracy: 0.86 - ETA: 0s - loss: 0.3275 - accuracy: 0.85 - ETA: 0s - loss: 0.3266 - accuracy: 0.86 - 1s 83us/step - loss: 0.3265 - accuracy: 0.8602 - val_loss: 0.3449 - val_accuracy: 0.8543\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.24919\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2477 - accuracy: 0.88 - ETA: 0s - loss: 0.2944 - accuracy: 0.87 - ETA: 0s - loss: 0.2880 - accuracy: 0.88 - ETA: 0s - loss: 0.2896 - accuracy: 0.87 - ETA: 0s - loss: 0.2969 - accuracy: 0.87 - ETA: 0s - loss: 0.3099 - accuracy: 0.86 - ETA: 0s - loss: 0.3013 - accuracy: 0.87 - ETA: 0s - loss: 0.3098 - accuracy: 0.87 - ETA: 0s - loss: 0.3114 - accuracy: 0.86 - ETA: 0s - loss: 0.3111 - accuracy: 0.86 - ETA: 0s - loss: 0.3108 - accuracy: 0.86 - ETA: 0s - loss: 0.3114 - accuracy: 0.86 - ETA: 0s - loss: 0.3113 - accuracy: 0.86 - ETA: 0s - loss: 0.3122 - accuracy: 0.86 - 1s 82us/step - loss: 0.3104 - accuracy: 0.8670 - val_loss: 0.2907 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24919\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3886 - accuracy: 0.84 - ETA: 0s - loss: 0.3268 - accuracy: 0.85 - ETA: 0s - loss: 0.3179 - accuracy: 0.85 - ETA: 0s - loss: 0.3090 - accuracy: 0.86 - ETA: 0s - loss: 0.3079 - accuracy: 0.86 - ETA: 0s - loss: 0.3058 - accuracy: 0.86 - ETA: 0s - loss: 0.3082 - accuracy: 0.86 - ETA: 0s - loss: 0.3049 - accuracy: 0.86 - ETA: 0s - loss: 0.3053 - accuracy: 0.86 - ETA: 0s - loss: 0.3020 - accuracy: 0.86 - ETA: 0s - loss: 0.2980 - accuracy: 0.87 - ETA: 0s - loss: 0.2999 - accuracy: 0.87 - ETA: 0s - loss: 0.3026 - accuracy: 0.86 - ETA: 0s - loss: 0.2995 - accuracy: 0.87 - 1s 76us/step - loss: 0.3002 - accuracy: 0.8709 - val_loss: 0.3199 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24919\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2724 - accuracy: 0.88 - ETA: 0s - loss: 0.2804 - accuracy: 0.86 - ETA: 0s - loss: 0.2800 - accuracy: 0.87 - ETA: 0s - loss: 0.2884 - accuracy: 0.87 - ETA: 0s - loss: 0.2794 - accuracy: 0.87 - ETA: 0s - loss: 0.2821 - accuracy: 0.87 - ETA: 0s - loss: 0.2854 - accuracy: 0.87 - ETA: 0s - loss: 0.2835 - accuracy: 0.87 - ETA: 0s - loss: 0.2890 - accuracy: 0.87 - ETA: 0s - loss: 0.2847 - accuracy: 0.87 - ETA: 0s - loss: 0.2836 - accuracy: 0.87 - ETA: 0s - loss: 0.2807 - accuracy: 0.87 - ETA: 0s - loss: 0.2829 - accuracy: 0.87 - ETA: 0s - loss: 0.2819 - accuracy: 0.87 - ETA: 0s - loss: 0.2827 - accuracy: 0.87 - ETA: 0s - loss: 0.2851 - accuracy: 0.87 - ETA: 0s - loss: 0.2849 - accuracy: 0.87 - 1s 94us/step - loss: 0.2846 - accuracy: 0.8768 - val_loss: 0.2872 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24919\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2524 - accuracy: 0.88 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - ETA: 0s - loss: 0.2674 - accuracy: 0.89 - ETA: 0s - loss: 0.2706 - accuracy: 0.89 - ETA: 0s - loss: 0.2710 - accuracy: 0.88 - ETA: 0s - loss: 0.2729 - accuracy: 0.88 - ETA: 0s - loss: 0.2778 - accuracy: 0.88 - ETA: 0s - loss: 0.2799 - accuracy: 0.88 - ETA: 0s - loss: 0.2777 - accuracy: 0.88 - ETA: 0s - loss: 0.2817 - accuracy: 0.88 - ETA: 0s - loss: 0.2799 - accuracy: 0.88 - ETA: 0s - loss: 0.2792 - accuracy: 0.88 - ETA: 0s - loss: 0.2799 - accuracy: 0.88 - ETA: 0s - loss: 0.2783 - accuracy: 0.88 - 1s 78us/step - loss: 0.2783 - accuracy: 0.8838 - val_loss: 0.2812 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24919\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2333 - accuracy: 0.88 - ETA: 0s - loss: 0.2691 - accuracy: 0.88 - ETA: 0s - loss: 0.2784 - accuracy: 0.87 - ETA: 0s - loss: 0.2817 - accuracy: 0.87 - ETA: 0s - loss: 0.2801 - accuracy: 0.87 - ETA: 0s - loss: 0.2812 - accuracy: 0.87 - ETA: 0s - loss: 0.2828 - accuracy: 0.87 - ETA: 0s - loss: 0.2783 - accuracy: 0.88 - ETA: 0s - loss: 0.2756 - accuracy: 0.88 - ETA: 0s - loss: 0.2781 - accuracy: 0.88 - ETA: 0s - loss: 0.2793 - accuracy: 0.88 - ETA: 0s - loss: 0.2779 - accuracy: 0.88 - ETA: 0s - loss: 0.2769 - accuracy: 0.88 - ETA: 0s - loss: 0.2744 - accuracy: 0.88 - 1s 79us/step - loss: 0.2754 - accuracy: 0.8821 - val_loss: 0.2768 - val_accuracy: 0.8736\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24919\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.91 - ETA: 0s - loss: 0.2655 - accuracy: 0.88 - ETA: 0s - loss: 0.2583 - accuracy: 0.88 - ETA: 0s - loss: 0.2471 - accuracy: 0.89 - ETA: 0s - loss: 0.2575 - accuracy: 0.88 - ETA: 0s - loss: 0.2570 - accuracy: 0.88 - ETA: 0s - loss: 0.2667 - accuracy: 0.88 - ETA: 0s - loss: 0.2641 - accuracy: 0.88 - ETA: 0s - loss: 0.2644 - accuracy: 0.88 - ETA: 0s - loss: 0.2649 - accuracy: 0.88 - ETA: 0s - loss: 0.2643 - accuracy: 0.88 - ETA: 0s - loss: 0.2650 - accuracy: 0.88 - ETA: 0s - loss: 0.2658 - accuracy: 0.88 - 1s 76us/step - loss: 0.2641 - accuracy: 0.8845 - val_loss: 0.2615 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24919\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2358 - accuracy: 0.91 - ETA: 0s - loss: 0.2951 - accuracy: 0.88 - ETA: 0s - loss: 0.2762 - accuracy: 0.89 - ETA: 0s - loss: 0.2726 - accuracy: 0.89 - ETA: 0s - loss: 0.2624 - accuracy: 0.89 - ETA: 0s - loss: 0.2647 - accuracy: 0.89 - ETA: 0s - loss: 0.2642 - accuracy: 0.88 - ETA: 0s - loss: 0.2612 - accuracy: 0.89 - ETA: 0s - loss: 0.2668 - accuracy: 0.88 - ETA: 0s - loss: 0.2659 - accuracy: 0.88 - ETA: 0s - loss: 0.2651 - accuracy: 0.88 - ETA: 0s - loss: 0.2633 - accuracy: 0.88 - 1s 72us/step - loss: 0.2619 - accuracy: 0.8870 - val_loss: 0.2852 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.24919\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2971 - accuracy: 0.88 - ETA: 0s - loss: 0.2568 - accuracy: 0.88 - ETA: 0s - loss: 0.2458 - accuracy: 0.89 - ETA: 0s - loss: 0.2462 - accuracy: 0.89 - ETA: 0s - loss: 0.2594 - accuracy: 0.89 - ETA: 0s - loss: 0.2592 - accuracy: 0.89 - ETA: 0s - loss: 0.2605 - accuracy: 0.89 - ETA: 0s - loss: 0.2588 - accuracy: 0.89 - ETA: 0s - loss: 0.2552 - accuracy: 0.89 - ETA: 0s - loss: 0.2596 - accuracy: 0.88 - ETA: 0s - loss: 0.2594 - accuracy: 0.88 - ETA: 0s - loss: 0.2586 - accuracy: 0.89 - 1s 73us/step - loss: 0.2577 - accuracy: 0.8898 - val_loss: 0.2324 - val_accuracy: 0.9023\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.24919 to 0.23241, saving model to DenseNet_bee_1.h5\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.87 - ETA: 0s - loss: 0.2652 - accuracy: 0.88 - ETA: 0s - loss: 0.2561 - accuracy: 0.88 - ETA: 0s - loss: 0.2624 - accuracy: 0.88 - ETA: 0s - loss: 0.2514 - accuracy: 0.88 - ETA: 0s - loss: 0.2531 - accuracy: 0.88 - ETA: 0s - loss: 0.2543 - accuracy: 0.88 - ETA: 0s - loss: 0.2521 - accuracy: 0.88 - ETA: 0s - loss: 0.2541 - accuracy: 0.88 - ETA: 0s - loss: 0.2518 - accuracy: 0.88 - ETA: 0s - loss: 0.2521 - accuracy: 0.88 - ETA: 0s - loss: 0.2498 - accuracy: 0.89 - ETA: 0s - loss: 0.2519 - accuracy: 0.89 - ETA: 0s - loss: 0.2510 - accuracy: 0.89 - 1s 79us/step - loss: 0.2533 - accuracy: 0.8898 - val_loss: 0.3119 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.23241\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.88 - ETA: 0s - loss: 0.2655 - accuracy: 0.88 - ETA: 0s - loss: 0.2487 - accuracy: 0.88 - ETA: 0s - loss: 0.2473 - accuracy: 0.88 - ETA: 0s - loss: 0.2484 - accuracy: 0.88 - ETA: 0s - loss: 0.2471 - accuracy: 0.88 - ETA: 0s - loss: 0.2434 - accuracy: 0.89 - ETA: 0s - loss: 0.2424 - accuracy: 0.89 - ETA: 0s - loss: 0.2413 - accuracy: 0.89 - ETA: 0s - loss: 0.2418 - accuracy: 0.89 - ETA: 0s - loss: 0.2373 - accuracy: 0.89 - ETA: 0s - loss: 0.2399 - accuracy: 0.89 - ETA: 0s - loss: 0.2430 - accuracy: 0.89 - ETA: 0s - loss: 0.2454 - accuracy: 0.89 - ETA: 0s - loss: 0.2450 - accuracy: 0.89 - ETA: 0s - loss: 0.2440 - accuracy: 0.89 - 1s 83us/step - loss: 0.2437 - accuracy: 0.8956 - val_loss: 0.2971 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.23241\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.1474 - accuracy: 0.94 - ETA: 0s - loss: 0.2533 - accuracy: 0.88 - ETA: 0s - loss: 0.2449 - accuracy: 0.89 - ETA: 0s - loss: 0.2441 - accuracy: 0.89 - ETA: 0s - loss: 0.2398 - accuracy: 0.89 - ETA: 0s - loss: 0.2413 - accuracy: 0.89 - ETA: 0s - loss: 0.2432 - accuracy: 0.89 - ETA: 0s - loss: 0.2433 - accuracy: 0.89 - ETA: 0s - loss: 0.2449 - accuracy: 0.89 - ETA: 0s - loss: 0.2424 - accuracy: 0.89 - ETA: 0s - loss: 0.2445 - accuracy: 0.89 - ETA: 0s - loss: 0.2440 - accuracy: 0.89 - ETA: 0s - loss: 0.2436 - accuracy: 0.89 - ETA: 0s - loss: 0.2443 - accuracy: 0.89 - 1s 79us/step - loss: 0.2442 - accuracy: 0.8971 - val_loss: 0.2312 - val_accuracy: 0.9023\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.23241 to 0.23120, saving model to DenseNet_bee_1.h5\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.88 - ETA: 0s - loss: 0.2526 - accuracy: 0.89 - ETA: 0s - loss: 0.2387 - accuracy: 0.89 - ETA: 0s - loss: 0.2367 - accuracy: 0.89 - ETA: 0s - loss: 0.2289 - accuracy: 0.89 - ETA: 0s - loss: 0.2330 - accuracy: 0.89 - ETA: 0s - loss: 0.2364 - accuracy: 0.89 - ETA: 0s - loss: 0.2364 - accuracy: 0.89 - ETA: 0s - loss: 0.2403 - accuracy: 0.89 - ETA: 0s - loss: 0.2397 - accuracy: 0.89 - ETA: 0s - loss: 0.2384 - accuracy: 0.89 - ETA: 0s - loss: 0.2365 - accuracy: 0.89 - ETA: 0s - loss: 0.2391 - accuracy: 0.89 - 1s 75us/step - loss: 0.2391 - accuracy: 0.8989 - val_loss: 0.2991 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.23120\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.91 - ETA: 0s - loss: 0.2099 - accuracy: 0.91 - ETA: 0s - loss: 0.2199 - accuracy: 0.91 - ETA: 0s - loss: 0.2181 - accuracy: 0.90 - ETA: 0s - loss: 0.2189 - accuracy: 0.90 - ETA: 0s - loss: 0.2235 - accuracy: 0.90 - ETA: 0s - loss: 0.2254 - accuracy: 0.90 - ETA: 0s - loss: 0.2284 - accuracy: 0.90 - ETA: 0s - loss: 0.2258 - accuracy: 0.90 - ETA: 0s - loss: 0.2282 - accuracy: 0.90 - ETA: 0s - loss: 0.2272 - accuracy: 0.90 - ETA: 0s - loss: 0.2319 - accuracy: 0.90 - ETA: 0s - loss: 0.2335 - accuracy: 0.90 - 1s 75us/step - loss: 0.2331 - accuracy: 0.9018 - val_loss: 0.2951 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.23120\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.85 - ETA: 0s - loss: 0.2244 - accuracy: 0.90 - ETA: 0s - loss: 0.2251 - accuracy: 0.90 - ETA: 0s - loss: 0.2235 - accuracy: 0.90 - ETA: 0s - loss: 0.2266 - accuracy: 0.90 - ETA: 0s - loss: 0.2311 - accuracy: 0.90 - ETA: 0s - loss: 0.2341 - accuracy: 0.90 - ETA: 0s - loss: 0.2334 - accuracy: 0.90 - ETA: 0s - loss: 0.2309 - accuracy: 0.90 - ETA: 0s - loss: 0.2323 - accuracy: 0.90 - ETA: 0s - loss: 0.2344 - accuracy: 0.90 - ETA: 0s - loss: 0.2368 - accuracy: 0.90 - 1s 71us/step - loss: 0.2367 - accuracy: 0.9012 - val_loss: 0.3258 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.23120\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.1730 - accuracy: 0.93 - ETA: 0s - loss: 0.2506 - accuracy: 0.89 - ETA: 0s - loss: 0.2387 - accuracy: 0.90 - ETA: 0s - loss: 0.2339 - accuracy: 0.90 - ETA: 0s - loss: 0.2264 - accuracy: 0.90 - ETA: 0s - loss: 0.2268 - accuracy: 0.90 - ETA: 0s - loss: 0.2328 - accuracy: 0.90 - ETA: 0s - loss: 0.2348 - accuracy: 0.90 - ETA: 0s - loss: 0.2327 - accuracy: 0.90 - ETA: 0s - loss: 0.2310 - accuracy: 0.90 - ETA: 0s - loss: 0.2301 - accuracy: 0.90 - ETA: 0s - loss: 0.2316 - accuracy: 0.90 - 1s 72us/step - loss: 0.2315 - accuracy: 0.9024 - val_loss: 0.2245 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.23120 to 0.22453, saving model to DenseNet_bee_1.h5\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2127 - accuracy: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.90 - ETA: 0s - loss: 0.2361 - accuracy: 0.90 - ETA: 0s - loss: 0.2368 - accuracy: 0.90 - ETA: 0s - loss: 0.2335 - accuracy: 0.90 - ETA: 0s - loss: 0.2386 - accuracy: 0.90 - ETA: 0s - loss: 0.2396 - accuracy: 0.90 - ETA: 0s - loss: 0.2413 - accuracy: 0.90 - ETA: 0s - loss: 0.2388 - accuracy: 0.90 - ETA: 0s - loss: 0.2407 - accuracy: 0.90 - ETA: 0s - loss: 0.2358 - accuracy: 0.90 - ETA: 0s - loss: 0.2333 - accuracy: 0.90 - ETA: 0s - loss: 0.2341 - accuracy: 0.90 - 1s 76us/step - loss: 0.2334 - accuracy: 0.9013 - val_loss: 0.2563 - val_accuracy: 0.8863\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22453\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2206 - accuracy: 0.88 - ETA: 0s - loss: 0.2464 - accuracy: 0.89 - ETA: 0s - loss: 0.2477 - accuracy: 0.89 - ETA: 0s - loss: 0.2372 - accuracy: 0.90 - ETA: 0s - loss: 0.2320 - accuracy: 0.90 - ETA: 0s - loss: 0.2266 - accuracy: 0.90 - ETA: 0s - loss: 0.2254 - accuracy: 0.90 - ETA: 0s - loss: 0.2266 - accuracy: 0.90 - ETA: 0s - loss: 0.2331 - accuracy: 0.90 - ETA: 0s - loss: 0.2332 - accuracy: 0.90 - ETA: 0s - loss: 0.2321 - accuracy: 0.90 - ETA: 0s - loss: 0.2301 - accuracy: 0.90 - 1s 72us/step - loss: 0.2316 - accuracy: 0.9045 - val_loss: 0.2340 - val_accuracy: 0.9006\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.22453\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2125 - accuracy: 0.91 - ETA: 0s - loss: 0.1958 - accuracy: 0.91 - ETA: 0s - loss: 0.1936 - accuracy: 0.91 - ETA: 0s - loss: 0.2118 - accuracy: 0.91 - ETA: 0s - loss: 0.2108 - accuracy: 0.91 - ETA: 0s - loss: 0.2140 - accuracy: 0.90 - ETA: 0s - loss: 0.2178 - accuracy: 0.90 - ETA: 0s - loss: 0.2247 - accuracy: 0.90 - ETA: 0s - loss: 0.2231 - accuracy: 0.90 - ETA: 0s - loss: 0.2274 - accuracy: 0.90 - ETA: 0s - loss: 0.2252 - accuracy: 0.90 - ETA: 0s - loss: 0.2247 - accuracy: 0.90 - 1s 72us/step - loss: 0.2247 - accuracy: 0.9057 - val_loss: 0.2192 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.22453 to 0.21917, saving model to DenseNet_bee_1.h5\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.1868 - accuracy: 0.92 - ETA: 0s - loss: 0.2126 - accuracy: 0.90 - ETA: 0s - loss: 0.2263 - accuracy: 0.90 - ETA: 0s - loss: 0.2355 - accuracy: 0.89 - ETA: 0s - loss: 0.2270 - accuracy: 0.90 - ETA: 0s - loss: 0.2271 - accuracy: 0.90 - ETA: 0s - loss: 0.2252 - accuracy: 0.90 - ETA: 0s - loss: 0.2218 - accuracy: 0.90 - ETA: 0s - loss: 0.2210 - accuracy: 0.90 - ETA: 0s - loss: 0.2220 - accuracy: 0.90 - ETA: 0s - loss: 0.2241 - accuracy: 0.90 - ETA: 0s - loss: 0.2239 - accuracy: 0.90 - 1s 70us/step - loss: 0.2228 - accuracy: 0.9054 - val_loss: 0.2242 - val_accuracy: 0.9065\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.21917\n",
      "Epoch 24/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.90 - ETA: 0s - loss: 0.2062 - accuracy: 0.91 - ETA: 0s - loss: 0.2081 - accuracy: 0.90 - ETA: 0s - loss: 0.2189 - accuracy: 0.90 - ETA: 0s - loss: 0.2202 - accuracy: 0.90 - ETA: 0s - loss: 0.2184 - accuracy: 0.90 - ETA: 0s - loss: 0.2172 - accuracy: 0.90 - ETA: 0s - loss: 0.2189 - accuracy: 0.90 - ETA: 0s - loss: 0.2196 - accuracy: 0.90 - ETA: 0s - loss: 0.2211 - accuracy: 0.90 - ETA: 0s - loss: 0.2218 - accuracy: 0.90 - ETA: 0s - loss: 0.2186 - accuracy: 0.90 - 1s 72us/step - loss: 0.2193 - accuracy: 0.9051 - val_loss: 0.3635 - val_accuracy: 0.8492\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.21917\n",
      "Epoch 25/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.1943 - accuracy: 0.89 - ETA: 0s - loss: 0.2028 - accuracy: 0.90 - ETA: 0s - loss: 0.2033 - accuracy: 0.90 - ETA: 0s - loss: 0.2090 - accuracy: 0.90 - ETA: 0s - loss: 0.2145 - accuracy: 0.90 - ETA: 0s - loss: 0.2154 - accuracy: 0.90 - ETA: 0s - loss: 0.2166 - accuracy: 0.90 - ETA: 0s - loss: 0.2155 - accuracy: 0.90 - ETA: 0s - loss: 0.2167 - accuracy: 0.90 - ETA: 0s - loss: 0.2171 - accuracy: 0.90 - ETA: 0s - loss: 0.2172 - accuracy: 0.90 - ETA: 0s - loss: 0.2182 - accuracy: 0.90 - ETA: 0s - loss: 0.2186 - accuracy: 0.90 - ETA: 0s - loss: 0.2165 - accuracy: 0.90 - 1s 77us/step - loss: 0.2180 - accuracy: 0.9057 - val_loss: 0.2520 - val_accuracy: 0.8939\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.21917\n",
      "Epoch 26/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2135 - accuracy: 0.91 - ETA: 0s - loss: 0.2034 - accuracy: 0.91 - ETA: 0s - loss: 0.2077 - accuracy: 0.91 - ETA: 0s - loss: 0.2075 - accuracy: 0.91 - ETA: 0s - loss: 0.2124 - accuracy: 0.90 - ETA: 0s - loss: 0.2096 - accuracy: 0.91 - ETA: 0s - loss: 0.2117 - accuracy: 0.91 - ETA: 0s - loss: 0.2095 - accuracy: 0.91 - ETA: 0s - loss: 0.2090 - accuracy: 0.91 - ETA: 0s - loss: 0.2119 - accuracy: 0.91 - ETA: 0s - loss: 0.2128 - accuracy: 0.90 - ETA: 0s - loss: 0.2135 - accuracy: 0.90 - ETA: 0s - loss: 0.2135 - accuracy: 0.90 - ETA: 0s - loss: 0.2108 - accuracy: 0.91 - 1s 81us/step - loss: 0.2124 - accuracy: 0.9101 - val_loss: 0.2461 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.21917\n",
      "Epoch 27/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1893 - accuracy: 0.91 - ETA: 0s - loss: 0.2323 - accuracy: 0.90 - ETA: 0s - loss: 0.2131 - accuracy: 0.91 - ETA: 0s - loss: 0.2126 - accuracy: 0.91 - ETA: 0s - loss: 0.2193 - accuracy: 0.91 - ETA: 0s - loss: 0.2221 - accuracy: 0.90 - ETA: 0s - loss: 0.2220 - accuracy: 0.90 - ETA: 0s - loss: 0.2213 - accuracy: 0.90 - ETA: 0s - loss: 0.2208 - accuracy: 0.90 - ETA: 0s - loss: 0.2198 - accuracy: 0.90 - ETA: 0s - loss: 0.2198 - accuracy: 0.90 - ETA: 0s - loss: 0.2206 - accuracy: 0.90 - ETA: 0s - loss: 0.2183 - accuracy: 0.90 - ETA: 0s - loss: 0.2166 - accuracy: 0.90 - 1s 77us/step - loss: 0.2167 - accuracy: 0.9089 - val_loss: 0.2888 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.21917\n",
      "Epoch 00027: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 64us/step\n",
      "[2020-05-18 17:22:13 RAM64.5% 0.57GB] Val Score : [0.294743113914523, 0.8695982098579407]\n",
      "[2020-05-18 17:22:13 RAM64.5% 0.57GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:22:13 RAM64.5% 0.57GB] Training on Fold : 5\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 41s - loss: 1.2364 - accuracy: 0.413 - ETA: 10s - loss: 1.3537 - accuracy: 0.608 - ETA: 5s - loss: 0.9773 - accuracy: 0.686 - ETA: 3s - loss: 0.7980 - accuracy: 0.72 - ETA: 2s - loss: 0.7072 - accuracy: 0.74 - ETA: 1s - loss: 0.6757 - accuracy: 0.75 - ETA: 1s - loss: 0.6555 - accuracy: 0.75 - ETA: 1s - loss: 0.6299 - accuracy: 0.75 - ETA: 1s - loss: 0.6035 - accuracy: 0.76 - ETA: 0s - loss: 0.5885 - accuracy: 0.76 - ETA: 0s - loss: 0.5700 - accuracy: 0.77 - ETA: 0s - loss: 0.5552 - accuracy: 0.77 - ETA: 0s - loss: 0.5456 - accuracy: 0.78 - ETA: 0s - loss: 0.5325 - accuracy: 0.78 - ETA: 0s - loss: 0.5211 - accuracy: 0.78 - 2s 147us/step - loss: 0.5167 - accuracy: 0.7904 - val_loss: 1.8387 - val_accuracy: 0.5796\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.21917\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3433 - accuracy: 0.83 - ETA: 0s - loss: 0.3707 - accuracy: 0.84 - ETA: 0s - loss: 0.3917 - accuracy: 0.83 - ETA: 0s - loss: 0.3892 - accuracy: 0.83 - ETA: 0s - loss: 0.3862 - accuracy: 0.83 - ETA: 0s - loss: 0.3965 - accuracy: 0.83 - ETA: 0s - loss: 0.3877 - accuracy: 0.83 - ETA: 0s - loss: 0.3942 - accuracy: 0.83 - ETA: 0s - loss: 0.3991 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.83 - ETA: 0s - loss: 0.3886 - accuracy: 0.83 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3918 - accuracy: 0.82 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - 1s 82us/step - loss: 0.3884 - accuracy: 0.8280 - val_loss: 0.9189 - val_accuracy: 0.6251\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.21917\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.81 - ETA: 0s - loss: 0.4096 - accuracy: 0.81 - ETA: 0s - loss: 0.3849 - accuracy: 0.83 - ETA: 0s - loss: 0.3690 - accuracy: 0.83 - ETA: 0s - loss: 0.3662 - accuracy: 0.83 - ETA: 0s - loss: 0.3661 - accuracy: 0.83 - ETA: 0s - loss: 0.3704 - accuracy: 0.83 - ETA: 0s - loss: 0.3717 - accuracy: 0.83 - ETA: 0s - loss: 0.3684 - accuracy: 0.84 - ETA: 0s - loss: 0.3689 - accuracy: 0.83 - ETA: 0s - loss: 0.3655 - accuracy: 0.84 - ETA: 0s - loss: 0.3632 - accuracy: 0.84 - ETA: 0s - loss: 0.3601 - accuracy: 0.84 - ETA: 0s - loss: 0.3590 - accuracy: 0.84 - ETA: 0s - loss: 0.3618 - accuracy: 0.84 - ETA: 0s - loss: 0.3583 - accuracy: 0.84 - ETA: 0s - loss: 0.3585 - accuracy: 0.84 - ETA: 0s - loss: 0.3583 - accuracy: 0.84 - 1s 94us/step - loss: 0.3565 - accuracy: 0.8449 - val_loss: 0.5260 - val_accuracy: 0.7767\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21917\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.4134 - accuracy: 0.79 - ETA: 0s - loss: 0.3544 - accuracy: 0.83 - ETA: 0s - loss: 0.3496 - accuracy: 0.84 - ETA: 0s - loss: 0.3312 - accuracy: 0.85 - ETA: 0s - loss: 0.3306 - accuracy: 0.85 - ETA: 0s - loss: 0.3274 - accuracy: 0.85 - ETA: 0s - loss: 0.3309 - accuracy: 0.85 - ETA: 0s - loss: 0.3337 - accuracy: 0.85 - ETA: 0s - loss: 0.3287 - accuracy: 0.85 - ETA: 0s - loss: 0.3295 - accuracy: 0.85 - ETA: 0s - loss: 0.3317 - accuracy: 0.85 - ETA: 0s - loss: 0.3294 - accuracy: 0.85 - ETA: 0s - loss: 0.3265 - accuracy: 0.85 - ETA: 0s - loss: 0.3268 - accuracy: 0.85 - ETA: 0s - loss: 0.3269 - accuracy: 0.85 - 1s 79us/step - loss: 0.3258 - accuracy: 0.8552 - val_loss: 0.3569 - val_accuracy: 0.8382\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21917\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.87 - ETA: 0s - loss: 0.2973 - accuracy: 0.87 - ETA: 0s - loss: 0.3013 - accuracy: 0.87 - ETA: 0s - loss: 0.3154 - accuracy: 0.86 - ETA: 0s - loss: 0.3168 - accuracy: 0.86 - ETA: 0s - loss: 0.3123 - accuracy: 0.86 - ETA: 0s - loss: 0.3129 - accuracy: 0.86 - ETA: 0s - loss: 0.3137 - accuracy: 0.86 - ETA: 0s - loss: 0.3156 - accuracy: 0.86 - ETA: 0s - loss: 0.3128 - accuracy: 0.86 - ETA: 0s - loss: 0.3095 - accuracy: 0.86 - ETA: 0s - loss: 0.3100 - accuracy: 0.86 - ETA: 0s - loss: 0.3109 - accuracy: 0.86 - 1s 74us/step - loss: 0.3124 - accuracy: 0.8644 - val_loss: 0.3435 - val_accuracy: 0.8509\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21917\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2760 - accuracy: 0.87 - ETA: 0s - loss: 0.2904 - accuracy: 0.88 - ETA: 0s - loss: 0.2964 - accuracy: 0.87 - ETA: 0s - loss: 0.2881 - accuracy: 0.87 - ETA: 0s - loss: 0.2934 - accuracy: 0.87 - ETA: 0s - loss: 0.2916 - accuracy: 0.87 - ETA: 0s - loss: 0.2933 - accuracy: 0.87 - ETA: 0s - loss: 0.2887 - accuracy: 0.87 - ETA: 0s - loss: 0.2959 - accuracy: 0.86 - ETA: 0s - loss: 0.3012 - accuracy: 0.86 - ETA: 0s - loss: 0.3007 - accuracy: 0.86 - ETA: 0s - loss: 0.3016 - accuracy: 0.86 - ETA: 0s - loss: 0.3035 - accuracy: 0.86 - ETA: 0s - loss: 0.3024 - accuracy: 0.86 - ETA: 0s - loss: 0.3015 - accuracy: 0.86 - 1s 79us/step - loss: 0.3004 - accuracy: 0.8671 - val_loss: 0.3699 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21917\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3047 - accuracy: 0.84 - ETA: 0s - loss: 0.2800 - accuracy: 0.87 - ETA: 0s - loss: 0.2847 - accuracy: 0.87 - ETA: 0s - loss: 0.2758 - accuracy: 0.88 - ETA: 0s - loss: 0.2825 - accuracy: 0.88 - ETA: 0s - loss: 0.2856 - accuracy: 0.88 - ETA: 0s - loss: 0.2878 - accuracy: 0.88 - ETA: 0s - loss: 0.2835 - accuracy: 0.88 - ETA: 0s - loss: 0.2891 - accuracy: 0.88 - ETA: 0s - loss: 0.2860 - accuracy: 0.88 - ETA: 0s - loss: 0.2858 - accuracy: 0.88 - ETA: 0s - loss: 0.2864 - accuracy: 0.87 - ETA: 0s - loss: 0.2870 - accuracy: 0.88 - ETA: 0s - loss: 0.2896 - accuracy: 0.87 - ETA: 0s - loss: 0.2895 - accuracy: 0.87 - 1s 79us/step - loss: 0.2887 - accuracy: 0.8782 - val_loss: 0.3032 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21917\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2164 - accuracy: 0.89 - ETA: 0s - loss: 0.2843 - accuracy: 0.87 - ETA: 0s - loss: 0.2921 - accuracy: 0.87 - ETA: 0s - loss: 0.2949 - accuracy: 0.87 - ETA: 0s - loss: 0.2923 - accuracy: 0.87 - ETA: 0s - loss: 0.2891 - accuracy: 0.87 - ETA: 0s - loss: 0.2951 - accuracy: 0.87 - ETA: 0s - loss: 0.2938 - accuracy: 0.87 - ETA: 0s - loss: 0.2922 - accuracy: 0.87 - ETA: 0s - loss: 0.2859 - accuracy: 0.87 - ETA: 0s - loss: 0.2837 - accuracy: 0.87 - ETA: 0s - loss: 0.2855 - accuracy: 0.87 - ETA: 0s - loss: 0.2829 - accuracy: 0.87 - ETA: 0s - loss: 0.2831 - accuracy: 0.87 - ETA: 0s - loss: 0.2805 - accuracy: 0.87 - 1s 82us/step - loss: 0.2807 - accuracy: 0.8767 - val_loss: 0.5038 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21917\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3163 - accuracy: 0.84 - ETA: 0s - loss: 0.2770 - accuracy: 0.87 - ETA: 0s - loss: 0.2868 - accuracy: 0.87 - ETA: 0s - loss: 0.2874 - accuracy: 0.87 - ETA: 0s - loss: 0.2741 - accuracy: 0.87 - ETA: 0s - loss: 0.2738 - accuracy: 0.88 - ETA: 0s - loss: 0.2723 - accuracy: 0.88 - ETA: 0s - loss: 0.2679 - accuracy: 0.88 - ETA: 0s - loss: 0.2706 - accuracy: 0.88 - ETA: 0s - loss: 0.2723 - accuracy: 0.88 - ETA: 0s - loss: 0.2720 - accuracy: 0.88 - ETA: 0s - loss: 0.2712 - accuracy: 0.88 - ETA: 0s - loss: 0.2742 - accuracy: 0.88 - ETA: 0s - loss: 0.2725 - accuracy: 0.88 - ETA: 0s - loss: 0.2739 - accuracy: 0.88 - ETA: 0s - loss: 0.2730 - accuracy: 0.88 - ETA: 0s - loss: 0.2738 - accuracy: 0.88 - 1s 92us/step - loss: 0.2740 - accuracy: 0.8836 - val_loss: 0.3204 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21917\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.84 - ETA: 0s - loss: 0.2648 - accuracy: 0.88 - ETA: 0s - loss: 0.2900 - accuracy: 0.87 - ETA: 0s - loss: 0.2783 - accuracy: 0.88 - ETA: 0s - loss: 0.2762 - accuracy: 0.88 - ETA: 0s - loss: 0.2661 - accuracy: 0.88 - ETA: 0s - loss: 0.2569 - accuracy: 0.89 - ETA: 0s - loss: 0.2546 - accuracy: 0.89 - ETA: 0s - loss: 0.2516 - accuracy: 0.89 - ETA: 0s - loss: 0.2526 - accuracy: 0.89 - ETA: 0s - loss: 0.2543 - accuracy: 0.89 - ETA: 0s - loss: 0.2531 - accuracy: 0.89 - ETA: 0s - loss: 0.2537 - accuracy: 0.89 - ETA: 0s - loss: 0.2561 - accuracy: 0.89 - ETA: 0s - loss: 0.2588 - accuracy: 0.88 - ETA: 0s - loss: 0.2596 - accuracy: 0.88 - 1s 87us/step - loss: 0.2609 - accuracy: 0.8884 - val_loss: 0.2904 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21917\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.90 - ETA: 0s - loss: 0.2160 - accuracy: 0.89 - ETA: 0s - loss: 0.2293 - accuracy: 0.89 - ETA: 0s - loss: 0.2412 - accuracy: 0.89 - ETA: 0s - loss: 0.2541 - accuracy: 0.89 - ETA: 0s - loss: 0.2554 - accuracy: 0.88 - ETA: 0s - loss: 0.2607 - accuracy: 0.88 - ETA: 0s - loss: 0.2643 - accuracy: 0.88 - ETA: 0s - loss: 0.2653 - accuracy: 0.88 - ETA: 0s - loss: 0.2632 - accuracy: 0.88 - ETA: 0s - loss: 0.2613 - accuracy: 0.88 - ETA: 0s - loss: 0.2614 - accuracy: 0.88 - ETA: 0s - loss: 0.2647 - accuracy: 0.88 - ETA: 0s - loss: 0.2634 - accuracy: 0.88 - ETA: 0s - loss: 0.2630 - accuracy: 0.88 - ETA: 0s - loss: 0.2626 - accuracy: 0.88 - 1s 83us/step - loss: 0.2629 - accuracy: 0.8874 - val_loss: 0.3030 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21917\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.86 - ETA: 0s - loss: 0.2997 - accuracy: 0.86 - ETA: 0s - loss: 0.2692 - accuracy: 0.88 - ETA: 0s - loss: 0.2579 - accuracy: 0.88 - ETA: 0s - loss: 0.2511 - accuracy: 0.89 - ETA: 0s - loss: 0.2483 - accuracy: 0.89 - ETA: 0s - loss: 0.2450 - accuracy: 0.89 - ETA: 0s - loss: 0.2447 - accuracy: 0.89 - ETA: 0s - loss: 0.2438 - accuracy: 0.89 - ETA: 0s - loss: 0.2441 - accuracy: 0.89 - ETA: 0s - loss: 0.2485 - accuracy: 0.89 - ETA: 0s - loss: 0.2518 - accuracy: 0.89 - ETA: 0s - loss: 0.2546 - accuracy: 0.89 - ETA: 0s - loss: 0.2539 - accuracy: 0.89 - ETA: 0s - loss: 0.2560 - accuracy: 0.89 - 1s 81us/step - loss: 0.2567 - accuracy: 0.8897 - val_loss: 0.3039 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21917\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.89 - ETA: 0s - loss: 0.2567 - accuracy: 0.89 - ETA: 0s - loss: 0.2422 - accuracy: 0.90 - ETA: 0s - loss: 0.2403 - accuracy: 0.90 - ETA: 0s - loss: 0.2399 - accuracy: 0.90 - ETA: 0s - loss: 0.2381 - accuracy: 0.90 - ETA: 0s - loss: 0.2451 - accuracy: 0.89 - ETA: 0s - loss: 0.2484 - accuracy: 0.89 - ETA: 0s - loss: 0.2467 - accuracy: 0.89 - ETA: 0s - loss: 0.2477 - accuracy: 0.89 - ETA: 0s - loss: 0.2503 - accuracy: 0.89 - ETA: 0s - loss: 0.2534 - accuracy: 0.89 - ETA: 0s - loss: 0.2516 - accuracy: 0.89 - ETA: 0s - loss: 0.2535 - accuracy: 0.89 - 1s 75us/step - loss: 0.2522 - accuracy: 0.8917 - val_loss: 0.2850 - val_accuracy: 0.8964\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.21917\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.86 - ETA: 0s - loss: 0.2813 - accuracy: 0.88 - ETA: 0s - loss: 0.2600 - accuracy: 0.89 - ETA: 0s - loss: 0.2573 - accuracy: 0.89 - ETA: 0s - loss: 0.2556 - accuracy: 0.89 - ETA: 0s - loss: 0.2478 - accuracy: 0.89 - ETA: 0s - loss: 0.2497 - accuracy: 0.89 - ETA: 0s - loss: 0.2502 - accuracy: 0.89 - ETA: 0s - loss: 0.2541 - accuracy: 0.89 - ETA: 0s - loss: 0.2509 - accuracy: 0.89 - ETA: 0s - loss: 0.2513 - accuracy: 0.89 - ETA: 0s - loss: 0.2502 - accuracy: 0.89 - ETA: 0s - loss: 0.2539 - accuracy: 0.89 - ETA: 0s - loss: 0.2531 - accuracy: 0.89 - ETA: 0s - loss: 0.2512 - accuracy: 0.89 - 1s 78us/step - loss: 0.2500 - accuracy: 0.8931 - val_loss: 0.4486 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.21917\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.87 - ETA: 0s - loss: 0.2443 - accuracy: 0.90 - ETA: 0s - loss: 0.2483 - accuracy: 0.89 - ETA: 0s - loss: 0.2524 - accuracy: 0.89 - ETA: 0s - loss: 0.2494 - accuracy: 0.89 - ETA: 0s - loss: 0.2467 - accuracy: 0.89 - ETA: 0s - loss: 0.2470 - accuracy: 0.89 - ETA: 0s - loss: 0.2431 - accuracy: 0.89 - ETA: 0s - loss: 0.2432 - accuracy: 0.89 - ETA: 0s - loss: 0.2430 - accuracy: 0.90 - ETA: 0s - loss: 0.2411 - accuracy: 0.89 - ETA: 0s - loss: 0.2404 - accuracy: 0.89 - ETA: 0s - loss: 0.2433 - accuracy: 0.89 - ETA: 0s - loss: 0.2438 - accuracy: 0.89 - ETA: 0s - loss: 0.2427 - accuracy: 0.89 - 1s 80us/step - loss: 0.2439 - accuracy: 0.8964 - val_loss: 0.4237 - val_accuracy: 0.8644\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21917\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.91 - ETA: 0s - loss: 0.2427 - accuracy: 0.88 - ETA: 0s - loss: 0.2450 - accuracy: 0.89 - ETA: 0s - loss: 0.2465 - accuracy: 0.89 - ETA: 0s - loss: 0.2412 - accuracy: 0.89 - ETA: 0s - loss: 0.2420 - accuracy: 0.89 - ETA: 0s - loss: 0.2405 - accuracy: 0.89 - ETA: 0s - loss: 0.2399 - accuracy: 0.89 - ETA: 0s - loss: 0.2370 - accuracy: 0.89 - ETA: 0s - loss: 0.2387 - accuracy: 0.89 - ETA: 0s - loss: 0.2391 - accuracy: 0.89 - ETA: 0s - loss: 0.2391 - accuracy: 0.89 - ETA: 0s - loss: 0.2385 - accuracy: 0.89 - ETA: 0s - loss: 0.2389 - accuracy: 0.89 - ETA: 0s - loss: 0.2409 - accuracy: 0.89 - 1s 79us/step - loss: 0.2408 - accuracy: 0.8975 - val_loss: 0.2876 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.21917\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.92 - ETA: 0s - loss: 0.2566 - accuracy: 0.89 - ETA: 0s - loss: 0.2576 - accuracy: 0.88 - ETA: 0s - loss: 0.2395 - accuracy: 0.89 - ETA: 0s - loss: 0.2412 - accuracy: 0.89 - ETA: 0s - loss: 0.2369 - accuracy: 0.89 - ETA: 0s - loss: 0.2362 - accuracy: 0.89 - ETA: 0s - loss: 0.2347 - accuracy: 0.89 - ETA: 0s - loss: 0.2337 - accuracy: 0.90 - ETA: 0s - loss: 0.2379 - accuracy: 0.89 - ETA: 0s - loss: 0.2380 - accuracy: 0.89 - ETA: 0s - loss: 0.2372 - accuracy: 0.89 - 1s 72us/step - loss: 0.2394 - accuracy: 0.8975 - val_loss: 0.4759 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21917\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.87 - ETA: 0s - loss: 0.2574 - accuracy: 0.88 - ETA: 0s - loss: 0.2488 - accuracy: 0.89 - ETA: 0s - loss: 0.2461 - accuracy: 0.89 - ETA: 0s - loss: 0.2376 - accuracy: 0.90 - ETA: 0s - loss: 0.2378 - accuracy: 0.90 - ETA: 0s - loss: 0.2277 - accuracy: 0.90 - ETA: 0s - loss: 0.2299 - accuracy: 0.90 - ETA: 0s - loss: 0.2305 - accuracy: 0.90 - ETA: 0s - loss: 0.2306 - accuracy: 0.90 - ETA: 0s - loss: 0.2326 - accuracy: 0.90 - ETA: 0s - loss: 0.2321 - accuracy: 0.90 - ETA: 0s - loss: 0.2331 - accuracy: 0.90 - ETA: 0s - loss: 0.2339 - accuracy: 0.90 - ETA: 0s - loss: 0.2339 - accuracy: 0.90 - 1s 80us/step - loss: 0.2341 - accuracy: 0.9031 - val_loss: 0.4015 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21917\n",
      "Epoch 00018: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 66us/step\n",
      "[2020-05-18 17:22:31 RAM65.3% 0.62GB] Val Score : [0.27103341796871067, 0.8794541358947754]\n",
      "[2020-05-18 17:22:31 RAM65.3% 0.62GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:22:31 RAM65.3% 0.62GB] Training on Fold : 6\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 44s - loss: 1.0823 - accuracy: 0.544 - ETA: 7s - loss: 0.9444 - accuracy: 0.698 - ETA: 3s - loss: 0.7695 - accuracy: 0.72 - ETA: 2s - loss: 0.6643 - accuracy: 0.75 - ETA: 1s - loss: 0.6046 - accuracy: 0.77 - ETA: 1s - loss: 0.5828 - accuracy: 0.77 - ETA: 0s - loss: 0.5562 - accuracy: 0.78 - ETA: 0s - loss: 0.5313 - accuracy: 0.79 - ETA: 0s - loss: 0.5163 - accuracy: 0.79 - ETA: 0s - loss: 0.5052 - accuracy: 0.79 - ETA: 0s - loss: 0.4992 - accuracy: 0.80 - ETA: 0s - loss: 0.4921 - accuracy: 0.80 - ETA: 0s - loss: 0.4883 - accuracy: 0.80 - 1s 137us/step - loss: 0.4888 - accuracy: 0.8020 - val_loss: 1.0377 - val_accuracy: 0.5670\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.21917\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3083 - accuracy: 0.86 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - ETA: 0s - loss: 0.4077 - accuracy: 0.82 - ETA: 0s - loss: 0.4064 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.83 - ETA: 0s - loss: 0.3866 - accuracy: 0.83 - ETA: 0s - loss: 0.3856 - accuracy: 0.83 - ETA: 0s - loss: 0.3832 - accuracy: 0.83 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3854 - accuracy: 0.82 - ETA: 0s - loss: 0.3812 - accuracy: 0.83 - ETA: 0s - loss: 0.3808 - accuracy: 0.83 - ETA: 0s - loss: 0.3800 - accuracy: 0.83 - ETA: 0s - loss: 0.3796 - accuracy: 0.83 - 1s 83us/step - loss: 0.3775 - accuracy: 0.8336 - val_loss: 0.7532 - val_accuracy: 0.6546\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.21917\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3125 - accuracy: 0.86 - ETA: 0s - loss: 0.3858 - accuracy: 0.82 - ETA: 0s - loss: 0.3764 - accuracy: 0.82 - ETA: 0s - loss: 0.3628 - accuracy: 0.83 - ETA: 0s - loss: 0.3596 - accuracy: 0.84 - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 0s - loss: 0.3570 - accuracy: 0.84 - ETA: 0s - loss: 0.3621 - accuracy: 0.84 - ETA: 0s - loss: 0.3593 - accuracy: 0.84 - ETA: 0s - loss: 0.3616 - accuracy: 0.84 - ETA: 0s - loss: 0.3611 - accuracy: 0.84 - ETA: 0s - loss: 0.3597 - accuracy: 0.84 - ETA: 0s - loss: 0.3549 - accuracy: 0.84 - ETA: 0s - loss: 0.3553 - accuracy: 0.84 - 1s 82us/step - loss: 0.3545 - accuracy: 0.8444 - val_loss: 0.5221 - val_accuracy: 0.7557\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21917\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3216 - accuracy: 0.86 - ETA: 0s - loss: 0.3400 - accuracy: 0.84 - ETA: 0s - loss: 0.3383 - accuracy: 0.84 - ETA: 0s - loss: 0.3318 - accuracy: 0.85 - ETA: 0s - loss: 0.3308 - accuracy: 0.85 - ETA: 0s - loss: 0.3298 - accuracy: 0.85 - ETA: 0s - loss: 0.3288 - accuracy: 0.85 - ETA: 0s - loss: 0.3343 - accuracy: 0.85 - ETA: 0s - loss: 0.3304 - accuracy: 0.85 - ETA: 0s - loss: 0.3343 - accuracy: 0.85 - ETA: 0s - loss: 0.3340 - accuracy: 0.85 - ETA: 0s - loss: 0.3333 - accuracy: 0.85 - ETA: 0s - loss: 0.3322 - accuracy: 0.85 - ETA: 0s - loss: 0.3282 - accuracy: 0.85 - 1s 82us/step - loss: 0.3272 - accuracy: 0.8587 - val_loss: 0.3938 - val_accuracy: 0.8071\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21917\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.85 - ETA: 0s - loss: 0.3080 - accuracy: 0.86 - ETA: 0s - loss: 0.3098 - accuracy: 0.85 - ETA: 0s - loss: 0.3000 - accuracy: 0.86 - ETA: 0s - loss: 0.3114 - accuracy: 0.86 - ETA: 0s - loss: 0.3104 - accuracy: 0.86 - ETA: 0s - loss: 0.3091 - accuracy: 0.86 - ETA: 0s - loss: 0.3124 - accuracy: 0.86 - ETA: 0s - loss: 0.3132 - accuracy: 0.86 - ETA: 0s - loss: 0.3180 - accuracy: 0.86 - ETA: 0s - loss: 0.3156 - accuracy: 0.86 - ETA: 0s - loss: 0.3097 - accuracy: 0.86 - ETA: 0s - loss: 0.3080 - accuracy: 0.86 - ETA: 0s - loss: 0.3085 - accuracy: 0.86 - ETA: 0s - loss: 0.3107 - accuracy: 0.86 - ETA: 0s - loss: 0.3122 - accuracy: 0.86 - ETA: 0s - loss: 0.3108 - accuracy: 0.86 - 1s 90us/step - loss: 0.3110 - accuracy: 0.8648 - val_loss: 0.2835 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21917\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.88 - ETA: 0s - loss: 0.2734 - accuracy: 0.88 - ETA: 0s - loss: 0.2981 - accuracy: 0.87 - ETA: 0s - loss: 0.3033 - accuracy: 0.87 - ETA: 0s - loss: 0.2993 - accuracy: 0.87 - ETA: 0s - loss: 0.2976 - accuracy: 0.87 - ETA: 0s - loss: 0.2988 - accuracy: 0.87 - ETA: 0s - loss: 0.2972 - accuracy: 0.87 - ETA: 0s - loss: 0.2966 - accuracy: 0.87 - ETA: 0s - loss: 0.2974 - accuracy: 0.87 - ETA: 0s - loss: 0.2938 - accuracy: 0.87 - ETA: 0s - loss: 0.2964 - accuracy: 0.87 - ETA: 0s - loss: 0.2940 - accuracy: 0.87 - ETA: 0s - loss: 0.2955 - accuracy: 0.87 - ETA: 0s - loss: 0.2971 - accuracy: 0.87 - 1s 79us/step - loss: 0.2944 - accuracy: 0.8726 - val_loss: 0.2876 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21917\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2721 - accuracy: 0.86 - ETA: 0s - loss: 0.2858 - accuracy: 0.88 - ETA: 0s - loss: 0.2915 - accuracy: 0.87 - ETA: 0s - loss: 0.3043 - accuracy: 0.87 - ETA: 0s - loss: 0.2975 - accuracy: 0.87 - ETA: 0s - loss: 0.3036 - accuracy: 0.86 - ETA: 0s - loss: 0.2998 - accuracy: 0.87 - ETA: 0s - loss: 0.2954 - accuracy: 0.87 - ETA: 0s - loss: 0.2935 - accuracy: 0.87 - ETA: 0s - loss: 0.2908 - accuracy: 0.87 - ETA: 0s - loss: 0.2882 - accuracy: 0.87 - ETA: 0s - loss: 0.2867 - accuracy: 0.87 - ETA: 0s - loss: 0.2879 - accuracy: 0.87 - ETA: 0s - loss: 0.2878 - accuracy: 0.87 - ETA: 0s - loss: 0.2867 - accuracy: 0.87 - 1s 75us/step - loss: 0.2862 - accuracy: 0.8775 - val_loss: 0.2834 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21917\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.86 - ETA: 0s - loss: 0.2685 - accuracy: 0.89 - ETA: 0s - loss: 0.2648 - accuracy: 0.89 - ETA: 0s - loss: 0.2769 - accuracy: 0.88 - ETA: 0s - loss: 0.2805 - accuracy: 0.88 - ETA: 0s - loss: 0.2783 - accuracy: 0.88 - ETA: 0s - loss: 0.2853 - accuracy: 0.88 - ETA: 0s - loss: 0.2832 - accuracy: 0.88 - ETA: 0s - loss: 0.2863 - accuracy: 0.87 - ETA: 0s - loss: 0.2838 - accuracy: 0.88 - ETA: 0s - loss: 0.2807 - accuracy: 0.88 - ETA: 0s - loss: 0.2803 - accuracy: 0.88 - ETA: 0s - loss: 0.2814 - accuracy: 0.88 - ETA: 0s - loss: 0.2769 - accuracy: 0.88 - ETA: 0s - loss: 0.2761 - accuracy: 0.88 - 1s 79us/step - loss: 0.2763 - accuracy: 0.8830 - val_loss: 0.2565 - val_accuracy: 0.8812\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21917\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.84 - ETA: 0s - loss: 0.2688 - accuracy: 0.87 - ETA: 0s - loss: 0.2722 - accuracy: 0.88 - ETA: 0s - loss: 0.2714 - accuracy: 0.88 - ETA: 0s - loss: 0.2684 - accuracy: 0.88 - ETA: 0s - loss: 0.2702 - accuracy: 0.88 - ETA: 0s - loss: 0.2732 - accuracy: 0.88 - ETA: 0s - loss: 0.2733 - accuracy: 0.88 - ETA: 0s - loss: 0.2789 - accuracy: 0.87 - ETA: 0s - loss: 0.2762 - accuracy: 0.88 - ETA: 0s - loss: 0.2746 - accuracy: 0.88 - ETA: 0s - loss: 0.2714 - accuracy: 0.88 - ETA: 0s - loss: 0.2682 - accuracy: 0.88 - ETA: 0s - loss: 0.2713 - accuracy: 0.88 - ETA: 0s - loss: 0.2693 - accuracy: 0.88 - ETA: 0s - loss: 0.2716 - accuracy: 0.88 - 1s 82us/step - loss: 0.2712 - accuracy: 0.8839 - val_loss: 0.2936 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21917\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.89 - ETA: 0s - loss: 0.2276 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.89 - ETA: 0s - loss: 0.2584 - accuracy: 0.89 - ETA: 0s - loss: 0.2720 - accuracy: 0.88 - ETA: 0s - loss: 0.2642 - accuracy: 0.88 - ETA: 0s - loss: 0.2674 - accuracy: 0.88 - ETA: 0s - loss: 0.2625 - accuracy: 0.88 - ETA: 0s - loss: 0.2655 - accuracy: 0.88 - ETA: 0s - loss: 0.2677 - accuracy: 0.88 - ETA: 0s - loss: 0.2684 - accuracy: 0.88 - ETA: 0s - loss: 0.2689 - accuracy: 0.88 - ETA: 0s - loss: 0.2691 - accuracy: 0.88 - 1s 74us/step - loss: 0.2685 - accuracy: 0.8853 - val_loss: 0.2560 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21917\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2748 - accuracy: 0.88 - ETA: 0s - loss: 0.2695 - accuracy: 0.88 - ETA: 0s - loss: 0.2645 - accuracy: 0.88 - ETA: 0s - loss: 0.2679 - accuracy: 0.88 - ETA: 0s - loss: 0.2631 - accuracy: 0.89 - ETA: 0s - loss: 0.2644 - accuracy: 0.88 - ETA: 0s - loss: 0.2666 - accuracy: 0.88 - ETA: 0s - loss: 0.2631 - accuracy: 0.89 - ETA: 0s - loss: 0.2624 - accuracy: 0.89 - ETA: 0s - loss: 0.2607 - accuracy: 0.89 - ETA: 0s - loss: 0.2629 - accuracy: 0.88 - 1s 69us/step - loss: 0.2648 - accuracy: 0.8889 - val_loss: 0.2972 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21917\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2184 - accuracy: 0.90 - ETA: 0s - loss: 0.2508 - accuracy: 0.88 - ETA: 0s - loss: 0.2675 - accuracy: 0.88 - ETA: 0s - loss: 0.2639 - accuracy: 0.88 - ETA: 0s - loss: 0.2622 - accuracy: 0.88 - ETA: 0s - loss: 0.2532 - accuracy: 0.89 - ETA: 0s - loss: 0.2498 - accuracy: 0.89 - ETA: 0s - loss: 0.2530 - accuracy: 0.89 - ETA: 0s - loss: 0.2550 - accuracy: 0.89 - ETA: 0s - loss: 0.2554 - accuracy: 0.89 - ETA: 0s - loss: 0.2600 - accuracy: 0.88 - ETA: 0s - loss: 0.2610 - accuracy: 0.88 - 1s 73us/step - loss: 0.2601 - accuracy: 0.8904 - val_loss: 0.2554 - val_accuracy: 0.8863\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21917\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2367 - accuracy: 0.89 - ETA: 0s - loss: 0.2722 - accuracy: 0.87 - ETA: 0s - loss: 0.2793 - accuracy: 0.87 - ETA: 0s - loss: 0.2720 - accuracy: 0.87 - ETA: 0s - loss: 0.2712 - accuracy: 0.87 - ETA: 0s - loss: 0.2646 - accuracy: 0.88 - ETA: 0s - loss: 0.2646 - accuracy: 0.88 - ETA: 0s - loss: 0.2646 - accuracy: 0.88 - ETA: 0s - loss: 0.2601 - accuracy: 0.88 - ETA: 0s - loss: 0.2601 - accuracy: 0.88 - ETA: 0s - loss: 0.2513 - accuracy: 0.88 - ETA: 0s - loss: 0.2488 - accuracy: 0.89 - 1s 72us/step - loss: 0.2492 - accuracy: 0.8903 - val_loss: 0.2314 - val_accuracy: 0.8964\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.21917\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3021 - accuracy: 0.88 - ETA: 0s - loss: 0.2603 - accuracy: 0.88 - ETA: 0s - loss: 0.2469 - accuracy: 0.89 - ETA: 0s - loss: 0.2462 - accuracy: 0.89 - ETA: 0s - loss: 0.2400 - accuracy: 0.89 - ETA: 0s - loss: 0.2503 - accuracy: 0.89 - ETA: 0s - loss: 0.2510 - accuracy: 0.89 - ETA: 0s - loss: 0.2484 - accuracy: 0.89 - ETA: 0s - loss: 0.2503 - accuracy: 0.89 - ETA: 0s - loss: 0.2529 - accuracy: 0.89 - ETA: 0s - loss: 0.2514 - accuracy: 0.89 - ETA: 0s - loss: 0.2499 - accuracy: 0.89 - 1s 69us/step - loss: 0.2503 - accuracy: 0.8933 - val_loss: 0.3164 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.21917\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.90 - ETA: 0s - loss: 0.2410 - accuracy: 0.90 - ETA: 0s - loss: 0.2427 - accuracy: 0.89 - ETA: 0s - loss: 0.2495 - accuracy: 0.89 - ETA: 0s - loss: 0.2562 - accuracy: 0.89 - ETA: 0s - loss: 0.2456 - accuracy: 0.89 - ETA: 0s - loss: 0.2452 - accuracy: 0.89 - ETA: 0s - loss: 0.2425 - accuracy: 0.89 - ETA: 0s - loss: 0.2400 - accuracy: 0.89 - ETA: 0s - loss: 0.2415 - accuracy: 0.89 - ETA: 0s - loss: 0.2443 - accuracy: 0.89 - ETA: 0s - loss: 0.2441 - accuracy: 0.89 - 1s 70us/step - loss: 0.2448 - accuracy: 0.8961 - val_loss: 0.2461 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21917\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2396 - accuracy: 0.89 - ETA: 0s - loss: 0.2536 - accuracy: 0.88 - ETA: 0s - loss: 0.2401 - accuracy: 0.89 - ETA: 0s - loss: 0.2375 - accuracy: 0.89 - ETA: 0s - loss: 0.2349 - accuracy: 0.89 - ETA: 0s - loss: 0.2394 - accuracy: 0.89 - ETA: 0s - loss: 0.2395 - accuracy: 0.89 - ETA: 0s - loss: 0.2450 - accuracy: 0.89 - ETA: 0s - loss: 0.2389 - accuracy: 0.89 - ETA: 0s - loss: 0.2373 - accuracy: 0.89 - ETA: 0s - loss: 0.2406 - accuracy: 0.89 - ETA: 0s - loss: 0.2405 - accuracy: 0.89 - 1s 72us/step - loss: 0.2402 - accuracy: 0.8956 - val_loss: 0.2278 - val_accuracy: 0.9031\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.21917\n",
      "Epoch 17/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2634 - accuracy: 0.88 - ETA: 0s - loss: 0.2391 - accuracy: 0.90 - ETA: 0s - loss: 0.2409 - accuracy: 0.89 - ETA: 0s - loss: 0.2500 - accuracy: 0.89 - ETA: 0s - loss: 0.2424 - accuracy: 0.89 - ETA: 0s - loss: 0.2387 - accuracy: 0.89 - ETA: 0s - loss: 0.2427 - accuracy: 0.89 - ETA: 0s - loss: 0.2439 - accuracy: 0.89 - ETA: 0s - loss: 0.2426 - accuracy: 0.89 - ETA: 0s - loss: 0.2414 - accuracy: 0.89 - ETA: 0s - loss: 0.2384 - accuracy: 0.89 - ETA: 0s - loss: 0.2385 - accuracy: 0.90 - ETA: 0s - loss: 0.2373 - accuracy: 0.90 - ETA: 0s - loss: 0.2382 - accuracy: 0.90 - ETA: 0s - loss: 0.2371 - accuracy: 0.90 - ETA: 0s - loss: 0.2379 - accuracy: 0.89 - 1s 87us/step - loss: 0.2369 - accuracy: 0.9000 - val_loss: 0.2549 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21917\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2431 - accuracy: 0.91 - ETA: 0s - loss: 0.2438 - accuracy: 0.90 - ETA: 0s - loss: 0.2320 - accuracy: 0.90 - ETA: 0s - loss: 0.2310 - accuracy: 0.90 - ETA: 0s - loss: 0.2270 - accuracy: 0.90 - ETA: 0s - loss: 0.2321 - accuracy: 0.90 - ETA: 0s - loss: 0.2330 - accuracy: 0.90 - ETA: 0s - loss: 0.2385 - accuracy: 0.90 - ETA: 0s - loss: 0.2410 - accuracy: 0.90 - ETA: 0s - loss: 0.2399 - accuracy: 0.90 - ETA: 0s - loss: 0.2392 - accuracy: 0.90 - ETA: 0s - loss: 0.2377 - accuracy: 0.90 - ETA: 0s - loss: 0.2372 - accuracy: 0.90 - ETA: 0s - loss: 0.2348 - accuracy: 0.90 - ETA: 0s - loss: 0.2350 - accuracy: 0.90 - 1s 82us/step - loss: 0.2351 - accuracy: 0.9010 - val_loss: 0.2270 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21917\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2237 - accuracy: 0.91 - ETA: 0s - loss: 0.2174 - accuracy: 0.90 - ETA: 0s - loss: 0.2144 - accuracy: 0.91 - ETA: 0s - loss: 0.2184 - accuracy: 0.91 - ETA: 0s - loss: 0.2163 - accuracy: 0.91 - ETA: 0s - loss: 0.2193 - accuracy: 0.91 - ETA: 0s - loss: 0.2218 - accuracy: 0.91 - ETA: 0s - loss: 0.2242 - accuracy: 0.90 - ETA: 0s - loss: 0.2249 - accuracy: 0.90 - ETA: 0s - loss: 0.2235 - accuracy: 0.90 - ETA: 0s - loss: 0.2242 - accuracy: 0.90 - ETA: 0s - loss: 0.2257 - accuracy: 0.90 - ETA: 0s - loss: 0.2254 - accuracy: 0.90 - 1s 75us/step - loss: 0.2256 - accuracy: 0.9069 - val_loss: 0.2628 - val_accuracy: 0.8913\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21917\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2074 - accuracy: 0.90 - ETA: 0s - loss: 0.2356 - accuracy: 0.90 - ETA: 0s - loss: 0.2362 - accuracy: 0.90 - ETA: 0s - loss: 0.2383 - accuracy: 0.89 - ETA: 0s - loss: 0.2313 - accuracy: 0.90 - ETA: 0s - loss: 0.2320 - accuracy: 0.90 - ETA: 0s - loss: 0.2368 - accuracy: 0.89 - ETA: 0s - loss: 0.2377 - accuracy: 0.89 - ETA: 0s - loss: 0.2371 - accuracy: 0.89 - ETA: 0s - loss: 0.2329 - accuracy: 0.90 - ETA: 0s - loss: 0.2330 - accuracy: 0.90 - ETA: 0s - loss: 0.2276 - accuracy: 0.90 - ETA: 0s - loss: 0.2296 - accuracy: 0.90 - 1s 74us/step - loss: 0.2291 - accuracy: 0.9032 - val_loss: 0.2783 - val_accuracy: 0.8846\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.21917\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.1798 - accuracy: 0.91 - ETA: 0s - loss: 0.2360 - accuracy: 0.89 - ETA: 0s - loss: 0.2289 - accuracy: 0.90 - ETA: 0s - loss: 0.2252 - accuracy: 0.90 - ETA: 0s - loss: 0.2250 - accuracy: 0.90 - ETA: 0s - loss: 0.2298 - accuracy: 0.89 - ETA: 0s - loss: 0.2287 - accuracy: 0.89 - ETA: 0s - loss: 0.2228 - accuracy: 0.90 - ETA: 0s - loss: 0.2245 - accuracy: 0.90 - ETA: 0s - loss: 0.2256 - accuracy: 0.90 - ETA: 0s - loss: 0.2241 - accuracy: 0.90 - ETA: 0s - loss: 0.2235 - accuracy: 0.90 - 1s 72us/step - loss: 0.2235 - accuracy: 0.9040 - val_loss: 0.2538 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.21917\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2516 - accuracy: 0.87 - ETA: 0s - loss: 0.2511 - accuracy: 0.88 - ETA: 0s - loss: 0.2172 - accuracy: 0.90 - ETA: 0s - loss: 0.2266 - accuracy: 0.90 - ETA: 0s - loss: 0.2240 - accuracy: 0.90 - ETA: 0s - loss: 0.2211 - accuracy: 0.90 - ETA: 0s - loss: 0.2218 - accuracy: 0.90 - ETA: 0s - loss: 0.2232 - accuracy: 0.90 - ETA: 0s - loss: 0.2241 - accuracy: 0.90 - ETA: 0s - loss: 0.2228 - accuracy: 0.90 - ETA: 0s - loss: 0.2226 - accuracy: 0.90 - ETA: 0s - loss: 0.2220 - accuracy: 0.90 - ETA: 0s - loss: 0.2208 - accuracy: 0.90 - ETA: 0s - loss: 0.2207 - accuracy: 0.90 - 1s 79us/step - loss: 0.2215 - accuracy: 0.9072 - val_loss: 0.2463 - val_accuracy: 0.8896\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.21917\n",
      "Epoch 23/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2711 - accuracy: 0.89 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2214 - accuracy: 0.91 - ETA: 0s - loss: 0.2105 - accuracy: 0.91 - ETA: 0s - loss: 0.2143 - accuracy: 0.91 - ETA: 0s - loss: 0.2152 - accuracy: 0.91 - ETA: 0s - loss: 0.2186 - accuracy: 0.91 - ETA: 0s - loss: 0.2169 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.90 - ETA: 0s - loss: 0.2202 - accuracy: 0.90 - ETA: 0s - loss: 0.2190 - accuracy: 0.90 - ETA: 0s - loss: 0.2170 - accuracy: 0.90 - ETA: 0s - loss: 0.2165 - accuracy: 0.90 - 1s 74us/step - loss: 0.2156 - accuracy: 0.9092 - val_loss: 0.2647 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.21917\n",
      "Epoch 00023: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 55us/step\n",
      "[2020-05-18 17:22:53 RAM66.0% 0.67GB] Val Score : [0.2534168760508999, 0.8855193257331848]\n",
      "[2020-05-18 17:22:53 RAM66.0% 0.67GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:22:53 RAM66.0% 0.67GB] Training on Fold : 7\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 37s - loss: 1.2068 - accuracy: 0.448 - ETA: 7s - loss: 1.1228 - accuracy: 0.675 - ETA: 5s - loss: 0.9152 - accuracy: 0.70 - ETA: 3s - loss: 0.8021 - accuracy: 0.73 - ETA: 2s - loss: 0.7111 - accuracy: 0.75 - ETA: 2s - loss: 0.6638 - accuracy: 0.76 - ETA: 1s - loss: 0.6336 - accuracy: 0.77 - ETA: 1s - loss: 0.6082 - accuracy: 0.77 - ETA: 1s - loss: 0.5823 - accuracy: 0.78 - ETA: 0s - loss: 0.5627 - accuracy: 0.78 - ETA: 0s - loss: 0.5496 - accuracy: 0.78 - ETA: 0s - loss: 0.5424 - accuracy: 0.79 - ETA: 0s - loss: 0.5364 - accuracy: 0.79 - ETA: 0s - loss: 0.5235 - accuracy: 0.79 - ETA: 0s - loss: 0.5207 - accuracy: 0.79 - ETA: 0s - loss: 0.5129 - accuracy: 0.79 - 2s 145us/step - loss: 0.5081 - accuracy: 0.7955 - val_loss: 1.4482 - val_accuracy: 0.5939\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.21917\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3801 - accuracy: 0.82 - ETA: 0s - loss: 0.4000 - accuracy: 0.84 - ETA: 0s - loss: 0.3933 - accuracy: 0.83 - ETA: 0s - loss: 0.4005 - accuracy: 0.83 - ETA: 0s - loss: 0.3980 - accuracy: 0.83 - ETA: 0s - loss: 0.3845 - accuracy: 0.83 - ETA: 0s - loss: 0.3859 - accuracy: 0.83 - ETA: 0s - loss: 0.3849 - accuracy: 0.83 - ETA: 0s - loss: 0.3798 - accuracy: 0.83 - ETA: 0s - loss: 0.3871 - accuracy: 0.83 - ETA: 0s - loss: 0.3904 - accuracy: 0.83 - ETA: 0s - loss: 0.3883 - accuracy: 0.83 - ETA: 0s - loss: 0.3902 - accuracy: 0.83 - ETA: 0s - loss: 0.3899 - accuracy: 0.83 - ETA: 0s - loss: 0.3869 - accuracy: 0.83 - ETA: 0s - loss: 0.3894 - accuracy: 0.83 - 1s 92us/step - loss: 0.3888 - accuracy: 0.8317 - val_loss: 0.5953 - val_accuracy: 0.7144\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.21917\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3248 - accuracy: 0.82 - ETA: 0s - loss: 0.3509 - accuracy: 0.83 - ETA: 0s - loss: 0.3582 - accuracy: 0.83 - ETA: 0s - loss: 0.3507 - accuracy: 0.83 - ETA: 0s - loss: 0.3469 - accuracy: 0.84 - ETA: 0s - loss: 0.3580 - accuracy: 0.83 - ETA: 0s - loss: 0.3571 - accuracy: 0.84 - ETA: 0s - loss: 0.3534 - accuracy: 0.84 - ETA: 0s - loss: 0.3521 - accuracy: 0.84 - ETA: 0s - loss: 0.3531 - accuracy: 0.84 - ETA: 0s - loss: 0.3519 - accuracy: 0.84 - ETA: 0s - loss: 0.3547 - accuracy: 0.84 - ETA: 0s - loss: 0.3518 - accuracy: 0.84 - ETA: 0s - loss: 0.3515 - accuracy: 0.84 - ETA: 0s - loss: 0.3498 - accuracy: 0.84 - ETA: 0s - loss: 0.3515 - accuracy: 0.84 - 1s 85us/step - loss: 0.3512 - accuracy: 0.8453 - val_loss: 0.4030 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21917\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.4943 - accuracy: 0.78 - ETA: 0s - loss: 0.3482 - accuracy: 0.83 - ETA: 0s - loss: 0.3564 - accuracy: 0.84 - ETA: 0s - loss: 0.3479 - accuracy: 0.84 - ETA: 0s - loss: 0.3424 - accuracy: 0.85 - ETA: 0s - loss: 0.3395 - accuracy: 0.85 - ETA: 0s - loss: 0.3318 - accuracy: 0.85 - ETA: 0s - loss: 0.3345 - accuracy: 0.85 - ETA: 0s - loss: 0.3336 - accuracy: 0.85 - ETA: 0s - loss: 0.3374 - accuracy: 0.85 - ETA: 0s - loss: 0.3372 - accuracy: 0.85 - ETA: 0s - loss: 0.3332 - accuracy: 0.85 - ETA: 0s - loss: 0.3318 - accuracy: 0.85 - ETA: 0s - loss: 0.3307 - accuracy: 0.85 - ETA: 0s - loss: 0.3290 - accuracy: 0.85 - ETA: 0s - loss: 0.3296 - accuracy: 0.85 - ETA: 0s - loss: 0.3300 - accuracy: 0.85 - 1s 91us/step - loss: 0.3294 - accuracy: 0.8560 - val_loss: 0.3782 - val_accuracy: 0.8248\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21917\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3206 - accuracy: 0.83 - ETA: 0s - loss: 0.3114 - accuracy: 0.86 - ETA: 0s - loss: 0.3056 - accuracy: 0.86 - ETA: 0s - loss: 0.3074 - accuracy: 0.86 - ETA: 0s - loss: 0.3059 - accuracy: 0.87 - ETA: 0s - loss: 0.3094 - accuracy: 0.86 - ETA: 0s - loss: 0.3116 - accuracy: 0.86 - ETA: 0s - loss: 0.3144 - accuracy: 0.86 - ETA: 0s - loss: 0.3110 - accuracy: 0.86 - ETA: 0s - loss: 0.3078 - accuracy: 0.86 - ETA: 0s - loss: 0.3106 - accuracy: 0.86 - ETA: 0s - loss: 0.3113 - accuracy: 0.86 - ETA: 0s - loss: 0.3107 - accuracy: 0.86 - ETA: 0s - loss: 0.3111 - accuracy: 0.86 - ETA: 0s - loss: 0.3096 - accuracy: 0.86 - 1s 85us/step - loss: 0.3116 - accuracy: 0.8668 - val_loss: 0.2965 - val_accuracy: 0.8694\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21917\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.82 - ETA: 0s - loss: 0.2939 - accuracy: 0.86 - ETA: 0s - loss: 0.2913 - accuracy: 0.86 - ETA: 0s - loss: 0.2995 - accuracy: 0.86 - ETA: 0s - loss: 0.2993 - accuracy: 0.86 - ETA: 0s - loss: 0.2968 - accuracy: 0.86 - ETA: 0s - loss: 0.2921 - accuracy: 0.86 - ETA: 0s - loss: 0.2983 - accuracy: 0.86 - ETA: 0s - loss: 0.2980 - accuracy: 0.86 - ETA: 0s - loss: 0.2964 - accuracy: 0.86 - ETA: 0s - loss: 0.2948 - accuracy: 0.86 - ETA: 0s - loss: 0.2982 - accuracy: 0.86 - ETA: 0s - loss: 0.2966 - accuracy: 0.87 - ETA: 0s - loss: 0.2937 - accuracy: 0.87 - ETA: 0s - loss: 0.2956 - accuracy: 0.87 - 1s 79us/step - loss: 0.2972 - accuracy: 0.8722 - val_loss: 0.3674 - val_accuracy: 0.8441\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21917\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.86 - ETA: 0s - loss: 0.2924 - accuracy: 0.87 - ETA: 0s - loss: 0.2846 - accuracy: 0.88 - ETA: 0s - loss: 0.2883 - accuracy: 0.87 - ETA: 0s - loss: 0.2785 - accuracy: 0.87 - ETA: 0s - loss: 0.2889 - accuracy: 0.87 - ETA: 0s - loss: 0.2914 - accuracy: 0.87 - ETA: 0s - loss: 0.2923 - accuracy: 0.87 - ETA: 0s - loss: 0.2939 - accuracy: 0.87 - ETA: 0s - loss: 0.2915 - accuracy: 0.87 - ETA: 0s - loss: 0.2890 - accuracy: 0.87 - ETA: 0s - loss: 0.2920 - accuracy: 0.87 - ETA: 0s - loss: 0.2908 - accuracy: 0.87 - ETA: 0s - loss: 0.2884 - accuracy: 0.87 - ETA: 0s - loss: 0.2884 - accuracy: 0.87 - 1s 78us/step - loss: 0.2883 - accuracy: 0.8743 - val_loss: 0.4977 - val_accuracy: 0.7751\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21917\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.85 - ETA: 0s - loss: 0.3109 - accuracy: 0.86 - ETA: 0s - loss: 0.2978 - accuracy: 0.86 - ETA: 0s - loss: 0.2890 - accuracy: 0.86 - ETA: 0s - loss: 0.2803 - accuracy: 0.88 - ETA: 0s - loss: 0.2845 - accuracy: 0.87 - ETA: 0s - loss: 0.2854 - accuracy: 0.87 - ETA: 0s - loss: 0.2811 - accuracy: 0.88 - ETA: 0s - loss: 0.2839 - accuracy: 0.87 - ETA: 0s - loss: 0.2848 - accuracy: 0.87 - ETA: 0s - loss: 0.2856 - accuracy: 0.87 - ETA: 0s - loss: 0.2824 - accuracy: 0.88 - ETA: 0s - loss: 0.2868 - accuracy: 0.88 - ETA: 0s - loss: 0.2839 - accuracy: 0.88 - ETA: 0s - loss: 0.2854 - accuracy: 0.87 - ETA: 0s - loss: 0.2865 - accuracy: 0.87 - 1s 81us/step - loss: 0.2860 - accuracy: 0.8779 - val_loss: 0.2972 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21917\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.92 - ETA: 0s - loss: 0.2682 - accuracy: 0.88 - ETA: 0s - loss: 0.2634 - accuracy: 0.88 - ETA: 0s - loss: 0.2740 - accuracy: 0.87 - ETA: 0s - loss: 0.2771 - accuracy: 0.87 - ETA: 0s - loss: 0.2735 - accuracy: 0.88 - ETA: 0s - loss: 0.2738 - accuracy: 0.88 - ETA: 0s - loss: 0.2735 - accuracy: 0.88 - ETA: 0s - loss: 0.2766 - accuracy: 0.87 - ETA: 0s - loss: 0.2747 - accuracy: 0.88 - ETA: 0s - loss: 0.2720 - accuracy: 0.88 - ETA: 0s - loss: 0.2716 - accuracy: 0.88 - ETA: 0s - loss: 0.2694 - accuracy: 0.88 - ETA: 0s - loss: 0.2735 - accuracy: 0.88 - ETA: 0s - loss: 0.2707 - accuracy: 0.88 - 1s 82us/step - loss: 0.2686 - accuracy: 0.8829 - val_loss: 0.4588 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21917\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3576 - accuracy: 0.84 - ETA: 0s - loss: 0.2792 - accuracy: 0.88 - ETA: 0s - loss: 0.2604 - accuracy: 0.88 - ETA: 0s - loss: 0.2664 - accuracy: 0.88 - ETA: 0s - loss: 0.2707 - accuracy: 0.88 - ETA: 0s - loss: 0.2614 - accuracy: 0.88 - ETA: 0s - loss: 0.2604 - accuracy: 0.88 - ETA: 0s - loss: 0.2602 - accuracy: 0.89 - ETA: 0s - loss: 0.2566 - accuracy: 0.89 - ETA: 0s - loss: 0.2592 - accuracy: 0.88 - ETA: 0s - loss: 0.2640 - accuracy: 0.88 - ETA: 0s - loss: 0.2711 - accuracy: 0.88 - ETA: 0s - loss: 0.2662 - accuracy: 0.88 - ETA: 0s - loss: 0.2664 - accuracy: 0.88 - 1s 79us/step - loss: 0.2656 - accuracy: 0.8870 - val_loss: 0.3559 - val_accuracy: 0.8677\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21917\n",
      "Epoch 00010: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 59us/step\n",
      "[2020-05-18 17:23:04 RAM66.6% 0.72GB] Val Score : [0.3152107181249241, 0.8749052286148071]\n",
      "[2020-05-18 17:23:04 RAM66.6% 0.72GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:23:04 RAM66.6% 0.72GB] Training on Fold : 8\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 41s - loss: 0.8450 - accuracy: 0.558 - ETA: 8s - loss: 1.0258 - accuracy: 0.710 - ETA: 4s - loss: 0.8222 - accuracy: 0.73 - ETA: 3s - loss: 0.7540 - accuracy: 0.74 - ETA: 2s - loss: 0.6882 - accuracy: 0.75 - ETA: 2s - loss: 0.6510 - accuracy: 0.75 - ETA: 1s - loss: 0.6244 - accuracy: 0.76 - ETA: 1s - loss: 0.6118 - accuracy: 0.76 - ETA: 1s - loss: 0.5883 - accuracy: 0.77 - ETA: 0s - loss: 0.5724 - accuracy: 0.77 - ETA: 0s - loss: 0.5557 - accuracy: 0.77 - ETA: 0s - loss: 0.5422 - accuracy: 0.78 - ETA: 0s - loss: 0.5337 - accuracy: 0.78 - ETA: 0s - loss: 0.5240 - accuracy: 0.78 - ETA: 0s - loss: 0.5188 - accuracy: 0.78 - ETA: 0s - loss: 0.5138 - accuracy: 0.78 - ETA: 0s - loss: 0.5023 - accuracy: 0.79 - 2s 152us/step - loss: 0.4966 - accuracy: 0.7944 - val_loss: 1.3293 - val_accuracy: 0.5535\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.21917\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3202 - accuracy: 0.85 - ETA: 0s - loss: 0.3602 - accuracy: 0.83 - ETA: 0s - loss: 0.3676 - accuracy: 0.83 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3822 - accuracy: 0.83 - ETA: 0s - loss: 0.3783 - accuracy: 0.83 - ETA: 0s - loss: 0.3736 - accuracy: 0.83 - ETA: 0s - loss: 0.3725 - accuracy: 0.83 - ETA: 0s - loss: 0.3728 - accuracy: 0.83 - ETA: 0s - loss: 0.3729 - accuracy: 0.83 - ETA: 0s - loss: 0.3737 - accuracy: 0.83 - ETA: 0s - loss: 0.3712 - accuracy: 0.83 - ETA: 0s - loss: 0.3676 - accuracy: 0.83 - ETA: 0s - loss: 0.3684 - accuracy: 0.83 - 1s 84us/step - loss: 0.3711 - accuracy: 0.8373 - val_loss: 0.5956 - val_accuracy: 0.7136\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.21917\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3031 - accuracy: 0.86 - ETA: 0s - loss: 0.2997 - accuracy: 0.86 - ETA: 0s - loss: 0.3402 - accuracy: 0.84 - ETA: 0s - loss: 0.3546 - accuracy: 0.84 - ETA: 0s - loss: 0.3499 - accuracy: 0.84 - ETA: 0s - loss: 0.3484 - accuracy: 0.84 - ETA: 0s - loss: 0.3423 - accuracy: 0.84 - ETA: 0s - loss: 0.3434 - accuracy: 0.85 - ETA: 0s - loss: 0.3441 - accuracy: 0.85 - ETA: 0s - loss: 0.3431 - accuracy: 0.85 - ETA: 0s - loss: 0.3421 - accuracy: 0.85 - ETA: 0s - loss: 0.3432 - accuracy: 0.85 - 1s 73us/step - loss: 0.3400 - accuracy: 0.8530 - val_loss: 0.3957 - val_accuracy: 0.8138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21917\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3677 - accuracy: 0.83 - ETA: 0s - loss: 0.3190 - accuracy: 0.86 - ETA: 0s - loss: 0.3174 - accuracy: 0.86 - ETA: 0s - loss: 0.3303 - accuracy: 0.85 - ETA: 0s - loss: 0.3241 - accuracy: 0.85 - ETA: 0s - loss: 0.3216 - accuracy: 0.85 - ETA: 0s - loss: 0.3226 - accuracy: 0.85 - ETA: 0s - loss: 0.3211 - accuracy: 0.85 - ETA: 0s - loss: 0.3196 - accuracy: 0.85 - ETA: 0s - loss: 0.3174 - accuracy: 0.85 - ETA: 0s - loss: 0.3168 - accuracy: 0.85 - ETA: 0s - loss: 0.3177 - accuracy: 0.85 - ETA: 0s - loss: 0.3173 - accuracy: 0.86 - ETA: 0s - loss: 0.3151 - accuracy: 0.86 - 1s 81us/step - loss: 0.3141 - accuracy: 0.8623 - val_loss: 0.3050 - val_accuracy: 0.8686\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21917\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3106 - accuracy: 0.85 - ETA: 0s - loss: 0.3293 - accuracy: 0.85 - ETA: 0s - loss: 0.3223 - accuracy: 0.85 - ETA: 0s - loss: 0.3176 - accuracy: 0.85 - ETA: 0s - loss: 0.3129 - accuracy: 0.86 - ETA: 0s - loss: 0.3095 - accuracy: 0.86 - ETA: 0s - loss: 0.3115 - accuracy: 0.86 - ETA: 0s - loss: 0.3057 - accuracy: 0.87 - ETA: 0s - loss: 0.3088 - accuracy: 0.86 - ETA: 0s - loss: 0.3083 - accuracy: 0.86 - ETA: 0s - loss: 0.3099 - accuracy: 0.86 - ETA: 0s - loss: 0.3084 - accuracy: 0.86 - ETA: 0s - loss: 0.3093 - accuracy: 0.86 - ETA: 0s - loss: 0.3075 - accuracy: 0.86 - ETA: 0s - loss: 0.3072 - accuracy: 0.86 - 1s 84us/step - loss: 0.3045 - accuracy: 0.8708 - val_loss: 0.3387 - val_accuracy: 0.8517\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21917\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.1945 - accuracy: 0.92 - ETA: 0s - loss: 0.2743 - accuracy: 0.88 - ETA: 0s - loss: 0.2918 - accuracy: 0.87 - ETA: 0s - loss: 0.2903 - accuracy: 0.87 - ETA: 0s - loss: 0.2923 - accuracy: 0.87 - ETA: 0s - loss: 0.2978 - accuracy: 0.86 - ETA: 0s - loss: 0.2953 - accuracy: 0.87 - ETA: 0s - loss: 0.2945 - accuracy: 0.87 - ETA: 0s - loss: 0.2946 - accuracy: 0.87 - ETA: 0s - loss: 0.2958 - accuracy: 0.87 - ETA: 0s - loss: 0.2915 - accuracy: 0.87 - ETA: 0s - loss: 0.2893 - accuracy: 0.87 - ETA: 0s - loss: 0.2875 - accuracy: 0.87 - ETA: 0s - loss: 0.2888 - accuracy: 0.87 - ETA: 0s - loss: 0.2924 - accuracy: 0.87 - 1s 83us/step - loss: 0.2924 - accuracy: 0.8721 - val_loss: 0.4643 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21917\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3030 - accuracy: 0.86 - ETA: 0s - loss: 0.3095 - accuracy: 0.86 - ETA: 0s - loss: 0.2961 - accuracy: 0.87 - ETA: 0s - loss: 0.3069 - accuracy: 0.87 - ETA: 0s - loss: 0.2894 - accuracy: 0.87 - ETA: 0s - loss: 0.2833 - accuracy: 0.87 - ETA: 0s - loss: 0.2836 - accuracy: 0.87 - ETA: 0s - loss: 0.2810 - accuracy: 0.88 - ETA: 0s - loss: 0.2820 - accuracy: 0.88 - ETA: 0s - loss: 0.2828 - accuracy: 0.88 - ETA: 0s - loss: 0.2821 - accuracy: 0.88 - ETA: 0s - loss: 0.2814 - accuracy: 0.88 - ETA: 0s - loss: 0.2814 - accuracy: 0.88 - ETA: 0s - loss: 0.2841 - accuracy: 0.88 - 1s 79us/step - loss: 0.2841 - accuracy: 0.8800 - val_loss: 0.2908 - val_accuracy: 0.8719\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21917\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.81 - ETA: 0s - loss: 0.3315 - accuracy: 0.85 - ETA: 0s - loss: 0.3022 - accuracy: 0.87 - ETA: 0s - loss: 0.2898 - accuracy: 0.88 - ETA: 0s - loss: 0.2851 - accuracy: 0.88 - ETA: 0s - loss: 0.2845 - accuracy: 0.88 - ETA: 0s - loss: 0.2827 - accuracy: 0.88 - ETA: 0s - loss: 0.2869 - accuracy: 0.88 - ETA: 0s - loss: 0.2884 - accuracy: 0.87 - ETA: 0s - loss: 0.2855 - accuracy: 0.87 - ETA: 0s - loss: 0.2815 - accuracy: 0.88 - ETA: 0s - loss: 0.2809 - accuracy: 0.87 - ETA: 0s - loss: 0.2783 - accuracy: 0.88 - ETA: 0s - loss: 0.2782 - accuracy: 0.87 - 1s 80us/step - loss: 0.2786 - accuracy: 0.8792 - val_loss: 0.3654 - val_accuracy: 0.8340\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21917\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2538 - accuracy: 0.89 - ETA: 0s - loss: 0.2632 - accuracy: 0.89 - ETA: 0s - loss: 0.2542 - accuracy: 0.89 - ETA: 0s - loss: 0.2717 - accuracy: 0.88 - ETA: 0s - loss: 0.2657 - accuracy: 0.89 - ETA: 0s - loss: 0.2603 - accuracy: 0.89 - ETA: 0s - loss: 0.2535 - accuracy: 0.89 - ETA: 0s - loss: 0.2620 - accuracy: 0.88 - ETA: 0s - loss: 0.2729 - accuracy: 0.88 - ETA: 0s - loss: 0.2716 - accuracy: 0.88 - ETA: 0s - loss: 0.2686 - accuracy: 0.88 - ETA: 0s - loss: 0.2686 - accuracy: 0.88 - ETA: 0s - loss: 0.2690 - accuracy: 0.88 - 1s 75us/step - loss: 0.2675 - accuracy: 0.8875 - val_loss: 0.2699 - val_accuracy: 0.8896\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21917\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2651 - accuracy: 0.89 - ETA: 0s - loss: 0.2816 - accuracy: 0.88 - ETA: 0s - loss: 0.2663 - accuracy: 0.88 - ETA: 0s - loss: 0.2639 - accuracy: 0.88 - ETA: 0s - loss: 0.2641 - accuracy: 0.88 - ETA: 0s - loss: 0.2696 - accuracy: 0.88 - ETA: 0s - loss: 0.2677 - accuracy: 0.88 - ETA: 0s - loss: 0.2696 - accuracy: 0.88 - ETA: 0s - loss: 0.2699 - accuracy: 0.88 - ETA: 0s - loss: 0.2651 - accuracy: 0.88 - ETA: 0s - loss: 0.2640 - accuracy: 0.88 - ETA: 0s - loss: 0.2625 - accuracy: 0.88 - ETA: 0s - loss: 0.2602 - accuracy: 0.88 - ETA: 0s - loss: 0.2600 - accuracy: 0.88 - ETA: 0s - loss: 0.2608 - accuracy: 0.88 - ETA: 0s - loss: 0.2636 - accuracy: 0.88 - 1s 88us/step - loss: 0.2656 - accuracy: 0.8857 - val_loss: 0.3560 - val_accuracy: 0.8256\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21917\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.91 - ETA: 0s - loss: 0.2638 - accuracy: 0.89 - ETA: 0s - loss: 0.2675 - accuracy: 0.88 - ETA: 0s - loss: 0.2720 - accuracy: 0.88 - ETA: 0s - loss: 0.2719 - accuracy: 0.88 - ETA: 0s - loss: 0.2807 - accuracy: 0.87 - ETA: 0s - loss: 0.2723 - accuracy: 0.88 - ETA: 0s - loss: 0.2711 - accuracy: 0.88 - ETA: 0s - loss: 0.2660 - accuracy: 0.88 - ETA: 0s - loss: 0.2614 - accuracy: 0.88 - ETA: 0s - loss: 0.2642 - accuracy: 0.88 - ETA: 0s - loss: 0.2615 - accuracy: 0.88 - ETA: 0s - loss: 0.2613 - accuracy: 0.88 - ETA: 0s - loss: 0.2586 - accuracy: 0.88 - 1s 81us/step - loss: 0.2593 - accuracy: 0.8891 - val_loss: 0.4384 - val_accuracy: 0.8239\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21917\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3581 - accuracy: 0.84 - ETA: 0s - loss: 0.2578 - accuracy: 0.89 - ETA: 0s - loss: 0.2465 - accuracy: 0.89 - ETA: 0s - loss: 0.2458 - accuracy: 0.89 - ETA: 0s - loss: 0.2597 - accuracy: 0.89 - ETA: 0s - loss: 0.2680 - accuracy: 0.88 - ETA: 0s - loss: 0.2629 - accuracy: 0.88 - ETA: 0s - loss: 0.2580 - accuracy: 0.89 - ETA: 0s - loss: 0.2564 - accuracy: 0.89 - ETA: 0s - loss: 0.2568 - accuracy: 0.89 - ETA: 0s - loss: 0.2556 - accuracy: 0.89 - ETA: 0s - loss: 0.2552 - accuracy: 0.89 - ETA: 0s - loss: 0.2526 - accuracy: 0.89 - ETA: 0s - loss: 0.2540 - accuracy: 0.89 - 1s 82us/step - loss: 0.2541 - accuracy: 0.8915 - val_loss: 0.2899 - val_accuracy: 0.8829\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21917\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2618 - accuracy: 0.90 - ETA: 0s - loss: 0.2558 - accuracy: 0.88 - ETA: 0s - loss: 0.2522 - accuracy: 0.88 - ETA: 0s - loss: 0.2465 - accuracy: 0.89 - ETA: 0s - loss: 0.2455 - accuracy: 0.89 - ETA: 0s - loss: 0.2445 - accuracy: 0.89 - ETA: 0s - loss: 0.2435 - accuracy: 0.89 - ETA: 0s - loss: 0.2430 - accuracy: 0.89 - ETA: 0s - loss: 0.2436 - accuracy: 0.89 - ETA: 0s - loss: 0.2441 - accuracy: 0.89 - ETA: 0s - loss: 0.2447 - accuracy: 0.89 - ETA: 0s - loss: 0.2463 - accuracy: 0.89 - ETA: 0s - loss: 0.2443 - accuracy: 0.89 - ETA: 0s - loss: 0.2458 - accuracy: 0.89 - ETA: 0s - loss: 0.2455 - accuracy: 0.89 - 1s 91us/step - loss: 0.2455 - accuracy: 0.8961 - val_loss: 0.2681 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.21917\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2575 - accuracy: 0.91 - ETA: 0s - loss: 0.2297 - accuracy: 0.90 - ETA: 0s - loss: 0.2372 - accuracy: 0.90 - ETA: 0s - loss: 0.2426 - accuracy: 0.89 - ETA: 0s - loss: 0.2429 - accuracy: 0.89 - ETA: 0s - loss: 0.2469 - accuracy: 0.89 - ETA: 0s - loss: 0.2459 - accuracy: 0.89 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2485 - accuracy: 0.89 - ETA: 0s - loss: 0.2508 - accuracy: 0.89 - ETA: 0s - loss: 0.2493 - accuracy: 0.89 - ETA: 0s - loss: 0.2522 - accuracy: 0.89 - ETA: 0s - loss: 0.2523 - accuracy: 0.89 - ETA: 0s - loss: 0.2496 - accuracy: 0.89 - 1s 80us/step - loss: 0.2494 - accuracy: 0.8941 - val_loss: 0.4534 - val_accuracy: 0.7885\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.21917\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3431 - accuracy: 0.88 - ETA: 0s - loss: 0.2471 - accuracy: 0.90 - ETA: 0s - loss: 0.2516 - accuracy: 0.89 - ETA: 0s - loss: 0.2492 - accuracy: 0.89 - ETA: 0s - loss: 0.2568 - accuracy: 0.89 - ETA: 0s - loss: 0.2635 - accuracy: 0.88 - ETA: 0s - loss: 0.2612 - accuracy: 0.88 - ETA: 0s - loss: 0.2622 - accuracy: 0.88 - ETA: 0s - loss: 0.2621 - accuracy: 0.89 - ETA: 0s - loss: 0.2593 - accuracy: 0.89 - ETA: 0s - loss: 0.2604 - accuracy: 0.89 - ETA: 0s - loss: 0.2575 - accuracy: 0.89 - ETA: 0s - loss: 0.2537 - accuracy: 0.89 - ETA: 0s - loss: 0.2536 - accuracy: 0.89 - ETA: 0s - loss: 0.2529 - accuracy: 0.89 - ETA: 0s - loss: 0.2525 - accuracy: 0.89 - 1s 88us/step - loss: 0.2505 - accuracy: 0.8947 - val_loss: 0.2726 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21917\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2782 - accuracy: 0.89 - ETA: 1s - loss: 0.2572 - accuracy: 0.90 - ETA: 1s - loss: 0.2434 - accuracy: 0.90 - ETA: 0s - loss: 0.2380 - accuracy: 0.89 - ETA: 0s - loss: 0.2404 - accuracy: 0.89 - ETA: 0s - loss: 0.2321 - accuracy: 0.90 - ETA: 0s - loss: 0.2308 - accuracy: 0.90 - ETA: 0s - loss: 0.2386 - accuracy: 0.89 - ETA: 0s - loss: 0.2363 - accuracy: 0.89 - ETA: 0s - loss: 0.2372 - accuracy: 0.89 - ETA: 0s - loss: 0.2377 - accuracy: 0.90 - ETA: 0s - loss: 0.2383 - accuracy: 0.89 - ETA: 0s - loss: 0.2392 - accuracy: 0.89 - ETA: 0s - loss: 0.2388 - accuracy: 0.90 - ETA: 0s - loss: 0.2399 - accuracy: 0.89 - ETA: 0s - loss: 0.2399 - accuracy: 0.89 - ETA: 0s - loss: 0.2396 - accuracy: 0.90 - 1s 92us/step - loss: 0.2398 - accuracy: 0.9001 - val_loss: 0.2584 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.21917\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.93 - ETA: 0s - loss: 0.2265 - accuracy: 0.89 - ETA: 0s - loss: 0.2437 - accuracy: 0.89 - ETA: 0s - loss: 0.2451 - accuracy: 0.89 - ETA: 0s - loss: 0.2401 - accuracy: 0.89 - ETA: 0s - loss: 0.2449 - accuracy: 0.89 - ETA: 0s - loss: 0.2489 - accuracy: 0.89 - ETA: 0s - loss: 0.2479 - accuracy: 0.89 - ETA: 0s - loss: 0.2488 - accuracy: 0.89 - ETA: 0s - loss: 0.2440 - accuracy: 0.89 - ETA: 0s - loss: 0.2443 - accuracy: 0.89 - ETA: 0s - loss: 0.2431 - accuracy: 0.89 - ETA: 0s - loss: 0.2435 - accuracy: 0.89 - ETA: 0s - loss: 0.2445 - accuracy: 0.89 - ETA: 0s - loss: 0.2445 - accuracy: 0.89 - ETA: 0s - loss: 0.2442 - accuracy: 0.89 - 1s 85us/step - loss: 0.2445 - accuracy: 0.8949 - val_loss: 0.2307 - val_accuracy: 0.9031\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21917\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.88 - ETA: 0s - loss: 0.2251 - accuracy: 0.90 - ETA: 0s - loss: 0.2197 - accuracy: 0.90 - ETA: 0s - loss: 0.2280 - accuracy: 0.90 - ETA: 0s - loss: 0.2379 - accuracy: 0.89 - ETA: 0s - loss: 0.2327 - accuracy: 0.90 - ETA: 0s - loss: 0.2305 - accuracy: 0.90 - ETA: 0s - loss: 0.2328 - accuracy: 0.89 - ETA: 0s - loss: 0.2321 - accuracy: 0.89 - ETA: 0s - loss: 0.2315 - accuracy: 0.89 - ETA: 0s - loss: 0.2265 - accuracy: 0.90 - ETA: 0s - loss: 0.2282 - accuracy: 0.90 - ETA: 0s - loss: 0.2297 - accuracy: 0.89 - ETA: 0s - loss: 0.2328 - accuracy: 0.89 - ETA: 0s - loss: 0.2326 - accuracy: 0.89 - ETA: 0s - loss: 0.2345 - accuracy: 0.89 - 1s 84us/step - loss: 0.2334 - accuracy: 0.8991 - val_loss: 0.2475 - val_accuracy: 0.8989\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21917\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.93 - ETA: 0s - loss: 0.2390 - accuracy: 0.90 - ETA: 0s - loss: 0.2221 - accuracy: 0.90 - ETA: 0s - loss: 0.2306 - accuracy: 0.90 - ETA: 0s - loss: 0.2304 - accuracy: 0.90 - ETA: 0s - loss: 0.2323 - accuracy: 0.90 - ETA: 0s - loss: 0.2297 - accuracy: 0.90 - ETA: 0s - loss: 0.2299 - accuracy: 0.90 - ETA: 0s - loss: 0.2301 - accuracy: 0.90 - ETA: 0s - loss: 0.2283 - accuracy: 0.90 - ETA: 0s - loss: 0.2276 - accuracy: 0.90 - ETA: 0s - loss: 0.2291 - accuracy: 0.90 - ETA: 0s - loss: 0.2297 - accuracy: 0.90 - ETA: 0s - loss: 0.2276 - accuracy: 0.90 - ETA: 0s - loss: 0.2298 - accuracy: 0.90 - ETA: 0s - loss: 0.2310 - accuracy: 0.90 - 1s 91us/step - loss: 0.2305 - accuracy: 0.9019 - val_loss: 0.3908 - val_accuracy: 0.8543\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21917\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2092 - accuracy: 0.91 - ETA: 0s - loss: 0.2399 - accuracy: 0.90 - ETA: 0s - loss: 0.2260 - accuracy: 0.90 - ETA: 0s - loss: 0.2286 - accuracy: 0.90 - ETA: 0s - loss: 0.2238 - accuracy: 0.91 - ETA: 0s - loss: 0.2232 - accuracy: 0.90 - ETA: 0s - loss: 0.2279 - accuracy: 0.90 - ETA: 0s - loss: 0.2272 - accuracy: 0.90 - ETA: 0s - loss: 0.2264 - accuracy: 0.90 - ETA: 0s - loss: 0.2276 - accuracy: 0.90 - ETA: 0s - loss: 0.2270 - accuracy: 0.90 - ETA: 0s - loss: 0.2295 - accuracy: 0.90 - ETA: 0s - loss: 0.2293 - accuracy: 0.90 - ETA: 0s - loss: 0.2271 - accuracy: 0.90 - ETA: 0s - loss: 0.2263 - accuracy: 0.90 - 1s 83us/step - loss: 0.2255 - accuracy: 0.9052 - val_loss: 0.3540 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.21917\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 2s - loss: 0.1579 - accuracy: 0.95 - ETA: 0s - loss: 0.2005 - accuracy: 0.91 - ETA: 0s - loss: 0.2142 - accuracy: 0.90 - ETA: 0s - loss: 0.2256 - accuracy: 0.89 - ETA: 0s - loss: 0.2243 - accuracy: 0.90 - ETA: 0s - loss: 0.2240 - accuracy: 0.90 - ETA: 0s - loss: 0.2263 - accuracy: 0.90 - ETA: 0s - loss: 0.2346 - accuracy: 0.89 - ETA: 0s - loss: 0.2314 - accuracy: 0.90 - ETA: 0s - loss: 0.2322 - accuracy: 0.89 - ETA: 0s - loss: 0.2299 - accuracy: 0.90 - ETA: 0s - loss: 0.2312 - accuracy: 0.90 - ETA: 0s - loss: 0.2303 - accuracy: 0.90 - ETA: 0s - loss: 0.2288 - accuracy: 0.90 - ETA: 0s - loss: 0.2299 - accuracy: 0.90 - ETA: 0s - loss: 0.2297 - accuracy: 0.90 - 1s 84us/step - loss: 0.2297 - accuracy: 0.9018 - val_loss: 0.2717 - val_accuracy: 0.8896\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.21917\n",
      "Epoch 22/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.88 - ETA: 0s - loss: 0.2276 - accuracy: 0.90 - ETA: 0s - loss: 0.2438 - accuracy: 0.89 - ETA: 0s - loss: 0.2421 - accuracy: 0.89 - ETA: 0s - loss: 0.2364 - accuracy: 0.89 - ETA: 0s - loss: 0.2280 - accuracy: 0.90 - ETA: 0s - loss: 0.2268 - accuracy: 0.90 - ETA: 0s - loss: 0.2244 - accuracy: 0.90 - ETA: 0s - loss: 0.2245 - accuracy: 0.90 - ETA: 0s - loss: 0.2243 - accuracy: 0.90 - ETA: 0s - loss: 0.2218 - accuracy: 0.90 - ETA: 0s - loss: 0.2230 - accuracy: 0.90 - ETA: 0s - loss: 0.2204 - accuracy: 0.90 - ETA: 0s - loss: 0.2236 - accuracy: 0.90 - ETA: 0s - loss: 0.2228 - accuracy: 0.90 - ETA: 0s - loss: 0.2216 - accuracy: 0.90 - 1s 81us/step - loss: 0.2224 - accuracy: 0.9045 - val_loss: 0.3705 - val_accuracy: 0.8509\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.21917\n",
      "Epoch 00022: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 58us/step\n",
      "[2020-05-18 17:23:27 RAM67.4% 0.77GB] Val Score : [0.3028923691087279, 0.857467770576477]\n",
      "[2020-05-18 17:23:27 RAM67.4% 0.77GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:23:27 RAM67.4% 0.77GB] Training on Fold : 9\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 42s - loss: 0.8283 - accuracy: 0.572 - ETA: 8s - loss: 1.0153 - accuracy: 0.685 - ETA: 5s - loss: 0.8547 - accuracy: 0.70 - ETA: 3s - loss: 0.7260 - accuracy: 0.73 - ETA: 2s - loss: 0.6432 - accuracy: 0.75 - ETA: 1s - loss: 0.6061 - accuracy: 0.76 - ETA: 1s - loss: 0.5803 - accuracy: 0.77 - ETA: 0s - loss: 0.5558 - accuracy: 0.77 - ETA: 0s - loss: 0.5404 - accuracy: 0.78 - ETA: 0s - loss: 0.5316 - accuracy: 0.78 - ETA: 0s - loss: 0.5224 - accuracy: 0.78 - ETA: 0s - loss: 0.5126 - accuracy: 0.79 - ETA: 0s - loss: 0.5066 - accuracy: 0.79 - ETA: 0s - loss: 0.5040 - accuracy: 0.79 - ETA: 0s - loss: 0.4933 - accuracy: 0.79 - 2s 147us/step - loss: 0.4922 - accuracy: 0.7950 - val_loss: 2.1499 - val_accuracy: 0.4457\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.21917\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.4751 - accuracy: 0.80 - ETA: 0s - loss: 0.4043 - accuracy: 0.82 - ETA: 0s - loss: 0.3768 - accuracy: 0.83 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.4004 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3818 - accuracy: 0.83 - ETA: 0s - loss: 0.3765 - accuracy: 0.83 - ETA: 0s - loss: 0.3831 - accuracy: 0.83 - ETA: 0s - loss: 0.3809 - accuracy: 0.83 - ETA: 0s - loss: 0.3799 - accuracy: 0.83 - ETA: 0s - loss: 0.3820 - accuracy: 0.83 - ETA: 0s - loss: 0.3820 - accuracy: 0.83 - ETA: 0s - loss: 0.3797 - accuracy: 0.83 - ETA: 0s - loss: 0.3763 - accuracy: 0.83 - 1s 84us/step - loss: 0.3764 - accuracy: 0.8364 - val_loss: 0.8182 - val_accuracy: 0.6849\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.21917\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.88 - ETA: 0s - loss: 0.3112 - accuracy: 0.87 - ETA: 0s - loss: 0.3266 - accuracy: 0.86 - ETA: 0s - loss: 0.3514 - accuracy: 0.85 - ETA: 0s - loss: 0.3457 - accuracy: 0.85 - ETA: 0s - loss: 0.3448 - accuracy: 0.85 - ETA: 0s - loss: 0.3486 - accuracy: 0.85 - ETA: 0s - loss: 0.3466 - accuracy: 0.85 - ETA: 0s - loss: 0.3436 - accuracy: 0.85 - ETA: 0s - loss: 0.3436 - accuracy: 0.85 - ETA: 0s - loss: 0.3470 - accuracy: 0.85 - ETA: 0s - loss: 0.3525 - accuracy: 0.85 - ETA: 0s - loss: 0.3520 - accuracy: 0.85 - ETA: 0s - loss: 0.3476 - accuracy: 0.85 - ETA: 0s - loss: 0.3471 - accuracy: 0.85 - 1s 82us/step - loss: 0.3459 - accuracy: 0.8502 - val_loss: 0.4051 - val_accuracy: 0.8197\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21917\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3344 - accuracy: 0.82 - ETA: 0s - loss: 0.2895 - accuracy: 0.87 - ETA: 0s - loss: 0.3032 - accuracy: 0.87 - ETA: 0s - loss: 0.3226 - accuracy: 0.86 - ETA: 0s - loss: 0.3233 - accuracy: 0.86 - ETA: 0s - loss: 0.3248 - accuracy: 0.86 - ETA: 0s - loss: 0.3188 - accuracy: 0.86 - ETA: 0s - loss: 0.3238 - accuracy: 0.86 - ETA: 0s - loss: 0.3208 - accuracy: 0.86 - ETA: 0s - loss: 0.3217 - accuracy: 0.86 - ETA: 0s - loss: 0.3227 - accuracy: 0.86 - ETA: 0s - loss: 0.3249 - accuracy: 0.86 - ETA: 0s - loss: 0.3240 - accuracy: 0.86 - ETA: 0s - loss: 0.3232 - accuracy: 0.86 - ETA: 0s - loss: 0.3257 - accuracy: 0.86 - ETA: 0s - loss: 0.3242 - accuracy: 0.86 - 1s 84us/step - loss: 0.3246 - accuracy: 0.8611 - val_loss: 0.5167 - val_accuracy: 0.7515\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21917\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.84 - ETA: 0s - loss: 0.3340 - accuracy: 0.87 - ETA: 0s - loss: 0.3392 - accuracy: 0.86 - ETA: 0s - loss: 0.3267 - accuracy: 0.86 - ETA: 0s - loss: 0.3174 - accuracy: 0.86 - ETA: 0s - loss: 0.3133 - accuracy: 0.87 - ETA: 0s - loss: 0.3125 - accuracy: 0.86 - ETA: 0s - loss: 0.3127 - accuracy: 0.86 - ETA: 0s - loss: 0.3171 - accuracy: 0.86 - ETA: 0s - loss: 0.3165 - accuracy: 0.86 - ETA: 0s - loss: 0.3106 - accuracy: 0.86 - ETA: 0s - loss: 0.3103 - accuracy: 0.86 - ETA: 0s - loss: 0.3134 - accuracy: 0.86 - 1s 75us/step - loss: 0.3127 - accuracy: 0.8670 - val_loss: 0.3549 - val_accuracy: 0.8475\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21917\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.86 - ETA: 0s - loss: 0.3007 - accuracy: 0.86 - ETA: 0s - loss: 0.3084 - accuracy: 0.86 - ETA: 0s - loss: 0.3033 - accuracy: 0.86 - ETA: 0s - loss: 0.2989 - accuracy: 0.86 - ETA: 0s - loss: 0.2994 - accuracy: 0.86 - ETA: 0s - loss: 0.2992 - accuracy: 0.87 - ETA: 0s - loss: 0.2991 - accuracy: 0.87 - ETA: 0s - loss: 0.3029 - accuracy: 0.86 - ETA: 0s - loss: 0.3029 - accuracy: 0.86 - ETA: 0s - loss: 0.3021 - accuracy: 0.86 - ETA: 0s - loss: 0.2985 - accuracy: 0.87 - ETA: 0s - loss: 0.2971 - accuracy: 0.87 - 1s 76us/step - loss: 0.2966 - accuracy: 0.8710 - val_loss: 0.3654 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21917\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2283 - accuracy: 0.90 - ETA: 0s - loss: 0.2440 - accuracy: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.88 - ETA: 0s - loss: 0.2646 - accuracy: 0.88 - ETA: 0s - loss: 0.2756 - accuracy: 0.88 - ETA: 0s - loss: 0.2825 - accuracy: 0.87 - ETA: 0s - loss: 0.2755 - accuracy: 0.87 - ETA: 0s - loss: 0.2814 - accuracy: 0.87 - ETA: 0s - loss: 0.2815 - accuracy: 0.87 - ETA: 0s - loss: 0.2837 - accuracy: 0.87 - ETA: 0s - loss: 0.2821 - accuracy: 0.87 - ETA: 0s - loss: 0.2830 - accuracy: 0.87 - ETA: 0s - loss: 0.2823 - accuracy: 0.87 - 1s 76us/step - loss: 0.2820 - accuracy: 0.8792 - val_loss: 0.4030 - val_accuracy: 0.8020\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21917\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2397 - accuracy: 0.91 - ETA: 0s - loss: 0.3023 - accuracy: 0.87 - ETA: 0s - loss: 0.2951 - accuracy: 0.87 - ETA: 0s - loss: 0.2836 - accuracy: 0.88 - ETA: 0s - loss: 0.2783 - accuracy: 0.88 - ETA: 0s - loss: 0.2738 - accuracy: 0.88 - ETA: 0s - loss: 0.2804 - accuracy: 0.88 - ETA: 0s - loss: 0.2795 - accuracy: 0.88 - ETA: 0s - loss: 0.2775 - accuracy: 0.88 - ETA: 0s - loss: 0.2781 - accuracy: 0.88 - ETA: 0s - loss: 0.2769 - accuracy: 0.88 - ETA: 0s - loss: 0.2768 - accuracy: 0.88 - ETA: 0s - loss: 0.2753 - accuracy: 0.88 - 1s 79us/step - loss: 0.2745 - accuracy: 0.8841 - val_loss: 0.2753 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21917\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.87 - ETA: 0s - loss: 0.3038 - accuracy: 0.87 - ETA: 0s - loss: 0.2796 - accuracy: 0.88 - ETA: 0s - loss: 0.2688 - accuracy: 0.88 - ETA: 0s - loss: 0.2704 - accuracy: 0.88 - ETA: 0s - loss: 0.2709 - accuracy: 0.88 - ETA: 0s - loss: 0.2698 - accuracy: 0.88 - ETA: 0s - loss: 0.2717 - accuracy: 0.88 - ETA: 0s - loss: 0.2701 - accuracy: 0.88 - ETA: 0s - loss: 0.2716 - accuracy: 0.88 - ETA: 0s - loss: 0.2704 - accuracy: 0.88 - ETA: 0s - loss: 0.2695 - accuracy: 0.88 - ETA: 0s - loss: 0.2708 - accuracy: 0.88 - ETA: 0s - loss: 0.2714 - accuracy: 0.88 - ETA: 0s - loss: 0.2690 - accuracy: 0.88 - ETA: 0s - loss: 0.2694 - accuracy: 0.88 - 1s 81us/step - loss: 0.2694 - accuracy: 0.8845 - val_loss: 0.2631 - val_accuracy: 0.8880\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21917\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2036 - accuracy: 0.91 - ETA: 0s - loss: 0.2762 - accuracy: 0.88 - ETA: 0s - loss: 0.2567 - accuracy: 0.88 - ETA: 0s - loss: 0.2543 - accuracy: 0.89 - ETA: 0s - loss: 0.2663 - accuracy: 0.88 - ETA: 0s - loss: 0.2624 - accuracy: 0.88 - ETA: 0s - loss: 0.2607 - accuracy: 0.89 - ETA: 0s - loss: 0.2635 - accuracy: 0.89 - ETA: 0s - loss: 0.2625 - accuracy: 0.89 - ETA: 0s - loss: 0.2643 - accuracy: 0.88 - ETA: 0s - loss: 0.2664 - accuracy: 0.88 - ETA: 0s - loss: 0.2644 - accuracy: 0.88 - ETA: 0s - loss: 0.2624 - accuracy: 0.88 - ETA: 0s - loss: 0.2623 - accuracy: 0.88 - ETA: 0s - loss: 0.2626 - accuracy: 0.88 - 1s 80us/step - loss: 0.2621 - accuracy: 0.8892 - val_loss: 0.3251 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21917\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.90 - ETA: 0s - loss: 0.2520 - accuracy: 0.90 - ETA: 0s - loss: 0.2372 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.89 - ETA: 0s - loss: 0.2593 - accuracy: 0.88 - ETA: 0s - loss: 0.2672 - accuracy: 0.88 - ETA: 0s - loss: 0.2650 - accuracy: 0.88 - ETA: 0s - loss: 0.2708 - accuracy: 0.88 - ETA: 0s - loss: 0.2703 - accuracy: 0.88 - ETA: 0s - loss: 0.2686 - accuracy: 0.88 - ETA: 0s - loss: 0.2663 - accuracy: 0.88 - ETA: 0s - loss: 0.2662 - accuracy: 0.88 - ETA: 0s - loss: 0.2633 - accuracy: 0.88 - ETA: 0s - loss: 0.2605 - accuracy: 0.88 - ETA: 0s - loss: 0.2603 - accuracy: 0.88 - 1s 82us/step - loss: 0.2596 - accuracy: 0.8894 - val_loss: 0.3074 - val_accuracy: 0.8871\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21917\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2811 - accuracy: 0.85 - ETA: 0s - loss: 0.2758 - accuracy: 0.87 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2449 - accuracy: 0.89 - ETA: 0s - loss: 0.2487 - accuracy: 0.89 - ETA: 0s - loss: 0.2495 - accuracy: 0.89 - ETA: 0s - loss: 0.2522 - accuracy: 0.89 - ETA: 0s - loss: 0.2533 - accuracy: 0.89 - ETA: 0s - loss: 0.2534 - accuracy: 0.89 - ETA: 0s - loss: 0.2520 - accuracy: 0.89 - ETA: 0s - loss: 0.2519 - accuracy: 0.89 - ETA: 0s - loss: 0.2529 - accuracy: 0.89 - ETA: 0s - loss: 0.2530 - accuracy: 0.89 - ETA: 0s - loss: 0.2523 - accuracy: 0.89 - ETA: 0s - loss: 0.2515 - accuracy: 0.89 - 1s 83us/step - loss: 0.2515 - accuracy: 0.8930 - val_loss: 0.3513 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21917\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.89 - ETA: 0s - loss: 0.2874 - accuracy: 0.87 - ETA: 0s - loss: 0.2632 - accuracy: 0.88 - ETA: 0s - loss: 0.2513 - accuracy: 0.89 - ETA: 0s - loss: 0.2503 - accuracy: 0.89 - ETA: 0s - loss: 0.2494 - accuracy: 0.89 - ETA: 0s - loss: 0.2514 - accuracy: 0.88 - ETA: 0s - loss: 0.2516 - accuracy: 0.88 - ETA: 0s - loss: 0.2546 - accuracy: 0.88 - ETA: 0s - loss: 0.2533 - accuracy: 0.88 - ETA: 0s - loss: 0.2530 - accuracy: 0.89 - ETA: 0s - loss: 0.2534 - accuracy: 0.88 - ETA: 0s - loss: 0.2545 - accuracy: 0.88 - ETA: 0s - loss: 0.2556 - accuracy: 0.88 - ETA: 0s - loss: 0.2538 - accuracy: 0.88 - ETA: 0s - loss: 0.2542 - accuracy: 0.88 - 1s 86us/step - loss: 0.2539 - accuracy: 0.8898 - val_loss: 0.2772 - val_accuracy: 0.8955\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.21917\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2454 - accuracy: 0.88 - ETA: 0s - loss: 0.2453 - accuracy: 0.88 - ETA: 0s - loss: 0.2571 - accuracy: 0.88 - ETA: 0s - loss: 0.2545 - accuracy: 0.89 - ETA: 0s - loss: 0.2526 - accuracy: 0.89 - ETA: 0s - loss: 0.2573 - accuracy: 0.89 - ETA: 0s - loss: 0.2553 - accuracy: 0.88 - ETA: 0s - loss: 0.2567 - accuracy: 0.89 - ETA: 0s - loss: 0.2571 - accuracy: 0.89 - ETA: 0s - loss: 0.2519 - accuracy: 0.89 - ETA: 0s - loss: 0.2511 - accuracy: 0.89 - ETA: 0s - loss: 0.2523 - accuracy: 0.89 - ETA: 0s - loss: 0.2521 - accuracy: 0.89 - ETA: 0s - loss: 0.2484 - accuracy: 0.89 - ETA: 0s - loss: 0.2486 - accuracy: 0.89 - ETA: 0s - loss: 0.2450 - accuracy: 0.89 - ETA: 0s - loss: 0.2459 - accuracy: 0.89 - ETA: 0s - loss: 0.2456 - accuracy: 0.89 - ETA: 0s - loss: 0.2449 - accuracy: 0.89 - 1s 102us/step - loss: 0.2469 - accuracy: 0.8943 - val_loss: 0.2727 - val_accuracy: 0.8955\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.21917\n",
      "Epoch 00014: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 64us/step\n",
      "[2020-05-18 17:23:42 RAM68.4% 0.82GB] Val Score : [0.22362806479121086, 0.8984078764915466]\n",
      "[2020-05-18 17:23:42 RAM68.4% 0.82GB] ============================================================================================================================================================\n",
      "\n",
      "\n",
      "[2020-05-18 17:23:42 RAM68.4% 0.82GB] Training on Fold : 10\n",
      "Train on 10677 samples, validate on 1187 samples\n",
      "Epoch 1/50\n",
      "10677/10677 [==============================] - ETA: 1:08 - loss: 1.0372 - accuracy: 0.58 - ETA: 13s - loss: 0.9954 - accuracy: 0.6497 - ETA: 7s - loss: 0.8389 - accuracy: 0.708 - ETA: 5s - loss: 0.7567 - accuracy: 0.72 - ETA: 4s - loss: 0.6836 - accuracy: 0.74 - ETA: 3s - loss: 0.6487 - accuracy: 0.74 - ETA: 2s - loss: 0.6060 - accuracy: 0.76 - ETA: 1s - loss: 0.5818 - accuracy: 0.76 - ETA: 1s - loss: 0.5583 - accuracy: 0.77 - ETA: 1s - loss: 0.5459 - accuracy: 0.77 - ETA: 0s - loss: 0.5270 - accuracy: 0.78 - ETA: 0s - loss: 0.5226 - accuracy: 0.78 - ETA: 0s - loss: 0.5167 - accuracy: 0.78 - ETA: 0s - loss: 0.5055 - accuracy: 0.79 - ETA: 0s - loss: 0.4994 - accuracy: 0.79 - ETA: 0s - loss: 0.4930 - accuracy: 0.79 - ETA: 0s - loss: 0.4924 - accuracy: 0.79 - 2s 191us/step - loss: 0.4912 - accuracy: 0.7949 - val_loss: 1.4177 - val_accuracy: 0.5880\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.21917\n",
      "Epoch 2/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.4247 - accuracy: 0.82 - ETA: 0s - loss: 0.4162 - accuracy: 0.83 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.4053 - accuracy: 0.83 - ETA: 0s - loss: 0.4000 - accuracy: 0.83 - ETA: 0s - loss: 0.3928 - accuracy: 0.83 - ETA: 0s - loss: 0.3845 - accuracy: 0.83 - ETA: 0s - loss: 0.3830 - accuracy: 0.83 - ETA: 0s - loss: 0.3904 - accuracy: 0.83 - ETA: 0s - loss: 0.3848 - accuracy: 0.83 - ETA: 0s - loss: 0.3823 - accuracy: 0.83 - ETA: 0s - loss: 0.3777 - accuracy: 0.83 - ETA: 0s - loss: 0.3756 - accuracy: 0.83 - ETA: 0s - loss: 0.3822 - accuracy: 0.83 - ETA: 0s - loss: 0.3814 - accuracy: 0.83 - ETA: 0s - loss: 0.3824 - accuracy: 0.83 - ETA: 0s - loss: 0.3829 - accuracy: 0.83 - ETA: 0s - loss: 0.3835 - accuracy: 0.83 - ETA: 0s - loss: 0.3810 - accuracy: 0.83 - 1s 100us/step - loss: 0.3816 - accuracy: 0.8333 - val_loss: 0.6710 - val_accuracy: 0.7161\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.21917\n",
      "Epoch 3/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.84 - ETA: 0s - loss: 0.3618 - accuracy: 0.83 - ETA: 0s - loss: 0.3412 - accuracy: 0.84 - ETA: 0s - loss: 0.3526 - accuracy: 0.84 - ETA: 0s - loss: 0.3485 - accuracy: 0.84 - ETA: 0s - loss: 0.3440 - accuracy: 0.84 - ETA: 0s - loss: 0.3425 - accuracy: 0.84 - ETA: 0s - loss: 0.3493 - accuracy: 0.84 - ETA: 0s - loss: 0.3467 - accuracy: 0.84 - ETA: 0s - loss: 0.3465 - accuracy: 0.84 - ETA: 0s - loss: 0.3493 - accuracy: 0.84 - ETA: 0s - loss: 0.3501 - accuracy: 0.84 - ETA: 0s - loss: 0.3514 - accuracy: 0.84 - ETA: 0s - loss: 0.3500 - accuracy: 0.84 - ETA: 0s - loss: 0.3478 - accuracy: 0.84 - ETA: 0s - loss: 0.3483 - accuracy: 0.84 - ETA: 0s - loss: 0.3493 - accuracy: 0.84 - 1s 92us/step - loss: 0.3477 - accuracy: 0.8487 - val_loss: 0.4079 - val_accuracy: 0.8248\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21917\n",
      "Epoch 4/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2927 - accuracy: 0.84 - ETA: 1s - loss: 0.3405 - accuracy: 0.85 - ETA: 0s - loss: 0.3650 - accuracy: 0.83 - ETA: 0s - loss: 0.3534 - accuracy: 0.84 - ETA: 0s - loss: 0.3497 - accuracy: 0.84 - ETA: 0s - loss: 0.3494 - accuracy: 0.84 - ETA: 0s - loss: 0.3356 - accuracy: 0.85 - ETA: 0s - loss: 0.3335 - accuracy: 0.85 - ETA: 0s - loss: 0.3329 - accuracy: 0.85 - ETA: 0s - loss: 0.3320 - accuracy: 0.85 - ETA: 0s - loss: 0.3300 - accuracy: 0.85 - ETA: 0s - loss: 0.3300 - accuracy: 0.85 - ETA: 0s - loss: 0.3318 - accuracy: 0.85 - ETA: 0s - loss: 0.3305 - accuracy: 0.85 - ETA: 0s - loss: 0.3279 - accuracy: 0.85 - ETA: 0s - loss: 0.3275 - accuracy: 0.85 - ETA: 0s - loss: 0.3252 - accuracy: 0.85 - 1s 92us/step - loss: 0.3253 - accuracy: 0.8565 - val_loss: 0.3221 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21917\n",
      "Epoch 5/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.84 - ETA: 0s - loss: 0.3391 - accuracy: 0.84 - ETA: 0s - loss: 0.3121 - accuracy: 0.85 - ETA: 0s - loss: 0.3081 - accuracy: 0.85 - ETA: 0s - loss: 0.3066 - accuracy: 0.86 - ETA: 0s - loss: 0.3072 - accuracy: 0.86 - ETA: 0s - loss: 0.3123 - accuracy: 0.86 - ETA: 0s - loss: 0.3134 - accuracy: 0.86 - ETA: 0s - loss: 0.3151 - accuracy: 0.85 - ETA: 0s - loss: 0.3081 - accuracy: 0.86 - ETA: 0s - loss: 0.3079 - accuracy: 0.86 - ETA: 0s - loss: 0.3088 - accuracy: 0.86 - ETA: 0s - loss: 0.3054 - accuracy: 0.86 - ETA: 0s - loss: 0.3063 - accuracy: 0.86 - ETA: 0s - loss: 0.3067 - accuracy: 0.86 - ETA: 0s - loss: 0.3074 - accuracy: 0.86 - 1s 83us/step - loss: 0.3069 - accuracy: 0.8654 - val_loss: 0.2761 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.21917\n",
      "Epoch 6/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.91 - ETA: 0s - loss: 0.2664 - accuracy: 0.88 - ETA: 0s - loss: 0.2613 - accuracy: 0.89 - ETA: 0s - loss: 0.2585 - accuracy: 0.89 - ETA: 0s - loss: 0.2775 - accuracy: 0.88 - ETA: 0s - loss: 0.2816 - accuracy: 0.87 - ETA: 0s - loss: 0.2836 - accuracy: 0.87 - ETA: 0s - loss: 0.2871 - accuracy: 0.87 - ETA: 0s - loss: 0.2880 - accuracy: 0.87 - ETA: 0s - loss: 0.2891 - accuracy: 0.87 - ETA: 0s - loss: 0.2909 - accuracy: 0.87 - ETA: 0s - loss: 0.2932 - accuracy: 0.87 - ETA: 0s - loss: 0.2935 - accuracy: 0.87 - ETA: 0s - loss: 0.2961 - accuracy: 0.87 - ETA: 0s - loss: 0.2961 - accuracy: 0.87 - ETA: 0s - loss: 0.2968 - accuracy: 0.87 - 1s 83us/step - loss: 0.2986 - accuracy: 0.8714 - val_loss: 0.3103 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21917\n",
      "Epoch 7/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3251 - accuracy: 0.84 - ETA: 0s - loss: 0.2984 - accuracy: 0.87 - ETA: 0s - loss: 0.2842 - accuracy: 0.87 - ETA: 0s - loss: 0.2920 - accuracy: 0.87 - ETA: 0s - loss: 0.2827 - accuracy: 0.87 - ETA: 0s - loss: 0.2884 - accuracy: 0.87 - ETA: 0s - loss: 0.2872 - accuracy: 0.87 - ETA: 0s - loss: 0.2902 - accuracy: 0.87 - ETA: 0s - loss: 0.2914 - accuracy: 0.87 - ETA: 0s - loss: 0.2901 - accuracy: 0.87 - ETA: 0s - loss: 0.2903 - accuracy: 0.87 - ETA: 0s - loss: 0.2911 - accuracy: 0.87 - ETA: 0s - loss: 0.2900 - accuracy: 0.87 - ETA: 0s - loss: 0.2894 - accuracy: 0.87 - ETA: 0s - loss: 0.2885 - accuracy: 0.87 - ETA: 0s - loss: 0.2885 - accuracy: 0.87 - 1s 87us/step - loss: 0.2887 - accuracy: 0.8781 - val_loss: 0.2917 - val_accuracy: 0.8703\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21917\n",
      "Epoch 8/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2784 - accuracy: 0.88 - ETA: 0s - loss: 0.2793 - accuracy: 0.88 - ETA: 0s - loss: 0.2988 - accuracy: 0.87 - ETA: 0s - loss: 0.3057 - accuracy: 0.86 - ETA: 0s - loss: 0.2902 - accuracy: 0.87 - ETA: 0s - loss: 0.2899 - accuracy: 0.87 - ETA: 0s - loss: 0.2909 - accuracy: 0.87 - ETA: 0s - loss: 0.2863 - accuracy: 0.87 - ETA: 0s - loss: 0.2814 - accuracy: 0.87 - ETA: 0s - loss: 0.2877 - accuracy: 0.87 - ETA: 0s - loss: 0.2849 - accuracy: 0.87 - ETA: 0s - loss: 0.2820 - accuracy: 0.87 - ETA: 0s - loss: 0.2787 - accuracy: 0.88 - ETA: 0s - loss: 0.2811 - accuracy: 0.87 - ETA: 0s - loss: 0.2781 - accuracy: 0.87 - ETA: 0s - loss: 0.2780 - accuracy: 0.87 - 1s 84us/step - loss: 0.2786 - accuracy: 0.8790 - val_loss: 0.3328 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21917\n",
      "Epoch 9/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2487 - accuracy: 0.88 - ETA: 0s - loss: 0.2718 - accuracy: 0.88 - ETA: 0s - loss: 0.2893 - accuracy: 0.87 - ETA: 0s - loss: 0.2813 - accuracy: 0.87 - ETA: 0s - loss: 0.2814 - accuracy: 0.87 - ETA: 0s - loss: 0.2819 - accuracy: 0.87 - ETA: 0s - loss: 0.2787 - accuracy: 0.87 - ETA: 0s - loss: 0.2752 - accuracy: 0.88 - ETA: 0s - loss: 0.2728 - accuracy: 0.88 - ETA: 0s - loss: 0.2761 - accuracy: 0.88 - ETA: 0s - loss: 0.2742 - accuracy: 0.88 - ETA: 0s - loss: 0.2745 - accuracy: 0.88 - ETA: 0s - loss: 0.2732 - accuracy: 0.88 - ETA: 0s - loss: 0.2732 - accuracy: 0.88 - ETA: 0s - loss: 0.2730 - accuracy: 0.88 - ETA: 0s - loss: 0.2729 - accuracy: 0.88 - 1s 85us/step - loss: 0.2739 - accuracy: 0.8822 - val_loss: 0.3055 - val_accuracy: 0.8576\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21917\n",
      "Epoch 10/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.88 - ETA: 0s - loss: 0.2463 - accuracy: 0.89 - ETA: 0s - loss: 0.2485 - accuracy: 0.89 - ETA: 0s - loss: 0.2495 - accuracy: 0.89 - ETA: 0s - loss: 0.2563 - accuracy: 0.89 - ETA: 0s - loss: 0.2596 - accuracy: 0.89 - ETA: 0s - loss: 0.2526 - accuracy: 0.89 - ETA: 0s - loss: 0.2618 - accuracy: 0.89 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2596 - accuracy: 0.89 - ETA: 0s - loss: 0.2591 - accuracy: 0.89 - ETA: 0s - loss: 0.2603 - accuracy: 0.89 - ETA: 0s - loss: 0.2594 - accuracy: 0.89 - ETA: 0s - loss: 0.2587 - accuracy: 0.89 - ETA: 0s - loss: 0.2620 - accuracy: 0.89 - ETA: 0s - loss: 0.2652 - accuracy: 0.88 - ETA: 0s - loss: 0.2634 - accuracy: 0.88 - 1s 94us/step - loss: 0.2624 - accuracy: 0.8893 - val_loss: 0.2406 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21917\n",
      "Epoch 11/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.84 - ETA: 0s - loss: 0.2684 - accuracy: 0.87 - ETA: 0s - loss: 0.2643 - accuracy: 0.88 - ETA: 0s - loss: 0.2661 - accuracy: 0.88 - ETA: 0s - loss: 0.2591 - accuracy: 0.88 - ETA: 0s - loss: 0.2590 - accuracy: 0.88 - ETA: 0s - loss: 0.2550 - accuracy: 0.89 - ETA: 0s - loss: 0.2568 - accuracy: 0.89 - ETA: 0s - loss: 0.2571 - accuracy: 0.89 - ETA: 0s - loss: 0.2579 - accuracy: 0.89 - ETA: 0s - loss: 0.2560 - accuracy: 0.89 - ETA: 0s - loss: 0.2583 - accuracy: 0.88 - ETA: 0s - loss: 0.2561 - accuracy: 0.89 - ETA: 0s - loss: 0.2602 - accuracy: 0.88 - ETA: 0s - loss: 0.2606 - accuracy: 0.88 - ETA: 0s - loss: 0.2597 - accuracy: 0.88 - 1s 89us/step - loss: 0.2578 - accuracy: 0.8897 - val_loss: 0.2647 - val_accuracy: 0.8863\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21917\n",
      "Epoch 12/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.94 - ETA: 0s - loss: 0.2568 - accuracy: 0.88 - ETA: 0s - loss: 0.2559 - accuracy: 0.88 - ETA: 0s - loss: 0.2660 - accuracy: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.88 - ETA: 0s - loss: 0.2667 - accuracy: 0.88 - ETA: 0s - loss: 0.2621 - accuracy: 0.88 - ETA: 0s - loss: 0.2641 - accuracy: 0.88 - ETA: 0s - loss: 0.2607 - accuracy: 0.88 - ETA: 0s - loss: 0.2633 - accuracy: 0.88 - ETA: 0s - loss: 0.2620 - accuracy: 0.88 - ETA: 0s - loss: 0.2590 - accuracy: 0.88 - ETA: 0s - loss: 0.2579 - accuracy: 0.88 - ETA: 0s - loss: 0.2595 - accuracy: 0.88 - ETA: 0s - loss: 0.2597 - accuracy: 0.88 - 1s 81us/step - loss: 0.2593 - accuracy: 0.8862 - val_loss: 0.3096 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21917\n",
      "Epoch 13/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.88 - ETA: 0s - loss: 0.2367 - accuracy: 0.90 - ETA: 0s - loss: 0.2459 - accuracy: 0.89 - ETA: 0s - loss: 0.2382 - accuracy: 0.90 - ETA: 0s - loss: 0.2350 - accuracy: 0.90 - ETA: 0s - loss: 0.2421 - accuracy: 0.89 - ETA: 0s - loss: 0.2431 - accuracy: 0.89 - ETA: 0s - loss: 0.2446 - accuracy: 0.89 - ETA: 0s - loss: 0.2483 - accuracy: 0.89 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2486 - accuracy: 0.89 - ETA: 0s - loss: 0.2493 - accuracy: 0.89 - ETA: 0s - loss: 0.2506 - accuracy: 0.89 - ETA: 0s - loss: 0.2498 - accuracy: 0.89 - ETA: 0s - loss: 0.2514 - accuracy: 0.89 - 1s 83us/step - loss: 0.2516 - accuracy: 0.8914 - val_loss: 0.2554 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.21917\n",
      "Epoch 14/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.88 - ETA: 0s - loss: 0.2618 - accuracy: 0.87 - ETA: 0s - loss: 0.2633 - accuracy: 0.87 - ETA: 0s - loss: 0.2506 - accuracy: 0.88 - ETA: 0s - loss: 0.2432 - accuracy: 0.89 - ETA: 0s - loss: 0.2439 - accuracy: 0.89 - ETA: 0s - loss: 0.2483 - accuracy: 0.89 - ETA: 0s - loss: 0.2473 - accuracy: 0.89 - ETA: 0s - loss: 0.2480 - accuracy: 0.89 - ETA: 0s - loss: 0.2484 - accuracy: 0.89 - ETA: 0s - loss: 0.2531 - accuracy: 0.88 - ETA: 0s - loss: 0.2535 - accuracy: 0.89 - ETA: 0s - loss: 0.2567 - accuracy: 0.88 - ETA: 0s - loss: 0.2564 - accuracy: 0.88 - ETA: 0s - loss: 0.2538 - accuracy: 0.89 - ETA: 0s - loss: 0.2525 - accuracy: 0.89 - 1s 82us/step - loss: 0.2521 - accuracy: 0.8923 - val_loss: 0.2346 - val_accuracy: 0.9065\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.21917\n",
      "Epoch 15/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.87 - ETA: 0s - loss: 0.2385 - accuracy: 0.89 - ETA: 0s - loss: 0.2400 - accuracy: 0.89 - ETA: 0s - loss: 0.2410 - accuracy: 0.90 - ETA: 0s - loss: 0.2327 - accuracy: 0.90 - ETA: 0s - loss: 0.2379 - accuracy: 0.89 - ETA: 0s - loss: 0.2455 - accuracy: 0.89 - ETA: 0s - loss: 0.2459 - accuracy: 0.89 - ETA: 0s - loss: 0.2436 - accuracy: 0.89 - ETA: 0s - loss: 0.2439 - accuracy: 0.89 - ETA: 0s - loss: 0.2432 - accuracy: 0.89 - ETA: 0s - loss: 0.2423 - accuracy: 0.89 - ETA: 0s - loss: 0.2413 - accuracy: 0.89 - ETA: 0s - loss: 0.2407 - accuracy: 0.89 - ETA: 0s - loss: 0.2413 - accuracy: 0.89 - ETA: 0s - loss: 0.2435 - accuracy: 0.89 - 1s 82us/step - loss: 0.2440 - accuracy: 0.8949 - val_loss: 0.2628 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21917\n",
      "Epoch 16/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.89 - ETA: 0s - loss: 0.2488 - accuracy: 0.89 - ETA: 1s - loss: 0.2359 - accuracy: 0.90 - ETA: 0s - loss: 0.2373 - accuracy: 0.90 - ETA: 0s - loss: 0.2317 - accuracy: 0.90 - ETA: 0s - loss: 0.2321 - accuracy: 0.90 - ETA: 0s - loss: 0.2298 - accuracy: 0.90 - ETA: 0s - loss: 0.2347 - accuracy: 0.90 - ETA: 0s - loss: 0.2359 - accuracy: 0.89 - ETA: 0s - loss: 0.2388 - accuracy: 0.89 - ETA: 0s - loss: 0.2372 - accuracy: 0.89 - ETA: 0s - loss: 0.2376 - accuracy: 0.89 - ETA: 0s - loss: 0.2361 - accuracy: 0.89 - ETA: 0s - loss: 0.2385 - accuracy: 0.89 - ETA: 0s - loss: 0.2406 - accuracy: 0.89 - ETA: 0s - loss: 0.2414 - accuracy: 0.89 - 1s 82us/step - loss: 0.2425 - accuracy: 0.8959 - val_loss: 0.2323 - val_accuracy: 0.9006\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.21917\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10677/10677 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.88 - ETA: 0s - loss: 0.2468 - accuracy: 0.90 - ETA: 0s - loss: 0.2467 - accuracy: 0.89 - ETA: 0s - loss: 0.2531 - accuracy: 0.89 - ETA: 0s - loss: 0.2533 - accuracy: 0.89 - ETA: 0s - loss: 0.2484 - accuracy: 0.89 - ETA: 0s - loss: 0.2416 - accuracy: 0.89 - ETA: 0s - loss: 0.2443 - accuracy: 0.89 - ETA: 0s - loss: 0.2430 - accuracy: 0.89 - ETA: 0s - loss: 0.2413 - accuracy: 0.89 - ETA: 0s - loss: 0.2421 - accuracy: 0.89 - ETA: 0s - loss: 0.2405 - accuracy: 0.89 - ETA: 0s - loss: 0.2416 - accuracy: 0.89 - ETA: 0s - loss: 0.2426 - accuracy: 0.89 - 1s 77us/step - loss: 0.2427 - accuracy: 0.8979 - val_loss: 0.2560 - val_accuracy: 0.8989\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21917\n",
      "Epoch 18/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2322 - accuracy: 0.89 - ETA: 0s - loss: 0.2359 - accuracy: 0.90 - ETA: 0s - loss: 0.2411 - accuracy: 0.89 - ETA: 0s - loss: 0.2359 - accuracy: 0.90 - ETA: 0s - loss: 0.2285 - accuracy: 0.90 - ETA: 0s - loss: 0.2273 - accuracy: 0.90 - ETA: 0s - loss: 0.2293 - accuracy: 0.90 - ETA: 0s - loss: 0.2305 - accuracy: 0.90 - ETA: 0s - loss: 0.2312 - accuracy: 0.90 - ETA: 0s - loss: 0.2323 - accuracy: 0.90 - ETA: 0s - loss: 0.2312 - accuracy: 0.90 - ETA: 0s - loss: 0.2334 - accuracy: 0.89 - ETA: 0s - loss: 0.2310 - accuracy: 0.90 - ETA: 0s - loss: 0.2320 - accuracy: 0.90 - 1s 75us/step - loss: 0.2321 - accuracy: 0.9005 - val_loss: 0.2426 - val_accuracy: 0.9006\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21917\n",
      "Epoch 19/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.2330 - accuracy: 0.88 - ETA: 0s - loss: 0.1957 - accuracy: 0.91 - ETA: 0s - loss: 0.2066 - accuracy: 0.91 - ETA: 0s - loss: 0.2223 - accuracy: 0.90 - ETA: 0s - loss: 0.2185 - accuracy: 0.91 - ETA: 0s - loss: 0.2202 - accuracy: 0.90 - ETA: 0s - loss: 0.2187 - accuracy: 0.90 - ETA: 0s - loss: 0.2201 - accuracy: 0.90 - ETA: 0s - loss: 0.2248 - accuracy: 0.90 - ETA: 0s - loss: 0.2260 - accuracy: 0.90 - ETA: 0s - loss: 0.2245 - accuracy: 0.90 - ETA: 0s - loss: 0.2249 - accuracy: 0.90 - ETA: 0s - loss: 0.2254 - accuracy: 0.90 - ETA: 0s - loss: 0.2264 - accuracy: 0.90 - 1s 75us/step - loss: 0.2275 - accuracy: 0.9049 - val_loss: 0.2569 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21917\n",
      "Epoch 20/50\n",
      "10677/10677 [==============================] - ETA: 1s - loss: 0.3259 - accuracy: 0.83 - ETA: 0s - loss: 0.2415 - accuracy: 0.88 - ETA: 0s - loss: 0.2355 - accuracy: 0.89 - ETA: 0s - loss: 0.2272 - accuracy: 0.90 - ETA: 0s - loss: 0.2307 - accuracy: 0.90 - ETA: 0s - loss: 0.2289 - accuracy: 0.90 - ETA: 0s - loss: 0.2341 - accuracy: 0.89 - ETA: 0s - loss: 0.2321 - accuracy: 0.89 - ETA: 0s - loss: 0.2327 - accuracy: 0.89 - ETA: 0s - loss: 0.2332 - accuracy: 0.89 - ETA: 0s - loss: 0.2349 - accuracy: 0.89 - ETA: 0s - loss: 0.2338 - accuracy: 0.89 - ETA: 0s - loss: 0.2325 - accuracy: 0.89 - 1s 74us/step - loss: 0.2332 - accuracy: 0.8987 - val_loss: 0.4627 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.21917\n",
      "Epoch 21/50\n",
      "10677/10677 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.93 - ETA: 0s - loss: 0.2012 - accuracy: 0.92 - ETA: 0s - loss: 0.2265 - accuracy: 0.90 - ETA: 0s - loss: 0.2201 - accuracy: 0.90 - ETA: 0s - loss: 0.2212 - accuracy: 0.90 - ETA: 0s - loss: 0.2219 - accuracy: 0.90 - ETA: 0s - loss: 0.2210 - accuracy: 0.90 - ETA: 0s - loss: 0.2210 - accuracy: 0.90 - ETA: 0s - loss: 0.2257 - accuracy: 0.90 - ETA: 0s - loss: 0.2250 - accuracy: 0.90 - ETA: 0s - loss: 0.2271 - accuracy: 0.90 - ETA: 0s - loss: 0.2257 - accuracy: 0.90 - ETA: 0s - loss: 0.2263 - accuracy: 0.90 - 1s 73us/step - loss: 0.2261 - accuracy: 0.9043 - val_loss: 0.2585 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.21917\n",
      "Epoch 00021: early stopping\n",
      "1319/1319 [==============================] - ETA:  - ETA:  - 0s 57us/step\n",
      "[2020-05-18 17:24:04 RAM70.4% 0.87GB] Val Score : [0.2228285336953117, 0.9075056910514832]\n",
      "[2020-05-18 17:24:04 RAM70.4% 0.87GB] ============================================================================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds=10\n",
    "epochs=50\n",
    "batch_size=145\n",
    "\n",
    "# save the model history in a list after fitting so we can plot later \n",
    "model_history=[]\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold :\", i+1)\n",
    "    x_t, x_v, y_t, y_v = train_test_split(train_x, train_y, test_size=0.1, random_state = 2020)\n",
    "    model_history.append(fit_and_evaluate(x_t, x_v, y_t, y_v, epochs, batch_size))\n",
    "    print(\"=============\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-73b24992616a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DenseNet_bee_1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model2 = load_model('DenseNet_bee_1.h5')\n",
    "model2.evaluate(test_x, test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels: \n",
    "predicted_classes= model2.predict(test_x)\n",
    "\n",
    "predicted_classes=np.argmax(np.round(predicted_classes), axis=1)\n",
    "rounded_predictions = model2.predict_classes(test_x, batch_size=128, verbose=0)\n",
    "rounded_labels=np.argmax(test_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 17:25:45 RAM70.0% 0.89GB] Confusion matrix, without normalization\n",
      "[2020-05-18 17:25:45 RAM70.0% 0.89GB] [[ 665  153]\n",
      " [  26 3674]]\n",
      "[2020-05-18 17:25:45 RAM70.0% 0.89GB] Normalized confusion matrix\n",
      "[2020-05-18 17:25:45 RAM70.0% 0.89GB] [[0.81 0.19]\n",
      " [0.01 0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-18 17:25:46 RAM70.0% 0.89GB] \n",
      "Clasification report TTBOX+ DenseNet:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "missing_queen       0.96      0.81      0.88       818\n",
      "       active       0.96      0.99      0.98      3700\n",
      "\n",
      "     accuracy                           0.96      4518\n",
      "    macro avg       0.96      0.90      0.93      4518\n",
      " weighted avg       0.96      0.96      0.96      4518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(rounded_labels, rounded_predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='TTBOX + DenseNet Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "target_names=['missing_queen', 'active']\n",
    "print ('\\nClasification report TTBOX+ DenseNet:\\n', classification_report(rounded_labels, rounded_predictions, target_names=target_names ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN  Ruche decomposition : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'write_sample_ids_perHive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cd9e89ca3e17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# retrieves unique hives names and also writes these to a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhives\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwrite_sample_ids_perHive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_ids\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpath_save_audio_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#hives=get_uniqueHives_names_from_File(path_save_audio_labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'write_sample_ids_perHive' is not defined"
     ]
    }
   ],
   "source": [
    "# retrieves unique hives names and also writes these to a file\n",
    "hives=write_sample_ids_perHive(sample_ids , path_save_audio_labels)  \n",
    "#hives=get_uniqueHives_names_from_File(path_save_audio_labels)\n",
    "print(hives.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% hives for test , 50% for training \n",
    "for i in range(3):\n",
    "    split_dict = split_samples_byHive(0.1, 0.7, hives, path_save_audio_labels+'split_byHive_'+str(i))\n",
    "    #print(split_dict )\n",
    "    print(len(split_dict['test']) )  \n",
    "    print(len(split_dict['train']) )\n",
    "    print(len(split_dict['val']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_id_perSet(pathSplitFile):  # reads split_id file\n",
    "\n",
    "   \n",
    "    split_dict=json.load(open (pathSplitFile, 'r'))\n",
    "    \n",
    "    sample_ids_test = split_dict['test'] \n",
    "    sample_ids_train = split_dict['train'] \n",
    "    sample_ids_val = split_dict['val']\n",
    "    return sample_ids_test, sample_ids_train, sample_ids_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids_test, sample_ids_train, sample_ids_val = get_samples_id_perSet(path_workingFolder+'split_byHive_0.json') \n",
    "print(len(sample_ids_test),len( sample_ids_train) ,len( sample_ids_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "def train_test_split_ruche(X,yy, sample_ids, sample_ids_train, sample_ids_test, sample_ids_val):\n",
    "   \n",
    "    for sample, xi ,yi in enumerate(sample_ids, X, yy):\n",
    "        if sample==sample_ids_train:\n",
    "            x_train=xi\n",
    "            y_train=yi\n",
    "        elif sample==sample_ids_test:\n",
    "            x_test=xi\n",
    "            y_test=yi\n",
    "        else x_val=xi\n",
    "             y_val=yi\n",
    "    # delete sample_ids_test from (path_workingFolder+'split_byHive_0.json')\n",
    "    os.remove(path_workingFolder+'split_byHive_0.json'+'test')\n",
    "        \n",
    "    return  x_train,y_train, x_test, y_test, x_val, y_val ,  sample_ids_test_ruche\n",
    "for i in n_fold:\n",
    "    sample_ids_test, sample_ids_train, sample_ids_val=get_samples_id_perSet(path_workingFolder+'split_byHive_0.json') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_features_from_samples(path_workingFolder, sample_ids_train, 'MFCCs20', 'NO', 0)\n",
    "X_val = get_features_from_samples(path_workingFolder, sample_ids_val, 'MFCCs20', 'NO', 0)\n",
    "X_test = get_features_from_samples(path_workingFolder, sample_ids_test, 'MFCCs20', 'NO', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = get_GT_labels_fromFiles(path_workingFolder, sample_ids_train, labels2read)\n",
    "Y_train= labels2binary('active', labels_train)\n",
    "labels_val = get_GT_labels_fromFiles(path_workingFolder, sample_ids_val, labels2read)\n",
    "Y_val= labels2binary('active', labels_val)\n",
    "labels_test = get_GT_labels_fromFiles(path_workingFolder, sample_ids_test, labels2read)\n",
    "Y_test= labels2binary('active', labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ruche=np.concatenate((X_train, X_val))\n",
    "Y_train_ruche=np.concatenate((Y_train, Y_val))\n",
    "X_test_ruche=np.array(X_test)\n",
    "Y_test_ruche= np.array(Y_test)\n",
    "X_train_ruche.shape, Y_train_ruche.shape, X_test_ruche.shape, Y_test_ruche.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ruche= X_train_ruche.reshape(-1, 20, 44, 1)\n",
    "X_test_ruche= X_test_ruche.reshape(-1, 20, 44, 1)\n",
    "Y_train_ruche=Y_train_ruche.reshape(-1, 1)\n",
    "Y_test_ruche=Y_test_ruche.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ruche.shape, X_test_ruche.shape, Y_train_ruche.shape, Y_test_ruche.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise and list all data in history\n",
    "# ---------------------------------------------------    \n",
    "print(adam.history.keys())\n",
    "\n",
    "accuracy= adam.history['accuracy']\n",
    "val_accuracy=adam.history['val_accuracy']\n",
    "loss=adam.history['loss']\n",
    "\n",
    "val_loss=adam.history['val_loss']\n",
    "\n",
    "epochs=range(len(accuracy))\n",
    "fig = plt.figure()\n",
    "plt.plot(epochs, accuracy,'ro',  label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "fig.savefig('accuracy.png')\n",
    "plt.legend()\n",
    "fig = plt.figure()\n",
    "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "fig.savefig('loss.png')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
